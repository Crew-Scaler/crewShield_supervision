[
  {
    "id": "http://arxiv.org/abs/2512.20554v1",
    "arxiv_id": "2512.20554v1",
    "title": "Hardware-aware and Resource-efficient Circuit Packing and Scheduling on Trapped-Ion Quantum Computers",
    "summary": "The rapid expansion of quantum cloud services has led to long job queues due to single-tenant execution models that underutilize hardware resources. Quantum multi-programming (QMP) mitigates this by executing multiple circuits in parallel on a single device, but existing methods target superconducting systems with limited connectivity, high crosstalk, and lower gate fidelity. Trapped-ion architectures, with all-to-all connectivity, long coherence times, and high-fidelity mid-circuit measurement properties, presents itself as a more suitable platform for scalable QMP. We present CircPack, a hardware-aware circuit packing framework designed for modular trapped-ion devices based on the Quantum Charge-Coupled Device (QCCD) architecture. CircPack formulates static circuit scheduling as a two-dimensional packing problem with hardware-specific shuttling constraints. Compared to superconducting-based QMP approaches, CircPack achieves up to 70.72% better fidelity, 62.67% higher utilization, and 32.80% improved layer reduction. This framework is also capable of scalable, balanced scheduling across a cluster of independent QCCD modules, highlighting trapped-ion systems' potential in improving the throughput of quantum cloud computing in the near future.",
    "published": "2025-12-23T17:53:47Z",
    "updated": "2025-12-23T17:53:47Z",
    "authors": [
      "Miguel Palma",
      "Shuwen Kan",
      "Wenqi Wei",
      "Juntao Chen",
      "Kaixun Hua",
      "Sara Mouradian",
      "Ying Mao"
    ],
    "affiliations": [
      "fordham university",
      "fordham university",
      "fordham university",
      "fordham university",
      "university of south florida",
      "university of washington",
      "fordham university"
    ],
    "first_author": "Miguel Palma",
    "pdf_url": "https://arxiv.org/pdf/2512.20554v1",
    "primary_category": "quant-ph",
    "relevance_score": 18.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20586v1",
    "arxiv_id": "2512.20586v1",
    "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
    "summary": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.",
    "published": "2025-12-23T18:32:17Z",
    "updated": "2025-12-23T18:32:17Z",
    "authors": [
      "Humza Nusrat",
      "Luke Francisco",
      "Bing Luo",
      "Hassan Bagher-Ebadian",
      "Joshua Kim",
      "Karen Chin-Snyder",
      "Salim Siddiqui",
      "Mira Shah",
      "Eric Mellon",
      "Mohammad Ghassemi",
      "Anthony Doemer",
      "Benjamin Movsas",
      "Kundan Thind"
    ],
    "affiliations": [],
    "first_author": "Humza Nusrat",
    "pdf_url": "https://arxiv.org/pdf/2512.20586v1",
    "primary_category": "cs.AI",
    "relevance_score": 14.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20469v1",
    "arxiv_id": "2512.20469v1",
    "title": "Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale",
    "summary": "AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \\emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.   However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.   We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \\emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.   We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.",
    "published": "2025-12-23T16:04:41Z",
    "updated": "2025-12-23T16:04:41Z",
    "authors": [
      "Linfeng Zhang",
      "Siheng Chen",
      "Yuzhu Cai",
      "Jingyi Chai",
      "Junhan Chang",
      "Kun Chen",
      "Zhi X. Chen",
      "Zhaohan Ding",
      "Yuwen Du",
      "Yuanpeng Gao",
      "Yuan Gao",
      "Jing Gao",
      "Zhifeng Gao",
      "Qiangqiang Gu",
      "Yanhui Hong",
      "Yuan Huang",
      "Xi Fang",
      "Xiaohong Ji",
      "Guolin Ke",
      "Zixing Lei",
      "Xinyu Li",
      "Yongge Li",
      "Ruoxue Liao",
      "Hang Lin",
      "Xiaolu Lin",
      "Yuxiang Liu",
      "Xinzijian Liu",
      "Zexi Liu",
      "Jintan Lu",
      "Tingjia Miao",
      "Haohui Que",
      "Weijie Sun",
      "Yanfeng Wang",
      "Bingyang Wu",
      "Tianju Xue",
      "Rui Ye",
      "Jinzhe Zeng",
      "Duo Zhang",
      "Jiahui Zhang",
      "Linfeng Zhang",
      "Tianhan Zhang",
      "Wenchang Zhang",
      "Yuzhi Zhang",
      "Zezhong Zhang",
      "Hang Zheng",
      "Hui Zhou",
      "Tong Zhu",
      "Xinyu Zhu",
      "Qingguo Zhou",
      "Weinan E"
    ],
    "affiliations": [],
    "first_author": "Linfeng Zhang",
    "pdf_url": "https://arxiv.org/pdf/2512.20469v1",
    "primary_category": "cs.AI",
    "relevance_score": 14.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20615v1",
    "arxiv_id": "2512.20615v1",
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "summary": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
    "published": "2025-12-23T18:59:16Z",
    "updated": "2025-12-23T18:59:16Z",
    "authors": [
      "Xuanhua He",
      "Tianyu Yang",
      "Ke Cao",
      "Ruiqi Wu",
      "Cheng Meng",
      "Yong Zhang",
      "Zhuoliang Kang",
      "Xiaoming Wei",
      "Qifeng Chen"
    ],
    "affiliations": [],
    "first_author": "Xuanhua He",
    "pdf_url": "https://arxiv.org/pdf/2512.20615v1",
    "primary_category": "cs.CV",
    "relevance_score": 12.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20578v1",
    "arxiv_id": "2512.20578v1",
    "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
    "published": "2025-12-23T18:21:32Z",
    "updated": "2025-12-23T18:21:32Z",
    "authors": [
      "Amirhosein Ghasemabadi",
      "Di Niu"
    ],
    "affiliations": [],
    "first_author": "Amirhosein Ghasemabadi",
    "pdf_url": "https://arxiv.org/pdf/2512.20578v1",
    "primary_category": "cs.CL",
    "relevance_score": 12.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20573v1",
    "arxiv_id": "2512.20573v1",
    "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
    "summary": "Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.",
    "published": "2025-12-23T18:16:58Z",
    "updated": "2025-12-23T18:16:58Z",
    "authors": [
      "Rui Pan",
      "Zhuofu Chen",
      "Ravi Netravali"
    ],
    "affiliations": [],
    "first_author": "Rui Pan",
    "pdf_url": "https://arxiv.org/pdf/2512.20573v1",
    "primary_category": "cs.LG",
    "relevance_score": 12.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20494v1",
    "arxiv_id": "2512.20494v1",
    "title": "On Link-irregular Digraphs",
    "summary": "We extend the study of link-irregular graphs to directed graphs (digraphs), where a digraph is link-irregular if no two vertices have isomorphic directed links. We establish that link-irregular digraphs exist on $n$ vertices if and only if $n \\geq 5$, and prove that their underlying graphs must contain 3-cycles. We conjecture that link-irregular tournaments exist if and only if $n \\geq 6$, providing explicit constructions for $n \\leq 8$ and computational verification for $n \\leq 100$. We derive lower bounds on the minimum degree and outdegree required for link-irregularity, establish that almost all link-irregular digraphs are nonplanar, and prove that any link-irregular orientable graph admits a link-irregular labeling. Additionally, we construct explicit examples of link-irregular digraphs with constant outdegree and regular tournaments.",
    "published": "2025-12-23T16:42:00Z",
    "updated": "2025-12-23T16:42:00Z",
    "authors": [
      "Alexander Bastien",
      "Omid Khormali"
    ],
    "affiliations": [],
    "first_author": "Alexander Bastien",
    "pdf_url": "https://arxiv.org/pdf/2512.20494v1",
    "primary_category": "math.CO",
    "relevance_score": 12.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20491v1",
    "arxiv_id": "2512.20491v1",
    "title": "Step-DeepResearch Technical Report",
    "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
    "published": "2025-12-23T16:32:27Z",
    "updated": "2025-12-23T16:32:27Z",
    "authors": [
      "Chen Hu",
      "Haikuo Du",
      "Heng Wang",
      "Lin Lin",
      "Mingrui Chen",
      "Peng Liu",
      "Ruihang Miao",
      "Tianchi Yue",
      "Wang You",
      "Wei Ji",
      "Wei Yuan",
      "Wenjin Deng",
      "Xiaojian Yuan",
      "Xiaoyun Zhang",
      "Xiangyu Liu",
      "Xikai Liu",
      "Yanming Xu",
      "Yicheng Cao",
      "Yifei Zhang",
      "Yongyao Wang",
      "Yubo Shu",
      "Yurong Zhang",
      "Yuxiang Zhang",
      "Zheng Gong",
      "Zhichao Chang",
      "Binyan Li",
      "Dan Ma",
      "Furong Jia",
      "Hongyuan Wang",
      "Jiayu Liu",
      "Jing Bai",
      "Junlan Liu",
      "Manjiao Liu",
      "Na Wang",
      "Qiuping Wu",
      "Qinxin Du",
      "Shiwei Li",
      "Wen Sun",
      "Yifeng Gong",
      "Yonglin Chen",
      "Yuling Zhao",
      "Yuxuan Lin",
      "Ziqi Ren",
      "Zixuan Wang",
      "Aihu Zhang",
      "Brian Li",
      "Buyun Ma",
      "Kang An",
      "Li Xie",
      "Mingliang Li",
      "Pan Li",
      "Shidong Yang",
      "Xi Chen",
      "Xiaojia Liu",
      "Yuchu Luo",
      "Yuan Song",
      "YuanHao Ding",
      "Yuanwei Liang",
      "Zexi Li",
      "Zhaoning Zhang",
      "Zixin Zhang",
      "Binxing Jiao",
      "Daxin Jiang",
      "Jiansheng Chen",
      "Jing Li",
      "Xiangyu Zhang",
      "Yibo Zhu"
    ],
    "affiliations": [],
    "first_author": "Chen Hu",
    "pdf_url": "https://arxiv.org/pdf/2512.20491v1",
    "primary_category": "cs.CL",
    "relevance_score": 12.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20489v1",
    "arxiv_id": "2512.20489v1",
    "title": "A High-Dimensional Quantum Blockchain Protocol Based on Time- Entanglement",
    "summary": "Rapid advancements in quantum computing and machine learning threaten the long-term security of classical blockchain systems, whose protection mechanisms largely rely on computational difficulties. In this study, we propose a quantum blockchain protocol whose protection mechanism is directly derived from quantum mechanical principles. The protocol combines high-dimensional Bell states, time-entanglement, entanglement switching, and high-dimensional superdense coding. Encoding classical block information into time-delimited qudit states allows block identity and data verification to be implemented through the causal sequencing of quantum measurements instead of cryptographic hash functions. High-dimensional coding increases the information capacity per quantum carrier and improves noise resistance. Time-entanglement provides distributed authentication, non-repudiation, and tamper detection across the blockchain. Each block derives its own public-private key pair directly from the observed quantum correlations by performing high-dimensional Bell state measurements in successive time steps. Because these keys are dependent on the time ordering of measurements, attempts to alter block data or disrupt the protocol's timing structure inevitably affect the reconstructed correlations and are revealed during validation. Recent advances in the creation and detection of high-dimensional time-slice entanglement demonstrate that the necessary quantum resources are compatible with emerging quantum communication platforms. Taken together, these considerations suggest that the proposed framework can be evaluated as a viable and scalable candidate for quantum-secure blockchain architectures in future quantum network environments.",
    "published": "2025-12-23T16:31:12Z",
    "updated": "2025-12-23T16:31:12Z",
    "authors": [
      " Akta\u015f",
      " Arzu",
      " Y\u0131lmaz",
      " \u0130hsan"
    ],
    "affiliations": [],
    "first_author": " Akta\u015f",
    "pdf_url": "https://arxiv.org/pdf/2512.20489v1",
    "primary_category": "quant-ph",
    "relevance_score": 12.0
  },
  {
    "id": "http://arxiv.org/abs/2512.20481v1",
    "arxiv_id": "2512.20481v1",
    "title": "Coherence in the brain unfolds across separable temporal regimes",
    "summary": "Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.",
    "published": "2025-12-23T16:16:42Z",
    "updated": "2025-12-23T16:16:42Z",
    "authors": [
      "Davide Stauba",
      "Finn Rabe",
      "Akhil Misra",
      "Yves Pauli",
      "Roya H\u00fcppi",
      "Nils Lang",
      "Lars Michels",
      "Victoria Edkins",
      "Sascha Fr\u00fchholz",
      "Iris Sommer",
      "Wolfram Hinzen",
      "Philipp Homan"
    ],
    "affiliations": [],
    "first_author": "Davide Stauba",
    "pdf_url": "https://arxiv.org/pdf/2512.20481v1",
    "primary_category": "q-bio.NC",
    "relevance_score": 12.0
  }
]