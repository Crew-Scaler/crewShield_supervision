**KSI-AIM-02-Q01:** What evidence demonstrates your organization monitors AI model baseline degradation in production identifying drift within hours? Document: (a) monitoring mechanisms (accuracy tracking, confidence calibration, data drift detection), (b) drift detection methodology (Population Stability Index, entropy, explainable drift TRIPODD-style), (c) baseline establishment procedure (learning period, clean data validation), (d) degradation threshold (triggers for alerts, research target <24 hour MTTD), (e) examples of drift detected and remediated.

**KSI-AIM-02-Q02:** How do you distinguish concept drift (legitimate infrastructure changes) from adversarial drift (slow-and-low attacker evasion) in production models? Explain: (a) drift attribution methodology (which features changed?), (b) explainability integration (TRIPODD-style feature importance), (c) natural vs. adversarial classification (detection accuracy), (d) differential response (legitimate drift → baseline refresh; adversarial → investigation), (e) research findings (85.77% concept drift detection possible with explainability).

**KSI-AIM-02-Q03:** What procedures prevent attackers from poisoning baselines during initial learning phase making attacks "normal"? Document: (a) baseline learning protection (ensemble voting, outlier filtering), (b) adversarial testing (injecting known attacks during learning validation), (c) clean data validation before learning, (d) baseline versioning (rollback if poisoning discovered), (e) frequency of learning cycles and protective procedures.

**KSI-AIM-02-Q04:** What is your mean time to detect (MTTD) model degradation in production AI systems? Provide: (a) baseline accuracy before deployment, (b) degradation threshold triggering alerts, (c) detection latency (from degradation to alert, research target 24 hours or less), (d) measurement methodology (continuous validation, drift metrics), (e) comparison to research findings (91% systems degrade undetected; 75% lack drift detection).

**KSI-AIM-02-Q05:** What testing validates AI asset classification models against known data poisoning scenarios? Provide: (a) poisoning attack simulation methodology, (b) attack impact quantification (0.001% poisoning = 7-11% performance degradation research finding), (c) model robustness testing with poisoned samples, (d) detection accuracy post-poisoning, (e) evidence of models surviving poisoning attacks or quick recovery.

**KSI-AIM-02-Q06:** How robust are your failure prediction models against adversarial input and data poisoning attacks? Document: (a) adversarial testing of models (can attackers craft system metrics to fool failure prediction?), (b) red team results showing evasion techniques that successfully evaded the model, (c) model robustness against data poisoning (training data corruption), (d) Byzantine-resilient techniques protecting against poisoned samples, (e) evidence of models surviving robustness testing or quick recovery mechanisms.

**KSI-AIM-02-Q07:** How does the organization integrate model drift detection with communication integrity validation in agent communication systems? Explain: (a) model drift triggers for agent communication channels (concept drift, data drift, accuracy degradation thresholds), (b) detection mechanisms for drift in AI-generated function calls and API parameters, (c) automated response procedures (communication quarantine, retraining, or model rollback when drift detected), (d) impact on downstream systems when agents generate different outputs due to drift, (e) audit trail documenting when agents degraded and were remediated.

**KSI-AIM-02-Q08:** How does your organization monitor for model degradation, distribution shift, and semantic decay in AI systems used for critical functions, and what triggers model retraining or rollback decisions? Determine capability to: (a) track model accuracy, confidence scores, and false positive/negative rates over operational lifetime; (b) detect distribution shift where model inputs diverge from training data characteristics; (c) measure semantic decay in language models where training data becomes less relevant; (d) compare AI-generated outputs against ground truth to identify performance degradation; (e) establish performance thresholds triggering escalation and potential model retraining; (f) document decisions to retrain, replace, or roll back degraded models; (g) provide metrics demonstrating model performance remains within acceptable ranges.
