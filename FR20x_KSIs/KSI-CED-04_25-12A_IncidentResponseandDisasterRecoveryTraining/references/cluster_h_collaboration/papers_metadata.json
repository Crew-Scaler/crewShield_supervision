{
  "cluster": "cluster_h_collaboration",
  "execution_date": "2026-01-11T09:14:45.681899",
  "total_papers": 288,
  "green_papers_count": 41,
  "papers": [
    {
      "arxiv_id": "2310.19778v3",
      "title": "Human-AI collaboration is not very collaborative yet: A taxonomy of interaction patterns in AI-assisted decision making from a systematic review",
      "authors": [
        "Catalina Gomez",
        "Sue Min Cho",
        "Shichang Ke",
        "Chien-Ming Huang",
        "Mathias Unberath"
      ],
      "published": "2023-10-30T17:46:38Z",
      "categories": "",
      "summary": "Leveraging Artificial Intelligence (AI) in decision support systems has disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. A human-centered perspective attempts to alleviate this concern by designing AI solutions for seamless integration with existing processes. Determining what information AI should provide to aid humans is vital, a concept underscored by explainable AI's efforts to justify AI predictions. However, how the information is presented, e.g., the sequence of recommendations and solicitation ...",
      "pdf_url": "https://arxiv.org/pdf/2310.19778v3.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2310.19778v3_paper.pdf"
    },
    {
      "arxiv_id": "2412.07241v1",
      "title": "Human-Computer Interaction and Human-AI Collaboration in Advanced Air Mobility: A Comprehensive Review",
      "authors": [
        "Fatma Yamac Sagirli",
        "Xiaopeng Zhao",
        "Zhenbo Wang"
      ],
      "published": "2024-12-10T07:06:52Z",
      "categories": "",
      "summary": "The increasing rates of global urbanization and vehicle usage are leading to a shift of mobility to the third dimension-through Advanced Air Mobility (AAM)-offering a promising solution for faster, safer, cleaner, and more efficient transportation. As air transportation continues to evolve with more automated and autonomous systems, advancements in AAM require a deep understanding of human-computer interaction and human-AI collaboration to ensure safe and effective operations in complex urban and regional environments. There has been a significant increase in publications regarding these emerg...",
      "pdf_url": "https://arxiv.org/pdf/2412.07241v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2412.07241v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.06224v1",
      "title": "Exploring Human-AI Collaboration Using Mental Models of Early Adopters of Multi-Agent Generative AI Tools",
      "authors": [
        "Suchismita Naik",
        "Austin L. Toombs",
        "Amanda Snellinger",
        "Scott Saponas",
        "Amanda K. Hall"
      ],
      "published": "2025-09-10T05:35:38Z",
      "categories": "",
      "summary": "With recent advancements in multi-agent generative AI (Gen AI), technology organizations like Microsoft are adopting these complex tools, redefining AI agents as active collaborators in complex workflows rather than as passive tools. In this study, we investigated how early adopters and developers conceptualize multi-agent Gen AI tools, focusing on how they understand human-AI collaboration mechanisms, general collaboration dynamics, and transparency in the context of AI tools. We conducted semi-structured interviews with 13 developers, all early adopters of multi-agent Gen AI technology who w...",
      "pdf_url": "https://arxiv.org/pdf/2510.06224v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2510.06224v1_paper.pdf"
    },
    {
      "arxiv_id": "2208.07960v1",
      "title": "Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making",
      "authors": [
        "Kori Inkpen",
        "Shreya Chappidi",
        "Keri Mallari",
        "Besmira Nushi",
        "Divya Ramesh",
        "Pietro Michelucci",
        "Vani Mandava",
        "Libu\u0161e Hannah Vep\u0159ek",
        "Gabrielle Quinn"
      ],
      "published": "2022-08-16T21:39:58Z",
      "categories": "",
      "summary": "Human-AI collaboration for decision-making strives to achieve team performance that exceeds the performance of humans or AI alone. However, many factors can impact success of Human-AI teams, including a user's domain expertise, mental models of an AI system, trust in recommendations, and more. This work examines users' interaction with three simulated algorithmic models, all with similar accuracy but different tuning on their true positive and true negative rates. Our study examined user performance in a non-trivial blood vessel labeling task where participants indicated whether a given blood ...",
      "pdf_url": "https://arxiv.org/pdf/2208.07960v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2208.07960v1_paper.pdf"
    },
    {
      "arxiv_id": "2501.10909v1",
      "title": "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition",
      "authors": [
        "Gaole He",
        "Patrick Hemmer",
        "Michael V\u00f6ssing",
        "Max Schemmer",
        "Ujwal Gadiraju"
      ],
      "published": "2025-01-19T01:03:09Z",
      "categories": "",
      "summary": "In recent years, the rapid development of AI systems has brought about the benefits of intelligent services but also concerns about security and reliability. By fostering appropriate user reliance on an AI system, both complementary team performance and reduced human workload can be achieved. Previous empirical studies have extensively analyzed the impact of factors ranging from task, system, and human behavior on user trust and appropriate reliance in the context of one-step decision making. However, user reliance on AI systems in tasks with complex semantics that require multi-step workflows...",
      "pdf_url": "https://arxiv.org/pdf/2501.10909v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2501.10909v1_paper.pdf"
    },
    {
      "arxiv_id": "2509.16772v2",
      "title": "AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",
      "authors": [
        "Eason Chen",
        "Jeffrey Li",
        "Scarlett Huang",
        "Xinyi Tang",
        "Jionghao Lin",
        "Paulo Carvalho",
        "Kenneth Koedinger"
      ],
      "published": "2025-09-20T18:38:54Z",
      "categories": "",
      "summary": "We present an empirical study of how both experienced tutors and non-tutors judge the correctness of tutor praise responses under different Artificial Intelligence (AI)-assisted interfaces, types of explanation (textual explanations vs. inline highlighting). We first fine-tuned several Large Language Models (LLMs) to produce binary correctness labels and explanations, achieving up to 88% accuracy and 0.92 F1 score with GPT-4. We then let the GPT-4 models assist 95 participants in tutoring decision-making tasks by offering different types of explanations. Our findings show that although human-A...",
      "pdf_url": "https://arxiv.org/pdf/2509.16772v2.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2509.16772v2_paper.pdf"
    },
    {
      "arxiv_id": "2401.07058v1",
      "title": "Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making",
      "authors": [
        "Zhuoran Lu",
        "Dakuo Wang",
        "Ming Yin"
      ],
      "published": "2024-01-13T12:19:01Z",
      "categories": "",
      "summary": "AI assistance in decision-making has become popular, yet people's inappropriate reliance on AI often leads to unsatisfactory human-AI collaboration performance. In this paper, through three pre-registered, randomized human subject experiments, we explore whether and how the provision of {second opinions} may affect decision-makers' behavior and performance in AI-assisted decision-making. We find that if both the AI model's decision recommendation and a second opinion are always presented together, decision-makers reduce their over-reliance on AI while increase their under-reliance on AI, regar...",
      "pdf_url": "https://arxiv.org/pdf/2401.07058v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2401.07058v1_paper.pdf"
    },
    {
      "arxiv_id": "2509.21436v1",
      "title": "Position: Human Factors Reshape Adversarial Analysis in Human-AI Decision-Making Systems",
      "authors": [
        "Shutong Fan",
        "Lan Zhang",
        "Xiaoyong Yuan"
      ],
      "published": "2025-09-25T19:08:01Z",
      "categories": "",
      "summary": "As Artificial Intelligence (AI) increasingly supports human decision-making, its vulnerability to adversarial attacks grows. However, the existing adversarial analysis predominantly focuses on fully autonomous AI systems, where decisions are executed without human intervention. This narrow focus overlooks the complexities of human-AI collaboration, where humans interpret, adjust, and act upon AI-generated decisions. Trust, expectations, and cognitive behaviors influence how humans interact with AI, creating dynamic feedback loops that adversaries can exploit. To strengthen the robustness of AI...",
      "pdf_url": "https://arxiv.org/pdf/2509.21436v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2509.21436v1_paper.pdf"
    },
    {
      "arxiv_id": "2505.23397v2",
      "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
      "authors": [
        "Ahmad Mohsin",
        "Helge Janicke",
        "Ahmed Ibrahim",
        "Iqbal H. Sarker",
        "Seyit Camtepe"
      ],
      "published": "2025-05-29T12:35:08Z",
      "categories": "",
      "summary": "This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration. To address these limitations, we propose a novel autonomy tie...",
      "pdf_url": "https://arxiv.org/pdf/2505.23397v2.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2505.23397v2_paper.pdf"
    },
    {
      "arxiv_id": "2506.03707v1",
      "title": "My Advisor, Her AI and Me: Evidence from a Field Experiment on Human-AI Collaboration and Investment Decisions",
      "authors": [
        " Cathy",
        " Yang",
        "Kevin Bauer",
        "Xitong Li",
        "Oliver Hinz"
      ],
      "published": "2025-06-04T08:40:11Z",
      "categories": "",
      "summary": "Amid ongoing policy and managerial debates on keeping humans in the loop of AI decision-making, we investigate whether human involvement in AI-based service production benefits downstream consumers. Partnering with a large savings bank in Europe, we produced pure AI and human-AI collaborative investment advice, passed it to customers, and examined their advice-taking in a field experiment. On the production side, contrary to concerns that humans might inefficiently override AI output, we find that giving a human banker the final say over AI-generated financial advice does not compromise its qu...",
      "pdf_url": "https://arxiv.org/pdf/2506.03707v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2506.03707v1_paper.pdf"
    },
    {
      "arxiv_id": "2508.09033v1",
      "title": "Beyond Predictions: A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration",
      "authors": [
        "Tina Behzad",
        "Nikolos Gurney",
        "Ning Wang",
        "David V. Pynadath"
      ],
      "published": "2025-08-12T15:54:48Z",
      "categories": "",
      "summary": "The promise of human-AI teaming lies in humans and AI working together to achieve performance levels neither could accomplish alone. Effective communication between AI and humans is crucial for teamwork, enabling users to efficiently benefit from AI assistance. This paper investigates how AI communication impacts human-AI team performance. We examine AI explanations that convey an awareness of its strengths and limitations. To achieve this, we train a decision tree on the model's mistakes, allowing it to recognize and explain where and why it might err. Through a user study on an income predic...",
      "pdf_url": "https://arxiv.org/pdf/2508.09033v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2508.09033v1_paper.pdf"
    },
    {
      "arxiv_id": "2310.13544v1",
      "title": "A Diachronic Perspective on User Trust in AI under Uncertainty",
      "authors": [
        "Shehzaad Dhuliawala",
        "Vil\u00e9m Zouhar",
        "Mennatallah El-Assady",
        "Mrinmaya Sachan"
      ],
      "published": "2023-10-20T14:41:46Z",
      "categories": "",
      "summary": "In a human-AI collaboration, users build a mental model of the AI system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. Modern NLP systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. In order to build trustworthy AI, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. We study the evolution of user trust in response to these trust-eroding events using a betting game. We find that even a few inc...",
      "pdf_url": "https://arxiv.org/pdf/2310.13544v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2310.13544v1_paper.pdf"
    },
    {
      "arxiv_id": "2409.14377v1",
      "title": "To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance on AI Systems",
      "authors": [
        "Gaole He",
        "Abri Bharos",
        "Ujwal Gadiraju"
      ],
      "published": "2024-09-22T09:43:27Z",
      "categories": "",
      "summary": "Powerful predictive AI systems have demonstrated great potential in augmenting human decision making. Recent empirical work has argued that the vision for optimal human-AI collaboration requires 'appropriate reliance' of humans on AI systems. However, accurately estimating the trustworthiness of AI advice at the instance level is quite challenging, especially in the absence of performance feedback pertaining to the AI system. In practice, the performance disparity of machine learning models on out-of-distribution data makes the dataset-specific performance feedback unreliable in human-AI colla...",
      "pdf_url": "https://arxiv.org/pdf/2409.14377v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2409.14377v1_paper.pdf"
    },
    {
      "arxiv_id": "2501.16627v1",
      "title": "Engaging with AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision-Making",
      "authors": [
        "Zichen Chen",
        "Yunhao Luo",
        "Misha Sra"
      ],
      "published": "2025-01-28T02:03:00Z",
      "categories": "",
      "summary": "As reliance on AI systems for decision-making grows, it becomes critical to ensure that human users can appropriately balance trust in AI suggestions with their own judgment, especially in high-stakes domains like healthcare. However, human + AI teams have been shown to perform worse than AI alone, with evidence indicating automation bias as the reason for poorer performance, particularly because humans tend to follow AI's recommendations even when they are incorrect. In many existing human + AI systems, decision-making support is typically provided in the form of text explanations (XAI) to he...",
      "pdf_url": "https://arxiv.org/pdf/2501.16627v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2501.16627v1_paper.pdf"
    },
    {
      "arxiv_id": "2205.09696v1",
      "title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging",
      "authors": [
        "Riccardo Fogliato",
        "Shreya Chappidi",
        "Matthew Lungren",
        "Michael Fitzke",
        "Mark Parkinson",
        "Diane Wilson",
        "Paul Fisher",
        "Eric Horvitz",
        "Kori Inkpen",
        "Besmira Nushi"
      ],
      "published": "2022-05-19T16:59:25Z",
      "categories": "",
      "summary": "Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has ...",
      "pdf_url": "https://arxiv.org/pdf/2205.09696v1.pdf",
      "relevance_score": 99,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2205.09696v1_paper.pdf"
    },
    {
      "arxiv_id": "2404.01615v1",
      "title": "Collaborative human-AI trust (CHAI-T): A process framework for active management of trust in human-AI collaboration",
      "authors": [
        "Melanie J. McGrath",
        "Andreas Duenser",
        "Justine Lacey",
        "Cecile Paris"
      ],
      "published": "2024-04-02T03:39:06Z",
      "categories": "",
      "summary": "Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each. In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies. User trust is essential to creating and maintaining these collaborative relationships. Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecede...",
      "pdf_url": "https://arxiv.org/pdf/2404.01615v1.pdf",
      "relevance_score": 98,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2404.01615v1_paper.pdf"
    },
    {
      "arxiv_id": "2412.13405v2",
      "title": "What Human-Horse Interactions may Teach us About Effective Human-AI Interactions",
      "authors": [
        "Mohammad Hossein Jarrahi",
        "Stanley Ahalt"
      ],
      "published": "2024-12-18T00:39:16Z",
      "categories": "",
      "summary": "This article explores human-horse interactions as a metaphor for understanding and designing effective human-AI partnerships. Drawing on the long history of human collaboration with horses, we propose that AI, like horses, should complement rather than replace human capabilities. We move beyond traditional benchmarks such as the Turing test, which emphasize AI's ability to mimic human intelligence, and instead advocate for a symbiotic relationship where distinct intelligences enhance each other. We analyze key elements of human-horse relationships: trust, communication, and mutual adaptability...",
      "pdf_url": "https://arxiv.org/pdf/2412.13405v2.pdf",
      "relevance_score": 98,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2412.13405v2_paper.pdf"
    },
    {
      "arxiv_id": "2403.05911v2",
      "title": "Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning",
      "authors": [
        "Zana Bu\u00e7inca",
        "Siddharth Swaroop",
        "Amanda E. Paluch",
        "Susan A. Murphy",
        "Krzysztof Z. Gajos"
      ],
      "published": "2024-03-09T13:30:00Z",
      "categories": "",
      "summary": "Imagine if AI decision-support tools not only complemented our ability to make accurate decisions, but also improved our skills, boosted collaboration, and elevated the joy we derive from our tasks. Despite the potential to optimize a broad spectrum of such human-centric objectives, the design of current AI tools remains focused on decision accuracy alone. We propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize human-AI interaction for diverse objectives. RL can optimize such objectives by tailoring decision support, providing the ...",
      "pdf_url": "https://arxiv.org/pdf/2403.05911v2.pdf",
      "relevance_score": 97,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2403.05911v2_paper.pdf"
    },
    {
      "arxiv_id": "2204.13480v1",
      "title": "The Value of Measuring Trust in AI - A Socio-Technical System Perspective",
      "authors": [
        "Michaela Benk",
        "Suzanne Tolmeijer",
        "Florian von Wangenheim",
        "Andrea Ferrario"
      ],
      "published": "2022-04-28T13:13:48Z",
      "categories": "",
      "summary": "Building trust in AI-based systems is deemed critical for their adoption and appropriate use. Recent research has thus attempted to evaluate how various attributes of these systems affect user trust. However, limitations regarding the definition and measurement of trust in AI have hampered progress in the field, leading to results that are inconsistent or difficult to compare. In this work, we provide an overview of the main limitations in defining and measuring trust in AI. We focus on the attempt of giving trust in AI a numerical value and its utility in informing the design of real-world hu...",
      "pdf_url": "https://arxiv.org/pdf/2204.13480v1.pdf",
      "relevance_score": 97,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2204.13480v1_paper.pdf"
    },
    {
      "arxiv_id": "2311.03999v1",
      "title": "Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations",
      "authors": [
        "Lixiang Yan",
        "Vanessa Echeverria",
        "Gloria Fernandez Nieto",
        "Yueqiao Jin",
        "Zachari Swiecki",
        "Linxuan Zhao",
        "Dragan Ga\u0161evi\u0107",
        "Roberto Martinez-Maldonado"
      ],
      "published": "2023-11-07T13:54:56Z",
      "categories": "",
      "summary": "Generative artificial intelligence (GenAI) offers promising potential for advancing human-AI collaboration in qualitative research. However, existing works focused on conventional machine-learning and pattern-based AI systems, and little is known about how researchers interact with GenAI in qualitative research. This work delves into researchers' perceptions of their collaboration with GenAI, specifically ChatGPT. Through a user study involving ten qualitative researchers, we found ChatGPT to be a valuable collaborator for thematic analysis, enhancing coding efficiency, aiding initial data exp...",
      "pdf_url": "https://arxiv.org/pdf/2311.03999v1.pdf",
      "relevance_score": 93,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2311.03999v1_paper.pdf"
    },
    {
      "arxiv_id": "2502.13321v1",
      "title": "Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance",
      "authors": [
        "Tejas Srinivasan",
        "Jesse Thomason"
      ],
      "published": "2025-02-18T22:42:39Z",
      "categories": "",
      "summary": "Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance. For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. In two decision-making scenarios -- laypeople answering science questions and doctors making medical diagnoses -- we find that providing su...",
      "pdf_url": "https://arxiv.org/pdf/2502.13321v1.pdf",
      "relevance_score": 91,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2502.13321v1_paper.pdf"
    },
    {
      "arxiv_id": "2503.05926v1",
      "title": "What's So Human about Human-AI Collaboration, Anyway? Generative AI and Human-Computer Interaction",
      "authors": [
        "Elizabeth Anne Watkins",
        "Emanuel Moss",
        "Giuseppe Raffa",
        "Lama Nachman"
      ],
      "published": "2025-03-07T20:48:18Z",
      "categories": "",
      "summary": "While human-AI collaboration has been a longstanding goal and topic of study for computational research, the emergence of increasingly naturalistic generative AI language models has greatly inflected the trajectory of such research. In this paper we identify how, given the language capabilities of generative AI, common features of human-human collaboration derived from the social sciences can be applied to the study of human-computer interaction. We provide insights drawn from interviews with industry personnel working on building human-AI collaboration systems, as well as our collaborations w...",
      "pdf_url": "https://arxiv.org/pdf/2503.05926v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2503.05926v1_paper.pdf"
    },
    {
      "arxiv_id": "2204.13217v2",
      "title": "Understanding User Perceptions, Collaborative Experience and User Engagement in Different Human-AI Interaction Designs for Co-Creative Systems",
      "authors": [
        "Jeba Rezwana",
        "Mary Lou Maher"
      ],
      "published": "2022-04-27T22:37:44Z",
      "categories": "",
      "summary": "Human-AI co-creativity involves humans and AI collaborating on a shared creative product as partners. In a creative collaboration, communication is an essential component among collaborators. In many existing co-creative systems users can communicate with the AI, usually using buttons or sliders. Typically, the AI in co-creative systems cannot communicate back to humans, limiting their potential to be perceived as partners rather than just a tool. This paper presents a study with 38 participants to explore the impact of two interaction designs, with and without AI-to-human communication, on us...",
      "pdf_url": "https://arxiv.org/pdf/2204.13217v2.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2204.13217v2_paper.pdf"
    },
    {
      "arxiv_id": "2003.02622v1",
      "title": "Towards Effective Human-AI Collaboration in GUI-Based Interactive Task Learning Agents",
      "authors": [
        "Toby Jia-Jun Li",
        "Jingya Chen",
        "Tom M. Mitchell",
        "Brad A. Myers"
      ],
      "published": "2020-03-05T14:12:19Z",
      "categories": "",
      "summary": "We argue that a key challenge in enabling usable and useful interactive task learning for intelligent agents is to facilitate effective Human-AI collaboration. We reflect on our past 5 years of efforts on designing, developing and studying the SUGILITE system, discuss the issues on incorporating recent advances in AI with HCI principles in mixed-initiative interactions and multi-modal interactions, and summarize the lessons we learned. Lastly, we identify several challenges and opportunities, and describe our ongoing work",
      "pdf_url": "https://arxiv.org/pdf/2003.02622v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2003.02622v1_paper.pdf"
    },
    {
      "arxiv_id": "2502.12443v2",
      "title": "TherAIssist: Assisting Art Therapy Homework and Client-Practitioner Collaboration through Human-AI Interaction",
      "authors": [
        "Di Liu",
        "Jingwen Bai",
        "Zhuoyi Zhang",
        "Yilin Zhang",
        "Zhenhao Zhang",
        "Jian Zhao",
        "Pengcheng An"
      ],
      "published": "2025-02-18T02:26:12Z",
      "categories": "",
      "summary": "Art therapy homework is essential for fostering clients' reflection on daily experiences between sessions. However, current practices present challenges: clients often lack guidance for completing tasks that combine art-making and verbal expression, while therapists find it difficult to track and tailor homework. How HCI systems might support art therapy homework remains underexplored. To address this, we present TherAIssist, comprising a client-facing application leveraging human-AI co-creative art-making and conversational agents to facilitate homework, and a therapist-facing application ena...",
      "pdf_url": "https://arxiv.org/pdf/2502.12443v2.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2502.12443v2_paper.pdf"
    },
    {
      "arxiv_id": "2505.22477v1",
      "title": "Human-Centered Human-AI Collaboration (HCHAC)",
      "authors": [
        "Qi Gao",
        "Wei Xu",
        "Hanxi Pan",
        "Mowei Shen",
        "Zaifeng Gao"
      ],
      "published": "2025-05-28T15:27:52Z",
      "categories": "",
      "summary": "In the intelligent era, the interaction between humans and intelligent systems fundamentally involves collaboration with autonomous intelligent agents. Human-AI Collaboration (HAC) represents a novel type of human-machine relationship facilitated by autonomous intelligent machines equipped with AI technologies. In this paradigm, AI agents serve not only as auxiliary tools but also as active teammates, partnering with humans to accomplish tasks collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical leadership roles in the collaboration. This human-led collaboration impar...",
      "pdf_url": "https://arxiv.org/pdf/2505.22477v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2505.22477v1_paper.pdf"
    },
    {
      "arxiv_id": "2508.10919v1",
      "title": "Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?",
      "authors": [
        "Mohammed Saqr",
        "Kamila Misiejuk",
        "Sonsoles L\u00f3pez-Pernas"
      ],
      "published": "2025-08-03T11:43:01Z",
      "categories": "",
      "summary": "While research on human-AI collaboration exists, it mainly examined language learning and used traditional counting methods with little attention to evolution and dynamics of collaboration on cognitively demanding tasks. This study examines human-AI interactions while solving a complex problem. Student-AI interactions were qualitatively coded and analyzed with transition network analysis, sequence analysis and partial correlation networks as well as comparison of frequencies using chi-square and Person-residual shaded Mosaic plots to map interaction patterns, their evolution, and their relatio...",
      "pdf_url": "https://arxiv.org/pdf/2508.10919v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2508.10919v1_paper.pdf"
    },
    {
      "arxiv_id": "2306.12843v1",
      "title": "Critical-Reflective Human-AI Collaboration: Exploring Computational Tools for Art Historical Image Retrieval",
      "authors": [
        "Katrin Glinka",
        "Claudia M\u00fcller-Birn"
      ],
      "published": "2023-06-22T12:29:56Z",
      "categories": "",
      "summary": "Just as other disciplines, the humanities explore how computational research approaches and tools can meaningfully contribute to scholarly knowledge production. We approach the design of computational tools through the analytical lens of 'human-AI collaboration.' However, there is no generalizable concept of what constitutes 'meaningful' human-AI collaboration. In terms of genuinely human competencies, we consider criticality and reflection as guiding principles of scholarly knowledge production. Although (designing for) reflection is a recurring topic in CSCW and HCI discourses, it has not be...",
      "pdf_url": "https://arxiv.org/pdf/2306.12843v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2306.12843v1_paper.pdf"
    },
    {
      "arxiv_id": "2505.16023v3",
      "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild",
      "authors": [
        "Sheshera Mysore",
        "Debarati Das",
        "Hancheng Cao",
        "Bahareh Sarrafzadeh"
      ],
      "published": "2025-05-21T21:13:01Z",
      "categories": "",
      "summary": "As large language models (LLMs) are used in complex writing workflows, users engage in multi-turn interactions to steer generations to better fit their needs. Rather than passively accepting output, users actively refine, explore, and co-construct text. We conduct a large-scale analysis of this collaborative behavior for users engaged in writing tasks in the wild with two popular AI assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task classification or satisfaction estimation common in prior work and instead characterizes how users interact with LLMs through the course o...",
      "pdf_url": "https://arxiv.org/pdf/2505.16023v3.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2505.16023v3_paper.pdf"
    },
    {
      "arxiv_id": "2309.12368v2",
      "title": "Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis",
      "authors": [
        "Shao Zhang",
        "Jianing Yu",
        "Xuhai Xu",
        "Changchang Yin",
        "Yuxuan Lu",
        "Bingsheng Yao",
        "Melanie Tory",
        "Lace M. Padilla",
        "Jeffrey Caterino",
        "Ping Zhang",
        "Dakuo Wang"
      ],
      "published": "2023-09-17T19:19:39Z",
      "categories": "",
      "summary": "Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module ...",
      "pdf_url": "https://arxiv.org/pdf/2309.12368v2.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2309.12368v2_paper.pdf"
    },
    {
      "arxiv_id": "2507.14034v1",
      "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors",
      "authors": [
        "Jochen Wulf",
        "Jurg Meierhofer",
        "Frank Hannich"
      ],
      "published": "2025-07-18T16:06:03Z",
      "categories": "",
      "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organ...",
      "pdf_url": "https://arxiv.org/pdf/2507.14034v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2507.14034v1_paper.pdf"
    },
    {
      "arxiv_id": "2507.18374v1",
      "title": "Towards Effective Human-in-the-Loop Assistive AI Agents",
      "authors": [
        "Filippos Bellos",
        "Yayuan Li",
        "Cary Shu",
        "Ruey Day",
        "Jeffrey M. Siskind",
        "Jason J. Corso"
      ],
      "published": "2025-07-24T12:50:46Z",
      "categories": "",
      "summary": "Effective human-AI collaboration for physical task completion has significant potential in both everyday activities and professional domains. AI agents equipped with informative guidance can enhance human performance, but evaluating such collaboration remains challenging due to the complexity of human-in-the-loop interactions. In this work, we introduce an evaluation framework and a multimodal dataset of human-AI interactions designed to assess how AI guidance affects procedural task performance, error reduction and learning outcomes. Besides, we develop an augmented reality (AR)-equipped AI a...",
      "pdf_url": "https://arxiv.org/pdf/2507.18374v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2507.18374v1_paper.pdf"
    },
    {
      "arxiv_id": "2509.24718v1",
      "title": "Understanding Collaboration between Professional Designers and Decision-making AI: A Case Study in the Workplace",
      "authors": [
        "Nami Ogawa",
        "Yuki Okafuji",
        "Yuji Hatada",
        "Jun Baba"
      ],
      "published": "2025-09-29T12:46:13Z",
      "categories": "",
      "summary": "The rapid development of artificial intelligence (AI) has fundamentally transformed creative work practices in the design industry. Existing studies have identified both opportunities and challenges for creative practitioners in their collaboration with generative AI and explored ways to facilitate effective human-AI co-creation. However, there is still a limited understanding of designers' collaboration with AI that supports creative processes distinct from generative AI. To address these gaps, this study focuses on understanding designers' collaboration with decision-making AI, which support...",
      "pdf_url": "https://arxiv.org/pdf/2509.24718v1.pdf",
      "relevance_score": 86,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2509.24718v1_paper.pdf"
    },
    {
      "arxiv_id": "2409.15814v1",
      "title": "Interactive Example-based Explanations to Improve Health Professionals' Onboarding with AI for Human-AI Collaborative Decision Making",
      "authors": [
        "Min Hun Lee",
        "Renee Bao Xuan Ng",
        "Silvana Xinyi Choo",
        "Shamala Thilarajah"
      ],
      "published": "2024-09-24T07:20:09Z",
      "categories": "",
      "summary": "A growing research explores the usage of AI explanations on user's decision phases for human-AI collaborative decision-making. However, previous studies found the issues of overreliance on `wrong' AI outputs. In this paper, we propose interactive example-based explanations to improve health professionals' onboarding with AI for their better reliance on AI during AI-assisted decision-making. We implemented an AI-based decision support system that utilizes a neural network to assess the quality of post-stroke survivors' exercises and interactive example-based explanations that systematically sur...",
      "pdf_url": "https://arxiv.org/pdf/2409.15814v1.pdf",
      "relevance_score": 85,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2409.15814v1_paper.pdf"
    },
    {
      "arxiv_id": "2403.16812v2",
      "title": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
      "authors": [
        "Shuai Ma",
        "Qiaoyi Chen",
        "Xinru Wang",
        "Chengbo Zheng",
        "Zhenhui Peng",
        "Ming Yin",
        "Xiaojuan Ma"
      ],
      "published": "2024-03-25T14:34:06Z",
      "categories": "",
      "summary": "In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion eli...",
      "pdf_url": "https://arxiv.org/pdf/2403.16812v2.pdf",
      "relevance_score": 85,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2403.16812v2_paper.pdf"
    },
    {
      "arxiv_id": "2112.06751v2",
      "title": "Role of Human-AI Interaction in Selective Prediction",
      "authors": [
        "Elizabeth Bondi",
        "Raphael Koster",
        "Hannah Sheahan",
        "Martin Chadwick",
        "Yoram Bachrach",
        "Taylan Cemgil",
        "Ulrich Paquet",
        "Krishnamurthy Dvijotham"
      ],
      "published": "2021-12-13T16:03:13Z",
      "categories": "",
      "summary": "Recent work has shown the potential benefit of selective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation. However, most prior work assumes that human behavior remains unchanged when they solve a prediction task as part of a human-AI team as opposed to by themselves. We show that this is not the case by performing experiments to quantify human-AI interaction in the context of selective prediction. In particular, we study the impac...",
      "pdf_url": "https://arxiv.org/pdf/2112.06751v2.pdf",
      "relevance_score": 85,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2112.06751v2_paper.pdf"
    },
    {
      "arxiv_id": "2510.08104v1",
      "title": "Development of Mental Models in Human-AI Collaboration: A Conceptual Framework",
      "authors": [
        "Joshua Holstein",
        "Gerhard Satzger"
      ],
      "published": "2025-10-09T11:40:41Z",
      "categories": "",
      "summary": "Artificial intelligence has become integral to organizational decision-making and while research has explored many facets of this human-AI collaboration, the focus has mainly been on designing the AI agent(s) and the way the collaboration is set up - generally assuming a human decision-maker to be \"fixed\". However, it has largely been neglected that decision-makers' mental models evolve through their continuous interaction with AI systems. This paper addresses this gap by conceptualizing how the design of human-AI collaboration influences the development of three complementary and interdepende...",
      "pdf_url": "https://arxiv.org/pdf/2510.08104v1.pdf",
      "relevance_score": 84,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2510.08104v1_paper.pdf"
    },
    {
      "arxiv_id": "2506.05370v1",
      "title": "Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems",
      "authors": [
        "Kristy Wedel"
      ],
      "published": "2025-05-28T18:59:16Z",
      "categories": "",
      "summary": "A critical challenge remains unresolved as generative AI systems are quickly implemented in various organizational settings. Despite significant advances in memory components such as RAG, vector stores, and LLM agents, these systems still have substantial memory limitations. Gen AI workflows rarely store or reflect on the full context in which decisions are made. This leads to repeated errors and a general lack of clarity. This paper introduces Contextual Memory Intelligence (CMI) as a new foundational paradigm for building intelligent systems. It repositions memory as an adaptive infrastructu...",
      "pdf_url": "https://arxiv.org/pdf/2506.05370v1.pdf",
      "relevance_score": 84,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2506.05370v1_paper.pdf"
    },
    {
      "arxiv_id": "2401.05612v2",
      "title": "Designing for Appropriate Reliance: The Roles of AI Uncertainty Presentation, Initial User Decision, and User Demographics in AI-Assisted Decision-Making",
      "authors": [
        "Shiye Cao",
        "Anqi Liu",
        "Chien-Ming Huang"
      ],
      "published": "2024-01-11T01:28:43Z",
      "categories": "",
      "summary": "Appropriate reliance is critical to achieving synergistic human-AI collaboration. For instance, when users over-rely on AI assistance, their human-AI team performance is bounded by the model's capability. This work studies how the presentation of model uncertainty may steer users' decision-making toward fostering appropriate reliance. Our results demonstrate that showing the calibrated model uncertainty alone is inadequate. Rather, calibrating model uncertainty and presenting it in a frequency format allow users to adjust their reliance accordingly and help reduce the effect of confirmation bi...",
      "pdf_url": "https://arxiv.org/pdf/2401.05612v2.pdf",
      "relevance_score": 82,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2401.05612v2_paper.pdf"
    },
    {
      "arxiv_id": "2304.08804v4",
      "title": "AI Reliance and Decision Quality: Fundamentals, Interdependence, and the Effects of Interventions",
      "authors": [
        "Jakob Schoeffer",
        "Johannes Jakubik",
        "Michael Voessing",
        "Niklas Kuehl",
        "Gerhard Satzger"
      ],
      "published": "2023-04-18T08:08:05Z",
      "categories": "",
      "summary": "In AI-assisted decision-making, a central promise of having a human-in-the-loop is that they should be able to complement the AI system by overriding its wrong recommendations. In practice, however, we often see that humans cannot assess the correctness of AI recommendations and, as a result, adhere to wrong or override correct advice. Different ways of relying on AI recommendations have immediate, yet distinct, implications for decision quality. Unfortunately, reliance and decision quality are often inappropriately conflated in the current literature on AI-assisted decision-making. In this wo...",
      "pdf_url": "https://arxiv.org/pdf/2304.08804v4.pdf",
      "relevance_score": 82,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2304.08804v4_paper.pdf"
    },
    {
      "arxiv_id": "2411.00998v1",
      "title": "Automation Bias in AI-Assisted Medical Decision-Making under Time Pressure in Computational Pathology",
      "authors": [
        "Emely Rosbach",
        "Jonathan Ganz",
        "Jonas Ammeling",
        "Andreas Riener",
        "Marc Aubreville"
      ],
      "published": "2024-11-01T19:46:55Z",
      "categories": "",
      "summary": "Artificial intelligence (AI)-based clinical decision support systems (CDSS) promise to enhance diagnostic accuracy and efficiency in computational pathology. However, human-AI collaboration might introduce automation bias, where users uncritically follow automated cues. This bias may worsen when time pressure strains practitioners' cognitive resources. We quantified automation bias by measuring the adoption of negative system consultations and examined the role of time pressure in a web-based experiment, where trained pathology experts (n=28) estimated tumor cell percentages. Our results indic...",
      "pdf_url": "https://arxiv.org/pdf/2411.00998v1.pdf",
      "relevance_score": 80,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2411.00998v1_paper.pdf"
    },
    {
      "arxiv_id": "2409.14223v1",
      "title": "Collaborative Human-AI Risk Annotation: Co-Annotating Online Incivility with CHAIRA",
      "authors": [
        "Jinkyung Katie Park",
        "Rahul Dev Ellezhuthil",
        "Pamela Wisniewski",
        "Vivek Singh"
      ],
      "published": "2024-09-21T19:04:05Z",
      "categories": "",
      "summary": "Collaborative human-AI annotation is a promising approach for various tasks with large-scale and complex data. Tools and methods to support effective human-AI collaboration for data annotation are an important direction for research. In this paper, we present CHAIRA: a Collaborative Human-AI Risk Annotation tool that enables human and AI agents to collaboratively annotate online incivility. We leveraged Large Language Models (LLMs) to facilitate the interaction between human and AI annotators and examine four different prompting strategies. The developed CHAIRA system combines multiple prompti...",
      "pdf_url": "https://arxiv.org/pdf/2409.14223v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2404.00634v2",
      "title": "Designing Human-AI Systems: Anthropomorphism and Framing Bias on Human-AI Collaboration",
      "authors": [
        "Samuel Aleksander S\u00e1nchez Olszewski"
      ],
      "published": "2024-03-31T10:01:47Z",
      "categories": "",
      "summary": "AI is redefining how humans interact with technology, leading to a synergetic collaboration between the two. Nevertheless, the effects of human cognition on this collaboration remain unclear. This study investigates the implications of two cognitive biases, anthropomorphism and framing effect, on human-AI collaboration within a hiring setting. Subjects were asked to select job candidates with the help of an AI-powered recommendation tool. The tool was manipulated in a 3 x 3 between-subjects design to present three different AI identities (human-like, robot-like, generic) and three types of fra...",
      "pdf_url": "https://arxiv.org/pdf/2404.00634v2.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2412.08005v1",
      "title": "Survey on Human-Vehicle Interactions and AI Collaboration for Optimal Decision-Making in Automated Driving",
      "authors": [
        "Abu Jafar Md Muzahid",
        "Xiaopeng Zhao",
        "Zhenbo Wang"
      ],
      "published": "2024-12-11T01:19:43Z",
      "categories": "",
      "summary": "The capabilities of automated vehicles are advancing rapidly, yet achieving full autonomy remains a significant challenge, requiring ongoing human cognition in decision-making processes. Incorporating human cognition into control algorithms has become increasingly important, as researchers work to develop strategies that minimize conflicts between human drivers and AI systems. Despite notable progress, many challenges persist, underscoring the need for further innovation and refinement in this field. This review covers recent progress in human-vehicle interaction (HVI) and AI collaboration for...",
      "pdf_url": "https://arxiv.org/pdf/2412.08005v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.22570v1",
      "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration",
      "authors": [
        "Qi Mao",
        "Tinghan Yang",
        "Jiahao Li",
        "Bin Li",
        "Libiao Jin",
        "Yan Lu"
      ],
      "published": "2025-09-26T16:46:12Z",
      "categories": "",
      "summary": "The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the com...",
      "pdf_url": "https://arxiv.org/pdf/2509.22570v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2411.12527v2",
      "title": "Human-AI Co-Creativity: Exploring Synergies Across Levels of Creative Collaboration",
      "authors": [
        "Jennifer Haase",
        "Sebastian Pokutta"
      ],
      "published": "2024-11-19T14:19:43Z",
      "categories": "",
      "summary": "Human-AI co-creativity represents a transformative shift in how humans and generative AI tools collaborate in creative processes. This chapter explores the synergies between human ingenuity and AI capabilities across four levels of interaction: Digital Pen, AI Task Specialist, AI Assistant, and AI Co-Creator. While earlier digital tools primarily facilitated creativity, generative AI systems now contribute actively, demonstrating autonomous creativity in producing novel and valuable outcomes. Empirical evidence from mathematics showcases how AI can extend human creative potential, from computa...",
      "pdf_url": "https://arxiv.org/pdf/2411.12527v2.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2402.07632v4",
      "title": "Understanding the Effects of Miscalibrated AI Confidence on User Trust, Reliance, and Decision Efficacy",
      "authors": [
        "Jingshu Li",
        "Yitian Yang",
        "Renwen Zhang",
        "Q. Vera Liao",
        "Tianqi Song",
        "Zhengtao Xu",
        "Yi-chieh Lee"
      ],
      "published": "2024-02-12T13:16:30Z",
      "categories": "",
      "summary": "Providing well-calibrated AI confidence can help promote users' appropriate trust in and reliance on AI, which are essential for AI-assisted decision-making. However, calibrating AI confidence -- providing confidence score that accurately reflects the true likelihood of AI being correct -- is known to be challenging. To understand the effects of AI confidence miscalibration, we conducted our first experiment. The results indicate that miscalibrated AI confidence impairs users' appropriate reliance and reduces AI-assisted decision-making efficacy, and AI miscalibration is difficult for users to...",
      "pdf_url": "https://arxiv.org/pdf/2402.07632v4.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.07875v1",
      "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images",
      "authors": [
        "Shuo Han",
        "Ahmed Karam Eldaly",
        "Solomon Sunday Oyelere"
      ],
      "published": "2025-08-11T11:45:57Z",
      "categories": "",
      "summary": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer, and early, accurate diagnosis is critical to improving patient survival rates by guiding treatment decisions. Combining medical expertise with artificial intelligence (AI) holds significant promise for enhancing the precision and efficiency of IDC detection. In this work, we propose a human-in-the-loop (HITL) deep learning system designed to detect IDC in histopathology images. The system begins with an initial diagnosis provided by a high-performance EfficientNetV2S model, offering feedback from AI to the human exper...",
      "pdf_url": "https://arxiv.org/pdf/2508.07875v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2202.05302v1",
      "title": "Trust in AI: Interpretability is not necessary or sufficient, while black-box interaction is necessary and sufficient",
      "authors": [
        "Max W. Shen"
      ],
      "published": "2022-02-10T19:59:23Z",
      "categories": "",
      "summary": "The problem of human trust in artificial intelligence is one of the most fundamental problems in applied machine learning. Our processes for evaluating AI trustworthiness have substantial ramifications for ML's impact on science, health, and humanity, yet confusion surrounds foundational concepts. What does it mean to trust an AI, and how do humans assess AI trustworthiness? What are the mechanisms for building trustworthy AI? And what is the role of interpretable ML in trust? Here, we draw from statistical learning theory and sociological lenses on human-automation trust to motivate an AI-as-...",
      "pdf_url": "https://arxiv.org/pdf/2202.05302v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2401.12773v1",
      "title": "Generative AI Triggers Welfare-Reducing Decisions in Humans",
      "authors": [
        "Fabian Dvorak",
        "Regina Stumpf",
        "Sebastian Fehrler",
        "Urs Fischbacher"
      ],
      "published": "2024-01-23T13:59:36Z",
      "categories": "",
      "summary": "Generative artificial intelligence (AI) is poised to reshape the way individuals communicate and interact. While this form of AI has the potential to efficiently make numerous human decisions, there is limited understanding of how individuals respond to its use in social interaction. In particular, it remains unclear how individuals engage with algorithms when the interaction entails consequences for other people. Here, we report the results of a large-scale pre-registered online experiment (N = 3,552) indicating diminished fairness, trust, trustworthiness, cooperation, and coordination by hum...",
      "pdf_url": "https://arxiv.org/pdf/2401.12773v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2404.04570v1",
      "title": "A Map of Exploring Human Interaction patterns with LLM: Insights into Collaboration and Creativity",
      "authors": [
        "Jiayang Li",
        "Jiale Li"
      ],
      "published": "2024-04-06T09:34:30Z",
      "categories": "",
      "summary": "The outstanding performance capabilities of large language model have driven the evolution of current AI system interaction patterns. This has led to considerable discussion within the Human-AI Interaction (HAII) community. Numerous studies explore this interaction from technical, design, and empirical perspectives. However, the majority of current literature reviews concentrate on interactions across the wider spectrum of AI, with limited attention given to the specific realm of interaction with LLM. We searched for articles on human interaction with LLM, selecting 110 relevant publications m...",
      "pdf_url": "https://arxiv.org/pdf/2404.04570v1.pdf",
      "relevance_score": 77,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.05378v1",
      "title": "What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions",
      "authors": [
        "Reza Habibi",
        "Seung Wan Ha",
        "Zhiyu Lin",
        "Atieh Kashani",
        "Ala Shafia",
        "Lakshana Lakshmanarajan",
        "Chia-Fang Chung",
        "Magy Seif El-Nasr"
      ],
      "published": "2025-10-06T21:13:22Z",
      "categories": "",
      "summary": "Meaningful human-AI collaboration requires more than processing language; it demands a deeper understanding of symbols and their socially constructed meanings. While humans naturally interpret symbols through social interaction, AI systems often miss the dynamic interpretations that emerge in conversation. Drawing on Symbolic Interactionism theory, we conducted two studies to investigate how humans and AI co-construct symbols and their meanings. Findings provide empirical insights into how humans and conversational AI agents collaboratively shape meanings during interaction. We show how partic...",
      "pdf_url": "https://arxiv.org/pdf/2510.05378v1.pdf",
      "relevance_score": 77,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.01493v1",
      "title": "The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI Collaboration",
      "authors": [
        "Aung Pyae"
      ],
      "published": "2025-02-03T16:26:30Z",
      "categories": "",
      "summary": "Human-AI collaboration is evolving from a tool-based perspective to a partnership model where AI systems complement and enhance human capabilities. Traditional approaches often limit AI to a supportive role, missing the potential for reciprocal relationships where both human and AI inputs contribute to shared goals. Although Human-Centered AI (HcAI) frameworks emphasize transparency, ethics, and user experience, they often lack mechanisms for genuine, dynamic collaboration. The \"Human-AI Handshake Model\" addresses this gap by introducing a bi-directional, adaptive framework with five key attri...",
      "pdf_url": "https://arxiv.org/pdf/2502.01493v1.pdf",
      "relevance_score": 76,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2206.13202v2",
      "title": "Human-AI Collaboration in Decision-Making: Beyond Learning to Defer",
      "authors": [
        "Diogo Leit\u00e3o",
        "Pedro Saleiro",
        "M\u00e1rio A. T. Figueiredo",
        "Pedro Bizarro"
      ],
      "published": "2022-06-27T11:40:55Z",
      "categories": "",
      "summary": "Human-AI collaboration (HAIC) in decision-making aims to create synergistic teaming between human decision-makers and AI systems. Learning to defer (L2D) has been presented as a promising framework to determine who among humans and AI should make which decisions in order to optimize the performance and fairness of the combined system. Nevertheless, L2D entails several often unfeasible requirements, such as the availability of predictions from humans for every instance or ground-truth labels that are independent from said humans. Furthermore, neither L2D nor alternative approaches tackle fundam...",
      "pdf_url": "https://arxiv.org/pdf/2206.13202v2.pdf",
      "relevance_score": 74,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2404.12056v1",
      "title": "Deconstructing Human-AI Collaboration: Agency, Interaction, and Adaptation",
      "authors": [
        "Steffen Holter",
        "Mennatallah El-Assady"
      ],
      "published": "2024-04-18T10:12:18Z",
      "categories": "",
      "summary": "As full AI-based automation remains out of reach in most real-world applications, the focus has instead shifted to leveraging the strengths of both human and AI agents, creating effective collaborative systems. The rapid advances in this area have yielded increasingly more complex systems and frameworks, while the nuance of their characterization has gotten more vague. Similarly, the existing conceptual models no longer capture the elaborate processes of these systems nor describe the entire scope of their collaboration paradigms. In this paper, we propose a new unified set of dimensions throu...",
      "pdf_url": "https://arxiv.org/pdf/2404.12056v1.pdf",
      "relevance_score": 74,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.14222v1",
      "title": "tAIfa: Enhancing Team Effectiveness and Cohesion with AI-Generated Automated Feedback",
      "authors": [
        "Mohammed Almutairi",
        "Charles Chiang",
        "Yuxin Bai",
        "Diego Gomez-Zara"
      ],
      "published": "2025-04-19T08:06:48Z",
      "categories": "",
      "summary": "Providing timely and actionable feedback is crucial for effective collaboration, learning, and coordination within teams. However, many teams face challenges in receiving feedback that aligns with their goals and promotes cohesion. We introduce tAIfa (``Team AI Feedback Assistant''), an AI agent that uses Large Language Models (LLMs) to provide personalized, automated feedback to teams and their members. tAIfa analyzes team interactions, identifies strengths and areas for improvement, and delivers targeted feedback based on communication patterns. We conducted a between-subjects study with 18 ...",
      "pdf_url": "https://arxiv.org/pdf/2504.14222v1.pdf",
      "relevance_score": 74,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.09794v1",
      "title": "Augmenting Teamwork through AI Agents as Spatial Collaborators",
      "authors": [
        "Mariana Fernandez-Espinosa",
        "Diego Gomez-Zara"
      ],
      "published": "2025-03-12T19:58:44Z",
      "categories": "",
      "summary": "As Augmented Reality (AR) and Artificial Intelligence (AI) continue to converge, new opportunities emerge for AI agents to actively support human collaboration in immersive environments. While prior research has primarily focused on dyadic human-AI interactions, less attention has been given to Human-AI Teams (HATs) in AR, where AI acts as an adaptive teammate rather than a static tool. This position paper takes the perspective of team dynamics and work organization to propose that AI agents in AR should not only interact with individuals but also recognize and respond to team-level needs in r...",
      "pdf_url": "https://arxiv.org/pdf/2503.09794v1.pdf",
      "relevance_score": 74,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2002.01543v2",
      "title": "Transparency and Trust in Human-AI-Interaction: The Role of Model-Agnostic Explanations in Computer Vision-Based Decision Support",
      "authors": [
        "Christian Meske",
        "Enrico Bunde"
      ],
      "published": "2020-02-04T21:19:56Z",
      "categories": "",
      "summary": "Computer Vision, and hence Artificial Intelligence-based extraction of information from images, has increasingly received attention over the last years, for instance in medical diagnostics. While the algorithms' complexity is a reason for their increased performance, it also leads to the \"black box\" problem, consequently decreasing trust towards AI. In this regard, \"Explainable Artificial Intelligence\" (XAI) allows to open that black box and to improve the degree of AI transparency. In this paper, we first discuss the theoretical impact of explainability on trust towards AI, followed by showca...",
      "pdf_url": "https://arxiv.org/pdf/2002.01543v2.pdf",
      "relevance_score": 74,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2112.11471v1",
      "title": "Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies",
      "authors": [
        "Vivian Lai",
        "Chacha Chen",
        "Q. Vera Liao",
        "Alison Smith-Renner",
        "Chenhao Tan"
      ],
      "published": "2021-12-21T19:00:02Z",
      "categories": "",
      "summary": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a ...",
      "pdf_url": "https://arxiv.org/pdf/2112.11471v1.pdf",
      "relevance_score": 73,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.23476v1",
      "title": "Human-AI Collaborative Uncertainty Quantification",
      "authors": [
        "Sima Noorani",
        "Shayan Kiyani",
        "George Pappas",
        "Hamed Hassani"
      ],
      "published": "2025-10-27T16:11:23Z",
      "categories": "",
      "summary": "AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a k...",
      "pdf_url": "https://arxiv.org/pdf/2510.23476v1.pdf",
      "relevance_score": 72,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.00753v4",
      "title": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey",
      "authors": [
        "Henry Peng Zou",
        "Wei-Chieh Huang",
        "Yaozu Wu",
        "Yankai Chen",
        "Chunyu Miao",
        "Hoang Nguyen",
        "Yue Zhou",
        "Weizhi Zhang",
        "Liancheng Fang",
        "Langzhou He",
        "Yangning Li",
        "Dongyuan Li",
        "Renhe Jiang",
        "Xue Liu",
        "Philip S. Yu"
      ],
      "published": "2025-05-01T08:29:26Z",
      "categories": "",
      "summary": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reli...",
      "pdf_url": "https://arxiv.org/pdf/2505.00753v4.pdf",
      "relevance_score": 71,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.18193v2",
      "title": "FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo",
      "authors": [
        "Keivan Shariatmadar",
        "Ahmad Osman",
        "Ramin Ray",
        "Kisam Kim"
      ],
      "published": "2025-10-21T00:35:56Z",
      "categories": "",
      "summary": "Fair, transparent, and explainable decision-making remains a critical challenge in Olympic and Paralympic combat sports. This paper presents \\emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees, coaches, and athletes in real time during Taekwondo competitions and training. The system integrates {pose-based action recognition} using graph convolutional networks (GCNs), {epistemic uncertainty modeling} through credal sets, and {explainability overlays} for visual decision support. A set of {interactive dashboards} enables human--AI collaboration in referee evaluation, athl...",
      "pdf_url": "https://arxiv.org/pdf/2510.18193v2.pdf",
      "relevance_score": 71,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2403.01791v1",
      "title": "Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making",
      "authors": [
        "Shuai Ma",
        "Chenyi Zhang",
        "Xinru Wang",
        "Xiaojuan Ma",
        "Ming Yin"
      ],
      "published": "2024-03-04T07:32:28Z",
      "categories": "",
      "summary": "Artificial Intelligence (AI) is increasingly employed in various decision-making tasks, typically as a Recommender, providing recommendations that the AI deems correct. However, recent studies suggest this may diminish human analytical thinking and lead to humans' inappropriate reliance on AI, impairing the synergy in human-AI teams. In contrast, human advisors in group decision-making perform various roles, such as analyzing alternative options or criticizing decision-makers to encourage their critical thinking. This diversity of roles has not yet been empirically explored in AI assistance. I...",
      "pdf_url": "https://arxiv.org/pdf/2403.01791v1.pdf",
      "relevance_score": 70,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2403.09552v1",
      "title": "\"Are You Really Sure?\" Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making",
      "authors": [
        "Shuai Ma",
        "Xinru Wang",
        "Ying Lei",
        "Chuhan Shi",
        "Ming Yin",
        "Xiaojuan Ma"
      ],
      "published": "2024-03-14T16:39:23Z",
      "categories": "",
      "summary": "In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI. This paper approaches this problem from a human-centered perspective, \"human self-confidence calibration\". We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence. In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness. Then in our second study, We propose three calibration mechanisms and compare their effects on humans' self-confidence and user experience. Subseq...",
      "pdf_url": "https://arxiv.org/pdf/2403.09552v1.pdf",
      "relevance_score": 70,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2207.00497v1",
      "title": "Training Novices: The Role of Human-AI Collaboration and Knowledge Transfer",
      "authors": [
        "Philipp Spitzer",
        "Niklas K\u00fchl",
        "Marc Goutier"
      ],
      "published": "2022-07-01T15:34:23Z",
      "categories": "",
      "summary": "Across a multitude of work environments, expert knowledge is imperative for humans to conduct tasks with high performance and ensure business success. These humans possess task-specific expert knowledge (TSEK) and hence, represent subject matter experts (SMEs). However, not only demographic changes but also personnel downsizing strategies lead and will continue to lead to departures of SMEs within organizations, which constitutes the challenge of how to retain that expert knowledge and train novices to keep the competitive advantage elicited by that expert knowledge. SMEs training novices is t...",
      "pdf_url": "https://arxiv.org/pdf/2207.00497v1.pdf",
      "relevance_score": 69,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2112.12387v1",
      "title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization",
      "authors": [
        "Mingming Fan",
        "Xianyou Yang",
        "Tsz Tung Yu",
        "Vera Q. Liao",
        "Jian Zhao"
      ],
      "published": "2021-12-23T07:13:56Z",
      "categories": "",
      "summary": "Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX problems from a usability test session; synchronization refers to ...",
      "pdf_url": "https://arxiv.org/pdf/2112.12387v1.pdf",
      "relevance_score": 69,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2407.14769v1",
      "title": "A Two-Phase Visualization System for Continuous Human-AI Collaboration in Sequelae Analysis and Modeling",
      "authors": [
        "Yang Ouyang",
        "Chenyang Zhang",
        "He Wang",
        "Tianle Ma",
        "Chang Jiang",
        "Yuheng Yan",
        "Zuoqin Yan",
        "Xiaojuan Ma",
        "Chuhan Shi",
        "Quan Li"
      ],
      "published": "2024-07-20T06:20:13Z",
      "categories": "",
      "summary": "In healthcare, AI techniques are widely used for tasks like risk assessment and anomaly detection. Despite AI's potential as a valuable assistant, its role in complex medical data analysis often oversimplifies human-AI collaboration dynamics. To address this, we collaborated with a local hospital, engaging six physicians and one data scientist in a formative study. From this collaboration, we propose a framework integrating two-phase interactive visualization systems: one for Human-Led, AI-Assisted Retrospective Analysis and another for AI-Mediated, Human-Reviewed Iterative Modeling. This fram...",
      "pdf_url": "https://arxiv.org/pdf/2407.14769v1.pdf",
      "relevance_score": 69,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2406.06051v3",
      "title": "On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration",
      "authors": [
        "Guanghui Yu",
        "Robert Kasumba",
        "Chien-Ju Ho",
        "William Yeoh"
      ],
      "published": "2024-06-10T06:39:37Z",
      "categories": "",
      "summary": "To enable effective human-AI collaboration, merely optimizing AI performance without considering human factors is insufficient. Recent research has shown that designing AI agents that take human behavior into account leads to improved performance in human-AI collaboration. However, a limitation of most existing approaches is their assumption that human behavior remains static, regardless of the AI agent's actions. In reality, humans may adjust their actions based on their beliefs about the AI's intentions, specifically, the subtasks they perceive the AI to be attempting to complete based on it...",
      "pdf_url": "https://arxiv.org/pdf/2406.06051v3.pdf",
      "relevance_score": 69,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2310.11370v1",
      "title": "Improving Operator Situation Awareness when Working with AI Recommender Systems",
      "authors": [
        "Divya K. Srivastava",
        "J. Mason Lilly",
        "Karen M. Feigh"
      ],
      "published": "2023-10-17T16:13:07Z",
      "categories": "",
      "summary": "AI recommender systems are sought for decision support by providing suggestions to operators responsible for making final decisions. However, these systems are typically considered black boxes, and are often presented without any context or insight into the underlying algorithm. As a result, recommender systems can lead to miscalibrated user reliance and decreased situation awareness. Recent work has focused on improving the transparency of recommender systems in various ways such as improving the recommender's analysis and visualization of the figures of merit, providing explanations for the ...",
      "pdf_url": "https://arxiv.org/pdf/2310.11370v1.pdf",
      "relevance_score": 69,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2404.06432v4",
      "title": "Missing Pieces: How Do Designs that Expose Uncertainty Longitudinally Impact Trust in AI Decision Aids? An In Situ Study of Gig Drivers",
      "authors": [
        "Rex Chen",
        "Ruiyi Wang",
        "Fei Fang",
        "Norman Sadeh"
      ],
      "published": "2024-04-09T16:25:02Z",
      "categories": "",
      "summary": "Decision aids based on artificial intelligence (AI) induce a wide range of outcomes when they are deployed in uncertain environments. In this paper, we investigate how users' trust in recommendations from an AI decision aid is impacted over time by designs that expose uncertainty in predicted outcomes. Unlike previous work, we focus on gig driving - a real-world, repeated decision-making context. We report on a longitudinal mixed-methods study ($n=51$) where we measured gig drivers' trust as they interacted with an AI-based schedule recommendation tool. Our results show that participants' trus...",
      "pdf_url": "https://arxiv.org/pdf/2404.06432v4.pdf",
      "relevance_score": 69,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2204.05030v3",
      "title": "Assessing the communication gap between AI models and healthcare professionals: explainability, utility and trust in AI-driven clinical decision-making",
      "authors": [
        "Oskar Wysocki",
        "Jessica Katharine Davies",
        "Markel Vigo",
        "Anne Caroline Armstrong",
        "D\u00f3nal Landers",
        "Rebecca Lee",
        "Andr\u00e9 Freitas"
      ],
      "published": "2022-04-11T11:59:04Z",
      "categories": "",
      "summary": "This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study revealed a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context. Despite the general positive attitude of healthcare professionals (HCPs) towards explanations as a safety and trust mechanism, for a significant set of participants there were negative effects associated with confirmation bias, accentuating model over-reliance and increased effort to interact with the model. Also, contradicting one ...",
      "pdf_url": "https://arxiv.org/pdf/2204.05030v3.pdf",
      "relevance_score": 69,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2405.10460v1",
      "title": "The AI Collaborator: Bridging Human-AI Interaction in Educational and Professional Settings",
      "authors": [
        "Mohammad Amin Samadi",
        "Spencer JaQuay",
        "Jing Gu",
        "Nia Nixon"
      ],
      "published": "2024-05-16T22:14:54Z",
      "categories": "",
      "summary": "AI Collaborator, powered by OpenAI's GPT-4, is a groundbreaking tool designed for human-AI collaboration research. Its standout feature is the ability for researchers to create customized AI personas for diverse experimental setups using a user-friendly interface. This functionality is essential for simulating various interpersonal dynamics in team settings. AI Collaborator excels in mimicking different team behaviors, enabled by its advanced memory system and a sophisticated personality framework. Researchers can tailor AI personas along a spectrum from dominant to cooperative, enhancing the ...",
      "pdf_url": "https://arxiv.org/pdf/2405.10460v1.pdf",
      "relevance_score": 67,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2306.07458v3",
      "title": "Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure",
      "authors": [
        "Siddharth Swaroop",
        "Zana Bu\u00e7inca",
        "Krzysztof Z. Gajos",
        "Finale Doshi-Velez"
      ],
      "published": "2023-06-12T23:24:16Z",
      "categories": "",
      "summary": "In settings where users both need high accuracy and are time-pressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistanc...",
      "pdf_url": "https://arxiv.org/pdf/2306.07458v3.pdf",
      "relevance_score": 67,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2305.08598v1",
      "title": "Humans, AI, and Context: Understanding End-Users' Trust in a Real-World Computer Vision Application",
      "authors": [
        "Sunnie S. Y. Kim",
        "Elizabeth Anne Watkins",
        "Olga Russakovsky",
        "Ruth Fong",
        "Andr\u00e9s Monroy-Hern\u00e1ndez"
      ],
      "published": "2023-05-15T12:27:02Z",
      "categories": "",
      "summary": "Trust is an important factor in people's interactions with AI systems. However, there is a lack of empirical studies examining how real end-users trust or distrust the AI system they interact with. Most research investigates one aspect of trust in lab settings with hypothetical end-users. In this paper, we provide a holistic and nuanced understanding of trust in AI through a qualitative case study of a real-world computer vision application. We report findings from interviews with 20 end-users of a popular, AI-based bird identification app where we inquired about their trust in the app from ma...",
      "pdf_url": "https://arxiv.org/pdf/2305.08598v1.pdf",
      "relevance_score": 67,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2307.09885v1",
      "title": "Test-takers have a say: understanding the implications of the use of AI in language tests",
      "authors": [
        "Dawen Zhang",
        "Thong Hoang",
        "Shidong Pan",
        "Yongquan Hu",
        "Zhenchang Xing",
        "Mark Staples",
        "Xiwei Xu",
        "Qinghua Lu",
        "Aaron Quigley"
      ],
      "published": "2023-07-19T10:28:59Z",
      "categories": "",
      "summary": "Language tests measure a person's ability to use a language in terms of listening, speaking, reading, or writing. Such tests play an integral role in academic, professional, and immigration domains, with entities such as educational institutions, professional accreditation bodies, and governments using them to assess candidate language proficiency. Recent advances in Artificial Intelligence (AI) and the discipline of Natural Language Processing have prompted language test providers to explore AI's potential applicability within language testing, leading to transformative activity patterns surr...",
      "pdf_url": "https://arxiv.org/pdf/2307.09885v1.pdf",
      "relevance_score": 67,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2203.15514v1",
      "title": "Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias",
      "authors": [
        "David Solans",
        "Andrea Beretta",
        "Manuel Portela",
        "Carlos Castillo",
        "Anna Monreale"
      ],
      "published": "2022-03-24T11:05:55Z",
      "categories": "",
      "summary": "Artificial Intelligence (AI) is increasingly used to build Decision Support Systems (DSS) across many domains. This paper describes a series of experiments designed to observe human response to different characteristics of a DSS such as accuracy and bias, particularly the extent to which participants rely on the DSS, and the performance they achieve. In our experiments, participants play a simple online game inspired by so-called \"wildcat\" (i.e., exploratory) drilling for oil. The landscape has two layers: a visible layer describing the costs (terrain), and a hidden layer describing the reward...",
      "pdf_url": "https://arxiv.org/pdf/2203.15514v1.pdf",
      "relevance_score": 67,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.16770v1",
      "title": "DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions",
      "authors": [
        "Chaeyeon Lim"
      ],
      "published": "2025-04-23T14:41:31Z",
      "categories": "",
      "summary": "While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressi...",
      "pdf_url": "https://arxiv.org/pdf/2504.16770v1.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2501.01220v1",
      "title": "From Interaction to Attitude: Exploring the Impact of Human-AI Cooperation on Mental Illness Stigma",
      "authors": [
        "Tianqi Song",
        "Jack Jamieson",
        "Tianwen Zhu",
        "Naomi Yamashita",
        "Yi-Chieh Lee"
      ],
      "published": "2025-01-02T12:08:57Z",
      "categories": "",
      "summary": "AI conversational agents have demonstrated efficacy in social contact interventions for stigma reduction at a low cost. However, the underlying mechanisms of how interaction designs contribute to these effects remain unclear. This study investigates how participating in three human-chatbot interactions affects attitudes toward mental illness. We developed three chatbots capable of engaging in either one-way information dissemination from chatbot to a human or two-way cooperation where the chatbot and a human exchange thoughts and work together on a cooperation task. We then conducted a two-wee...",
      "pdf_url": "https://arxiv.org/pdf/2501.01220v1.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.06152v5",
      "title": "The Value of Information in Human-AI Decision-making",
      "authors": [
        "Ziyang Guo",
        "Yifan Wu",
        "Jason Hartline",
        "Jessica Hullman"
      ],
      "published": "2025-02-10T04:50:42Z",
      "categories": "",
      "summary": "Multiple agents are increasingly combined to make decisions with the expectation of achieving complementary performance, where the decisions they make together outperform those made individually. However, knowing how to improve the performance of collaborating agents requires knowing what information and strategies each agent employs. With a focus on human-AI pairings, we contribute a decision-theoretic framework for characterizing the value of information. By defining complementary information, our approach identifies opportunities for agents to better exploit available information in AI-assi...",
      "pdf_url": "https://arxiv.org/pdf/2502.06152v5.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2412.00372v1",
      "title": "2-Factor Retrieval for Improved Human-AI Decision Making in Radiology",
      "authors": [
        "Jim Solomon",
        "Laleh Jalilian",
        "Alexander Vilesov",
        "Meryl Mathew",
        "Tristan Grogan",
        "Arash Bedayat",
        "Achuta Kadambi"
      ],
      "published": "2024-11-30T06:44:42Z",
      "categories": "",
      "summary": "Human-machine teaming in medical AI requires us to understand to what degree a trained clinician should weigh AI predictions. While previous work has shown the potential of AI assistance at improving clinical predictions, existing clinical decision support systems either provide no explainability of their predictions or use techniques like saliency and Shapley values, which do not allow for physician-based verification. To address this gap, this study compares previously used explainable AI techniques with a newly proposed technique termed '2-factor retrieval (2FR)', which is a combination of ...",
      "pdf_url": "https://arxiv.org/pdf/2412.00372v1.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2205.05126v2",
      "title": "A Meta-Analysis of the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making",
      "authors": [
        "Max Schemmer",
        "Patrick Hemmer",
        "Maximilian Nitsche",
        "Niklas K\u00fchl",
        "Michael V\u00f6ssing"
      ],
      "published": "2022-05-10T19:08:10Z",
      "categories": "",
      "summary": "Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysi...",
      "pdf_url": "https://arxiv.org/pdf/2205.05126v2.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2306.13723v2",
      "title": "Human-AI Coevolution",
      "authors": [
        "Dino Pedreschi",
        "Luca Pappalardo",
        "Emanuele Ferragina",
        "Ricardo Baeza-Yates",
        "Albert-Laszlo Barabasi",
        "Frank Dignum",
        "Virginia Dignum",
        "Tina Eliassi-Rad",
        "Fosca Giannotti",
        "Janos Kertesz",
        "Alistair Knott",
        "Yannis Ioannidis",
        "Paul Lukowicz",
        "Andrea Passarella",
        "Alex Sandy Pentland",
        "John Shawe-Taylor",
        "Alessandro Vespignani"
      ],
      "published": "2023-06-23T18:10:54Z",
      "categories": "",
      "summary": "Human-AI coevolution, defined as a process in which humans and AI algorithms continuously influence each other, increasingly characterises our society, but is understudied in artificial intelligence and complexity science literature. Recommender systems and assistants play a prominent role in human-AI coevolution, as they permeate many facets of daily life and influence human choices on online platforms. The interaction between users and AI results in a potentially endless feedback loop, wherein users' choices generate data to train AI models, which, in turn, shape subsequent user preferences....",
      "pdf_url": "https://arxiv.org/pdf/2306.13723v2.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2408.12941v1",
      "title": "iSee: Advancing Multi-Shot Explainable AI Using Case-based Recommendations",
      "authors": [
        "Anjana Wijekoon",
        "Nirmalie Wiratunga",
        "David Corsar",
        "Kyle Martin",
        "Ikechukwu Nkisi-Orji",
        "Chamath Palihawadana",
        "Marta Caro-Mart\u00ednez",
        "Belen D\u00edaz-Agudo",
        "Derek Bridge",
        "Anne Liret"
      ],
      "published": "2024-08-23T09:44:57Z",
      "categories": "",
      "summary": "Explainable AI (XAI) can greatly enhance user trust and satisfaction in AI-assisted decision-making processes. Recent findings suggest that a single explainer may not meet the diverse needs of multiple users in an AI system; indeed, even individual users may require multiple explanations. This highlights the necessity for a \"multi-shot\" approach, employing a combination of explainers to form what we introduce as an \"explanation strategy\". Tailored to a specific user or a user group, an \"explanation experience\" describes interactions with personalised strategies designed to enhance their AI dec...",
      "pdf_url": "https://arxiv.org/pdf/2408.12941v1.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.02062v1",
      "title": "Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)",
      "authors": [
        "Prathamesh Muzumdar",
        "Apoorva Muley",
        "Kuldeep Singh",
        "Sumanth Cheemalapati"
      ],
      "published": "2025-05-04T10:40:05Z",
      "categories": "",
      "summary": "The adoption of Artificial Intelligence (AI) in the healthcare service industry presents numerous ethical challenges, yet current frameworks often fail to offer a comprehensive, empirical understanding of the multidimensional factors influencing ethical AI integration. Addressing this critical research gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM), a novel theoretical framework that categorizes 13 critical ethical variables across four foundational dimensions of Ethical AI Fair AI, Responsible AI, Explainable AI, and Sustainable AI. These dimensions are fur...",
      "pdf_url": "https://arxiv.org/pdf/2505.02062v1.pdf",
      "relevance_score": 65,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.02687v1",
      "title": "The Collaboration Gap",
      "authors": [
        "Tim R. Davidson",
        "Adam Fourney",
        "Saleema Amershi",
        "Robert West",
        "Eric Horvitz",
        "Ece Kamar"
      ],
      "published": "2025-11-04T16:10:57Z",
      "categories": "",
      "summary": "The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable autom...",
      "pdf_url": "https://arxiv.org/pdf/2511.02687v1.pdf",
      "relevance_score": 64,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.01537v1",
      "title": "Passing the Buck to AI: How Individuals' Decision-Making Patterns Affect Reliance on AI",
      "authors": [
        "Katelyn Xiaoying Mei",
        "Rock Yuren Pang",
        "Alex Lyford",
        "Lucy Lu Wang",
        "Katharina Reinecke"
      ],
      "published": "2025-05-02T18:58:27Z",
      "categories": "",
      "summary": "Psychological research has identified different patterns individuals have while making decisions, such as vigilance (making decisions after thorough information gathering), hypervigilance (rushed and anxious decision-making), and buckpassing (deferring decisions to others). We examine whether these decision-making patterns shape peoples' likelihood of seeking out or relying on AI. In an online experiment with 810 participants tasked with distinguishing food facts from myths, we found that a higher buckpassing tendency was positively correlated with both seeking out and relying on AI suggestion...",
      "pdf_url": "https://arxiv.org/pdf/2505.01537v1.pdf",
      "relevance_score": 63,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2304.13081v2",
      "title": "Organizational Governance of Emerging Technologies: AI Adoption in Healthcare",
      "authors": [
        "Jee Young Kim",
        "William Boag",
        "Freya Gulamali",
        "Alifia Hasan",
        "Henry David Jeffry Hogg",
        "Mark Lifson",
        "Deirdre Mulligan",
        "Manesh Patel",
        "Inioluwa Deborah Raji",
        "Ajai Sehgal",
        "Keo Shaw",
        "Danny Tobey",
        "Alexandra Valladares",
        "David Vidal",
        "Suresh Balu",
        "Mark Sendak"
      ],
      "published": "2023-04-25T18:30:47Z",
      "categories": "",
      "summary": "Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adop...",
      "pdf_url": "https://arxiv.org/pdf/2304.13081v2.pdf",
      "relevance_score": 63,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2204.07666v1",
      "title": "Designing Creative AI Partners with COFI: A Framework for Modeling Interaction in Human-AI Co-Creative Systems",
      "authors": [
        "Jeba Rezwana",
        "Mary Lou Maher"
      ],
      "published": "2022-04-15T22:35:23Z",
      "categories": "",
      "summary": "Human-AI co-creativity involves both humans and AI collaborating on a shared creative product as partners. In a creative collaboration, interaction dynamics, such as turn-taking, contribution type, and communication, are the driving forces of the co-creative process. Therefore the interaction model is a critical and essential component for effective co-creative systems. There is relatively little research about interaction design in the co-creativity field, which is reflected in a lack of focus on interaction design in many existing co-creative systems. The primary focus of co-creativity resea...",
      "pdf_url": "https://arxiv.org/pdf/2204.07666v1.pdf",
      "relevance_score": 62,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2401.15106v6",
      "title": "Underspecified Human Decision Experiments Considered Harmful",
      "authors": [
        "Jessica Hullman",
        "Alex Kale",
        "Jason Hartline"
      ],
      "published": "2024-01-25T16:21:37Z",
      "categories": "",
      "summary": "Decision-making with information displays is a key focus of research in areas like human-AI collaboration and data visualization. However, what constitutes a decision problem, and what is required for an experiment to conclude that decisions are flawed, remain imprecise. We present a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics. We claim that to attribute loss in human performance to bias, an experiment must provide the information that a rational agent would need to identify the normative decision. We evaluate whethe...",
      "pdf_url": "https://arxiv.org/pdf/2401.15106v6.pdf",
      "relevance_score": 62,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2308.05411v1",
      "title": "Explainable AI applications in the Medical Domain: a systematic review",
      "authors": [
        "Nicoletta Prentzas",
        "Antonis Kakas",
        "Constantinos S. Pattichis"
      ],
      "published": "2023-08-10T08:12:17Z",
      "categories": "",
      "summary": "Artificial Intelligence in Medicine has made significant progress with emerging applications in medical imaging, patient care, and other areas. While these applications have proven successful in retrospective studies, very few of them were applied in practice.The field of Medical AI faces various challenges, in terms of building user trust, complying with regulations, using data ethically.Explainable AI (XAI) aims to enable humans understand AI and trust its results. This paper presents a literature review on the recent developments of XAI solutions for medical decision support, based on a rep...",
      "pdf_url": "https://arxiv.org/pdf/2308.05411v1.pdf",
      "relevance_score": 61,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2204.02310v1",
      "title": "Improving Human-AI Partnerships in Child Welfare: Understanding Worker Practices, Challenges, and Desires for Algorithmic Decision Support",
      "authors": [
        "Anna Kawakami",
        "Venkatesh Sivaraman",
        "Hao-Fei Cheng",
        "Logan Stapleton",
        "Yanghuidi Cheng",
        "Diana Qing",
        "Adam Perer",
        "Zhiwei Steven Wu",
        "Haiyi Zhu",
        "Kenneth Holstein"
      ],
      "published": "2022-04-05T16:10:49Z",
      "categories": "",
      "summary": "AI-based decision support tools (ADS) are increasingly used to augment human decision-making in high-stakes, social contexts. As public sector agencies begin to adopt ADS, it is critical that we understand workers' experiences with these systems in practice. In this paper, we present findings from a series of interviews and contextual inquiries at a child welfare agency, to understand how they currently make AI-assisted child maltreatment screening decisions. Overall, we observe how workers' reliance upon the ADS is guided by (1) their knowledge of rich, contextual information beyond what the ...",
      "pdf_url": "https://arxiv.org/pdf/2204.02310v1.pdf",
      "relevance_score": 60,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.12500v1",
      "title": "Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public",
      "authors": [
        "Xuhai Xu",
        "Haoyu Hu",
        "Haoran Zhang",
        "Will Ke Wang",
        "Reina Wang",
        "Luis R. Soenksen",
        "Omar Badri",
        "Sheharbano Jafry",
        "Elise Burger",
        "Lotanna Nwandu",
        "Apoorva Mehta",
        "Erik P. Duhaime",
        "Asif Qasim",
        "Hause Lin",
        "Janis Pereira",
        "Jonathan Hershon",
        "Paulius Mui",
        "Alejandro A. Gru",
        "No\u00e9mie Elhadad",
        "Lena Mamykina",
        "Matthew Groh",
        "Philipp Tschandl",
        "Roxana Daneshjou",
        "Marzyeh Ghassemi"
      ],
      "published": "2025-12-14T00:06:06Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), infl...",
      "pdf_url": "https://arxiv.org/pdf/2512.12500v1.pdf",
      "relevance_score": 60,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.11979v1",
      "title": "Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)",
      "authors": [
        "Marc Scibelli",
        "Krystelle Gonzalez Papaux",
        "Julia Valenti",
        "Srishti Kush"
      ],
      "published": "2025-12-12T19:04:40Z",
      "categories": "",
      "summary": "The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initi...",
      "pdf_url": "https://arxiv.org/pdf/2512.11979v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2306.00074v4",
      "title": "Human-Aligned Calibration for AI-Assisted Decision Making",
      "authors": [
        "Nina L. Corvelo Benz",
        "Manuel Gomez Rodriguez"
      ],
      "published": "2023-05-31T18:00:14Z",
      "categories": "",
      "summary": "Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using thes...",
      "pdf_url": "https://arxiv.org/pdf/2306.00074v4.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2305.20076v3",
      "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
      "authors": [
        "Jessy Lin",
        "Nicholas Tomlin",
        "Jacob Andreas",
        "Jason Eisner"
      ],
      "published": "2023-05-31T17:50:02Z",
      "categories": "",
      "summary": "We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants ca...",
      "pdf_url": "https://arxiv.org/pdf/2305.20076v3.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2001.02114v1",
      "title": "Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making",
      "authors": [
        "Yunfeng Zhang",
        "Q. Vera Liao",
        "Rachel K. E. Bellamy"
      ],
      "published": "2020-01-07T15:33:48Z",
      "categories": "",
      "summary": "Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately \\textit{calibrate} human trust in the AI on a case-by-case bas...",
      "pdf_url": "https://arxiv.org/pdf/2001.02114v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.17930v1",
      "title": "How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations",
      "authors": [
        "Vahid Garousi",
        "Zafar Jafarov"
      ],
      "published": "2025-07-23T21:00:21Z",
      "categories": "",
      "summary": "Artificial Intelligence (AI) has the potential to transform Software Engineering (SE) by enhancing productivity, efficiency, and decision support. Tools like GitHub Copilot and ChatGPT have given rise to \"vibe coding\"-an exploratory, prompt-driven development style. Yet, how software engineers engage with these tools in daily tasks, especially in deciding whether to trust, refine, or reject AI-generated outputs, remains underexplored. This paper presents two complementary contributions. First, a pragmatic process model capturing real-world AI-assisted SE activities, including prompt design, in...",
      "pdf_url": "https://arxiv.org/pdf/2507.17930v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.02388v1",
      "title": "Bridging Human Cognition and AI: A Framework for Explainable Decision-Making Systems",
      "authors": [
        "N. Jean",
        "G. Le Pera"
      ],
      "published": "2025-09-02T14:53:37Z",
      "categories": "",
      "summary": "Explainability in AI and ML models is critical for fostering trust, ensuring accountability, and enabling informed decision making in high stakes domains. Yet this objective is often unmet in practice. This paper proposes a general purpose framework that bridges state of the art explainability techniques with Malle's five category model of behavior explanation: Knowledge Structures, Simulation/Projection, Covariation, Direct Recall, and Rationalization. The framework is designed to be applicable across AI assisted decision making systems, with the goal of enhancing transparency, interpretabili...",
      "pdf_url": "https://arxiv.org/pdf/2509.02388v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2204.12230v1",
      "title": "User Trust on an Explainable AI-based Medical Diagnosis Support System",
      "authors": [
        "Yao Rong",
        "Nora Castner",
        "Efe Bozkir",
        "Enkelejda Kasneci"
      ],
      "published": "2022-04-26T11:20:44Z",
      "categories": "",
      "summary": "Recent research has supported that system explainability improves user trust and willingness to use medical AI for diagnostic support. In this paper, we use chest disease diagnosis based on X-Ray images as a case study to investigate user trust and reliance. Building off explainability, we propose a support system where users (radiologists) can view causal explanations for final decisions. After observing these causal explanations, users provided their opinions of the model predictions and could correct explanations if they did not agree. We measured user trust as the agreement between the mod...",
      "pdf_url": "https://arxiv.org/pdf/2204.12230v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2301.05809v1",
      "title": "Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making",
      "authors": [
        "Shuai Ma",
        "Ying Lei",
        "Xinru Wang",
        "Chengbo Zheng",
        "Chuhan Shi",
        "Ming Yin",
        "Xiaojuan Ma"
      ],
      "published": "2023-01-14T02:51:01Z",
      "categories": "",
      "summary": "In AI-assisted decision-making, it is critical for human decision-makers to know when to trust AI and when to trust themselves. However, prior studies calibrated human trust only based on AI confidence indicating AI's correctness likelihood (CL) but ignored humans' CL, hindering optimal team decision-making. To mitigate this gap, we proposed to promote humans' appropriate trust based on the CL of both sides at a task-instance level. We first modeled humans' CL by approximating their decision-making models and computing their potential performance in similar instances. We demonstrated the feasi...",
      "pdf_url": "https://arxiv.org/pdf/2301.05809v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2204.13828v1",
      "title": "Designing for Responsible Trust in AI Systems: A Communication Perspective",
      "authors": [
        "Q. Vera Liao",
        "S. Shyam Sundar"
      ],
      "published": "2022-04-29T00:14:33Z",
      "categories": "",
      "summary": "Current literature and public discourse on \"trust in AI\" are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in technologies to develop a conceptual model called ...",
      "pdf_url": "https://arxiv.org/pdf/2204.13828v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.12384v1",
      "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries",
      "authors": [
        "Bo Wen",
        "Guoyun Gao",
        "Zhicheng Xu",
        "Ruibin Mao",
        "Xiaojuan Qi",
        "X. Sharon Hu",
        "Xunzhao Yin",
        "Can Li"
      ],
      "published": "2025-07-16T16:31:20Z",
      "categories": "",
      "summary": "The rapid advancement of artificial intelligence has raised concerns regarding its trustworthiness, especially in terms of interpretability and robustness. Tree-based models like Random Forest and XGBoost excel in interpretability and accuracy for tabular data, but scaling them remains computationally expensive due to poor data locality and high data dependence. Previous efforts to accelerate these models with analog content addressable memory (CAM) have struggled, due to the fact that the difficult-to-implement sharp decision boundaries are highly susceptible to device variations, which leads...",
      "pdf_url": "https://arxiv.org/pdf/2507.12384v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2301.06676v2",
      "title": "Explainable, Interpretable & Trustworthy AI for Intelligent Digital Twin: Case Study on Remaining Useful Life",
      "authors": [
        "Kazuma Kobayashi",
        "Syed Bahauddin Alam"
      ],
      "published": "2023-01-17T03:17:07Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) and Machine learning (ML) are increasingly used in energy and engineering systems, but these models must be fair, unbiased, and explainable. It is critical to have confidence in AI's trustworthiness. ML techniques have been useful in predicting important parameters and in improving model performance. However, for these AI techniques to be useful for making decisions, they need to be audited, accounted for, and easy to understand. Therefore, the use of explainable AI (XAI) and interpretable machine learning (IML) is crucial for the accurate prediction of prognostics...",
      "pdf_url": "https://arxiv.org/pdf/2301.06676v2.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.08699v1",
      "title": "Blockchain As a Platform For Artificial Intelligence (AI) Transparency",
      "authors": [
        "Afroja Akther",
        "Ayesha Arobee",
        "Abdullah Al Adnan",
        "Omum Auyon",
        "ASM Johirul Islam",
        "Farhad Akter"
      ],
      "published": "2025-03-07T01:57:26Z",
      "categories": "",
      "summary": "As artificial intelligence (AI) systems become increasingly complex and autonomous, concerns over transparency and accountability have intensified. The \"black box\" problem in AI decision-making limits stakeholders' ability to understand, trust, and verify outcomes, particularly in high-stakes sectors such as healthcare, finance, and autonomous systems. Blockchain technology, with its decentralized, immutable, and transparent characteristics, presents a potential solution to enhance AI transparency and auditability. This paper explores the integration of blockchain with AI to improve decision t...",
      "pdf_url": "https://arxiv.org/pdf/2503.08699v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2201.00692v1",
      "title": "Validation and Transparency in AI systems for pharmacovigilance: a case study applied to the medical literature monitoring of adverse events",
      "authors": [
        "Bruno Ohana",
        "Jack Sullivan",
        "Nicole Baker"
      ],
      "published": "2021-12-21T21:02:24Z",
      "categories": "",
      "summary": "Recent advances in artificial intelligence applied to biomedical text are opening exciting opportunities for improving pharmacovigilance activities currently burdened by the ever growing volumes of real world data. To fully realize these opportunities, existing regulatory guidance and industry best practices should be taken into consideration in order to increase the overall trustworthiness of the system and enable broader adoption. In this paper we present a case study on how to operationalize existing guidance for validated AI systems in pharmacovigilance focusing on the specific task of med...",
      "pdf_url": "https://arxiv.org/pdf/2201.00692v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.14632v1",
      "title": "Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and Future Potential",
      "authors": [
        "Jennifer Haase"
      ],
      "published": "2025-02-20T15:10:05Z",
      "categories": "",
      "summary": "The integration of generative AI (GenAI) tools, particularly large language models (LLMs), is transforming professional coaching workflows. This study explores how coaches use GenAI, the perceived benefits and limitations of these tools, and broader attitudes toward AI-assisted coaching. A survey of 205 coaching professionals reveals widespread adoption of GenAI for research, content creation, and administrative support, while its role in relational and interpretative coaching remains limited. Findings indicate that AI literacy and perceived AI impact strongly predict GenAI adoption, with posi...",
      "pdf_url": "https://arxiv.org/pdf/2502.14632v1.pdf",
      "relevance_score": 58,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.11919v1",
      "title": "From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis",
      "authors": [
        "Zhuoyan Li",
        "Hangxiao Zhu",
        "Zhuoran Lu",
        "Ziang Xiao",
        "Ming Yin"
      ],
      "published": "2025-02-17T15:32:54Z",
      "categories": "",
      "summary": "AI-assisted decision making becomes increasingly prevalent, yet individuals often fail to utilize AI-based decision aids appropriately especially when the AI explanations are absent, potentially as they do not %understand reflect on AI's decision recommendations critically. Large language models (LLMs), with their exceptional conversational and analytical capabilities, present great opportunities to enhance AI-assisted decision making in the absence of AI explanations by providing natural-language-based analysis of AI's decision recommendation, e.g., how each feature of a decision making task ...",
      "pdf_url": "https://arxiv.org/pdf/2502.11919v1.pdf",
      "relevance_score": 57,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2407.08908v1",
      "title": "Are They the Same Picture? Adapting Concept Bottleneck Models for Human-AI Collaboration in Image Retrieval",
      "authors": [
        "Vaibhav Balloli",
        "Sara Beery",
        "Elizabeth Bondi-Kelly"
      ],
      "published": "2024-07-12T00:59:32Z",
      "categories": "",
      "summary": "Image retrieval plays a pivotal role in applications from wildlife conservation to healthcare, for finding individual animals or relevant images to aid diagnosis. Although deep learning techniques for image retrieval have advanced significantly, their imperfect real-world performance often necessitates including human expertise. Human-in-the-loop approaches typically rely on humans completing the task independently and then combining their opinions with an AI model in various ways, as these models offer very little interpretability or \\textit{correctability}. To allow humans to intervene in th...",
      "pdf_url": "https://arxiv.org/pdf/2407.08908v1.pdf",
      "relevance_score": 57,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.18385v1",
      "title": "Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights",
      "authors": [
        "Jeba Rezwana",
        "Corey Ford"
      ],
      "published": "2025-05-23T21:19:37Z",
      "categories": "",
      "summary": "Effective communication between AI and humans is essential for successful human-AI co-creation. However, many current co-creative AI systems lack effective communication, which limits their potential for collaboration. This paper presents the initial design of the Framework for AI Communication (FAICO) for co-creative AI, developed through a systematic review of 107 full-length papers. FAICO presents key aspects of AI communication and their impact on user experience, offering preliminary guidelines for designing human-centered AI communication. To improve the framework, we conducted a prelimi...",
      "pdf_url": "https://arxiv.org/pdf/2505.18385v1.pdf",
      "relevance_score": 57,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.09858v1",
      "title": "Media and responsible AI governance: a game-theoretic and LLM analysis",
      "authors": [
        "Nataliya Balabanova",
        "Adeela Bashir",
        "Paolo Bova",
        "Alessio Buscemi",
        "Theodor Cimpeanu",
        "Henrique Correia da Fonseca",
        "Alessandro Di Stefano",
        "Manh Hong Duong",
        "Elias Fernandez Domingos",
        "Antonio Fernandes",
        "The Anh Han",
        "Marcus Krellner",
        "Ndidi Bianca Ogbo",
        "Simon T. Powers",
        "Daniele Proverbio",
        "Fernando P. Santos",
        "Zia Ush Shamszaman",
        "Zhao Song"
      ],
      "published": "2025-03-12T21:39:38Z",
      "categories": "",
      "summary": "This paper investigates the complex interplay between AI developers, regulators, users, and the media in fostering trustworthy AI systems. Using evolutionary game theory and large language models (LLMs), we model the strategic interactions among these actors under different regulatory regimes. The research explores two key mechanisms for achieving responsible governance, safe AI development and adoption of safe AI: incentivising effective regulation through media reporting, and conditioning user trust on commentariats' recommendation. The findings highlight the crucial role of the media in pro...",
      "pdf_url": "https://arxiv.org/pdf/2503.09858v1.pdf",
      "relevance_score": 57,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.10981v1",
      "title": "An Exploratory Study on AI-driven Visualisation Techniques on Decision Making in Extended Reality",
      "authors": [
        "Ze Dong",
        "Binyang Han",
        "Jingjing Zhang",
        "Ruoyu Wen",
        "Barrett Ens",
        "Adrian Clark",
        "Tham Piumsomboon"
      ],
      "published": "2025-07-15T04:53:09Z",
      "categories": "",
      "summary": "The integration of extended reality (XR) with artificial intelligence (AI) introduces a new paradigm for user interaction, enabling AI to perceive user intent, stimulate the senses, and influence decision-making. We explored the impact of four AI-driven visualisation techniques -- `Inform,' `Nudge,' `Recommend,' and `Instruct' -- on user decision-making in XR using the Meta Quest Pro. To test these techniques, we used a pre-recorded 360-degree video of a supermarket, overlaying each technique through a virtual interface. We aimed to investigate how these different visualisation techniques with...",
      "pdf_url": "https://arxiv.org/pdf/2507.10981v1.pdf",
      "relevance_score": 57,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2407.17481v1",
      "title": "Human Oversight of Artificial Intelligence and Technical Standardisation",
      "authors": [
        "Marion Ho-Dac",
        "Baptiste Martinez"
      ],
      "published": "2024-07-02T07:43:46Z",
      "categories": "",
      "summary": "The adoption of human oversight measures makes it possible to regulate, to varying degrees and in different ways, the decision-making process of Artificial Intelligence (AI) systems, for example by placing a human being in charge of supervising the system and, upstream, by developing the AI system to enable such supervision. Within the global governance of AI, the requirement for human oversight is embodied in several regulatory formats, within a diversity of normative sources. On the one hand, it reinforces the accountability of AI systems' users (for example, by requiring them to carry out c...",
      "pdf_url": "https://arxiv.org/pdf/2407.17481v1.pdf",
      "relevance_score": 55,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.06595v1",
      "title": "LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?",
      "authors": [
        "Irdin Pekaric",
        "Philipp Zech",
        "Tom Mattson"
      ],
      "published": "2025-09-08T12:06:06Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) are transforming human decision-making by acting as cognitive collaborators. Yet, this promise comes with a paradox: while LLMs can improve accuracy, they may also erode independent reasoning, promote over-reliance and homogenize decisions. In this paper, we investigate how LLMs shape human judgment in security-critical contexts. Through two exploratory focus groups (unaided and LLM-supported), we assess decision accuracy, behavioral resilience and reliance dynamics. Our findings reveal that while LLMs enhance accuracy and consistency in routine decisions, they can...",
      "pdf_url": "https://arxiv.org/pdf/2509.06595v1.pdf",
      "relevance_score": 55,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.02631v2",
      "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective",
      "authors": [
        "Haotian Li",
        "Yun Wang",
        "Huamin Qu"
      ],
      "published": "2025-03-04T13:56:18Z",
      "categories": "",
      "summary": "Human-AI collaborative tools attract attentions from the data storytelling community to lower the expertise barrier and streamline the workflow. The recent advance in large-scale generative AI techniques, e.g., large language models (LLMs) and text-to-image models, has the potential to enhance data storytelling with their power in visual and narration generation. After two years since these techniques were publicly available, it is important to reflect our progress of applying them and have an outlook for future opportunities. To achieve the goal, we compare the collaboration patterns of the l...",
      "pdf_url": "https://arxiv.org/pdf/2503.02631v2.pdf",
      "relevance_score": 54,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.02560v1",
      "title": "SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration",
      "authors": [
        "Dan Bohus",
        "Sean Andrist",
        "Ann Paradiso",
        "Nick Saw",
        "Tim Schoonbeek",
        "Maia Stiber"
      ],
      "published": "2025-11-04T13:30:15Z",
      "categories": "",
      "summary": "We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 h...",
      "pdf_url": "https://arxiv.org/pdf/2511.02560v1.pdf",
      "relevance_score": 54,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2404.07926v1",
      "title": "Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation",
      "authors": [
        "Jinkyung Park",
        "Pamela Wisniewski",
        "Vivek Singh"
      ],
      "published": "2024-04-11T17:20:57Z",
      "categories": "",
      "summary": "In this position paper, we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks. Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online ri...",
      "pdf_url": "https://arxiv.org/pdf/2404.07926v1.pdf",
      "relevance_score": 54,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.19743v1",
      "title": "Cultural Dimensions of Artificial Intelligence Adoption: Empirical Insights for Wave 1 from a Multinational Longitudinal Pilot Study",
      "authors": [
        "Michelle J. Cummings-Koether",
        "Franziska Durner",
        "Theophile Shyiramunda",
        "Matthias Huemmer"
      ],
      "published": "2025-10-22T16:31:28Z",
      "categories": "",
      "summary": "The swift diffusion of artificial intelligence (AI) raises critical questions about how cultural contexts shape adoption patterns and their consequences for human daily life. This study investigates the cultural dimensions of AI adoption and their influence on cognitive strategies across nine national contexts in Europe, Africa, Asia, and South America. Drawing on survey data from a diverse pilot sample (n = 21) and guided by cross-cultural psychology, digital ethics, and sociotechnical systems theory, we examine how demographic variables (age, gender, professional role) and cultural orientati...",
      "pdf_url": "https://arxiv.org/pdf/2510.19743v1.pdf",
      "relevance_score": 53,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.10847v2",
      "title": "A funny companion: Distinct neural responses to perceived AI- versus human-generated humor",
      "authors": [
        "Xiaohui Rao",
        "Hanlin Wu",
        "Zhenguang G. Cai"
      ],
      "published": "2025-09-13T15:05:57Z",
      "categories": "",
      "summary": "As AI companions become capable of human-like communication, including telling jokes, understanding how people cognitively and emotionally respond to AI humor becomes increasingly important. This study used electroencephalography (EEG) to compare how people process humor from AI versus human sources. Behavioral analysis revealed that participants rated AI and human humor as comparably funny. However, neurophysiological data showed that AI humor elicited a smaller N400 effect, suggesting reduced cognitive effort during the processing of incongruity. This was accompanied by a larger Late Positiv...",
      "pdf_url": "https://arxiv.org/pdf/2509.10847v2.pdf",
      "relevance_score": 52,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2305.15922v1",
      "title": "Towards a Capability Assessment Model for the Comprehension and Adoption of AI in Organisations",
      "authors": [
        " Butler",
        " Tom",
        " Espinoza-Lim\u00f3n",
        " Angelina",
        " Sepp\u00e4l\u00e4",
        " Selja"
      ],
      "published": "2023-05-25T10:43:54Z",
      "categories": "",
      "summary": "The comprehension and adoption of Artificial Intelligence (AI) are beset with practical and ethical problems. This article presents a 5-level AI Capability Assessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to assist practitioners in AI comprehension and adoption. These practical tools were developed with business executives, technologists, and other organisational stakeholders in mind. They are founded on a comprehensive conception of AI compared to those in other AI adoption models and are also open-source artefacts. Thus, the AI-CAM and AI-CM present an accessible resour...",
      "pdf_url": "https://arxiv.org/pdf/2305.15922v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.09473v1",
      "title": "An Efficient Interaction Human-AI Synergy System Bridging Visual Awareness and Large Language Model for Intensive Care Units",
      "authors": [
        "Yibowen Zhao",
        "Yiming Cao",
        "Zhiqi Shen",
        "Juan Du",
        "Yonghui Xu",
        "Lizhen Cui",
        "Cyril Leung"
      ],
      "published": "2025-12-10T09:50:58Z",
      "categories": "",
      "summary": "Intensive Care Units (ICUs) are critical environments characterized by high-stakes monitoring and complex data management. However, current practices often rely on manual data transcription and fragmented information systems, introducing potential risks to patient safety and operational efficiency. To address these issues, we propose a human-AI synergy system based on a cloud-edge-end architecture, which integrates visual-aware data extraction and semantic interaction mechanisms. Specifically, a visual-aware edge module non-invasively captures real-time physiological data from bedside monitors...",
      "pdf_url": "https://arxiv.org/pdf/2512.09473v1.pdf",
      "relevance_score": 50,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2401.05115v1",
      "title": "Unpacking Human-AI interactions: From interaction primitives to a design space",
      "authors": [
        "Kostas Tsiakas",
        "Dave Murray-Rust"
      ],
      "published": "2024-01-10T12:27:18Z",
      "categories": "",
      "summary": "This paper aims to develop a semi-formal design space for Human-AI interactions, by building a set of interaction primitives which specify the communication between users and AI systems during their interaction. We show how these primitives can be combined into a set of interaction patterns which can provide an abstract specification for exchanging messages between humans and AI/ML models to carry out purposeful interactions. The motivation behind this is twofold: firstly, to provide a compact generalisation of existing practices, that highlights the similarities and differences between system...",
      "pdf_url": "https://arxiv.org/pdf/2401.05115v1.pdf",
      "relevance_score": 50,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2411.10461v1",
      "title": "Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary",
      "authors": [
        "Zhuoyan Li",
        "Ming Yin"
      ],
      "published": "2024-11-02T18:33:28Z",
      "categories": "",
      "summary": "Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the ``black-box'' nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice. In this paper, we explore whether we can...",
      "pdf_url": "https://arxiv.org/pdf/2411.10461v1.pdf",
      "relevance_score": 50,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.07617v1",
      "title": "On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making",
      "authors": [
        "Sarah Jabbour",
        "David Fouhey",
        "Nikola Banovic",
        "Stephanie D. Shepard",
        "Ella Kazerooni",
        "Michael W. Sjoding",
        "Jenna Wiens"
      ],
      "published": "2025-08-11T04:53:13Z",
      "categories": "",
      "summary": "AI has the potential to augment human decision making. However, even high-performing models can produce inaccurate predictions when deployed. These inaccuracies, combined with automation bias, where humans overrely on AI predictions, can result in worse decisions. Selective prediction, in which potentially unreliable model predictions are hidden from users, has been proposed as a solution. This approach assumes that when AI abstains and informs the user so, humans make decisions as they would without AI involvement. To test this assumption, we study the effects of selective prediction on human...",
      "pdf_url": "https://arxiv.org/pdf/2508.07617v1.pdf",
      "relevance_score": 50,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2405.15804v1",
      "title": "Explainable Human-AI Interaction: A Planning Perspective",
      "authors": [
        "Sarath Sreedharan",
        "Anagha Kulkarni",
        "Subbarao Kambhampati"
      ],
      "published": "2024-05-19T22:22:21Z",
      "categories": "",
      "summary": "From its inception, AI has had a rather ambivalent relationship with humans -- swinging between their augmentation and replacement. Now, as AI technologies enter our everyday lives at an ever increasing pace, there is a greater need for AI systems to work synergistically with humans. One critical requirement for such synergistic human-AI interaction is that the AI systems be explainable to the humans in the loop. To do this effectively, AI agents need to go beyond planning with their own models of the world, and take into account the mental model of the human in the loop. Drawing from several ...",
      "pdf_url": "https://arxiv.org/pdf/2405.15804v1.pdf",
      "relevance_score": 50,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2406.14114v1",
      "title": "Dye4AI: Assuring Data Boundary on Generative AI Services",
      "authors": [
        "Shu Wang",
        "Kun Sun",
        "Yan Zhai"
      ],
      "published": "2024-06-20T08:55:23Z",
      "categories": "",
      "summary": "Generative artificial intelligence (AI) is versatile for various applications, but security and privacy concerns with third-party AI vendors hinder its broader adoption in sensitive scenarios. Hence, it is essential for users to validate the AI trustworthiness and ensure the security of data boundaries. In this paper, we present a dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution. Our dye testing procedure contains 3 stages: trigger generation, trigger inserti...",
      "pdf_url": "https://arxiv.org/pdf/2406.14114v1.pdf",
      "relevance_score": 50,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2312.15661v3",
      "title": "Unlocking the Potential of Large Language Models for Explainable Recommendations",
      "authors": [
        "Yucong Luo",
        "Mingyue Cheng",
        "Hao Zhang",
        "Junyu Lu",
        "Qi Liu",
        "Enhong Chen"
      ],
      "published": "2023-12-25T09:09:54Z",
      "categories": "",
      "summary": "Generating user-friendly explanations regarding why an item is recommended has become increasingly common, largely due to advances in language generation technology, which can enhance user trust and facilitate more informed decision-making when using online services. However, existing explainable recommendation systems focus on using small-size language models. It remains uncertain what impact replacing the explanation generator with the recently emerging large language models (LLMs) would have. Can we expect unprecedented results?   In this study, we propose LLMXRec, a simple yet effective tw...",
      "pdf_url": "https://arxiv.org/pdf/2312.15661v3.pdf",
      "relevance_score": 49,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2305.18307v1",
      "title": "Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study",
      "authors": [
        "Nicolas Scharowski",
        "Michaela Benk",
        "Swen J. K\u00fchne",
        "L\u00e9ane Wettstein",
        "Florian Br\u00fchlmann"
      ],
      "published": "2023-05-15T09:51:10Z",
      "categories": "",
      "summary": "Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users' attitudes toward certification labels and thei...",
      "pdf_url": "https://arxiv.org/pdf/2305.18307v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2403.09510v1",
      "title": "Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation",
      "authors": [
        "Zainab Alalawi",
        "Paolo Bova",
        "Theodor Cimpeanu",
        "Alessandro Di Stefano",
        "Manh Hong Duong",
        "Elias Fernandez Domingos",
        "The Anh Han",
        "Marcus Krellner",
        "Bianca Ogbo",
        "Simon T. Powers",
        "Filippo Zimmaro"
      ],
      "published": "2024-03-14T15:56:39Z",
      "categories": "",
      "summary": "There is general agreement that some form of regulation is necessary both for AI creators to be incentivised to develop trustworthy systems, and for users to actually trust those systems. But there is much debate about what form these regulations should take and how they should be implemented. Most work in this area has been qualitative, and has not been able to make formal predictions. Here, we propose that evolutionary game theory can be used to quantitatively model the dilemmas faced by users, AI creators, and regulators, and provide insights into the possible effects of different regulator...",
      "pdf_url": "https://arxiv.org/pdf/2403.09510v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2309.10318v1",
      "title": "Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness and Trust",
      "authors": [
        "Andreas Duenser",
        "David M. Douglas"
      ],
      "published": "2023-09-19T05:00:34Z",
      "categories": "",
      "summary": "We present an overview of the literature on trust in AI and AI trustworthiness and argue for the need to distinguish these concepts more clearly and to gather more empirically evidence on what contributes to people s trusting behaviours. We discuss that trust in AI involves not only reliance on the system itself, but also trust in the developers of the AI system. AI ethics principles such as explainability and transparency are often assumed to promote user trust, but empirical evidence of how such features actually affect how users perceive the system s trustworthiness is not as abundance or n...",
      "pdf_url": "https://arxiv.org/pdf/2309.10318v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2001.05375v1",
      "title": "AAAI FSS-19: Human-Centered AI: Trustworthiness of AI Models and Data Proceedings",
      "authors": [
        "Florian Buettner",
        "John Piorkowski",
        "Ian McCulloh",
        "Ulli Waltinger"
      ],
      "published": "2020-01-15T15:30:29Z",
      "categories": "",
      "summary": "To facilitate the widespread acceptance of AI systems guiding decision-making in real-world applications, it is key that solutions comprise trustworthy, integrated human-AI systems. Not only in safety-critical applications such as autonomous driving or medicine, but also in dynamic open world systems in industry and government it is crucial for predictive models to be uncertainty-aware and yield trustworthy predictions. Another key requirement for deployment of AI at enterprise scale is to realize the importance of integrating human-centered design into AI systems such that humans are able to ...",
      "pdf_url": "https://arxiv.org/pdf/2001.05375v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.05046v1",
      "title": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User Attributes, Trust Dimensions, Task Context, and Societal Perceptions among University Students",
      "authors": [
        "Kadija Bouyzourn",
        "Alexandra Birch"
      ],
      "published": "2025-07-07T14:29:54Z",
      "categories": "",
      "summary": "This mixed-methods inquiry examined four domains that shape university students' trust in ChatGPT: user attributes, seven delineated trust dimensions, task context, and perceived societal impact. Data were collected through a survey of 115 UK undergraduate and postgraduate students and four complementary semi-structured interviews. Behavioural engagement outweighed demographics: frequent use increased trust, whereas self-reported understanding of large-language-model mechanics reduced it. Among the dimensions, perceived expertise and ethical risk were the strongest predictors of overall trust;...",
      "pdf_url": "https://arxiv.org/pdf/2507.05046v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1912.05284v1",
      "title": "Interactive AI with a Theory of Mind",
      "authors": [
        "Mustafa Mert \u00c7elikok",
        "Tomi Peltola",
        "Pedram Daee",
        "Samuel Kaski"
      ],
      "published": "2019-12-01T19:26:48Z",
      "categories": "",
      "summary": "Understanding each other is the key to success in collaboration. For humans, attributing mental states to others, the theory of mind, provides the crucial advantage. We argue for formulating human--AI interaction as a multi-agent problem, endowing AI with a computational theory of mind to understand and anticipate the user. To differentiate the approach from previous work, we introduce a categorisation of user modelling approaches based on the level of agency learnt in the interaction. We describe our recent work in using nested multi-agent modelling to formulate user models for multi-armed ba...",
      "pdf_url": "https://arxiv.org/pdf/1912.05284v1.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2409.11911v2",
      "title": "AI vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok",
      "authors": [
        "Jiajun Wang",
        "Xiangzhe Yuan",
        "Siying Hu",
        "Zhicong Lu"
      ],
      "published": "2024-09-18T12:13:09Z",
      "categories": "",
      "summary": "With the development of generative AI technology, a vast array of AI-generated paintings (AIGP) have gone viral on social media like TikTok. However, some negative news about AIGP has also emerged. For example, in 2022, numerous painters worldwide organized a large-scale anti-AI movement because of the infringement in generative AI model training. This event reflected a social issue that, with the development and application of generative AI, public feedback and feelings towards it may have been overlooked. Therefore, to investigate public interactions and perceptions towards AIGP on social me...",
      "pdf_url": "https://arxiv.org/pdf/2409.11911v2.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2501.14035v1",
      "title": "Human-Alignment Influences the Utility of AI-assisted Decision Making",
      "authors": [
        "Nina L. Corvelo Benz",
        "Manuel Gomez Rodriguez"
      ],
      "published": "2025-01-23T19:01:47Z",
      "categories": "",
      "summary": "Whenever an AI model is used to predict a relevant (binary) outcome in AI-assisted decision making, it is widely agreed that, together with each prediction, the model should provide an AI confidence value. However, it has been unclear why decision makers have often difficulties to develop a good sense on when to trust a prediction using AI confidence values. Very recently, Corvelo Benz and Gomez Rodriguez have argued that, for rational decision makers, the utility of AI-assisted decision making is inherently bounded by the degree of alignment between the AI confidence values and the decision m...",
      "pdf_url": "https://arxiv.org/pdf/2501.14035v1.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.14537v1",
      "title": "ClearFairy: Capturing Creative Workflows through Decision Structuring, In-Situ Questioning, and Rationale Inference",
      "authors": [
        "Kihoon Son",
        "DaEun Choi",
        "Tae Soo Kim",
        "Young-Ho Kim",
        "Sangdoo Yun",
        "Juho Kim"
      ],
      "published": "2025-09-18T02:11:34Z",
      "categories": "",
      "summary": "Capturing professionals' decision-making in creative workflows is essential for reflection, collaboration, and knowledge sharing, yet existing methods often leave rationales incomplete and implicit decisions hidden. To address this, we present CLEAR framework that structures reasoning into cognitive decision steps-linked units of actions, artifacts, and self-explanations that make decisions traceable. Building on this framework, we introduce ClearFairy, a think-aloud AI assistant for UI design that detects weak explanations, asks lightweight clarifying questions, and infers missing rationales ...",
      "pdf_url": "https://arxiv.org/pdf/2509.14537v1.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2302.09688v1",
      "title": "AutoDOViz: Human-Centered Automation for Decision Optimization",
      "authors": [
        "Daniel Karl I. Weidele",
        "Shazia Afzal",
        "Abel N. Valente",
        "Cole Makuch",
        "Owen Cornec",
        "Long Vu",
        "Dharmashankar Subramanian",
        "Werner Geyer",
        "Rahul Nair",
        "Inge Vejsbjerg",
        "Radu Marinescu",
        "Paulito Palmes",
        "Elizabeth M. Daly",
        "Loraine Franke",
        "Daniel Haehn"
      ],
      "published": "2023-02-19T23:06:19Z",
      "categories": "",
      "summary": "We present AutoDOViz, an interactive user interface for automated decision optimization (AutoDO) using reinforcement learning (RL). Decision optimization (DO) has classically being practiced by dedicated DO researchers where experts need to spend long periods of time fine tuning a solution through trial-and-error. AutoML pipeline search has sought to make it easier for a data scientist to find the best machine learning pipeline by leveraging automation to search and tune the solution. More recently, these advances have been applied to the domain of AutoDO, with a similar goal to find the best ...",
      "pdf_url": "https://arxiv.org/pdf/2302.09688v1.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.02793v2",
      "title": "A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models",
      "authors": [
        "Gaurav Verma",
        "Jiawei Zhou",
        "Mohit Chandra",
        "Srijan Kumar",
        "Munmun De Choudhury"
      ],
      "published": "2025-04-03T17:40:11Z",
      "categories": "",
      "summary": "Large artificial intelligence (AI) models have garnered significant attention for their remarkable, often \"superhuman\", performance on standardized benchmarks. However, when these models are deployed in high-stakes verticals such as healthcare, education, and law, they often reveal notable limitations. For instance, they exhibit brittleness to minor variations in input data, present contextually uninformed decisions in critical settings, and undermine user trust by confidently producing or reproducing inaccuracies. These challenges in applying large models necessitate cross-disciplinary innova...",
      "pdf_url": "https://arxiv.org/pdf/2504.02793v2.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2305.05331v2",
      "title": "Explainable Recommender with Geometric Information Bottleneck",
      "authors": [
        "Hanqi Yan",
        "Lin Gui",
        "Menghan Wang",
        "Kun Zhang",
        "Yulan He"
      ],
      "published": "2023-05-09T10:38:36Z",
      "categories": "",
      "summary": "Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporat...",
      "pdf_url": "https://arxiv.org/pdf/2305.05331v2.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.03606v1",
      "title": "Demystifying Sequential Recommendations: Counterfactual Explanations via Genetic Algorithms",
      "authors": [
        "Domiziano Scarcelli",
        "Filippo Betello",
        "Giuseppe Perelli",
        "Fabrizio Silvestri",
        "Gabriele Tolomei"
      ],
      "published": "2025-08-05T16:22:45Z",
      "categories": "",
      "summary": "Sequential Recommender Systems (SRSs) have demonstrated remarkable effectiveness in capturing users' evolving preferences. However, their inherent complexity as \"black box\" models poses significant challenges for explainability. This work presents the first counterfactual explanation technique specifically developed for SRSs, introducing a novel approach in this space, addressing the key question: What minimal changes in a user's interaction history would lead to different recommendations? To achieve this, we introduce a specialized genetic algorithm tailored for discrete sequences and show th...",
      "pdf_url": "https://arxiv.org/pdf/2508.03606v1.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2205.15406v1",
      "title": "From Explanation to Recommendation: Ethical Standards for Algorithmic Recourse",
      "authors": [
        "Emily Sullivan",
        "Philippe Verreault-Julien"
      ],
      "published": "2022-05-30T20:09:42Z",
      "categories": "",
      "summary": "People are increasingly subject to algorithmic decisions, and it is generally agreed that end-users should be provided an explanation or rationale for these decisions. There are different purposes that explanations can have, such as increasing user trust in the system or allowing users to contest the decision. One specific purpose that is gaining more traction is algorithmic recourse. We first propose that recourse should be viewed as a recommendation problem, not an explanation problem. Then, we argue that the capability approach provides plausible and fruitful ethical standards for recourse....",
      "pdf_url": "https://arxiv.org/pdf/2205.15406v1.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2107.06416v2",
      "title": "Multi-Step Critiquing User Interface for Recommender Systems",
      "authors": [
        "Diana Petrescu",
        "Diego Antognini",
        "Boi Faltings"
      ],
      "published": "2021-07-13T22:19:38Z",
      "categories": "",
      "summary": "Recommendations with personalized explanations have been shown to increase user trust and perceived quality and help users make better decisions. Moreover, such explanations allow users to provide feedback by critiquing them. Several algorithms for recommender systems with multi-step critiquing have therefore been developed. However, providing a user-friendly interface based on personalized explanations and critiquing has not been addressed in the last decade. In this paper, we introduce four different web interfaces (available under https://lia.epfl.ch/critiquing/) helping users making decisi...",
      "pdf_url": "https://arxiv.org/pdf/2107.06416v2.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2204.08859v1",
      "title": "On the Influence of Explainable AI on Automation Bias",
      "authors": [
        "Max Schemmer",
        "Niklas K\u00fchl",
        "Carina Benz",
        "Gerhard Satzger"
      ],
      "published": "2022-04-19T12:54:23Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) is gaining momentum, and its importance for the future of work in many areas, such as medicine and banking, is continuously rising. However, insights on the effective collaboration of humans and AI are still rare. Typically, AI supports humans in decision-making by addressing human limitations. However, it may also evoke human bias, especially in the form of automation bias as an over-reliance on AI advice. We aim to shed light on the potential to influence automation bias by explainable AI (XAI). In this pre-test, we derive a research model and describe our study ...",
      "pdf_url": "https://arxiv.org/pdf/2204.08859v1.pdf",
      "relevance_score": 47,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.25137v2",
      "title": "The Iceberg Index: Measuring Skills-centered Exposure in the AI Economy",
      "authors": [
        "Ayush Chopra",
        "Santanu Bhattacharya",
        "DeAndrea Salvador",
        "Ayan Paul",
        "Teddy Wright",
        "Aditi Garg",
        "Feroz Ahmad",
        "Alice C. Schwarze",
        "Ramesh Raskar",
        "Prasanna Balaprakash"
      ],
      "published": "2025-10-29T03:30:45Z",
      "categories": "",
      "summary": "Artificial Intelligence is reshaping America's \\$9.4 trillion labor market, with cascading effects that extend far beyond visible technology sectors. When AI transforms quality control tasks in automotive plants, consequences spread through logistics networks, supply chains, and local service economies. Yet traditional workforce metrics cannot capture these ripple effects: they measure employment outcomes after disruption occurs, not where AI capabilities overlap with human skills before adoption crystallizes. Project Iceberg addresses this gap using Large Population Models to simulate the hum...",
      "pdf_url": "https://arxiv.org/pdf/2510.25137v2.pdf",
      "relevance_score": 46,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.13062v1",
      "title": "AI-Assisted Decision Making with Human Learning",
      "authors": [
        "Gali Noti",
        "Kate Donahue",
        "Jon Kleinberg",
        "Sigal Oren"
      ],
      "published": "2025-02-18T17:08:21Z",
      "categories": "",
      "summary": "AI systems increasingly support human decision-making. In many cases, despite the algorithm's superior performance, the final decision remains in human hands. For example, an AI may assist doctors in determining which diagnostic tests to run, but the doctor ultimately makes the diagnosis. This paper studies such AI-assisted decision-making settings, where the human learns through repeated interactions with the algorithm. In our framework, the algorithm -- designed to maximize decision accuracy according to its own model -- determines which features the human can consider. The human then makes ...",
      "pdf_url": "https://arxiv.org/pdf/2502.13062v1.pdf",
      "relevance_score": 45,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1810.12644v4",
      "title": "The Responsibility Quantification (ResQu) Model of Human Interaction with Automation",
      "authors": [
        "Nir Douer",
        "Joachim Meyer"
      ],
      "published": "2018-10-30T10:45:57Z",
      "categories": "",
      "summary": "Intelligent systems and advanced automation are involved in information collection and evaluation, in decision-making and in the implementation of chosen actions. In such systems, human responsibility becomes equivocal. Understanding human casual responsibility is particularly important when intelligent autonomous systems can harm people, as with autonomous vehicles or, most notably, with autonomous weapon systems (AWS). Using Information Theory, we develop a responsibility quantification (ResQu) model of human involvement in intelligent automated systems and demonstrate its applications on de...",
      "pdf_url": "https://arxiv.org/pdf/1810.12644v4.pdf",
      "relevance_score": 45,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.08424v1",
      "title": "When Medical AI Explanations Help and When They Harm",
      "authors": [
        "Manshu Khanna",
        "Ziyi Wang",
        "Lijia Wei",
        "Lian Xue"
      ],
      "published": "2025-12-09T09:50:39Z",
      "categories": "",
      "summary": "We document a fundamental paradox in AI transparency: explanations improve decisions when algorithms are correct but systematically worsen them when algorithms err. In an experiment with 257 medical students making 3,855 diagnostic decisions, we find explanations increase accuracy by 6.3 percentage points when AI is correct (73% of cases) but decrease it by 4.9 points when incorrect (27% of cases). This asymmetry arises because modern AI systems generate equally persuasive explanations regardless of recommendation quality-physicians cannot distinguish helpful from misleading guidance. We show ...",
      "pdf_url": "https://arxiv.org/pdf/2512.08424v1.pdf",
      "relevance_score": 45,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2302.09079v1",
      "title": "Advances in Automatically Rating the Trustworthiness of Text Processing Services",
      "authors": [
        "Biplav Srivastava",
        "Kausik Lakkaraju",
        "Mariana Bernagozzi",
        "Marco Valtorta"
      ],
      "published": "2023-02-04T14:27:46Z",
      "categories": "",
      "summary": "AI services are known to have unstable behavior when subjected to changes in data, models or users. Such behaviors, whether triggered by omission or commission, lead to trust issues when AI works with humans. The current approach of assessing AI services in a black box setting, where the consumer does not have access to the AI's source code or training data, is limited. The consumer has to rely on the AI developer's documentation and trust that the system has been built as stated. Further, if the AI consumer reuses the service to build other services which they sell to their customers, the con...",
      "pdf_url": "https://arxiv.org/pdf/2302.09079v1.pdf",
      "relevance_score": 44,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2107.06641v3",
      "title": "Trustworthy AI: A Computational Perspective",
      "authors": [
        "Haochen Liu",
        "Yiqi Wang",
        "Wenqi Fan",
        "Xiaorui Liu",
        "Yaxin Li",
        "Shaili Jain",
        "Yunhao Liu",
        "Anil K. Jain",
        "Jiliang Tang"
      ],
      "published": "2021-07-12T14:21:46Z",
      "categories": "",
      "summary": "In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone's daily life and profoundly altering the course of human society. The intention of developing AI is to benefit humans, by reducing human labor, bringing everyday convenience to human lives, and promoting social good. However, recent research and AI applications show that AI can cause unintentional harm to humans, such as making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against one group. Thus, trustworthy AI has a...",
      "pdf_url": "https://arxiv.org/pdf/2107.06641v3.pdf",
      "relevance_score": 44,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2410.01809v1",
      "title": "Enhancing transparency in AI-powered customer engagement",
      "authors": [
        "Tara DeZao"
      ],
      "published": "2024-09-13T20:26:11Z",
      "categories": "",
      "summary": "This paper addresses the critical challenge of building consumer trust in AI-powered customer engagement by emphasising the necessity for transparency and accountability. Despite the potential of AI to revolutionise business operations and enhance customer experiences, widespread concerns about misinformation and the opacity of AI decision-making processes hinder trust. Surveys highlight a significant lack of awareness among consumers regarding their interactions with AI, alongside apprehensions about bias and fairness in AI algorithms. The paper advocates for the development of explainable AI...",
      "pdf_url": "https://arxiv.org/pdf/2410.01809v1.pdf",
      "relevance_score": 44,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.19942v1",
      "title": "Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations",
      "authors": [
        "Martin Benderoth",
        "Patrick Gebhard",
        "Christian Keller",
        "C. Benjamin Nakhosteen",
        "Stefan Schaffer",
        "Tanja Schneeberger"
      ],
      "published": "2025-08-27T14:59:32Z",
      "categories": "",
      "summary": "This paper introduces a novel approach to tackle the challenges of preserving and transferring tacit knowledge--deep, experience-based insights that are hard to articulate but vital for decision-making, innovation, and problem-solving. Traditional methods rely heavily on human facilitators, which, while effective, are resource-intensive and lack scalability. A promising alternative is the use of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer facilitators. These agents interact autonomously and socially intelligently with users through multimodal behaviors (verbal, paraverba...",
      "pdf_url": "https://arxiv.org/pdf/2508.19942v1.pdf",
      "relevance_score": 44,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2307.06081v2",
      "title": "Navigating the Complexity of Generative AI Adoption in Software Engineering",
      "authors": [
        "Daniel Russo"
      ],
      "published": "2023-07-12T11:05:19Z",
      "categories": "",
      "summary": "In this paper, the adoption patterns of Generative Artificial Intelligence (AI) tools within software engineering are investigated. Influencing factors at the individual, technological, and societal levels are analyzed using a mixed-methods approach for an extensive comprehension of AI adoption. An initial structured interview was conducted with 100 software engineers, employing the Technology Acceptance Model (TAM), the Diffusion of Innovations theory (DOI), and the Social Cognitive Theory (SCT) as guiding theories. A theoretical model named the Human-AI Collaboration and Adaptation Framework...",
      "pdf_url": "https://arxiv.org/pdf/2307.06081v2.pdf",
      "relevance_score": 43,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.20273v1",
      "title": "Ten Principles of AI Agent Economics",
      "authors": [
        "Ke Yang",
        "ChengXiang Zhai"
      ],
      "published": "2025-05-26T17:52:44Z",
      "categories": "",
      "summary": "The rapid rise of AI-based autonomous agents is transforming human society and economic systems, as these entities increasingly exhibit human-like or superhuman intelligence. From excelling at complex games like Go to tackling diverse general-purpose tasks with large language and multimodal models, AI agents are evolving from specialized tools into dynamic participants in social and economic ecosystems. Their autonomy and decision-making capabilities are poised to impact industries, professions, and human lives profoundly, raising critical questions about their integration into economic activi...",
      "pdf_url": "https://arxiv.org/pdf/2505.20273v1.pdf",
      "relevance_score": 42,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2506.16199v1",
      "title": "Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)",
      "authors": [
        "Mohammad Naiseh",
        "Huseyin Dogan",
        "Stephen Giff",
        "Nan Jiang"
      ],
      "published": "2025-06-19T10:26:58Z",
      "categories": "",
      "summary": "Explainable Artificial Intelligence (XAI) plays a critical role in fostering user trust and understanding in AI-driven systems. However, the design of effective XAI interfaces presents significant challenges, particularly for UX professionals who may lack technical expertise in AI or machine learning. Existing explanation methods, such as SHAP, LIME, and counterfactual explanations, often rely on complex technical language and assumptions that are difficult for non-expert users to interpret. To address these gaps, we propose a UX Research (UXR) Playbook for XAI - a practical framework aimed at...",
      "pdf_url": "https://arxiv.org/pdf/2506.16199v1.pdf",
      "relevance_score": 42,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2005.13275v1",
      "title": "Who is this Explanation for? Human Intelligence and Knowledge Graphs for eXplainable AI",
      "authors": [
        "Irene Celino"
      ],
      "published": "2020-05-27T10:47:15Z",
      "categories": "",
      "summary": "eXplainable AI focuses on generating explanations for the output of an AI algorithm to a user, usually a decision-maker. Such user needs to interpret the AI system in order to decide whether to trust the machine outcome. When addressing this challenge, therefore, proper attention should be given to produce explanations that are interpretable by the target community of users. In this chapter, we claim for the need to better investigate what constitutes a human explanation, i.e. a justification of the machine behaviour that is interpretable and actionable by the human decision makers. In particu...",
      "pdf_url": "https://arxiv.org/pdf/2005.13275v1.pdf",
      "relevance_score": 40,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1708.05122v1",
      "title": "Evaluating Visual Conversational Agents via Cooperative Human-AI Games",
      "authors": [
        "Prithvijit Chattopadhyay",
        "Deshraj Yadav",
        "Viraj Prabhu",
        "Arjun Chandrasekaran",
        "Abhishek Das",
        "Stefan Lee",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "published": "2017-08-17T03:27:53Z",
      "categories": "",
      "summary": "As AI continues to advance, human-AI teams are inevitable. However, progress in AI is routinely measured in isolation, without a human in the loop. It is crucial to benchmark progress in AI, not just in isolation, but also in terms of how it translates to helping humans perform certain tasks, i.e., the performance of human-AI teams.   In this work, we design a cooperative game - GuessWhich - to measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the AI. The AI, which we call ALICE, i...",
      "pdf_url": "https://arxiv.org/pdf/1708.05122v1.pdf",
      "relevance_score": 40,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2407.09512v1",
      "title": "Design and evaluation of AI copilots -- case studies of retail copilot templates",
      "authors": [
        "Michal Furmakiewicz",
        "Chang Liu",
        "Angus Taylor",
        "Ilya Venger"
      ],
      "published": "2024-06-17T17:31:33Z",
      "categories": "",
      "summary": "Building a successful AI copilot requires a systematic approach. This paper is divided into two sections, covering the design and evaluation of a copilot respectively. A case study of developing copilot templates for the retail domain by Microsoft is used to illustrate the role and importance of each aspect. The first section explores the key technical components of a copilot's architecture, including the LLM, plugins for knowledge retrieval and actions, orchestration, system prompts, and responsible AI guardrails. The second section discusses testing and evaluation as a principled way to prom...",
      "pdf_url": "https://arxiv.org/pdf/2407.09512v1.pdf",
      "relevance_score": 40,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.15769v1",
      "title": "Preliminary Quantitative Study on Explainability and Trust in AI Systems",
      "authors": [
        "Allen Daniel Sunny"
      ],
      "published": "2025-10-17T15:59:28Z",
      "categories": "",
      "summary": "Large-scale AI models such as GPT-4 have accelerated the deployment of artificial intelligence across critical domains including law, healthcare, and finance, raising urgent questions about trust and transparency. This study investigates the relationship between explainability and user trust in AI systems through a quantitative experimental design. Using an interactive, web-based loan approval simulation, we compare how different types of explanations, ranging from basic feature importance to interactive counterfactuals influence perceived trust. Results suggest that interactivity enhances bot...",
      "pdf_url": "https://arxiv.org/pdf/2510.15769v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2307.04386v1",
      "title": "Counterfactual Explanation for Fairness in Recommendation",
      "authors": [
        "Xiangmeng Wang",
        "Qian Li",
        "Dianer Yu",
        "Qing Li",
        "Guandong Xu"
      ],
      "published": "2023-07-10T07:45:06Z",
      "categories": "",
      "summary": "Fairness-aware recommendation eliminates discrimination issues to build trustworthy recommendation systems.Explaining the causes of unfair recommendations is critical, as it promotes fairness diagnostics, and thus secures users' trust in recommendation models. Existing fairness explanation methods suffer high computation burdens due to the large-scale search space and the greedy nature of the explanation search process. Besides, they perform score-based optimizations with continuous values, which are not applicable to discrete attributes such as gender and race. In this work, we adopt the nove...",
      "pdf_url": "https://arxiv.org/pdf/2307.04386v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.24366v1",
      "title": "On the Factual Consistency of Text-based Explainable Recommendation Models",
      "authors": [
        "Ben Kabongo",
        "Vincent Guigue"
      ],
      "published": "2025-12-30T17:25:15Z",
      "categories": "",
      "summary": "Text-based explainable recommendation aims to generate natural-language explanations that justify item recommendations, to improve user trust and system transparency. Although recent advances leverage LLMs to produce fluent outputs, a critical question remains underexplored: are these explanations factually consistent with the available evidence? We introduce a comprehensive framework for evaluating the factual consistency of text-based explainable recommenders. We design a prompting-based pipeline that uses LLMs to extract atomic explanatory statements from reviews, thereby constructing a gro...",
      "pdf_url": "https://arxiv.org/pdf/2512.24366v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2101.04251v2",
      "title": "Vis Ex Machina: An Analysis of Trust in Human versus Algorithmically Generated Visualization Recommendations",
      "authors": [
        "Rachael Zehrung",
        "Astha Singhal",
        "Michael Correll",
        "Leilani Battle"
      ],
      "published": "2021-01-12T01:06:39Z",
      "categories": "",
      "summary": "More visualization systems are simplifying the data analysis process by automatically suggesting relevant visualizations. However, little work has been done to understand if users trust these automated recommendations. In this paper, we present the results of a crowd-sourced study exploring preferences and perceived quality of recommendations that have been positioned as either human-curated or algorithmically generated. We observe that while participants initially prefer human recommenders, their actions suggest an indifference for recommendation source when evaluating visualization recommend...",
      "pdf_url": "https://arxiv.org/pdf/2101.04251v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.21293v2",
      "title": "Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles",
      "authors": [
        "Siddharth Mehrotra",
        "Jin Huang",
        "Xuelong Fu",
        "Roel Dobbe",
        "Clara I. S\u00e1nchez",
        "Maarten de Rijke"
      ],
      "published": "2025-10-24T09:40:38Z",
      "categories": "",
      "summary": "Background: Trustworthy AI serves as a foundational pillar for two major AI ethics conferences: AIES and FAccT. However, current research often adopts techno-centric approaches, focusing primarily on technical attributes such as reliability, robustness, and fairness, while overlooking the sociotechnical dimensions critical to understanding AI trustworthiness in real-world contexts.   Objectives: This scoping review aims to examine how the AIES and FAccT communities conceptualize, measure, and validate AI trustworthiness, identifying major gaps and opportunities for advancing a holistic underst...",
      "pdf_url": "https://arxiv.org/pdf/2510.21293v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.15097v2",
      "title": "MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm",
      "authors": [
        "Vineeth Sai Narajala",
        "Manish Bhatt",
        "Idan Habler",
        "Ronald F. Del Rosario",
        "Ads Dawson"
      ],
      "published": "2025-11-19T04:10:32Z",
      "categories": "",
      "summary": "The AI trustworthiness crisis threatens to derail the artificial intelligence revolution, with regulatory barriers, security vulnerabilities, and accountability gaps preventing deployment in critical domains. Current AI systems operate on opaque data structures that lack the audit trails, provenance tracking, or explainability required by emerging regulations like the EU AI Act. We propose an artifact-centric AI agent paradigm where behavior is driven by persistent, verifiable data artifacts rather than ephemeral tasks, solving the trustworthiness problem at the data architecture level. Centra...",
      "pdf_url": "https://arxiv.org/pdf/2511.15097v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2307.04677v1",
      "title": "Practical Trustworthiness Model for DNN in Dedicated 6G Application",
      "authors": [
        "Anouar Nechi",
        "Ahmed Mahmoudi",
        "Christoph Herold",
        "Daniel Widmer",
        "Thomas K\u00fcrner",
        "Mladen Berekovic",
        "Saleh Mulhem"
      ],
      "published": "2023-07-10T16:21:35Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) is considered an efficient response to several challenges facing 6G technology. However, AI still suffers from a huge trust issue due to its ambiguous way of making predictions. Therefore, there is a need for a method to evaluate the AI's trustworthiness in practice for future 6G applications. This paper presents a practical model to analyze the trustworthiness of AI in a dedicated 6G application. In particular, we present two customized Deep Neural Networks (DNNs) to solve the Automatic Modulation Recognition (AMR) problem in Terahertz communications-based 6G tech...",
      "pdf_url": "https://arxiv.org/pdf/2307.04677v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2412.16369v2",
      "title": "Navigating AI to Unpack Youth Privacy Concerns: An In-Depth Exploration and Systematic Review",
      "authors": [
        "Ajay Kumar Shrestha",
        "Ankur Barthwal",
        "Molly Campbell",
        "Austin Shouli",
        "Saad Syed",
        "Sandhya Joshi",
        "Julita Vassileva"
      ],
      "published": "2024-12-20T22:00:06Z",
      "categories": "",
      "summary": "This systematic literature review investigates perceptions, concerns, and expectations of young digital citizens regarding privacy in artificial intelligence (AI) systems, focusing on social media platforms, educational technology, gaming systems, and recommendation algorithms. Using a rigorous methodology, the review started with 2,000 papers, narrowed down to 552 after initial screening, and finally refined to 108 for detailed analysis. Data extraction focused on privacy concerns, data-sharing practices, the balance between privacy and utility, trust factors in AI, transparency expectations,...",
      "pdf_url": "https://arxiv.org/pdf/2412.16369v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2206.00474v1",
      "title": "Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness",
      "authors": [
        "Yuri Nakao",
        "Lorenzo Strappelli",
        "Simone Stumpf",
        "Aisha Naseer",
        "Daniele Regoli",
        "Giulia Del Gamba"
      ],
      "published": "2022-06-01T13:08:37Z",
      "categories": "",
      "summary": "With Artificial intelligence (AI) to aid or automate decision-making advancing rapidly, a particular concern is its fairness. In order to create reliable, safe and trustworthy systems through human-centred artificial intelligence (HCAI) design, recent efforts have produced user interfaces (UIs) for AI experts to investigate the fairness of AI models. In this work, we provide a design space exploration that supports not only data scientists but also domain experts to investigate AI fairness. Using loan applications as an example, we held a series of workshops with loan officers and data scienti...",
      "pdf_url": "https://arxiv.org/pdf/2206.00474v1.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.19443v1",
      "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI",
      "authors": [
        "Ranjan Sapkota",
        "Konstantinos I. Roumeliotis",
        "Manoj Karkee"
      ],
      "published": "2025-05-26T03:00:21Z",
      "categories": "",
      "summary": "This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and ...",
      "pdf_url": "https://arxiv.org/pdf/2505.19443v1.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.12931v1",
      "title": "Explainable AI in Usable Privacy and Security: Challenges and Opportunities",
      "authors": [
        "Vincent Freiberger",
        "Arthur Fleig",
        "Erik Buchmann"
      ],
      "published": "2025-04-17T13:28:01Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) are increasingly being used for automated evaluations and explaining them. However, concerns about explanation quality, consistency, and hallucinations remain open research challenges, particularly in high-stakes contexts like privacy and security, where user trust and decision-making are at stake. In this paper, we investigate these issues in the context of PRISMe, an interactive privacy policy assessment tool that leverages LLMs to evaluate and explain website privacy policies. Based on a prior user study with 22 participants, we identify key concerns regarding L...",
      "pdf_url": "https://arxiv.org/pdf/2504.12931v1.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.08051v1",
      "title": "Counterfactual Language Reasoning for Explainable Recommendation Systems",
      "authors": [
        "Guanrong Li",
        "Haolin Yang",
        "Xinyu Liu",
        "Zhen Wu",
        "Xinyu Dai"
      ],
      "published": "2025-03-11T05:15:37Z",
      "categories": "",
      "summary": "Explainable recommendation systems leverage transparent reasoning to foster user trust and improve decision-making processes. Current approaches typically decouple recommendation generation from explanation creation, violating causal precedence principles where explanatory factors should logically precede outcomes. This paper introduces a novel framework integrating structural causal models with large language models to establish causal consistency in recommendation pipelines. Our methodology enforces explanation factors as causal antecedents to recommendation predictions through causal graph ...",
      "pdf_url": "https://arxiv.org/pdf/2503.08051v1.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1812.04608v2",
      "title": "Metrics for Explainable AI: Challenges and Prospects",
      "authors": [
        "Robert R. Hoffman",
        "Shane T. Mueller",
        "Gary Klein",
        "Jordan Litman"
      ],
      "published": "2018-12-11T18:50:02Z",
      "categories": "",
      "summary": "The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's tr...",
      "pdf_url": "https://arxiv.org/pdf/1812.04608v2.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2010.04990v2",
      "title": "The emergence of Explainability of Intelligent Systems: Delivering Explainable and Personalised Recommendations for Energy Efficiency",
      "authors": [
        "Christos Sardianos",
        "Iraklis Varlamis",
        "Christos Chronis",
        "George Dimitrakopoulos",
        "Abdullah Alsalemi",
        "Yassine Himeur",
        "Faycal Bensaali",
        "Abbes Amira"
      ],
      "published": "2020-10-10T13:11:43Z",
      "categories": "",
      "summary": "The recent advances in artificial intelligence namely in machine learning and deep learning, have boosted the performance of intelligent systems in several ways. This gave rise to human expectations, but also created the need for a deeper understanding of how intelligent systems think and decide. The concept of explainability appeared, in the extent of explaining the internal system mechanics in human terms. Recommendation systems are intelligent systems that support human decision making, and as such, they have to be explainable in order to increase user trust and improve the acceptance of re...",
      "pdf_url": "https://arxiv.org/pdf/2010.04990v2.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2009.09945v4",
      "title": "Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue",
      "authors": [
        "Wenjie Wang",
        "Fuli Feng",
        "Xiangnan He",
        "Hanwang Zhang",
        "Tat-Seng Chua"
      ],
      "published": "2020-09-21T15:08:10Z",
      "categories": "",
      "summary": "Recommendation is a prevalent and critical service in information systems. To provide personalized suggestions to users, industry players embrace machine learning, more specifically, building predictive models based on the click behavior data. This is known as the Click-Through Rate (CTR) prediction, which has become the gold standard for building personalized recommendation service. However, we argue that there is a significant gap between clicks and user satisfaction -- it is common that a user is \"cheated\" to click an item by the attractive title/cover of the item. This will severely hurt u...",
      "pdf_url": "https://arxiv.org/pdf/2009.09945v4.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2404.19093v1",
      "title": "Large Language Models as Conversational Movie Recommenders: A User Study",
      "authors": [
        "Ruixuan Sun",
        "Xinyi Li",
        "Avinash Akella",
        "Joseph A. Konstan"
      ],
      "published": "2024-04-29T20:17:06Z",
      "categories": "",
      "summary": "This paper explores the effectiveness of using large language models (LLMs) for personalized movie recommendations from users' perspectives in an online field experiment. Our study involves a combination of between-subject prompt and historic consumption assessments, along with within-subject recommendation scenario evaluations. By examining conversation and survey response data from 160 active users, we find that LLMs offer strong recommendation explainability but lack overall personalization, diversity, and user trust. Our results also indicate that different personalized prompting technique...",
      "pdf_url": "https://arxiv.org/pdf/2404.19093v1.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2001.00062v1",
      "title": "Visual Evaluation of Generative Adversarial Networks for Time Series Data",
      "authors": [
        "Hiba Arnout",
        "Johannes Kehrer",
        "Johanna Bronner",
        "Thomas Runkler"
      ],
      "published": "2019-12-23T13:59:33Z",
      "categories": "",
      "summary": "A crucial factor to trust Machine Learning (ML) algorithm decisions is a good representation of its application field by the training dataset. This is particularly true when parts of the training data have been artificially generated to overcome common training problems such as lack of data or imbalanced dataset. Over the last few years, Generative Adversarial Networks (GANs) have shown remarkable results in generating realistic data. However, this ML approach lacks an objective function to evaluate the quality of the generated data. Numerous GAN applications focus on generating image data mos...",
      "pdf_url": "https://arxiv.org/pdf/2001.00062v1.pdf",
      "relevance_score": 37,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.15647v2",
      "title": "Promoting Real-Time Reflection in Synchronous Communication with Generative AI",
      "authors": [
        "Yi Wen",
        "Meng Xia"
      ],
      "published": "2025-04-22T07:12:56Z",
      "categories": "",
      "summary": "Real-time reflection plays a vital role in synchronous communication. It enables users to adjust their communication strategies dynamically, thereby improving the effectiveness of their communication. Generative AI holds significant potential to enhance real-time reflection due to its ability to comprehensively understand the current context and generate personalized and nuanced content. However, it is challenging to design the way of interaction and information presentation to support the real-time workflow rather than disrupt it. In this position paper, we present a review of existing resear...",
      "pdf_url": "https://arxiv.org/pdf/2504.15647v2.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2304.06701v3",
      "title": "Learning Personalized Decision Support Policies",
      "authors": [
        "Umang Bhatt",
        "Valerie Chen",
        "Katherine M. Collins",
        "Parameswaran Kamalaruban",
        "Emma Kallina",
        "Adrian Weller",
        "Ameet Talwalkar"
      ],
      "published": "2023-04-13T17:53:34Z",
      "categories": "",
      "summary": "Individual human decision-makers may benefit from different forms of support to improve decision outcomes, but when each form of support will yield better outcomes? In this work, we posit that personalizing access to decision support tools can be an effective mechanism for instantiating the appropriate use of AI assistance. Specifically, we propose the general problem of learning a decision support policy that, for a given input, chooses which form of support to provide to decision-makers for whom we initially have no prior information. We develop $\\texttt{Modiste}$, an interactive tool to lea...",
      "pdf_url": "https://arxiv.org/pdf/2304.06701v3.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2410.23423v1",
      "title": "Dynamic Information Sub-Selection for Decision Support",
      "authors": [
        "Hung-Tien Huang",
        "Maxwell Lennon",
        "Shreyas Bhat Brahmavar",
        "Sean Sylvia",
        "Junier B. Oliva"
      ],
      "published": "2024-10-30T20:00:54Z",
      "categories": "",
      "summary": "We introduce Dynamic Information Sub-Selection (DISS), a novel framework of AI assistance designed to enhance the performance of black-box decision-makers by tailoring their information processing on a per-instance basis. Blackbox decision-makers (e.g., humans or real-time systems) often face challenges in processing all possible information at hand (e.g., due to cognitive biases or resource constraints), which can degrade decision efficacy. DISS addresses these challenges through policies that dynamically select the most effective features and options to forward to the black-box decision-make...",
      "pdf_url": "https://arxiv.org/pdf/2410.23423v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.12552v1",
      "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms",
      "authors": [
        "Jifei Liu",
        "Zhi Chen",
        "Yuanguang Zhong"
      ],
      "published": "2025-12-14T04:51:53Z",
      "categories": "",
      "summary": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experi...",
      "pdf_url": "https://arxiv.org/pdf/2512.12552v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2401.05840v1",
      "title": "Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making",
      "authors": [
        "Zhuoyan Li",
        "Zhuoran Lu",
        "Ming Yin"
      ],
      "published": "2024-01-11T11:22:36Z",
      "categories": "",
      "summary": "With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes. To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process. Meanwhile, methods that prior...",
      "pdf_url": "https://arxiv.org/pdf/2401.05840v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.22365v2",
      "title": "Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making",
      "authors": [
        "ZhaoBin Li",
        "Mark Steyvers"
      ],
      "published": "2025-07-30T04:05:50Z",
      "categories": "",
      "summary": "In settings where human decision-making relies on AI input, both the predictive accuracy of the AI system and the reliability of its confidence estimates influence decision quality. We highlight the role of AI metacognitive sensitivity -- its ability to assign confidence scores that accurately distinguish correct from incorrect predictions -- and introduce a theoretical framework for assessing the joint impact of AI's predictive accuracy and metacognitive sensitivity in hybrid decision-making settings. Our analysis identifies conditions under which an AI with lower predictive accuracy but high...",
      "pdf_url": "https://arxiv.org/pdf/2507.22365v2.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.00940v1",
      "title": "Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions",
      "authors": [
        "Vijay Keswani",
        "Vincent Conitzer",
        "Walter Sinnott-Armstrong",
        "Breanna K. Nguyen",
        "Hoda Heidari",
        "Jana Schaich Borg"
      ],
      "published": "2025-03-02T15:42:17Z",
      "categories": "",
      "summary": "A growing body of work in Ethical AI attempts to capture human moral judgments through simple computational models. The key question we address in this work is whether such simple AI models capture {the critical} nuances of moral decision-making by focusing on the use case of kidney allocation. We conducted twenty interviews where participants explained their rationale for their judgments about who should receive a kidney. We observe participants: (a) value patients' morally-relevant attributes to different degrees; (b) use diverse decision-making processes, citing heuristics to reduce decisio...",
      "pdf_url": "https://arxiv.org/pdf/2503.00940v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.11984v1",
      "title": "Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering",
      "authors": [
        "Alireza Joonbakhsh",
        "Alireza Rostami",
        "AmirMohammad Kamalinia",
        "Ali Nazeri",
        "Farshad Khunjush",
        "Bedir Tekinerdogan",
        "Siamak Farshidi"
      ],
      "published": "2025-12-12T19:08:04Z",
      "categories": "",
      "summary": "The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.   This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirement...",
      "pdf_url": "https://arxiv.org/pdf/2512.11984v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.03207v1",
      "title": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted Decision-Making",
      "authors": [
        "Zelun Tony Zhang",
        "Leon Reicherts"
      ],
      "published": "2025-04-04T06:40:03Z",
      "categories": "",
      "summary": "How can we use generative AI to design tools that augment rather than replace human cognition? In this position paper, we review our own research on AI-assisted decision-making for lessons to learn. We observe that in both AI-assisted decision-making and generative AI, a popular approach is to suggest AI-generated end-to-end solutions to users, which users can then accept, reject, or edit. Alternatively, AI tools could offer more incremental support to help users solve tasks themselves, which we call process-oriented support. We describe findings on the challenges of end-to-end solutions, and ...",
      "pdf_url": "https://arxiv.org/pdf/2504.03207v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.14219v1",
      "title": "Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks",
      "authors": [
        "Jiangen He",
        "Jiqun Liu"
      ],
      "published": "2025-02-20T03:15:54Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) are increasingly used in decision-making, yet their susceptibility to cognitive biases remains a pressing challenge. This study explores how personality traits influence these biases and evaluates the effectiveness of mitigation strategies across various model architectures. Our findings identify six prevalent cognitive biases, while the sunk cost and group attribution biases exhibit minimal impact. Personality traits play a crucial role in either amplifying or reducing biases, significantly affecting how LLMs respond to debiasing techniques. Notably, Conscientious...",
      "pdf_url": "https://arxiv.org/pdf/2502.14219v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.20733v2",
      "title": "E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing",
      "authors": [
        "Cheonsu Jeong",
        "Seongmin Sim",
        "Hyoyoung Cho",
        "Sungsu Kim",
        "Byounggwan Shin"
      ],
      "published": "2025-05-27T05:21:08Z",
      "categories": "",
      "summary": "This paper presents an intelligent work automation approach in the context of contemporary digital transformation by integrating generative AI and Intelligent Document Processing (IDP) technologies with an Automation Agent to realize End-to-End (E2E) automation of corporate financial expense processing tasks. While traditional Robotic Process Automation (RPA) has proven effective for repetitive, rule-based simple task automation, it faces limitations in handling unstructured data, exception management, and complex decision-making. This study designs and implements a four-stage integrated proce...",
      "pdf_url": "https://arxiv.org/pdf/2505.20733v2.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.02898v2",
      "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
      "authors": [
        "Lele Cao"
      ],
      "published": "2025-04-02T23:27:55Z",
      "categories": "",
      "summary": "Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline ...",
      "pdf_url": "https://arxiv.org/pdf/2504.02898v2.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2406.10942v4",
      "title": "Effective Generative AI: The Human-Algorithm Centaur",
      "authors": [
        "Soroush Saghafian",
        "Lihi Idan"
      ],
      "published": "2024-06-16T13:44:41Z",
      "categories": "",
      "summary": "Advanced analytics science methods have enabled combining the power of artificial and human intelligence, creating \\textit{centaurs} that allow superior decision-making. Centaurs are hybrid human-algorithm models that combine both formal analytics and human intuition in a symbiotic manner within their learning and reasoning process. We argue that the future of AI development and use in many domains needs to focus more on centaurs as opposed to other AI approaches. This paradigm shift towards centaur-based AI methods raises some fundamental questions: How are centaurs different from other human...",
      "pdf_url": "https://arxiv.org/pdf/2406.10942v4.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.05360v1",
      "title": "Building Effective Safety Guardrails in AI Education Tools",
      "authors": [
        "Hannah-Beth Clark",
        "Laura Benton",
        "Emma Searle",
        "Margaux Dowland",
        "Matthew Gregory",
        "Will Gayne",
        "John Roberts"
      ],
      "published": "2025-08-07T13:09:47Z",
      "categories": "",
      "summary": "There has been rapid development in generative AI tools across the education sector, which in turn is leading to increased adoption by teachers. However, this raises concerns regarding the safety and age-appropriateness of the AI-generated content that is being created for use in classrooms. This paper explores Oak National Academy's approach to addressing these concerns within the development of the UK Government's first publicly available generative AI tool - our AI-powered lesson planning assistant (Aila). Aila is intended to support teachers planning national curriculum-aligned lessons tha...",
      "pdf_url": "https://arxiv.org/pdf/2508.05360v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1909.06713v1",
      "title": "Human self-determination within algorithmic sociotechnical systems",
      "authors": [
        "Bogdana Rakova",
        "Rumman Chowdhury"
      ],
      "published": "2019-09-15T02:19:23Z",
      "categories": "",
      "summary": "In order to investigate the protection of human self-determination within algorithmic sociotechnical systems, we study the relationships between the concepts of mutability, bias, feedback loops, and power dynamics. We focus on the interactions between people and algorithmic systems in the case of Recommender Systems (RS) and provide novel theoretical analysis informed by human-in-the-loop system design and Supervisory Control, in order to question the dynamics in our interactions with RSs. We explore what meaningful reliability monitoring means in the context of RSs and elaborate on the need f...",
      "pdf_url": "https://arxiv.org/pdf/1909.06713v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2310.07534v2",
      "title": "Human-Centered Evaluation of XAI Methods",
      "authors": [
        "Karam Dawoud",
        "Wojciech Samek",
        "Peter Eisert",
        "Sebastian Lapuschkin",
        "Sebastian Bosse"
      ],
      "published": "2023-10-11T14:39:12Z",
      "categories": "",
      "summary": "In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called \"black boxes\" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salie...",
      "pdf_url": "https://arxiv.org/pdf/2310.07534v2.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2409.14570v1",
      "title": "Generative artificial intelligence usage by researchers at work: Effects of gender, career stage, type of workplace, and perceived barriers",
      "authors": [
        "Pablo Dorta-Gonz\u00e1lez",
        "Alexis Jorge L\u00f3pez-Puig",
        "Mar\u00eda Isabel Dorta-Gonz\u00e1lez",
        "Sara M. Gonz\u00e1lez-Betancor"
      ],
      "published": "2024-08-31T22:00:21Z",
      "categories": "",
      "summary": "The integration of generative artificial intelligence technology into research environments has become increasingly common in recent years, representing a significant shift in the way researchers approach their work. This paper seeks to explore the factors underlying the frequency of use of generative AI amongst researchers in their professional environments. As survey data may be influenced by a bias towards scientists interested in AI, potentially skewing the results towards the perspectives of these researchers, this study uses a regression model to isolate the impact of specific factors su...",
      "pdf_url": "https://arxiv.org/pdf/2409.14570v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2207.12515v2",
      "title": "A Survey on Trustworthy Recommender Systems",
      "authors": [
        "Yingqiang Ge",
        "Shuchang Liu",
        "Zuohui Fu",
        "Juntao Tan",
        "Zelong Li",
        "Shuyuan Xu",
        "Yunqi Li",
        "Yikun Xian",
        "Yongfeng Zhang"
      ],
      "published": "2022-07-25T20:23:25Z",
      "categories": "",
      "summary": "Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user's private data for personalization, just to name a few. All of these create an urgent need for Trus...",
      "pdf_url": "https://arxiv.org/pdf/2207.12515v2.pdf",
      "relevance_score": 34,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.27272v1",
      "title": "Inferring trust in recommendation systems from brain, behavioural, and physiological data",
      "authors": [
        "Vincent K. M. Cheung",
        "Pei-Cheng Shih",
        "Masato Hirano",
        "Masataka Goto",
        "Shinichi Furuya"
      ],
      "published": "2025-10-31T08:28:24Z",
      "categories": "",
      "summary": "As people nowadays increasingly rely on artificial intelligence (AI) to curate information and make decisions, assigning the appropriate amount of trust in automated intelligent systems has become ever more important. However, current measurements of trust in automation still largely rely on self-reports that are subjective and disruptive to the user. Here, we take music recommendation as a model to investigate the neural and cognitive processes underlying trust in automation. We observed that system accuracy was directly related to users' trust and modulated the influence of recommendation cu...",
      "pdf_url": "https://arxiv.org/pdf/2510.27272v1.pdf",
      "relevance_score": 34,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.00103v4",
      "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app",
      "authors": [
        "Guilherme Guerino",
        "Luiz Rodrigues",
        "Luana Bianchini",
        "Mariana Alves",
        "Marcelo Marinho",
        "Thomaz Veloso",
        "Valmir Macario",
        "Diego Dermeval",
        "Thales Vieira",
        "Ig Bittencourt",
        "Seiji Isotani"
      ],
      "published": "2025-07-31T18:56:01Z",
      "categories": "",
      "summary": "This study explores the integration of Augmented Intelligence (AuI) in Intelligent Tutoring Systems (ITS) to address challenges in Artificial Intelligence in Education (AIED), including teacher involvement, AI reliability, and resource accessibility. We present MathAIde, an ITS that uses computer vision and AI to correct mathematics exercises from student work photos and provide feedback. The system was designed through a collaborative process involving brainstorming with teachers, high-fidelity prototyping, A/B testing, and a real-world case study. Findings emphasize the importance of a teach...",
      "pdf_url": "https://arxiv.org/pdf/2508.00103v4.pdf",
      "relevance_score": 33,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.12107v1",
      "title": "Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice",
      "authors": [
        "Si Chen",
        "Isabel R. Molnar",
        "Peiyu Li",
        "Adam Acunin",
        "Ting Hua",
        "Alex Ambrose",
        "Nitesh V. Chawla",
        "Ronald Metoyer"
      ],
      "published": "2025-09-15T16:33:37Z",
      "categories": "",
      "summary": "Large language models (LLMs) typically generate direct answers, yet they are increasingly used as learning tools. Studying instructors' usage is critical, given their role in teaching and guiding AI adoption in education. We designed and evaluated TeaPT, an LLM for pedagogical purposes that supports instructors' professional development through two conversational approaches: a Socratic approach that uses guided questioning to foster reflection, and a Narrative approach that offers elaborated suggestions to extend externalized cognition. In a mixed-method study with 41 higher-education instruct...",
      "pdf_url": "https://arxiv.org/pdf/2509.12107v1.pdf",
      "relevance_score": 33,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2405.14767v2",
      "title": "FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models",
      "authors": [
        "Hongyang Yang",
        "Boyu Zhang",
        "Neng Wang",
        "Cheng Guo",
        "Xiaoli Zhang",
        "Likun Lin",
        "Junlin Wang",
        "Tianyu Zhou",
        "Mao Guan",
        "Runjia Zhang",
        "Christina Dan Wang"
      ],
      "published": "2024-05-23T16:35:20Z",
      "categories": "",
      "summary": "As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making. In this paper, we introd...",
      "pdf_url": "https://arxiv.org/pdf/2405.14767v2.pdf",
      "relevance_score": 33,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.15357v1",
      "title": "Integrating Generative AI in Cybersecurity Education: Case Study Insights on Pedagogical Strategies, Critical Thinking, and Responsible AI Use",
      "authors": [
        "Mahmoud Elkhodr",
        "Ergun Gide"
      ],
      "published": "2025-02-21T10:14:07Z",
      "categories": "",
      "summary": "The rapid advancement of Generative Artificial Intelligence (GenAI) has introduced new opportunities for transforming higher education, particularly in fields that require analytical reasoning and regulatory compliance, such as cybersecurity management. This study presents a structured framework for integrating GenAI tools into cybersecurity education, demonstrating their role in fostering critical thinking, real-world problem-solving, and regulatory awareness. The implementation strategy followed a two-stage approach, embedding GenAI within tutorial exercises and assessment tasks. Tutorials e...",
      "pdf_url": "https://arxiv.org/pdf/2502.15357v1.pdf",
      "relevance_score": 33,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.09090v1",
      "title": "AI and Human Oversight: A Risk-Based Framework for Alignment",
      "authors": [
        "Laxmiraju Kandikatla",
        "Branislav Radeljic"
      ],
      "published": "2025-10-10T07:36:44Z",
      "categories": "",
      "summary": "As Artificial Intelligence (AI) technologies continue to advance, protecting human autonomy and promoting ethical decision-making are essential to fostering trust and accountability. Human agency (the capacity of individuals to make informed decisions) should be actively preserved and reinforced by AI systems. This paper examines strategies for designing AI systems that uphold fundamental rights, strengthen human agency, and embed effective human oversight mechanisms. It discusses key oversight models, including Human-in-Command (HIC), Human-in-the-Loop (HITL), and Human-on-the-Loop (HOTL), an...",
      "pdf_url": "https://arxiv.org/pdf/2510.09090v1.pdf",
      "relevance_score": 32,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.06000v1",
      "title": "Differentiable Fuzzy Neural Networks for Recommender Systems",
      "authors": [
        "Stephan Bartl",
        "Kevin Innerebner",
        "Elisabeth Lex"
      ],
      "published": "2025-05-09T12:31:56Z",
      "categories": "",
      "summary": "As recommender systems become increasingly complex, transparency is essential to increase user trust, accountability, and regulatory compliance. Neuro-symbolic approaches that integrate symbolic reasoning with sub-symbolic learning offer a promising approach toward transparent and user-centric systems. In this work-in-progress, we investigate using fuzzy neural networks (FNNs) as a neuro-symbolic approach for recommendations that learn logic-based rules over predefined, human-readable atoms. Each rule corresponds to a fuzzy logic expression, making the recommender's decision process inherently...",
      "pdf_url": "https://arxiv.org/pdf/2505.06000v1.pdf",
      "relevance_score": 32,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.15142v1",
      "title": "Revisiting UTAUT for the Age of AI: Understanding Employees AI Adoption and Usage Patterns Through an Extended UTAUT Framework",
      "authors": [
        "Diana Wolfe",
        "Matt Price",
        "Alice Choe",
        "Fergus Kidd",
        "Hannah Wagner"
      ],
      "published": "2025-10-16T21:01:41Z",
      "categories": "",
      "summary": "This study investigates whether demographic factors shape adoption and attitudes among employees toward artificial intelligence (AI) technologies at work. Building on an extended Unified Theory of Acceptance and Use of Technology (UTAUT), which reintroduces affective dimensions such as attitude, self-efficacy, and anxiety, we surveyed 2,257 professionals across global regions and organizational levels within a multinational consulting firm. Non-parametric tests examined whether three demographic factors (i.e., years of experience, hierarchical level in the organization, and geographic region) ...",
      "pdf_url": "https://arxiv.org/pdf/2510.15142v1.pdf",
      "relevance_score": 31,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2412.04924v2",
      "title": "Follow the money: a startup-based measure of AI exposure across occupations, industries and regions",
      "authors": [
        "Enrico Maria Fenoaltea",
        "Dario Mazzilli",
        "Aurelio Patelli",
        "Angelica Sbardella",
        "Andrea Tacchella",
        "Andrea Zaccaria",
        "Marco Trombetti",
        "Luciano Pietronero"
      ],
      "published": "2024-12-06T10:25:05Z",
      "categories": "",
      "summary": "The integration of artificial intelligence (AI) into the workplace is advancing rapidly, necessitating robust metrics to evaluate its tangible impact on the labour market. Existing measures of AI occupational exposure largely focus on AI's theoretical potential to substitute or complement human labour on the basis of technical feasibility, providing limited insight into actual adoption and offering inadequate guidance for policymakers. To address this gap, we introduce the AI Startup Exposure (AISE) index-a novel metric based on occupational descriptions from O*NET and AI applications develope...",
      "pdf_url": "https://arxiv.org/pdf/2412.04924v2.pdf",
      "relevance_score": 31,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2501.00750v2",
      "title": "Beyond Text: Implementing Multimodal Large Language Model-Powered Multi-Agent Systems Using a No-Code Platform",
      "authors": [
        "Cheonsu Jeong"
      ],
      "published": "2025-01-01T06:36:56Z",
      "categories": "",
      "summary": "This study proposes the design and implementation of a multimodal LLM-based Multi-Agent System (MAS) leveraging a No-Code platform to address the practical constraints and significant entry barriers associated with AI adoption in enterprises. Advanced AI technologies, such as Large Language Models (LLMs), often pose challenges due to their technical complexity and high implementation costs, making them difficult for many organizations to adopt. To overcome these limitations, this research develops a No-Code-based Multi-Agent System designed to enable users without programming knowledge to easi...",
      "pdf_url": "https://arxiv.org/pdf/2501.00750v2.pdf",
      "relevance_score": 31,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.15894v1",
      "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making",
      "authors": [
        "Chengbo Zheng",
        "Tim Miller",
        "Alina Bialkowski",
        "H Peter Soyer",
        "Monika Janda"
      ],
      "published": "2025-04-22T13:36:06Z",
      "categories": "",
      "summary": "High stakes decision-making often requires a continuous interplay between evolving evidence and shifting hypotheses, a dynamic that is not well supported by current AI decision support systems. In this paper, we introduce a mixed-initiative framework for AI assisted decision making that is grounded in the data-frame theory of sensemaking and the evaluative AI paradigm. Our approach enables both humans and AI to collaboratively construct, validate, and adapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer diagnosis prototype that leverages a concept bottleneck model to ...",
      "pdf_url": "https://arxiv.org/pdf/2504.15894v1.pdf",
      "relevance_score": 30,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.06201v1",
      "title": "Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models",
      "authors": [
        "Rodrigo Gallardo",
        "Oz Fishman",
        "Alexander Htet Kyaw"
      ],
      "published": "2025-11-09T03:24:10Z",
      "categories": "",
      "summary": "This paper introduces a human-in-the-loop computer vision framework that uses generative AI to propose micro-scale design interventions in public space and support more continuous, local participation. Using Grounding DINO and a curated subset of the ADE20K dataset as a proxy for the urban built environment, the system detects urban objects and builds co-occurrence embeddings that reveal common spatial configurations. From this analysis, the user receives five statistically likely complements to a chosen anchor object. A vision language model then reasons over the scene image and the selected ...",
      "pdf_url": "https://arxiv.org/pdf/2511.06201v1.pdf",
      "relevance_score": 30,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2002.07786v1",
      "title": "Investigating Potential Factors Associated with Gender Discrimination in Collaborative Recommender Systems",
      "authors": [
        "Masoud Mansoury",
        "Himan Abdollahpouri",
        "Jessie Smith",
        "Arman Dehpanah",
        "Mykola Pechenizkiy",
        "Bamshad Mobasher"
      ],
      "published": "2020-02-18T18:30:17Z",
      "categories": "",
      "summary": "The proliferation of personalized recommendation technologies has raised concerns about discrepancies in their recommendation performance across different genders, age groups, and racial or ethnic populations. This varying degree of performance could impact users' trust in the system and may pose legal and ethical issues in domains where fairness and equity are critical concerns, like job recommendation. In this paper, we investigate several potential factors that could be associated with discriminatory performance of a recommendation algorithm for women versus men. We specifically study sever...",
      "pdf_url": "https://arxiv.org/pdf/2002.07786v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2202.07371v2",
      "title": "Personalized Prompt Learning for Explainable Recommendation",
      "authors": [
        "Lei Li",
        "Yongfeng Zhang",
        "Li Chen"
      ],
      "published": "2022-02-15T12:53:52Z",
      "categories": "",
      "summary": "Providing user-understandable explanations to justify recommendations could help users better understand the recommended items, increase the system's ease of use, and gain users' trust. A typical approach to realize it is natural language generation. However, previous works mostly adopt recurrent neural networks to meet the ends, leaving the potentially more effective pre-trained Transformer models under-explored. In fact, user and item IDs, as important identifiers in recommender systems, are inherently in different semantic space as words that pre-trained models were already trained on. Thus...",
      "pdf_url": "https://arxiv.org/pdf/2202.07371v2.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2407.02833v1",
      "title": "LANE: Logic Alignment of Non-tuning Large Language Models and Online Recommendation Systems for Explainable Reason Generation",
      "authors": [
        "Hongke Zhao",
        "Songming Zheng",
        "Likang Wu",
        "Bowen Yu",
        "Jing Wang"
      ],
      "published": "2024-07-03T06:20:31Z",
      "categories": "",
      "summary": "The explainability of recommendation systems is crucial for enhancing user trust and satisfaction. Leveraging large language models (LLMs) offers new opportunities for comprehensive recommendation logic generation. However, in existing related studies, fine-tuning LLM models for recommendation tasks incurs high computational costs and alignment issues with existing systems, limiting the application potential of proven proprietary/closed-source LLM models, such as GPT-4. In this work, our proposed effective strategy LANE aligns LLMs with online recommendation systems without additional LLMs tun...",
      "pdf_url": "https://arxiv.org/pdf/2407.02833v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.09188v1",
      "title": "Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation",
      "authors": [
        "Bangcheng Sun",
        "Yazhe Chen",
        "Jilin Yang",
        "Xiaodong Li",
        "Hui Li"
      ],
      "published": "2025-07-12T08:15:05Z",
      "categories": "",
      "summary": "Explainable Recommender System (ExRec) provides transparency to the recommendation process, increasing users' trust and boosting the operation of online services. With the rise of large language models (LLMs), whose extensive world knowledge and nuanced language understanding enable the generation of human-like, contextually grounded explanations, LLM-powered ExRec has gained great momentum. However, existing LLM-based ExRec models suffer from profile deviation and high retrieval overhead, hindering their deployment. To address these issues, we propose Retrieval-Augmented Recommendation Explan...",
      "pdf_url": "https://arxiv.org/pdf/2507.09188v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2004.08100v8",
      "title": "Recommendation system using a deep learning and graph analysis approach",
      "authors": [
        "Mahdi Kherad",
        "Amir Jalaly Bidgoly"
      ],
      "published": "2020-04-17T08:05:33Z",
      "categories": "",
      "summary": "When a user connects to the Internet to fulfill his needs, he often encounters a huge amount of related information. Recommender systems are the techniques for massively filtering information and offering the items that users find them satisfying and interesting. The advances in machine learning methods, especially deep learning, have led to great achievements in recommender systems, although these systems still suffer from challenges such as cold-start and sparsity problems. To solve these problems, context information such as user communication network is usually used. In this paper, we have...",
      "pdf_url": "https://arxiv.org/pdf/2004.08100v8.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2409.18690v1",
      "title": "Less is More: Towards Sustainability-Aware Persuasive Explanations in Recommender Systems",
      "authors": [
        "Thi Ngoc Trang Tran",
        "Seda Polat Erdeniz",
        "Alexander Felfernig",
        "Sebastian Lubos",
        "Merfat El-Mansi",
        "Viet-Man Le"
      ],
      "published": "2024-09-27T12:24:10Z",
      "categories": "",
      "summary": "Recommender systems play an important role in supporting the achievement of the United Nations sustainable development goals (SDGs). In recommender systems, explanations can support different goals, such as increasing a user's trust in a recommendation, persuading a user to purchase specific items, or increasing the understanding of the reasons behind a recommendation. In this paper, we discuss the concept of \"sustainability-aware persuasive explanations\" which we regard as a major concept to support the achievement of the mentioned SDGs. Such explanations are orthogonal to most existing expla...",
      "pdf_url": "https://arxiv.org/pdf/2409.18690v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.03169v1",
      "title": "Rashomon in the Streets: Explanation Ambiguity in Scene Understanding",
      "authors": [
        "Helge Spieker",
        "J\u00f8rn Eirik Betten",
        "Arnaud Gotlieb",
        "Nadjib Lazaar",
        "Nassim Belmecheri"
      ],
      "published": "2025-09-03T09:36:18Z",
      "categories": "",
      "summary": "Explainable AI (XAI) is essential for validating and trusting models in safety-critical applications like autonomous driving. However, the reliability of XAI is challenged by the Rashomon effect, where multiple, equally accurate models can offer divergent explanations for the same prediction. This paper provides the first empirical quantification of this effect for the task of action prediction in real-world driving scenes. Using Qualitative Explainable Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two distinct model classes: interpretable, pair-based gradient boo...",
      "pdf_url": "https://arxiv.org/pdf/2509.03169v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2307.08368v1",
      "title": "Gender mobility in the labor market with skills-based matching models",
      "authors": [
        "Ajaya Adhikari",
        "Steven Vethman",
        "Daan Vos",
        "Marc Lenz",
        "Ioana Cocu",
        "Ioannis Tolios",
        "Cor J. Veenman"
      ],
      "published": "2023-07-17T10:06:21Z",
      "categories": "",
      "summary": "Skills-based matching promises mobility of workers between different sectors and occupations in the labor market. In this case, job seekers can look for jobs they do not yet have experience in, but for which they do have relevant skills. Currently, there are multiple occupations with a skewed gender distribution. For skills-based matching, it is unclear if and how a shift in the gender distribution, which we call gender mobility, between occupations will be effected. It is expected that the skills-based matching approach will likely be data-driven, including computational language models and s...",
      "pdf_url": "https://arxiv.org/pdf/2307.08368v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.04107v1",
      "title": "Rethinking AI Evaluation in Education: The TEACH-AI Framework and Benchmark for Generative AI Assistants",
      "authors": [
        "Shi Ding",
        "Brian Magerko"
      ],
      "published": "2025-11-28T17:42:36Z",
      "categories": "",
      "summary": "As generative artificial intelligence (AI) continues to transform education, most existing AI evaluations rely primarily on technical performance metrics such as accuracy or task efficiency while overlooking human identity, learner agency, contextual learning processes, and ethical considerations. In this paper, we present TEACH-AI (Trustworthy and Effective AI Classroom Heuristics), a domain-independent, pedagogically grounded, and stakeholder-aligned framework with measurable indicators and a practical toolkit for guiding the design, development, and evaluation of generative AI systems in ed...",
      "pdf_url": "https://arxiv.org/pdf/2512.04107v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2405.02846v1",
      "title": "Responsible AI: Portraits with Intelligent Bibliometrics",
      "authors": [
        "Yi Zhang",
        "Mengjia Wu",
        "Guangquan Zhang",
        "Jie Lu"
      ],
      "published": "2024-05-05T08:40:22Z",
      "categories": "",
      "summary": "Shifting the focus from principles to practical implementation, responsible artificial intelligence (AI) has garnered considerable attention across academia, industry, and society at large. Despite being in its nascent stages, this emerging field grapples with nebulous concepts and intricate knowledge frameworks. By analyzing three prevailing concepts - explainable AI, trustworthy AI, and ethical AI, this study defined responsible AI and identified its core principles. Methodologically, this study successfully demonstrated the implementation of leveraging AI's capabilities into bibliometrics f...",
      "pdf_url": "https://arxiv.org/pdf/2405.02846v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2502.12386v1",
      "title": "Bridging the Data Gap in AI Reliability Research and Establishing DR-AIR, a Comprehensive Data Repository for AI Reliability",
      "authors": [
        "Simin Zheng",
        "Jared M. Clark",
        "Fatemeh Salboukh",
        "Priscila Silva",
        "Karen da Mata",
        "Fenglian Pan",
        "Jie Min",
        "Jiayi Lian",
        "Caleb B. King",
        "Lance Fiondella",
        "Jian Liu",
        "Xinwei Deng",
        "Yili Hong"
      ],
      "published": "2025-02-17T23:50:36Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) technology and systems have been advancing rapidly. However, ensuring the reliability of these systems is crucial for fostering public confidence in their use. This necessitates the modeling and analysis of reliability data specific to AI systems. A major challenge in AI reliability research, particularly for those in academia, is the lack of readily available AI reliability data. To address this gap, this paper focuses on conducting a comprehensive review of available AI reliability data and establishing DR-AIR: a data repository for AI reliability. Specifically, ...",
      "pdf_url": "https://arxiv.org/pdf/2502.12386v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2408.10229v1",
      "title": "AI Transparency in Academic Search Systems: An Initial Exploration",
      "authors": [
        "Yifan Liu",
        "Peter Sullivan",
        "Luanne Sinnamon"
      ],
      "published": "2024-08-02T19:33:47Z",
      "categories": "",
      "summary": "As AI-enhanced academic search systems become increasingly popular among researchers, investigating their AI transparency is crucial to ensure trust in the search outcomes, as well as the reliability and integrity of scholarly work. This study employs a qualitative content analysis approach to examine the websites of a sample of 10 AI-enhanced academic search systems identified through university library guides. The assessed level of transparency varies across these systems: five provide detailed information about their mechanisms, three offer partial information, and two provide little to no ...",
      "pdf_url": "https://arxiv.org/pdf/2408.10229v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2404.04963v1",
      "title": "SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials",
      "authors": [
        "Mael Jullien",
        "Marco Valentino",
        "Andr\u00e9 Freitas"
      ],
      "published": "2024-04-07T13:58:41Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) are at the forefront of NLP achievements but fall short in dealing with shortcut learning, factual inconsistency, and vulnerability to adversarial inputs.These shortcomings are especially critical in medical contexts, where they can misrepresent actual model capabilities. Addressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for ClinicalTrials. Our contributions include the refined NLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials - Perturbed), designed to challenge LLMs with interventional and causal reaso...",
      "pdf_url": "https://arxiv.org/pdf/2404.04963v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2303.11508v1",
      "title": "AI-in-the-Loop -- The impact of HMI in AI-based Application",
      "authors": [
        "Julius Sch\u00f6ning",
        "Clemens Westerkamp"
      ],
      "published": "2023-03-21T00:04:33Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded...",
      "pdf_url": "https://arxiv.org/pdf/2303.11508v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.00098v1",
      "title": "Operating advanced scientific instruments with AI agents that learn on the job",
      "authors": [
        "Aikaterini Vriza",
        "Michael H. Prince",
        "Tao Zhou",
        "Henry Chan",
        "Mathew J. Cherukara"
      ],
      "published": "2025-08-27T16:40:14Z",
      "categories": "",
      "summary": "Advanced scientific user facilities, such as next generation X-ray light sources and self-driving laboratories, are revolutionizing scientific discovery by automating routine tasks and enabling rapid experimentation and characterizations. However, these facilities must continuously evolve to support new experimental workflows, adapt to diverse user projects, and meet growing demands for more intricate instruments and experiments. This continuous development introduces significant operational complexity, necessitating a focus on usability, reproducibility, and intuitive human-instrument interac...",
      "pdf_url": "https://arxiv.org/pdf/2509.00098v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2302.03350v2",
      "title": "To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods",
      "authors": [
        "Dawen Zhang",
        "Shidong Pan",
        "Thong Hoang",
        "Zhenchang Xing",
        "Mark Staples",
        "Xiwei Xu",
        "Lina Yao",
        "Qinghua Lu",
        "Liming Zhu"
      ],
      "published": "2023-02-07T09:48:29Z",
      "categories": "",
      "summary": "The right to be forgotten (RTBF) is motivated by the desire of people not to be perpetually disadvantaged by their past deeds. For this, data deletion needs to be deep and permanent, and should be removed from machine learning models. Researchers have proposed machine unlearning algorithms which aim to erase specific data from trained models more efficiently. However, these methods modify how data is fed into the model and how training is done, which may subsequently compromise AI ethics from the fairness perspective. To help software engineers make responsible decisions when adopting these un...",
      "pdf_url": "https://arxiv.org/pdf/2302.03350v2.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.10154v1",
      "title": "Simulating Biases for Interpretable Fairness in Offline and Online Classifiers",
      "authors": [
        "Ricardo In\u00e1cio",
        "Zafeiris Kokkinogenis",
        "Vitor Cerqueira",
        "Carlos Soares"
      ],
      "published": "2025-07-14T11:04:24Z",
      "categories": "",
      "summary": "Predictive models often reinforce biases which were originally embedded in their training data, through skewed decisions. In such cases, mitigation methods are critical to ensure that, regardless of the prevailing disparities, model outcomes are adjusted to be fair. To assess this, datasets could be systematically generated with specific biases, to train machine learning classifiers. Then, predictive outcomes could aid in the understanding of this bias embedding process. Hence, an agent-based model (ABM), depicting a loan application process that represents various systemic biases across two d...",
      "pdf_url": "https://arxiv.org/pdf/2507.10154v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.21664v1",
      "title": "Expert Survey: AI Reliability & Security Research Priorities",
      "authors": [
        "Joe O'Brien",
        "Jeremy Dolan",
        "Jay Kim",
        "Jonah Dykhuizen",
        "Jeba Sania",
        "Sebastian Becker",
        "Jam Kraprayoon",
        "Cara Labrador"
      ],
      "published": "2025-05-27T18:44:30Z",
      "categories": "",
      "summary": "Our survey of 53 specialists across 105 AI reliability and security research areas identifies the most promising research prospects to guide strategic AI R&D investment. As companies are seeking to develop AI systems with broadly human-level capabilities, research on reliability and security is urgently needed to ensure AI's benefits can be safely and broadly realized and prevent severe harms. This study is the first to quantify expert priorities across a comprehensive taxonomy of AI safety and security research directions and to produce a data-driven ranking of their potential impact. These r...",
      "pdf_url": "https://arxiv.org/pdf/2505.21664v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2301.07098v1",
      "title": "The moral authority of ChatGPT",
      "authors": [
        "Sebastian Kr\u00fcgel",
        "Andreas Ostermaier",
        "Matthias Uhl"
      ],
      "published": "2023-01-13T20:24:38Z",
      "categories": "",
      "summary": "ChatGPT is not only fun to chat with, but it also searches information, answers questions, and gives advice. With consistent moral advice, it might improve the moral judgment and decisions of users, who often hold contradictory moral beliefs. Unfortunately, ChatGPT turns out highly inconsistent as a moral advisor. Nonetheless, it influences users' moral judgment, we find in an experiment, even if they know they are advised by a chatting bot, and they underestimate how much they are influenced. Thus, ChatGPT threatens to corrupt rather than improves users' judgment. These findings raise the que...",
      "pdf_url": "https://arxiv.org/pdf/2301.07098v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2302.10395v1",
      "title": "Designerly Understanding: Information Needs for Model Transparency to Support Design Ideation for AI-Powered User Experience",
      "authors": [
        "Q. Vera Liao",
        "Hariharan Subramonyam",
        "Jennifer Wang",
        "Jennifer Wortman Vaughan"
      ],
      "published": "2023-02-21T02:06:24Z",
      "categories": "",
      "summary": "Despite the widespread use of artificial intelligence (AI), designing user experiences (UX) for AI-powered systems remains challenging. UX designers face hurdles understanding AI technologies, such as pre-trained language models, as design materials. This limits their ability to ideate and make decisions about whether, where, and how to use AI. To address this problem, we bridge the literature on AI design and AI transparency to explore whether and how frameworks for transparent model reporting can support design ideation with pre-trained models. By interviewing 23 UX practitioners, we find th...",
      "pdf_url": "https://arxiv.org/pdf/2302.10395v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1910.07089v1",
      "title": "Challenges of Human-Aware AI Systems",
      "authors": [
        "Subbarao Kambhampati"
      ],
      "published": "2019-10-15T22:34:50Z",
      "categories": "",
      "summary": "From its inception, AI has had a rather ambivalent relationship to humans---swinging between their augmentation and replacement. Now, as AI technologies enter our everyday lives at an ever increasing pace, there is a greater need for AI systems to work synergistically with humans. To do this effectively, AI systems must pay more attention to aspects of intelligence that helped humans work with each other---including social intelligence. I will discuss the research challenges in designing such human-aware AI systems, including modeling the mental states of humans in the loop, recognizing their ...",
      "pdf_url": "https://arxiv.org/pdf/1910.07089v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.20671v1",
      "title": "Bridging the AI Trustworthiness Gap between Functions and Norms",
      "authors": [
        "Daan Di Scala",
        "Sophie Lathouwers",
        "Michael van Bekkum"
      ],
      "published": "2025-12-19T14:06:57Z",
      "categories": "",
      "summary": "Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also hel...",
      "pdf_url": "https://arxiv.org/pdf/2512.20671v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2110.01167v2",
      "title": "Trustworthy AI: From Principles to Practices",
      "authors": [
        "Bo Li",
        "Peng Qi",
        "Bo Liu",
        "Shuai Di",
        "Jingen Liu",
        "Jiquan Pei",
        "Jinfeng Yi",
        "Bowen Zhou"
      ],
      "published": "2021-10-04T03:20:39Z",
      "categories": "",
      "summary": "The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people's trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization...",
      "pdf_url": "https://arxiv.org/pdf/2110.01167v2.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.14741v1",
      "title": "CaTE Data Curation for Trustworthy AI",
      "authors": [
        "Mary Versa Clemens-Sewall",
        "Christopher Cervantes",
        "Emma Rafkin",
        "J. Neil Otte",
        "Tom Magelinski",
        "Libby Lewis",
        "Michelle Liu",
        "Dana Udwin",
        "Monique Kirkman-Bey"
      ],
      "published": "2025-08-20T14:40:21Z",
      "categories": "",
      "summary": "This report provides practical guidance to teams designing or developing AI-enabled systems for how to promote trustworthiness during the data curation phase of development. In this report, the authors first define data, the data curation phase, and trustworthiness. We then describe a series of steps that the development team, especially data scientists, can take to build a trustworthy AI-enabled system. We enumerate the sequence of core steps and trace parallel paths where alternatives exist. The descriptions of these steps include strengths, weaknesses, preconditions, outcomes, and relevant ...",
      "pdf_url": "https://arxiv.org/pdf/2508.14741v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2305.11581v1",
      "title": "Trustworthy, responsible, ethical AI in manufacturing and supply chains: synthesis and emerging research questions",
      "authors": [
        "Alexandra Brintrup",
        "George Baryannis",
        "Ashutosh Tiwari",
        "Svetan Ratchev",
        "Giovanna Martinez-Arellano",
        "Jatinder Singh"
      ],
      "published": "2023-05-19T10:43:06Z",
      "categories": "",
      "summary": "While the increased use of AI in the manufacturing sector has been widely noted, there is little understanding on the risks that it may raise in a manufacturing organisation. Although various high level frameworks and definitions have been proposed to consolidate potential risks, practitioners struggle with understanding and implementing them.   This lack of understanding exposes manufacturing to a multitude of risks, including the organisation, its workers, as well as suppliers and clients. In this paper, we explore and interpret the applicability of responsible, ethical, and trustworthy AI w...",
      "pdf_url": "https://arxiv.org/pdf/2305.11581v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2407.04481v1",
      "title": "Using Petri Nets as an Integrated Constraint Mechanism for Reinforcement Learning Tasks",
      "authors": [
        "Timon Sachweh",
        "Pierre Haritz",
        "Thomas Liebig"
      ],
      "published": "2024-07-05T13:04:06Z",
      "categories": "",
      "summary": "The lack of trust in algorithms is usually an issue when using Reinforcement Learning (RL) agents for control in real-world domains such as production plants, autonomous vehicles, or traffic-related infrastructure, partly due to the lack of verifiability of the model itself. In such scenarios, Petri nets (PNs) are often available for flowcharts or process steps, as they are versatile and standardized. In order to facilitate integration of RL models and as a step towards increasing AI trustworthiness, we propose an approach that uses PNs with three main advantages over typical RL approaches: Fi...",
      "pdf_url": "https://arxiv.org/pdf/2407.04481v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2506.03233v1",
      "title": "A Trustworthiness-based Metaphysics of Artificial Intelligence Systems",
      "authors": [
        "Andrea Ferrario"
      ],
      "published": "2025-06-03T15:45:46Z",
      "categories": "",
      "summary": "Modern AI systems are man-made objects that leverage machine learning to support our lives across a myriad of contexts and applications. Despite extensive epistemological and ethical debates, their metaphysical foundations remain relatively under explored. The orthodox view simply suggests that AI systems, as artifacts, lack well-posed identity and persistence conditions -- their metaphysical kinds are no real kinds. In this work, we challenge this perspective by introducing a theory of metaphysical identity of AI systems. We do so by characterizing their kinds and introducing identity criteri...",
      "pdf_url": "https://arxiv.org/pdf/2506.03233v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2002.06276v1",
      "title": "Trustworthy AI",
      "authors": [
        "Jeannette M. Wing"
      ],
      "published": "2020-02-14T22:45:36Z",
      "categories": "",
      "summary": "The promise of AI is huge. AI systems have already achieved good enough performance to be in our streets and in our homes. However, they can be brittle and unfair. For society to reap the benefits of AI systems, society needs to be able to trust them. Inspired by decades of progress in trustworthy computing, we suggest what trustworthy properties would be desired of AI systems. By enumerating a set of new research questions, we explore one approach--formal verification--for ensuring trust in AI. Trustworthy AI ups the ante on both trustworthy computing and formal methods.",
      "pdf_url": "https://arxiv.org/pdf/2002.06276v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2102.04221v1",
      "title": "The Sanction of Authority: Promoting Public Trust in AI",
      "authors": [
        "Bran Knowles",
        "John T. Richards"
      ],
      "published": "2021-01-22T22:01:30Z",
      "categories": "",
      "summary": "Trusted AI literature to date has focused on the trust needs of users who knowingly interact with discrete AIs. Conspicuously absent from the literature is a rigorous treatment of public trust in AI. We argue that public distrust of AI originates from the under-development of a regulatory ecosystem that would guarantee the trustworthiness of the AIs that pervade society. Drawing from structuration theory and literature on institutional trust, we offer a model of public trust in AI that differs starkly from models driving Trusted AI efforts. This model provides a theoretical scaffolding for Tru...",
      "pdf_url": "https://arxiv.org/pdf/2102.04221v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2402.07039v3",
      "title": "Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities",
      "authors": [
        "Sven Cattell",
        "Avijit Ghosh",
        "Lucie-Aim\u00e9e Kaffee"
      ],
      "published": "2024-02-10T20:39:04Z",
      "categories": "",
      "summary": "Harm reporting in Artificial Intelligence (AI) currently lacks a structured process for disclosing and addressing algorithmic flaws, relying largely on an ad-hoc approach. This contrasts sharply with the well-established Coordinated Vulnerability Disclosure (CVD) ecosystem in software security. While global efforts to establish frameworks for AI transparency and collaboration are underway, the unique challenges presented by machine learning (ML) models demand a specialized approach. To address this gap, we propose implementing a Coordinated Flaw Disclosure (CFD) framework tailored to the compl...",
      "pdf_url": "https://arxiv.org/pdf/2402.07039v3.pdf",
      "relevance_score": 24,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2501.11705v1",
      "title": "Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)",
      "authors": [
        "Brian E. Perron",
        "Lauri Goldkind",
        "Zia Qi",
        "Bryan G. Victor"
      ],
      "published": "2025-01-20T19:38:21Z",
      "categories": "",
      "summary": "This paper examines the responsible integration of artificial intelligence (AI) in human services organizations (HSOs), proposing a nuanced framework for evaluating AI applications across multiple dimensions of risk. The authors argue that ethical concerns about AI deployment -- including professional judgment displacement, environmental impact, model bias, and data laborer exploitation -- vary significantly based on implementation context and specific use cases. They challenge the binary view of AI adoption, demonstrating how different applications present varying levels of risk that can ofte...",
      "pdf_url": "https://arxiv.org/pdf/2501.11705v1.pdf",
      "relevance_score": 23,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2102.12413v1",
      "title": "Designing Explanations for Group Recommender Systems",
      "authors": [
        "A. Felfernig",
        "N. Tintarev",
        "T. N. T. Trang",
        "M. Stettinger"
      ],
      "published": "2021-02-24T17:05:39Z",
      "categories": "",
      "summary": "Explanations are used in recommender systems for various reasons. Users have to be supported in making (high-quality) decisions more quickly. Developers of recommender systems want to convince users to purchase specific items. Users should better understand how the recommender system works and why a specific item has been recommended. Users should also develop a more in-depth understanding of the item domain. Consequently, explanations are designed in order to achieve specific \\emph{goals} such as increasing the transparency of a recommendation or increasing a user's trust in the recommender s...",
      "pdf_url": "https://arxiv.org/pdf/2102.12413v1.pdf",
      "relevance_score": 22,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2406.15859v2",
      "title": "LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning",
      "authors": [
        "Guangsi Shi",
        "Xiaofeng Deng",
        "Linhao Luo",
        "Lijuan Xia",
        "Lei Bao",
        "Bei Ye",
        "Fei Du",
        "Shirui Pan",
        "Yuxiao Li"
      ],
      "published": "2024-06-22T14:14:03Z",
      "categories": "",
      "summary": "Recommender systems are pivotal in enhancing user experiences across various web applications by analyzing the complicated relationships between users and items. Knowledge graphs(KGs) have been widely used to enhance the performance of recommender systems. However, KGs are known to be noisy and incomplete, which are hard to provide reliable explanations for recommendation results. An explainable recommender system is crucial for the product development and subsequent decision-making. To address these challenges, we introduce a novel recommender that synergies Large Language Models (LLMs) and K...",
      "pdf_url": "https://arxiv.org/pdf/2406.15859v2.pdf",
      "relevance_score": 22,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2212.10136v1",
      "title": "A Comparison Between Tsetlin Machines and Deep Neural Networks in the Context of Recommendation Systems",
      "authors": [
        "Karl Audun Borgersen",
        "Morten Goodwin",
        "Jivitesh Sharma"
      ],
      "published": "2022-12-20T10:05:36Z",
      "categories": "",
      "summary": "Recommendation Systems (RSs) are ubiquitous in modern society and are one of the largest points of interaction between humans and AI. Modern RSs are often implemented using deep learning models, which are infamously difficult to interpret. This problem is particularly exasperated in the context of recommendation scenarios, as it erodes the user's trust in the RS. In contrast, the newly introduced Tsetlin Machines (TM) possess some valuable properties due to their inherent interpretability. TMs are still fairly young as a technology. As no RS has been developed for TMs before, it has become nec...",
      "pdf_url": "https://arxiv.org/pdf/2212.10136v1.pdf",
      "relevance_score": 22,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2307.08774v1",
      "title": "Reflections from the Workshop on AI-Assisted Decision Making for Conservation",
      "authors": [
        "Lily Xu",
        "Esther Rolf",
        "Sara Beery",
        "Joseph R. Bennett",
        "Tanya Berger-Wolf",
        "Tanya Birch",
        "Elizabeth Bondi-Kelly",
        "Justin Brashares",
        "Melissa Chapman",
        "Anthony Corso",
        "Andrew Davies",
        "Nikhil Garg",
        "Angela Gaylard",
        "Robert Heilmayr",
        "Hannah Kerner",
        "Konstantin Klemmer",
        "Vipin Kumar",
        "Lester Mackey",
        "Claire Monteleoni",
        "Paul Moorcroft",
        "Jonathan Palmer",
        "Andrew Perrault",
        "David Thau",
        "Milind Tambe"
      ],
      "published": "2023-07-17T18:41:03Z",
      "categories": "",
      "summary": "In this white paper, we synthesize key points made during presentations and discussions from the AI-Assisted Decision Making for Conservation workshop, hosted by the Center for Research on Computation and Society at Harvard University on October 20-21, 2022. We identify key open research questions in resource allocation, planning, and interventions for biodiversity conservation, highlighting conservation challenges that not only require AI solutions, but also require novel methodological advances. In addition to providing a summary of the workshop talks and discussions, we hope this document s...",
      "pdf_url": "https://arxiv.org/pdf/2307.08774v1.pdf",
      "relevance_score": 20,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2208.06327v1",
      "title": "Developing moral AI to support antimicrobial decision making",
      "authors": [
        "William J Bolton",
        "Cosmin Badea",
        "Pantelis Georgiou",
        "Alison Holmes",
        "Timothy M Rawson"
      ],
      "published": "2022-08-12T15:33:42Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) assisting with antimicrobial prescribing raises significant moral questions. Utilising ethical frameworks alongside AI-driven systems, while considering infection specific complexities, can support moral decision making to tackle antimicrobial resistance.",
      "pdf_url": "https://arxiv.org/pdf/2208.06327v1.pdf",
      "relevance_score": 20,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2407.09281v2",
      "title": "Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning",
      "authors": [
        "Thuy Ngoc Nguyen",
        "Kasturi Jamale",
        "Cleotilde Gonzalez"
      ],
      "published": "2024-07-12T14:13:06Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) have demonstrated their capabilities across various tasks, from language translation to complex reasoning. Understanding and predicting human behavior and biases are crucial for artificial intelligence (AI) assisted systems to provide useful assistance, yet it remains an open question whether these models can achieve this. This paper addresses this gap by leveraging the reasoning and generative capabilities of the LLMs to predict human behavior in two sequential decision-making tasks. These tasks involve balancing between exploitative and exploratory actions and ha...",
      "pdf_url": "https://arxiv.org/pdf/2407.09281v2.pdf",
      "relevance_score": 20,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1809.06606v1",
      "title": "Proceedings of the AI-HRI Symposium at AAAI-FSS 2018",
      "authors": [
        "Kalesha Bullard",
        "Nick DePalma",
        "Richard G. Freedman",
        "Bradley Hayes",
        "Luca Iocchi",
        "Katrin Lohan",
        "Ross Mead",
        "Emmanuel Senft",
        "Tom Williams"
      ],
      "published": "2018-09-18T09:16:54Z",
      "categories": "",
      "summary": "The goal of the Interactive Learning for Artificial Intelligence (AI) for Human-Robot Interaction (HRI) symposium is to bring together the large community of researchers working on interactive learning scenarios for interactive robotics. While current HRI research involves investigating ways for robots to effectively interact with people, HRI's overarching goal is to develop robots that are autonomous while intelligently modeling and learning from humans. These goals greatly overlap with some central goals of AI and interactive machine learning, such that HRI is an extremely challenging proble...",
      "pdf_url": "https://arxiv.org/pdf/1809.06606v1.pdf",
      "relevance_score": 20,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2211.13130v1",
      "title": "A Brief Overview of AI Governance for Responsible Machine Learning Systems",
      "authors": [
        "Navdeep Gill",
        "Abhishek Mathur",
        "Marcos V. Conde"
      ],
      "published": "2022-11-21T23:48:51Z",
      "categories": "",
      "summary": "Organizations of all sizes, across all industries and domains are leveraging artificial intelligence (AI) technologies to solve some of their biggest challenges around operations, customer experience, and much more. However, due to the probabilistic nature of AI, the risks associated with it are far greater than traditional technologies. Research has shown that these risks can range anywhere from regulatory, compliance, reputational, and user trust, to financial and even societal risks. Depending on the nature and size of the organization, AI technologies can pose a significant risk, if not us...",
      "pdf_url": "https://arxiv.org/pdf/2211.13130v1.pdf",
      "relevance_score": 20,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.07889v1",
      "title": "Towards Meaningful Transparency in Civic AI Systems",
      "authors": [
        "Dave Murray-Rust",
        "Kars Alfrink",
        "Cristina Zaga"
      ],
      "published": "2025-10-09T07:43:01Z",
      "categories": "",
      "summary": "Artificial intelligence has become a part of the provision of governmental services, from making decisions about benefits to issuing fines for parking violations. However, AI systems rarely live up to the promise of neutral optimisation, creating biased or incorrect outputs and reducing the agency of both citizens and civic workers to shape the way decisions are made. Transparency is a principle that can both help subjects understand decisions made about them and shape the processes behind those decisions. However, transparency as practiced around AI systems tends to focus on the production of...",
      "pdf_url": "https://arxiv.org/pdf/2510.07889v1.pdf",
      "relevance_score": 20,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.13330v1",
      "title": "The Future of Work is Blended, Not Hybrid",
      "authors": [
        "Marios Constantinides",
        "Himanshu Verma",
        "Shadan Sadeghian",
        "Abdallah El Ali"
      ],
      "published": "2025-04-17T20:40:43Z",
      "categories": "",
      "summary": "The way we work is no longer hybrid -- it is blended with AI co-workers, automated decisions, and virtual presence reshaping human roles, agency, and expertise. We now work through AI, with our outputs shaped by invisible algorithms. AI's infiltration into knowledge, creative, and service work is not just about automation, but concerns redistribution of agency, creativity, and control. How do we deal with physical and distributed AI-mediated workspaces? What happens when algorithms co-author reports, and draft our creative work? In this provocation, we argue that hybrid work is obsolete. Blend...",
      "pdf_url": "https://arxiv.org/pdf/2504.13330v1.pdf",
      "relevance_score": 20,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.20714v1",
      "title": "From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education",
      "authors": [
        "Iman Reihanian",
        "Yunfei Hou",
        "Qingquan Sun"
      ],
      "published": "2025-12-23T19:20:34Z",
      "categories": "",
      "summary": "Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-firs...",
      "pdf_url": "https://arxiv.org/pdf/2512.20714v1.pdf",
      "relevance_score": 18,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2505.14721v1",
      "title": "The emerging AI 'revolution tranquille' in America",
      "authors": [
        "Omar R. Malik"
      ],
      "published": "2025-05-19T15:49:32Z",
      "categories": "",
      "summary": "Using data from the U.S. Census Bureaus Business Trends and Outlook Survey (BTOS), I examine the adoption of AI among US firms at national, state, industry, and firm size levels. I find that adoption remains overall low (only around 7% of firms currently use AI), but is on a steady upward trajectory with a rising share of firms planning to implement AI. Adoption rates vary significantly across regions and sectors: some states are emerging as early adopters, while others lag, and knowledge-intensive industries (such as information technology and professional services) along with larger firms sh...",
      "pdf_url": "https://arxiv.org/pdf/2505.14721v1.pdf",
      "relevance_score": 16,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.13903v1",
      "title": "From Teacher to Colleague: How Coding Experience Shapes Developer Perceptions of AI Tools",
      "authors": [
        "Ilya Zakharov",
        "Ekaterina Koshchenko",
        "Agnia Sergeyuk"
      ],
      "published": "2025-04-08T08:58:06Z",
      "categories": "",
      "summary": "AI-assisted development tools promise productivity gains and improved code quality, yet their adoption among developers remains inconsistent. Prior research suggests that professional expertise influences technology adoption, but its role in shaping developers' perceptions of AI tools is unclear. We analyze survey data from 3380 developers to examine how coding experience relates to AI awareness, adoption, and the roles developers assign to AI in their workflow. Our findings reveal that coding experience does not predict AI adoption but significantly influences mental models of AI's role. Expe...",
      "pdf_url": "https://arxiv.org/pdf/2504.13903v1.pdf",
      "relevance_score": 16,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.18859v2",
      "title": "Toward Human-Centered AI-Assisted Terminology Work",
      "authors": [
        "Antonio San Martin"
      ],
      "published": "2025-12-21T19:16:40Z",
      "categories": "",
      "summary": "The rapid diffusion of generative artificial intelligence is transforming terminology work. While this technology promises gains in efficiency, its unstructured adoption risks weakening professional autonomy, amplifying bias, and eroding linguistic and conceptual diversity. This paper argues that a human-centered approach to artificial intelligence has become a necessity for terminology work. Building on research in artificial intelligence and translation studies, it proposes a human-centered framework that conceptualizes artificial intelligence as a means of amplifying the terminologist's cap...",
      "pdf_url": "https://arxiv.org/pdf/2512.18859v2.pdf",
      "relevance_score": 16,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2412.07727v4",
      "title": "Artificial Intelligence Tools Expand Scientists' Impact but Contract Science's Focus (Just accepted by Nature, to be online soon)",
      "authors": [
        "Qianyue Hao",
        "Fengli Xu",
        "Yong Li",
        "James Evans"
      ],
      "published": "2024-12-10T18:24:17Z",
      "categories": "",
      "summary": "Development in Artificial Intelligence (AI) has accelerated scientific discovery. Alongside recent AI-oriented Nobel prizes, these trends establish the role of AI tools in science. This advancement raises questions about the potential influences of AI tools on scientists and science as a whole, and highlights a potential conflict between individual and collective benefits. To evaluate, we used a pretrained language model to identify AI-augmented research, with an F1-score of 0.875 in validation against expert-labeled data. Using a dataset of 41.3 million research papers across natural science ...",
      "pdf_url": "https://arxiv.org/pdf/2412.07727v4.pdf",
      "relevance_score": 16,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2501.03092v1",
      "title": "Societal Adaptation to AI Human-Labor Automation",
      "authors": [
        "Yuval Rymon"
      ],
      "published": "2024-12-07T15:08:11Z",
      "categories": "",
      "summary": "AI is transforming human labor at an unprecedented pace - improving 10$\\times$ per year in training effectiveness. This paper analyzes how society can adapt to AI-driven human-labor automation (HLA), using Bernardi et al.'s societal adaptation framework. Drawing on literature from general automation economics and recent AI developments, the paper develops a \"threat model.\" The threat model is centered on mass unemployment and its socioeconomic consequences, and assumes a non-binary scenario between full AGI takeover and swift job creation. The analysis explores both \"capability-modifying inter...",
      "pdf_url": "https://arxiv.org/pdf/2501.03092v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.11143v1",
      "title": "Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis",
      "authors": [
        "Chuke Chen",
        "Biao Luo",
        "Nan Li",
        "Boxiang Wang",
        "Hang Yang",
        "Jing Guo",
        "Ming Xu"
      ],
      "published": "2025-10-13T08:32:43Z",
      "categories": "",
      "summary": "The rapid expansion of scientific data has widened the gap between analytical capability and research intent. Existing AI-based analysis tools, ranging from AutoML frameworks to agentic research assistants, either favor automation over transparency or depend on manual scripting that hinders scalability and reproducibility. We present ARIA (Automated Research Intelligence Assistant), a spec-driven, human-in-the-loop framework for automated and interpretable data analysis. ARIA integrates six interoperable layers, namely Command, Context, Code, Data, Orchestration, and AI Module, within a docume...",
      "pdf_url": "https://arxiv.org/pdf/2510.11143v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.27051v1",
      "title": "Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement",
      "authors": [
        "Aaditya Shukla",
        "Sidney Knowles",
        "Meenakshi Madugula",
        "Dave Farris",
        "Ryan Angilly",
        "Santiago Pombo",
        "Anbang Xu",
        "Lu An",
        "Abhinav Balasubramanian",
        "Tan Yu",
        "Jiaxiang Ren",
        "Rama Akkiraju"
      ],
      "published": "2025-10-30T23:41:06Z",
      "categories": "",
      "summary": "Enterprise AI agents must continuously adapt to maintain accuracy, reduce latency, and remain aligned with user needs. We present a practical implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts (MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a MAPE-driven data flywheel, we built a closed-loop system that systematically addresses failures in retrieval-augmented generation (RAG) pipelines and enables continuous learning. Over a 3-month post-deployment period, we monitored feedback and collected 495 negative samples. Analysis revealed two major...",
      "pdf_url": "https://arxiv.org/pdf/2510.27051v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.07950v1",
      "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis",
      "authors": [
        "Chen Shen",
        "Wanqing Zhang",
        "Kehan Li",
        "Erwen Huang",
        "Haitao Bi",
        "Aiying Fan",
        "Yiwen Shen",
        "Hongmei Dong",
        "Ji Zhang",
        "Yuming Shao",
        "Zengjia Liu",
        "Xinshe Liu",
        "Tao Li",
        "Chunxia Yan",
        "Shuanliang Fan",
        "Di Wu",
        "Jianhua Ma",
        "Bin Cong",
        "Zhenyuan Wang",
        "Chunfeng Lian"
      ],
      "published": "2025-08-11T13:05:59Z",
      "categories": "",
      "summary": "Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solve...",
      "pdf_url": "https://arxiv.org/pdf/2508.07950v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2412.14232v1",
      "title": "Human-in-the-loop or AI-in-the-loop? Automate or Collaborate?",
      "authors": [
        "Sriraam Natarajan",
        "Saurabh Mathur",
        "Sahil Sidheekh",
        "Wolfgang Stammer",
        "Kristian Kersting"
      ],
      "published": "2024-12-18T18:31:39Z",
      "categories": "",
      "summary": "Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop ($AI^2L$) systems, where the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critic...",
      "pdf_url": "https://arxiv.org/pdf/2412.14232v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.00970v1",
      "title": "AI-Educational Development Loop (AI-EDL): A Conceptual Framework to Bridge AI Capabilities with Classical Educational Theories",
      "authors": [
        "Ning Yu",
        "Jie Zhang",
        "Sandeep Mitra",
        "Rebecca Smith",
        "Adam Rich"
      ],
      "published": "2025-08-01T15:44:19Z",
      "categories": "",
      "summary": "This study introduces the AI-Educational Development Loop (AI-EDL), a theory-driven framework that integrates classical learning theories with human-in-the-loop artificial intelligence (AI) to support reflective, iterative learning. Implemented in EduAlly, an AI-assisted platform for writing-intensive and feedback-sensitive tasks, the framework emphasizes transparency, self-regulated learning, and pedagogical oversight. A mixed-methods study was piloted at a comprehensive public university to evaluate alignment between AI-generated feedback, instructor evaluations, and student self-assessments...",
      "pdf_url": "https://arxiv.org/pdf/2508.00970v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.07097v1",
      "title": "Agentic AI Sustainability Assessment for Supply Chain Document Insights",
      "authors": [
        "Diego Gosmar",
        "Anna Chiara Pallotta",
        "Giovanni Zenezini"
      ],
      "published": "2025-11-10T13:38:08Z",
      "categories": "",
      "summary": "This paper presents a comprehensive sustainability assessment framework for document intelligence within supply chain operations, centered on agentic artificial intelligence (AI). We address the dual objective of improving automation efficiency while providing measurable environmental performance in document-intensive workflows. The research compares three scenarios: fully manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical results show that AI-assisted HITL and agentic AI scenarios achieve redu...",
      "pdf_url": "https://arxiv.org/pdf/2511.07097v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2512.11295v3",
      "title": "AI Autonomy Coefficient ($\u03b1$): Defining Boundaries for Responsible AI Systems",
      "authors": [
        "Nattaya Mairittha",
        "Gabriel Phorncharoenmusikul",
        "Sorawit Worapradidth"
      ],
      "published": "2025-12-12T05:41:20Z",
      "categories": "",
      "summary": "The integrity of many contemporary AI systems is compromised by the misuse of Human-in-the-Loop (HITL) models to obscure systems that remain heavily dependent on human labor. We define this structural dependency as Human-Instead-of-AI (HISOAI), an ethically problematic and economically unsustainable design in which human workers function as concealed operational substitutes rather than intentional, high-value collaborators. To address this issue, we introduce the AI-First, Human-Empowered (AFHE) paradigm, which requires AI systems to demonstrate a quantifiable level of functional independence ...",
      "pdf_url": "https://arxiv.org/pdf/2512.11295v3.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2509.23994v2",
      "title": "Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents",
      "authors": [
        "Gauri Kholkar",
        "Ratinder Ahuja"
      ],
      "published": "2025-09-28T17:36:52Z",
      "categories": "",
      "summary": "As autonomous AI agents are used in regulated and safety-critical settings, organizations need effective ways to turn policy into enforceable controls. We introduce a regulatory machine learning framework that converts unstructured design artifacts (like PRDs, TDDs, and code) into verifiable runtime guardrails. Our Policy as Prompt method reads these documents and risk controls to build a source-linked policy tree. This tree is then compiled into lightweight, prompt-based classifiers for real-time runtime monitoring. The system is built to enforce least privilege and data minimization. For con...",
      "pdf_url": "https://arxiv.org/pdf/2509.23994v2.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2403.14049v1",
      "title": "A Roadmap Towards Automated and Regulated Robotic Systems",
      "authors": [
        "Yihao Liu",
        "Mehran Armand"
      ],
      "published": "2024-03-21T00:14:53Z",
      "categories": "",
      "summary": "The rapid development of generative technology opens up possibility for higher level of automation, and artificial intelligence (AI) embodiment in robotic systems is imminent. However, due to the blackbox nature of the generative technology, the generation of the knowledge and workflow scheme is uncontrolled, especially in a dynamic environment and a complex scene. This poses challenges to regulations in safety-demanding applications such as medical scenes. We argue that the unregulated generative processes from AI is fitted for low level end tasks, but intervention in the form of manual or au...",
      "pdf_url": "https://arxiv.org/pdf/2403.14049v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2308.01220v1",
      "title": "Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case",
      "authors": [
        "Rebekka G\u00f6rge",
        "Elena Haedecke",
        "Michael Mock"
      ],
      "published": "2023-08-02T15:26:08Z",
      "categories": "",
      "summary": "Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performanceand data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyse the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep ne...",
      "pdf_url": "https://arxiv.org/pdf/2308.01220v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2402.18139v3",
      "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
      "authors": [
        "Swagata Ashwani",
        "Kshiteesh Hegde",
        "Nishith Reddy Mannuru",
        "Mayank Jindal",
        "Dushyant Singh Sengar",
        "Krishna Chaitanya Rao Kathala",
        "Dishant Banga",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "published": "2024-02-28T08:02:14Z",
      "categories": "",
      "summary": "With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The propose...",
      "pdf_url": "https://arxiv.org/pdf/2402.18139v3.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.20437v1",
      "title": "On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating",
      "authors": [
        "Michael Widener",
        "Kausik Lakkaraju",
        "John Aydin",
        "Biplav Srivastava"
      ],
      "published": "2025-08-28T05:27:45Z",
      "categories": "",
      "summary": "Time-series forecasting models (TSFM) have evolved from classical statistical methods to sophisticated foundation models, yet understanding why and when these models succeed or fail remains challenging. Despite this known limitation, time series forecasting models are increasingly used to generate information that informs real-world actions with equally real consequences. Understanding the complexity, performance variability, and opaque nature of these models then becomes a valuable endeavor to combat serious concerns about how users should interact with and rely on these models' outputs. This...",
      "pdf_url": "https://arxiv.org/pdf/2508.20437v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1911.01917v1",
      "title": "Scenarios and Recommendations for Ethical Interpretive AI",
      "authors": [
        "John Licato",
        "Zaid Marji",
        "Sophia Abraham"
      ],
      "published": "2019-11-05T16:23:01Z",
      "categories": "",
      "summary": "Artificially intelligent systems, given a set of non-trivial ethical rules to follow, will inevitably be faced with scenarios which call into question the scope of those rules. In such cases, human reasoners typically will engage in interpretive reasoning, where interpretive arguments are used to support or attack claims that some rule should be understood a certain way. Artificially intelligent reasoners, however, currently lack the ability to carry out human-like interpretive reasoning, and we argue that bridging this gulf is of tremendous importance to human-centered AI. In order to better ...",
      "pdf_url": "https://arxiv.org/pdf/1911.01917v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1909.12838v2",
      "title": "Responsible AI by Design in Practice",
      "authors": [
        "Richard Benjamins",
        "Alberto Barbado",
        "Daniel Sierra"
      ],
      "published": "2019-09-27T16:28:01Z",
      "categories": "",
      "summary": "Recently, a lot of attention has been given to undesired consequences of Artificial Intelligence (AI), such as unfair bias leading to discrimination, or the lack of explanations of the results of AI systems. There are several important questions to answer before AI can be deployed at scale in our businesses and societies. Most of these issues are being discussed by experts and the wider communities, and it seems there is broad consensus on where they come from. There is, however, less consensus on, and experience with how to practically deal with those issues in organizations that develop and ...",
      "pdf_url": "https://arxiv.org/pdf/1909.12838v2.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2111.05391v1",
      "title": "Statistical Perspectives on Reliability of Artificial Intelligence Systems",
      "authors": [
        "Yili Hong",
        "Jiayi Lian",
        "Li Xu",
        "Jie Min",
        "Yueyao Wang",
        "Laura J. Freeman",
        "Xinwei Deng"
      ],
      "published": "2021-11-09T20:00:14Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) systems have become increasingly popular in many areas. Nevertheless, AI technologies are still in their developing stages, and many issues need to be addressed. Among those, the reliability of AI systems needs to be demonstrated so that the AI systems can be used with confidence by the general public. In this paper, we provide statistical perspectives on the reliability of AI systems. Different from other considerations, the reliability of AI systems focuses on the time dimension. That is, the system can perform its designed functionality for the intended period. ...",
      "pdf_url": "https://arxiv.org/pdf/2111.05391v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2411.05270v1",
      "title": "Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination Detection Systems",
      "authors": [
        "Alexander Thomas",
        "Seth Rosen",
        "Vishnu Vettrivel"
      ],
      "published": "2024-11-08T02:06:41Z",
      "categories": "",
      "summary": "This paper presents a comparative analysis of hallucination detection systems for AI, focusing on automatic summarization and question answering tasks for Large Language Models (LLMs). We evaluate different hallucination detection systems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics. Our results indicate that although advanced models can perform better they come at a much higher cost. We also demonstrate how an ideal hallucination detection system needs to maintain performance across different model sizes. Our findings highlight the importance of choosing a detection sy...",
      "pdf_url": "https://arxiv.org/pdf/2411.05270v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2510.06025v1",
      "title": "Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers",
      "authors": [
        "Kevin Raina",
        "Tanya Schmah"
      ],
      "published": "2025-10-07T15:23:05Z",
      "categories": "",
      "summary": "Out-of-Distribution (OOD) detection is critical to AI reliability and safety, yet in many practical settings, only a limited amount of training data is available. Bayesian Neural Networks (BNNs) are a promising class of model on which to base OOD detection, because they explicitly represent epistemic (i.e. model) uncertainty. In the small training data regime, BNNs are especially valuable because they can incorporate prior model information. We introduce a new family of Bayesian posthoc OOD scores based on expected logit vectors, and compare 5 Bayesian and 4 deterministic posthoc OOD scores. E...",
      "pdf_url": "https://arxiv.org/pdf/2510.06025v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.17645v1",
      "title": "A Modular Dataset to Demonstrate LLM Abstraction Capability",
      "authors": [
        "Adam Atanas",
        "Kai Liu"
      ],
      "published": "2025-03-22T04:25:30Z",
      "categories": "",
      "summary": "Large language models (LLMs) exhibit impressive capabilities but struggle with reasoning errors due to hallucinations and flawed logic. To investigate their internal representations of reasoning, we introduce ArrangementPuzzle, a novel puzzle dataset with structured solutions and automated stepwise correctness verification. We trained a classifier model on LLM activations on this dataset and found that it achieved over 80% accuracy in predicting reasoning correctness, implying that LLMs internally distinguish between correct and incorrect reasoning steps, with the strongest representations in ...",
      "pdf_url": "https://arxiv.org/pdf/2503.17645v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2503.06084v1",
      "title": "Exploring Interpretability for Visual Prompt Tuning with Hierarchical Concepts",
      "authors": [
        "Yubin Wang",
        "Xinyang Jiang",
        "De Cheng",
        "Xiangqian Zhao",
        "Zilong Wang",
        "Dongsheng Li",
        "Cairong Zhao"
      ],
      "published": "2025-03-08T06:12:50Z",
      "categories": "",
      "summary": "Visual prompt tuning offers significant advantages for adapting pre-trained visual foundation models to specific tasks. However, current research provides limited insight into the interpretability of this approach, which is essential for enhancing AI reliability and enabling AI-driven knowledge discovery. In this paper, rather than learning abstract prompt embeddings, we propose the first framework, named Interpretable Visual Prompt Tuning (IVPT), to explore interpretability for visual prompts, by introducing hierarchical concept prototypes. Specifically, visual prompts are linked to human-und...",
      "pdf_url": "https://arxiv.org/pdf/2503.06084v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2106.11345v1",
      "title": "Cogment: Open Source Framework For Distributed Multi-actor Training, Deployment & Operations",
      "authors": [
        "AI Redefined",
        "Sai Krishna Gottipati",
        "Sagar Kurandwad",
        "Clod\u00e9ric Mars",
        "Gregory Szriftgiser",
        "Fran\u00e7ois Chabot"
      ],
      "published": "2021-06-21T18:21:26Z",
      "categories": "",
      "summary": "Involving humans directly for the benefit of AI agents' training is getting traction thanks to several advances in reinforcement learning and human-in-the-loop learning. Humans can provide rewards to the agent, demonstrate tasks, design a curriculum, or act in the environment, but these benefits also come with architectural, functional design and engineering complexities. We present Cogment, a unifying open-source framework that introduces an actor formalism to support a variety of humans-agents collaboration typologies and training approaches. It is also scalable out of the box thanks to a di...",
      "pdf_url": "https://arxiv.org/pdf/2106.11345v1.pdf",
      "relevance_score": 12,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2307.13658v2",
      "title": "Towards an AI Accountability Policy",
      "authors": [
        "Przemyslaw Grabowicz",
        "Adrian Byrne",
        "Cyrus Cousins",
        "Nicholas Perello",
        "Yair Zick"
      ],
      "published": "2023-07-25T17:09:28Z",
      "categories": "",
      "summary": "We propose establishing an office to oversee AI systems by introducing a tiered system of explainability and benchmarking requirements for commercial AI systems. We examine how complex high-risk technologies have been successfully regulated at the national level. Specifically, we draw parallels to the existing regulation for the U.S. medical device industry and the pharmaceutical industry (regulated by the FDA), the proposed legislation for AI in the European Union (the AI Act), and the existing U.S. anti-discrimination legislation. To promote accountability and user trust, AI accountability m...",
      "pdf_url": "https://arxiv.org/pdf/2307.13658v2.pdf",
      "relevance_score": 12,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2401.10942v1",
      "title": "Machine Unlearning for Recommendation Systems: An Insight",
      "authors": [
        "Bhavika Sachdeva",
        "Harshita Rathee",
        " Sristi",
        "Arun Sharma",
        "Witold Wydma\u0144ski"
      ],
      "published": "2024-01-17T18:35:44Z",
      "categories": "",
      "summary": "This review explores machine unlearning (MUL) in recommendation systems, addressing adaptability, personalization, privacy, and bias challenges. Unlike traditional models, MUL dynamically adjusts system knowledge based on shifts in user preferences and ethical considerations. The paper critically examines MUL's basics, real-world applications, and challenges like algorithmic transparency. It sifts through literature, offering insights into how MUL could transform recommendations, discussing user trust, and suggesting paths for future research in responsible and user-focused artificial intellig...",
      "pdf_url": "https://arxiv.org/pdf/2401.10942v1.pdf",
      "relevance_score": 12,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2411.19576v2",
      "title": "On Explaining Recommendations with Large Language Models: A Review",
      "authors": [
        "Alan Said"
      ],
      "published": "2024-11-29T09:47:32Z",
      "categories": "",
      "summary": "The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has opened new opportunities for enhancing recommender systems through improved explainability. This paper provides a systematic literature review focused on leveraging LLMs to generate explanations for recommendations -- a critical aspect for fostering transparency and user trust. We conducted a comprehensive search within the ACM Guide to Computing Literature, covering publications from the launch of ChatGPT (November 2022) to the present (November 2024). Our search yielded 232 articles, but after applying inclusion criteri...",
      "pdf_url": "https://arxiv.org/pdf/2411.19576v2.pdf",
      "relevance_score": 12,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2309.15060v2",
      "title": "Constrained Deep Reinforcement Learning for Fronthaul Compression Optimization",
      "authors": [
        "Axel Gr\u00f6nland",
        "Alessio Russo",
        "Yassir Jedra",
        "Bleron Klaiqi",
        "Xavier Gelabert"
      ],
      "published": "2023-09-26T16:40:47Z",
      "categories": "",
      "summary": "In the Centralized-Radio Access Network (C-RAN) architecture, functions can be placed in the central or distributed locations. This architecture can offer higher capacity and cost savings but also puts strict requirements on the fronthaul (FH). Adaptive FH compression schemes that adapt the compression amount to varying FH traffic are promising approaches to deal with stringent FH requirements. In this work, we design such a compression scheme using a model-free off policy deep reinforcement learning algorithm which accounts for FH latency and packet loss constraints. Furthermore, this algorit...",
      "pdf_url": "https://arxiv.org/pdf/2309.15060v2.pdf",
      "relevance_score": 12,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.15763v1",
      "title": "Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications",
      "authors": [
        "Raymond K. Sheh",
        "Karen Geappen"
      ],
      "published": "2025-11-19T14:52:02Z",
      "categories": "",
      "summary": "Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.   W...",
      "pdf_url": "https://arxiv.org/pdf/2511.15763v1.pdf",
      "relevance_score": 12,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "1808.00089v2",
      "title": "Towards Composable Bias Rating of AI Services",
      "authors": [
        "Biplav Srivastava",
        "Francesca Rossi"
      ],
      "published": "2018-07-31T22:15:13Z",
      "categories": "",
      "summary": "A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance. However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf. A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias. Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about. The possibly biased ...",
      "pdf_url": "https://arxiv.org/pdf/1808.00089v2.pdf",
      "relevance_score": 10,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2412.15876v1",
      "title": "AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI",
      "authors": [
        "Katja B\u00fchler",
        "Thomas H\u00f6llt",
        "Thomas Schulz",
        "Pere-Pau V\u00e1zquez"
      ],
      "published": "2024-12-20T13:27:24Z",
      "categories": "",
      "summary": "AI is the workhorse of modern data analytics and omnipresent across many sectors. Large Language Models and multi-modal foundation models are today capable of generating code, charts, visualizations, etc. How will these massive developments of AI in data analytics shape future data visualizations and visual analytics workflows? What is the potential of AI to reshape methodology and design of future visual analytics applications? What will be our role as visualization researchers in the future? What are opportunities, open challenges and threats in the context of an increasingly powerful AI? Th...",
      "pdf_url": "https://arxiv.org/pdf/2412.15876v1.pdf",
      "relevance_score": 10,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.09178v1",
      "title": "Perspectives on a Reliability Monitoring Framework for Agentic AI Systems",
      "authors": [
        "Niclas Flehmig",
        "Mary Ann Lundteigen",
        "Shen Yin"
      ],
      "published": "2025-11-12T10:19:17Z",
      "categories": "",
      "summary": "The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operatio...",
      "pdf_url": "https://arxiv.org/pdf/2511.09178v1.pdf",
      "relevance_score": 10,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2504.13900v1",
      "title": "Supporting Students' Reading and Cognition with AI",
      "authors": [
        "Yue Fu",
        "Alexis Hiniker"
      ],
      "published": "2025-04-07T17:51:27Z",
      "categories": "",
      "summary": "With the rapid adoption of AI tools in learning contexts, it is vital to understand how these systems shape users' reading processes and cognitive engagement. We collected and analyzed text from 124 sessions with AI tools, in which students used these tools to support them as they read assigned readings for an undergraduate course. We categorized participants' prompts to AI according to Bloom's Taxonomy of educational objectives -- Remembering, Understanding, Applying, Analyzing, Evaluating. Our results show that ``Analyzing'' and ``Evaluating'' are more prevalent in users' second and third pr...",
      "pdf_url": "https://arxiv.org/pdf/2504.13900v1.pdf",
      "relevance_score": 8,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2601.00836v1",
      "title": "ESG Beliefs of Large Language Models: Evidence and Impact",
      "authors": [
        "Tong Li",
        "Luping Yu"
      ],
      "published": "2025-12-26T15:49:25Z",
      "categories": "",
      "summary": "We examine whether large language models (LLMs) hold systematic beliefs about environmental, social, and governance (ESG) issues and how these beliefs compare with-and potentially influence-those of human market participants. Based on established surveys originally administered to professional and retail investors, we show that major LLMs exhibit a strong pro-ESG orientation. Compared with human investors, LLMs assign greater financial relevance for ESG performance, expect larger return premia for high-ESG firms, and display a stronger willingness to sacrifice financial returns for ESG improve...",
      "pdf_url": "https://arxiv.org/pdf/2601.00836v1.pdf",
      "relevance_score": 8,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2203.04203v5",
      "title": "AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant",
      "authors": [
        "Benita Wong",
        "Joya Chen",
        "You Wu",
        "Stan Weixian Lei",
        "Dongxing Mao",
        "Difei Gao",
        "Mike Zheng Shou"
      ],
      "published": "2022-03-08T17:07:09Z",
      "categories": "",
      "summary": "A long-standing goal of intelligent assistants such as AR glasses/robots has been to assist users in affordance-centric real-world scenarios, such as \"how can I run the microwave for 1 minute?\". However, there is still no clear task definition and suitable benchmarks. In this paper, we define a new task called Affordance-centric Question-driven Task Completion, where the AI assistant should learn from instructional videos to provide step-by-step help in the user's view. To support the task, we constructed AssistQ, a new dataset comprising 531 question-answer samples from 100 newly filmed instr...",
      "pdf_url": "https://arxiv.org/pdf/2203.04203v5.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2306.13509v3",
      "title": "Exploring AI-enhanced Shared Control for an Assistive Robotic Arm",
      "authors": [
        "Max Pascher",
        "Kirill Kronhardt",
        "Jan Freienstein",
        "Jens Gerken"
      ],
      "published": "2023-06-23T14:19:56Z",
      "categories": "",
      "summary": "Assistive technologies and in particular assistive robotic arms have the potential to enable people with motor impairments to live a self-determined life. More and more of these systems have become available for end users in recent years, such as the Kinova Jaco robotic arm. However, they mostly require complex manual control, which can overwhelm users. As a result, researchers have explored ways to let such robots act autonomously. However, at least for this specific group of users, such an approach has shown to be futile. Here, users want to stay in control to achieve a higher level of perso...",
      "pdf_url": "https://arxiv.org/pdf/2306.13509v3.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2306.10052v1",
      "title": "Assigning AI: Seven Approaches for Students, with Prompts",
      "authors": [
        "Ethan Mollick",
        "Lilach Mollick"
      ],
      "published": "2023-06-13T03:36:36Z",
      "categories": "",
      "summary": "This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical as...",
      "pdf_url": "https://arxiv.org/pdf/2306.10052v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2507.19960v2",
      "title": "What Does 'Human-Centred AI' Mean?",
      "authors": [
        "Olivia Guest"
      ],
      "published": "2025-07-26T14:18:52Z",
      "categories": "",
      "summary": "While it seems sensible that human-centred artificial intelligence (AI) means centring \"human behaviour and experience,\" it cannot be any other way. AI, I argue, is usefully seen as a relationship between technology and humans where it appears that artifacts can perform, to a greater or lesser extent, human cognitive labour. This is evinced using examples that juxtapose technology with cognition, inter alia: abacus versus mental arithmetic; alarm clock versus knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel definitions and analyses, sociotechnical relationships can...",
      "pdf_url": "https://arxiv.org/pdf/2507.19960v2.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2201.11441v1",
      "title": "Human-centered mechanism design with Democratic AI",
      "authors": [
        "Raphael Koster",
        "Jan Balaguer",
        "Andrea Tacchetti",
        "Ari Weinstein",
        "Tina Zhu",
        "Oliver Hauser",
        "Duncan Williams",
        "Lucy Campbell-Gillingham",
        "Phoebe Thacker",
        "Matthew Botvinick",
        "Christopher Summerfield"
      ],
      "published": "2022-01-27T10:56:33Z",
      "categories": "",
      "summary": "Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here, we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechan...",
      "pdf_url": "https://arxiv.org/pdf/2201.11441v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2203.04159v1",
      "title": "AI for Next Generation Computing: Emerging Trends and Future Directions",
      "authors": [
        "Sukhpal Singh Gill",
        "Minxian Xu",
        "Carlo Ottaviani",
        "Panos Patros",
        "Rami Bahsoon",
        "Arash Shaghaghi",
        "Muhammed Golec",
        "Vlado Stankovski",
        "Huaming Wu",
        "Ajith Abraham",
        "Manmeet Singh",
        "Harshit Mehta",
        "Soumya K. Ghosh",
        "Thar Baker",
        "Ajith Kumar Parlikad",
        "Hanan Lutfiyya",
        "Salil S. Kanhere",
        "Rizos Sakellariou",
        "Schahram Dustdar",
        "Omer Rana",
        "Ivona Brandic",
        "Steve Uhlig"
      ],
      "published": "2022-03-05T16:59:43Z",
      "categories": "",
      "summary": "Autonomic computing investigates how systems can achieve (user) specified control outcomes on their own, without the intervention of a human operator. Autonomic computing fundamentals have been substantially influenced by those of control theory for closed and open-loop systems. In practice, complex systems may exhibit a number of concurrent and inter-dependent control loops. Despite research into autonomic models for managing computer resources, ranging from individual resources (e.g., web servers) to a resource ensemble (e.g., multiple resources within a data center), research into integrati...",
      "pdf_url": "https://arxiv.org/pdf/2203.04159v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2508.19967v1",
      "title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models",
      "authors": [
        "Oliver Grainge",
        "Sania Waheed",
        "Jack Stilgoe",
        "Michael Milford",
        "Shoaib Ehsan"
      ],
      "published": "2025-08-27T15:21:31Z",
      "categories": "",
      "summary": "Geo-localization is the task of identifying the location of an image using visual cues alone. It has beneficial applications, such as improving disaster response, enhancing navigation, and geography education. Recently, Vision-Language Models (VLMs) are increasingly demonstrating capabilities as accurate image geo-locators. This brings significant privacy risks, including those related to stalking and surveillance, considering the widespread uses of AI models and sharing of photos on social media. The precision of these models is likely to improve in the future. Despite these risks, there is l...",
      "pdf_url": "https://arxiv.org/pdf/2508.19967v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    },
    {
      "arxiv_id": "2511.08082v1",
      "title": "Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency",
      "authors": [
        "Stella C. Dong"
      ],
      "published": "2025-11-11T10:33:54Z",
      "categories": "",
      "summary": "This paper develops a prudential framework for assessing the reliability of large language models (LLMs) in reinsurance. A five-pillar architecture--governance, data lineage, assurance, resilience, and regulatory alignment--translates supervisory expectations from Solvency II, SR 11-7, and guidance from EIOPA (2025), NAIC (2023), and IAIS (2024) into measurable lifecycle controls. The framework is implemented through the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB), which evaluates whether governance-embedded LLMs meet prudential standards for grounding, transparency, and accoun...",
      "pdf_url": "https://arxiv.org/pdf/2511.08082v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_h_collaboration"
    }
  ],
  "green_papers": [
    {
      "arxiv_id": "2310.19778v3",
      "title": "Human-AI collaboration is not very collaborative yet: A taxonomy of interaction patterns in AI-assisted decision making from a systematic review",
      "authors": [
        "Catalina Gomez",
        "Sue Min Cho",
        "Shichang Ke",
        "Chien-Ming Huang",
        "Mathias Unberath"
      ],
      "published": "2023-10-30T17:46:38Z",
      "categories": "",
      "summary": "Leveraging Artificial Intelligence (AI) in decision support systems has disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. A human-centered perspective attempts to alleviate this concern by designing AI solutions for seamless integration with existing processes. Determining what information AI should provide to aid humans is vital, a concept underscored by explainable AI's efforts to justify AI predictions. However, how the information is presented, e.g., the sequence of recommendations and solicitation ...",
      "pdf_url": "https://arxiv.org/pdf/2310.19778v3.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2310.19778v3_paper.pdf"
    },
    {
      "arxiv_id": "2412.07241v1",
      "title": "Human-Computer Interaction and Human-AI Collaboration in Advanced Air Mobility: A Comprehensive Review",
      "authors": [
        "Fatma Yamac Sagirli",
        "Xiaopeng Zhao",
        "Zhenbo Wang"
      ],
      "published": "2024-12-10T07:06:52Z",
      "categories": "",
      "summary": "The increasing rates of global urbanization and vehicle usage are leading to a shift of mobility to the third dimension-through Advanced Air Mobility (AAM)-offering a promising solution for faster, safer, cleaner, and more efficient transportation. As air transportation continues to evolve with more automated and autonomous systems, advancements in AAM require a deep understanding of human-computer interaction and human-AI collaboration to ensure safe and effective operations in complex urban and regional environments. There has been a significant increase in publications regarding these emerg...",
      "pdf_url": "https://arxiv.org/pdf/2412.07241v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2412.07241v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.06224v1",
      "title": "Exploring Human-AI Collaboration Using Mental Models of Early Adopters of Multi-Agent Generative AI Tools",
      "authors": [
        "Suchismita Naik",
        "Austin L. Toombs",
        "Amanda Snellinger",
        "Scott Saponas",
        "Amanda K. Hall"
      ],
      "published": "2025-09-10T05:35:38Z",
      "categories": "",
      "summary": "With recent advancements in multi-agent generative AI (Gen AI), technology organizations like Microsoft are adopting these complex tools, redefining AI agents as active collaborators in complex workflows rather than as passive tools. In this study, we investigated how early adopters and developers conceptualize multi-agent Gen AI tools, focusing on how they understand human-AI collaboration mechanisms, general collaboration dynamics, and transparency in the context of AI tools. We conducted semi-structured interviews with 13 developers, all early adopters of multi-agent Gen AI technology who w...",
      "pdf_url": "https://arxiv.org/pdf/2510.06224v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2510.06224v1_paper.pdf"
    },
    {
      "arxiv_id": "2208.07960v1",
      "title": "Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making",
      "authors": [
        "Kori Inkpen",
        "Shreya Chappidi",
        "Keri Mallari",
        "Besmira Nushi",
        "Divya Ramesh",
        "Pietro Michelucci",
        "Vani Mandava",
        "Libu\u0161e Hannah Vep\u0159ek",
        "Gabrielle Quinn"
      ],
      "published": "2022-08-16T21:39:58Z",
      "categories": "",
      "summary": "Human-AI collaboration for decision-making strives to achieve team performance that exceeds the performance of humans or AI alone. However, many factors can impact success of Human-AI teams, including a user's domain expertise, mental models of an AI system, trust in recommendations, and more. This work examines users' interaction with three simulated algorithmic models, all with similar accuracy but different tuning on their true positive and true negative rates. Our study examined user performance in a non-trivial blood vessel labeling task where participants indicated whether a given blood ...",
      "pdf_url": "https://arxiv.org/pdf/2208.07960v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2208.07960v1_paper.pdf"
    },
    {
      "arxiv_id": "2501.10909v1",
      "title": "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition",
      "authors": [
        "Gaole He",
        "Patrick Hemmer",
        "Michael V\u00f6ssing",
        "Max Schemmer",
        "Ujwal Gadiraju"
      ],
      "published": "2025-01-19T01:03:09Z",
      "categories": "",
      "summary": "In recent years, the rapid development of AI systems has brought about the benefits of intelligent services but also concerns about security and reliability. By fostering appropriate user reliance on an AI system, both complementary team performance and reduced human workload can be achieved. Previous empirical studies have extensively analyzed the impact of factors ranging from task, system, and human behavior on user trust and appropriate reliance in the context of one-step decision making. However, user reliance on AI systems in tasks with complex semantics that require multi-step workflows...",
      "pdf_url": "https://arxiv.org/pdf/2501.10909v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2501.10909v1_paper.pdf"
    },
    {
      "arxiv_id": "2509.16772v2",
      "title": "AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",
      "authors": [
        "Eason Chen",
        "Jeffrey Li",
        "Scarlett Huang",
        "Xinyi Tang",
        "Jionghao Lin",
        "Paulo Carvalho",
        "Kenneth Koedinger"
      ],
      "published": "2025-09-20T18:38:54Z",
      "categories": "",
      "summary": "We present an empirical study of how both experienced tutors and non-tutors judge the correctness of tutor praise responses under different Artificial Intelligence (AI)-assisted interfaces, types of explanation (textual explanations vs. inline highlighting). We first fine-tuned several Large Language Models (LLMs) to produce binary correctness labels and explanations, achieving up to 88% accuracy and 0.92 F1 score with GPT-4. We then let the GPT-4 models assist 95 participants in tutoring decision-making tasks by offering different types of explanations. Our findings show that although human-A...",
      "pdf_url": "https://arxiv.org/pdf/2509.16772v2.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2509.16772v2_paper.pdf"
    },
    {
      "arxiv_id": "2401.07058v1",
      "title": "Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making",
      "authors": [
        "Zhuoran Lu",
        "Dakuo Wang",
        "Ming Yin"
      ],
      "published": "2024-01-13T12:19:01Z",
      "categories": "",
      "summary": "AI assistance in decision-making has become popular, yet people's inappropriate reliance on AI often leads to unsatisfactory human-AI collaboration performance. In this paper, through three pre-registered, randomized human subject experiments, we explore whether and how the provision of {second opinions} may affect decision-makers' behavior and performance in AI-assisted decision-making. We find that if both the AI model's decision recommendation and a second opinion are always presented together, decision-makers reduce their over-reliance on AI while increase their under-reliance on AI, regar...",
      "pdf_url": "https://arxiv.org/pdf/2401.07058v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2401.07058v1_paper.pdf"
    },
    {
      "arxiv_id": "2509.21436v1",
      "title": "Position: Human Factors Reshape Adversarial Analysis in Human-AI Decision-Making Systems",
      "authors": [
        "Shutong Fan",
        "Lan Zhang",
        "Xiaoyong Yuan"
      ],
      "published": "2025-09-25T19:08:01Z",
      "categories": "",
      "summary": "As Artificial Intelligence (AI) increasingly supports human decision-making, its vulnerability to adversarial attacks grows. However, the existing adversarial analysis predominantly focuses on fully autonomous AI systems, where decisions are executed without human intervention. This narrow focus overlooks the complexities of human-AI collaboration, where humans interpret, adjust, and act upon AI-generated decisions. Trust, expectations, and cognitive behaviors influence how humans interact with AI, creating dynamic feedback loops that adversaries can exploit. To strengthen the robustness of AI...",
      "pdf_url": "https://arxiv.org/pdf/2509.21436v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2509.21436v1_paper.pdf"
    },
    {
      "arxiv_id": "2505.23397v2",
      "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
      "authors": [
        "Ahmad Mohsin",
        "Helge Janicke",
        "Ahmed Ibrahim",
        "Iqbal H. Sarker",
        "Seyit Camtepe"
      ],
      "published": "2025-05-29T12:35:08Z",
      "categories": "",
      "summary": "This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration. To address these limitations, we propose a novel autonomy tie...",
      "pdf_url": "https://arxiv.org/pdf/2505.23397v2.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2505.23397v2_paper.pdf"
    },
    {
      "arxiv_id": "2506.03707v1",
      "title": "My Advisor, Her AI and Me: Evidence from a Field Experiment on Human-AI Collaboration and Investment Decisions",
      "authors": [
        " Cathy",
        " Yang",
        "Kevin Bauer",
        "Xitong Li",
        "Oliver Hinz"
      ],
      "published": "2025-06-04T08:40:11Z",
      "categories": "",
      "summary": "Amid ongoing policy and managerial debates on keeping humans in the loop of AI decision-making, we investigate whether human involvement in AI-based service production benefits downstream consumers. Partnering with a large savings bank in Europe, we produced pure AI and human-AI collaborative investment advice, passed it to customers, and examined their advice-taking in a field experiment. On the production side, contrary to concerns that humans might inefficiently override AI output, we find that giving a human banker the final say over AI-generated financial advice does not compromise its qu...",
      "pdf_url": "https://arxiv.org/pdf/2506.03707v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2506.03707v1_paper.pdf"
    },
    {
      "arxiv_id": "2508.09033v1",
      "title": "Beyond Predictions: A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration",
      "authors": [
        "Tina Behzad",
        "Nikolos Gurney",
        "Ning Wang",
        "David V. Pynadath"
      ],
      "published": "2025-08-12T15:54:48Z",
      "categories": "",
      "summary": "The promise of human-AI teaming lies in humans and AI working together to achieve performance levels neither could accomplish alone. Effective communication between AI and humans is crucial for teamwork, enabling users to efficiently benefit from AI assistance. This paper investigates how AI communication impacts human-AI team performance. We examine AI explanations that convey an awareness of its strengths and limitations. To achieve this, we train a decision tree on the model's mistakes, allowing it to recognize and explain where and why it might err. Through a user study on an income predic...",
      "pdf_url": "https://arxiv.org/pdf/2508.09033v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2508.09033v1_paper.pdf"
    },
    {
      "arxiv_id": "2310.13544v1",
      "title": "A Diachronic Perspective on User Trust in AI under Uncertainty",
      "authors": [
        "Shehzaad Dhuliawala",
        "Vil\u00e9m Zouhar",
        "Mennatallah El-Assady",
        "Mrinmaya Sachan"
      ],
      "published": "2023-10-20T14:41:46Z",
      "categories": "",
      "summary": "In a human-AI collaboration, users build a mental model of the AI system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. Modern NLP systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. In order to build trustworthy AI, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. We study the evolution of user trust in response to these trust-eroding events using a betting game. We find that even a few inc...",
      "pdf_url": "https://arxiv.org/pdf/2310.13544v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2310.13544v1_paper.pdf"
    },
    {
      "arxiv_id": "2409.14377v1",
      "title": "To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance on AI Systems",
      "authors": [
        "Gaole He",
        "Abri Bharos",
        "Ujwal Gadiraju"
      ],
      "published": "2024-09-22T09:43:27Z",
      "categories": "",
      "summary": "Powerful predictive AI systems have demonstrated great potential in augmenting human decision making. Recent empirical work has argued that the vision for optimal human-AI collaboration requires 'appropriate reliance' of humans on AI systems. However, accurately estimating the trustworthiness of AI advice at the instance level is quite challenging, especially in the absence of performance feedback pertaining to the AI system. In practice, the performance disparity of machine learning models on out-of-distribution data makes the dataset-specific performance feedback unreliable in human-AI colla...",
      "pdf_url": "https://arxiv.org/pdf/2409.14377v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2409.14377v1_paper.pdf"
    },
    {
      "arxiv_id": "2501.16627v1",
      "title": "Engaging with AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision-Making",
      "authors": [
        "Zichen Chen",
        "Yunhao Luo",
        "Misha Sra"
      ],
      "published": "2025-01-28T02:03:00Z",
      "categories": "",
      "summary": "As reliance on AI systems for decision-making grows, it becomes critical to ensure that human users can appropriately balance trust in AI suggestions with their own judgment, especially in high-stakes domains like healthcare. However, human + AI teams have been shown to perform worse than AI alone, with evidence indicating automation bias as the reason for poorer performance, particularly because humans tend to follow AI's recommendations even when they are incorrect. In many existing human + AI systems, decision-making support is typically provided in the form of text explanations (XAI) to he...",
      "pdf_url": "https://arxiv.org/pdf/2501.16627v1.pdf",
      "relevance_score": 100,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2501.16627v1_paper.pdf"
    },
    {
      "arxiv_id": "2205.09696v1",
      "title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging",
      "authors": [
        "Riccardo Fogliato",
        "Shreya Chappidi",
        "Matthew Lungren",
        "Michael Fitzke",
        "Mark Parkinson",
        "Diane Wilson",
        "Paul Fisher",
        "Eric Horvitz",
        "Kori Inkpen",
        "Besmira Nushi"
      ],
      "published": "2022-05-19T16:59:25Z",
      "categories": "",
      "summary": "Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has ...",
      "pdf_url": "https://arxiv.org/pdf/2205.09696v1.pdf",
      "relevance_score": 99,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2205.09696v1_paper.pdf"
    },
    {
      "arxiv_id": "2404.01615v1",
      "title": "Collaborative human-AI trust (CHAI-T): A process framework for active management of trust in human-AI collaboration",
      "authors": [
        "Melanie J. McGrath",
        "Andreas Duenser",
        "Justine Lacey",
        "Cecile Paris"
      ],
      "published": "2024-04-02T03:39:06Z",
      "categories": "",
      "summary": "Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each. In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies. User trust is essential to creating and maintaining these collaborative relationships. Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecede...",
      "pdf_url": "https://arxiv.org/pdf/2404.01615v1.pdf",
      "relevance_score": 98,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2404.01615v1_paper.pdf"
    },
    {
      "arxiv_id": "2412.13405v2",
      "title": "What Human-Horse Interactions may Teach us About Effective Human-AI Interactions",
      "authors": [
        "Mohammad Hossein Jarrahi",
        "Stanley Ahalt"
      ],
      "published": "2024-12-18T00:39:16Z",
      "categories": "",
      "summary": "This article explores human-horse interactions as a metaphor for understanding and designing effective human-AI partnerships. Drawing on the long history of human collaboration with horses, we propose that AI, like horses, should complement rather than replace human capabilities. We move beyond traditional benchmarks such as the Turing test, which emphasize AI's ability to mimic human intelligence, and instead advocate for a symbiotic relationship where distinct intelligences enhance each other. We analyze key elements of human-horse relationships: trust, communication, and mutual adaptability...",
      "pdf_url": "https://arxiv.org/pdf/2412.13405v2.pdf",
      "relevance_score": 98,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2412.13405v2_paper.pdf"
    },
    {
      "arxiv_id": "2403.05911v2",
      "title": "Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning",
      "authors": [
        "Zana Bu\u00e7inca",
        "Siddharth Swaroop",
        "Amanda E. Paluch",
        "Susan A. Murphy",
        "Krzysztof Z. Gajos"
      ],
      "published": "2024-03-09T13:30:00Z",
      "categories": "",
      "summary": "Imagine if AI decision-support tools not only complemented our ability to make accurate decisions, but also improved our skills, boosted collaboration, and elevated the joy we derive from our tasks. Despite the potential to optimize a broad spectrum of such human-centric objectives, the design of current AI tools remains focused on decision accuracy alone. We propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize human-AI interaction for diverse objectives. RL can optimize such objectives by tailoring decision support, providing the ...",
      "pdf_url": "https://arxiv.org/pdf/2403.05911v2.pdf",
      "relevance_score": 97,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2403.05911v2_paper.pdf"
    },
    {
      "arxiv_id": "2204.13480v1",
      "title": "The Value of Measuring Trust in AI - A Socio-Technical System Perspective",
      "authors": [
        "Michaela Benk",
        "Suzanne Tolmeijer",
        "Florian von Wangenheim",
        "Andrea Ferrario"
      ],
      "published": "2022-04-28T13:13:48Z",
      "categories": "",
      "summary": "Building trust in AI-based systems is deemed critical for their adoption and appropriate use. Recent research has thus attempted to evaluate how various attributes of these systems affect user trust. However, limitations regarding the definition and measurement of trust in AI have hampered progress in the field, leading to results that are inconsistent or difficult to compare. In this work, we provide an overview of the main limitations in defining and measuring trust in AI. We focus on the attempt of giving trust in AI a numerical value and its utility in informing the design of real-world hu...",
      "pdf_url": "https://arxiv.org/pdf/2204.13480v1.pdf",
      "relevance_score": 97,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2204.13480v1_paper.pdf"
    },
    {
      "arxiv_id": "2311.03999v1",
      "title": "Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations",
      "authors": [
        "Lixiang Yan",
        "Vanessa Echeverria",
        "Gloria Fernandez Nieto",
        "Yueqiao Jin",
        "Zachari Swiecki",
        "Linxuan Zhao",
        "Dragan Ga\u0161evi\u0107",
        "Roberto Martinez-Maldonado"
      ],
      "published": "2023-11-07T13:54:56Z",
      "categories": "",
      "summary": "Generative artificial intelligence (GenAI) offers promising potential for advancing human-AI collaboration in qualitative research. However, existing works focused on conventional machine-learning and pattern-based AI systems, and little is known about how researchers interact with GenAI in qualitative research. This work delves into researchers' perceptions of their collaboration with GenAI, specifically ChatGPT. Through a user study involving ten qualitative researchers, we found ChatGPT to be a valuable collaborator for thematic analysis, enhancing coding efficiency, aiding initial data exp...",
      "pdf_url": "https://arxiv.org/pdf/2311.03999v1.pdf",
      "relevance_score": 93,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2311.03999v1_paper.pdf"
    },
    {
      "arxiv_id": "2502.13321v1",
      "title": "Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance",
      "authors": [
        "Tejas Srinivasan",
        "Jesse Thomason"
      ],
      "published": "2025-02-18T22:42:39Z",
      "categories": "",
      "summary": "Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance. For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. In two decision-making scenarios -- laypeople answering science questions and doctors making medical diagnoses -- we find that providing su...",
      "pdf_url": "https://arxiv.org/pdf/2502.13321v1.pdf",
      "relevance_score": 91,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2502.13321v1_paper.pdf"
    },
    {
      "arxiv_id": "2503.05926v1",
      "title": "What's So Human about Human-AI Collaboration, Anyway? Generative AI and Human-Computer Interaction",
      "authors": [
        "Elizabeth Anne Watkins",
        "Emanuel Moss",
        "Giuseppe Raffa",
        "Lama Nachman"
      ],
      "published": "2025-03-07T20:48:18Z",
      "categories": "",
      "summary": "While human-AI collaboration has been a longstanding goal and topic of study for computational research, the emergence of increasingly naturalistic generative AI language models has greatly inflected the trajectory of such research. In this paper we identify how, given the language capabilities of generative AI, common features of human-human collaboration derived from the social sciences can be applied to the study of human-computer interaction. We provide insights drawn from interviews with industry personnel working on building human-AI collaboration systems, as well as our collaborations w...",
      "pdf_url": "https://arxiv.org/pdf/2503.05926v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2503.05926v1_paper.pdf"
    },
    {
      "arxiv_id": "2204.13217v2",
      "title": "Understanding User Perceptions, Collaborative Experience and User Engagement in Different Human-AI Interaction Designs for Co-Creative Systems",
      "authors": [
        "Jeba Rezwana",
        "Mary Lou Maher"
      ],
      "published": "2022-04-27T22:37:44Z",
      "categories": "",
      "summary": "Human-AI co-creativity involves humans and AI collaborating on a shared creative product as partners. In a creative collaboration, communication is an essential component among collaborators. In many existing co-creative systems users can communicate with the AI, usually using buttons or sliders. Typically, the AI in co-creative systems cannot communicate back to humans, limiting their potential to be perceived as partners rather than just a tool. This paper presents a study with 38 participants to explore the impact of two interaction designs, with and without AI-to-human communication, on us...",
      "pdf_url": "https://arxiv.org/pdf/2204.13217v2.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2204.13217v2_paper.pdf"
    },
    {
      "arxiv_id": "2003.02622v1",
      "title": "Towards Effective Human-AI Collaboration in GUI-Based Interactive Task Learning Agents",
      "authors": [
        "Toby Jia-Jun Li",
        "Jingya Chen",
        "Tom M. Mitchell",
        "Brad A. Myers"
      ],
      "published": "2020-03-05T14:12:19Z",
      "categories": "",
      "summary": "We argue that a key challenge in enabling usable and useful interactive task learning for intelligent agents is to facilitate effective Human-AI collaboration. We reflect on our past 5 years of efforts on designing, developing and studying the SUGILITE system, discuss the issues on incorporating recent advances in AI with HCI principles in mixed-initiative interactions and multi-modal interactions, and summarize the lessons we learned. Lastly, we identify several challenges and opportunities, and describe our ongoing work",
      "pdf_url": "https://arxiv.org/pdf/2003.02622v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2003.02622v1_paper.pdf"
    },
    {
      "arxiv_id": "2502.12443v2",
      "title": "TherAIssist: Assisting Art Therapy Homework and Client-Practitioner Collaboration through Human-AI Interaction",
      "authors": [
        "Di Liu",
        "Jingwen Bai",
        "Zhuoyi Zhang",
        "Yilin Zhang",
        "Zhenhao Zhang",
        "Jian Zhao",
        "Pengcheng An"
      ],
      "published": "2025-02-18T02:26:12Z",
      "categories": "",
      "summary": "Art therapy homework is essential for fostering clients' reflection on daily experiences between sessions. However, current practices present challenges: clients often lack guidance for completing tasks that combine art-making and verbal expression, while therapists find it difficult to track and tailor homework. How HCI systems might support art therapy homework remains underexplored. To address this, we present TherAIssist, comprising a client-facing application leveraging human-AI co-creative art-making and conversational agents to facilitate homework, and a therapist-facing application ena...",
      "pdf_url": "https://arxiv.org/pdf/2502.12443v2.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2502.12443v2_paper.pdf"
    },
    {
      "arxiv_id": "2505.22477v1",
      "title": "Human-Centered Human-AI Collaboration (HCHAC)",
      "authors": [
        "Qi Gao",
        "Wei Xu",
        "Hanxi Pan",
        "Mowei Shen",
        "Zaifeng Gao"
      ],
      "published": "2025-05-28T15:27:52Z",
      "categories": "",
      "summary": "In the intelligent era, the interaction between humans and intelligent systems fundamentally involves collaboration with autonomous intelligent agents. Human-AI Collaboration (HAC) represents a novel type of human-machine relationship facilitated by autonomous intelligent machines equipped with AI technologies. In this paradigm, AI agents serve not only as auxiliary tools but also as active teammates, partnering with humans to accomplish tasks collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical leadership roles in the collaboration. This human-led collaboration impar...",
      "pdf_url": "https://arxiv.org/pdf/2505.22477v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2505.22477v1_paper.pdf"
    },
    {
      "arxiv_id": "2508.10919v1",
      "title": "Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?",
      "authors": [
        "Mohammed Saqr",
        "Kamila Misiejuk",
        "Sonsoles L\u00f3pez-Pernas"
      ],
      "published": "2025-08-03T11:43:01Z",
      "categories": "",
      "summary": "While research on human-AI collaboration exists, it mainly examined language learning and used traditional counting methods with little attention to evolution and dynamics of collaboration on cognitively demanding tasks. This study examines human-AI interactions while solving a complex problem. Student-AI interactions were qualitatively coded and analyzed with transition network analysis, sequence analysis and partial correlation networks as well as comparison of frequencies using chi-square and Person-residual shaded Mosaic plots to map interaction patterns, their evolution, and their relatio...",
      "pdf_url": "https://arxiv.org/pdf/2508.10919v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2508.10919v1_paper.pdf"
    },
    {
      "arxiv_id": "2306.12843v1",
      "title": "Critical-Reflective Human-AI Collaboration: Exploring Computational Tools for Art Historical Image Retrieval",
      "authors": [
        "Katrin Glinka",
        "Claudia M\u00fcller-Birn"
      ],
      "published": "2023-06-22T12:29:56Z",
      "categories": "",
      "summary": "Just as other disciplines, the humanities explore how computational research approaches and tools can meaningfully contribute to scholarly knowledge production. We approach the design of computational tools through the analytical lens of 'human-AI collaboration.' However, there is no generalizable concept of what constitutes 'meaningful' human-AI collaboration. In terms of genuinely human competencies, we consider criticality and reflection as guiding principles of scholarly knowledge production. Although (designing for) reflection is a recurring topic in CSCW and HCI discourses, it has not be...",
      "pdf_url": "https://arxiv.org/pdf/2306.12843v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2306.12843v1_paper.pdf"
    },
    {
      "arxiv_id": "2505.16023v3",
      "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild",
      "authors": [
        "Sheshera Mysore",
        "Debarati Das",
        "Hancheng Cao",
        "Bahareh Sarrafzadeh"
      ],
      "published": "2025-05-21T21:13:01Z",
      "categories": "",
      "summary": "As large language models (LLMs) are used in complex writing workflows, users engage in multi-turn interactions to steer generations to better fit their needs. Rather than passively accepting output, users actively refine, explore, and co-construct text. We conduct a large-scale analysis of this collaborative behavior for users engaged in writing tasks in the wild with two popular AI assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task classification or satisfaction estimation common in prior work and instead characterizes how users interact with LLMs through the course o...",
      "pdf_url": "https://arxiv.org/pdf/2505.16023v3.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2505.16023v3_paper.pdf"
    },
    {
      "arxiv_id": "2309.12368v2",
      "title": "Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis",
      "authors": [
        "Shao Zhang",
        "Jianing Yu",
        "Xuhai Xu",
        "Changchang Yin",
        "Yuxuan Lu",
        "Bingsheng Yao",
        "Melanie Tory",
        "Lace M. Padilla",
        "Jeffrey Caterino",
        "Ping Zhang",
        "Dakuo Wang"
      ],
      "published": "2023-09-17T19:19:39Z",
      "categories": "",
      "summary": "Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module ...",
      "pdf_url": "https://arxiv.org/pdf/2309.12368v2.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2309.12368v2_paper.pdf"
    },
    {
      "arxiv_id": "2507.14034v1",
      "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors",
      "authors": [
        "Jochen Wulf",
        "Jurg Meierhofer",
        "Frank Hannich"
      ],
      "published": "2025-07-18T16:06:03Z",
      "categories": "",
      "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organ...",
      "pdf_url": "https://arxiv.org/pdf/2507.14034v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2507.14034v1_paper.pdf"
    },
    {
      "arxiv_id": "2507.18374v1",
      "title": "Towards Effective Human-in-the-Loop Assistive AI Agents",
      "authors": [
        "Filippos Bellos",
        "Yayuan Li",
        "Cary Shu",
        "Ruey Day",
        "Jeffrey M. Siskind",
        "Jason J. Corso"
      ],
      "published": "2025-07-24T12:50:46Z",
      "categories": "",
      "summary": "Effective human-AI collaboration for physical task completion has significant potential in both everyday activities and professional domains. AI agents equipped with informative guidance can enhance human performance, but evaluating such collaboration remains challenging due to the complexity of human-in-the-loop interactions. In this work, we introduce an evaluation framework and a multimodal dataset of human-AI interactions designed to assess how AI guidance affects procedural task performance, error reduction and learning outcomes. Besides, we develop an augmented reality (AR)-equipped AI a...",
      "pdf_url": "https://arxiv.org/pdf/2507.18374v1.pdf",
      "relevance_score": 89,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2507.18374v1_paper.pdf"
    },
    {
      "arxiv_id": "2509.24718v1",
      "title": "Understanding Collaboration between Professional Designers and Decision-making AI: A Case Study in the Workplace",
      "authors": [
        "Nami Ogawa",
        "Yuki Okafuji",
        "Yuji Hatada",
        "Jun Baba"
      ],
      "published": "2025-09-29T12:46:13Z",
      "categories": "",
      "summary": "The rapid development of artificial intelligence (AI) has fundamentally transformed creative work practices in the design industry. Existing studies have identified both opportunities and challenges for creative practitioners in their collaboration with generative AI and explored ways to facilitate effective human-AI co-creation. However, there is still a limited understanding of designers' collaboration with AI that supports creative processes distinct from generative AI. To address these gaps, this study focuses on understanding designers' collaboration with decision-making AI, which support...",
      "pdf_url": "https://arxiv.org/pdf/2509.24718v1.pdf",
      "relevance_score": 86,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2509.24718v1_paper.pdf"
    },
    {
      "arxiv_id": "2409.15814v1",
      "title": "Interactive Example-based Explanations to Improve Health Professionals' Onboarding with AI for Human-AI Collaborative Decision Making",
      "authors": [
        "Min Hun Lee",
        "Renee Bao Xuan Ng",
        "Silvana Xinyi Choo",
        "Shamala Thilarajah"
      ],
      "published": "2024-09-24T07:20:09Z",
      "categories": "",
      "summary": "A growing research explores the usage of AI explanations on user's decision phases for human-AI collaborative decision-making. However, previous studies found the issues of overreliance on `wrong' AI outputs. In this paper, we propose interactive example-based explanations to improve health professionals' onboarding with AI for their better reliance on AI during AI-assisted decision-making. We implemented an AI-based decision support system that utilizes a neural network to assess the quality of post-stroke survivors' exercises and interactive example-based explanations that systematically sur...",
      "pdf_url": "https://arxiv.org/pdf/2409.15814v1.pdf",
      "relevance_score": 85,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2409.15814v1_paper.pdf"
    },
    {
      "arxiv_id": "2403.16812v2",
      "title": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
      "authors": [
        "Shuai Ma",
        "Qiaoyi Chen",
        "Xinru Wang",
        "Chengbo Zheng",
        "Zhenhui Peng",
        "Ming Yin",
        "Xiaojuan Ma"
      ],
      "published": "2024-03-25T14:34:06Z",
      "categories": "",
      "summary": "In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion eli...",
      "pdf_url": "https://arxiv.org/pdf/2403.16812v2.pdf",
      "relevance_score": 85,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2403.16812v2_paper.pdf"
    },
    {
      "arxiv_id": "2112.06751v2",
      "title": "Role of Human-AI Interaction in Selective Prediction",
      "authors": [
        "Elizabeth Bondi",
        "Raphael Koster",
        "Hannah Sheahan",
        "Martin Chadwick",
        "Yoram Bachrach",
        "Taylan Cemgil",
        "Ulrich Paquet",
        "Krishnamurthy Dvijotham"
      ],
      "published": "2021-12-13T16:03:13Z",
      "categories": "",
      "summary": "Recent work has shown the potential benefit of selective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation. However, most prior work assumes that human behavior remains unchanged when they solve a prediction task as part of a human-AI team as opposed to by themselves. We show that this is not the case by performing experiments to quantify human-AI interaction in the context of selective prediction. In particular, we study the impac...",
      "pdf_url": "https://arxiv.org/pdf/2112.06751v2.pdf",
      "relevance_score": 85,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2112.06751v2_paper.pdf"
    },
    {
      "arxiv_id": "2510.08104v1",
      "title": "Development of Mental Models in Human-AI Collaboration: A Conceptual Framework",
      "authors": [
        "Joshua Holstein",
        "Gerhard Satzger"
      ],
      "published": "2025-10-09T11:40:41Z",
      "categories": "",
      "summary": "Artificial intelligence has become integral to organizational decision-making and while research has explored many facets of this human-AI collaboration, the focus has mainly been on designing the AI agent(s) and the way the collaboration is set up - generally assuming a human decision-maker to be \"fixed\". However, it has largely been neglected that decision-makers' mental models evolve through their continuous interaction with AI systems. This paper addresses this gap by conceptualizing how the design of human-AI collaboration influences the development of three complementary and interdepende...",
      "pdf_url": "https://arxiv.org/pdf/2510.08104v1.pdf",
      "relevance_score": 84,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2510.08104v1_paper.pdf"
    },
    {
      "arxiv_id": "2506.05370v1",
      "title": "Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems",
      "authors": [
        "Kristy Wedel"
      ],
      "published": "2025-05-28T18:59:16Z",
      "categories": "",
      "summary": "A critical challenge remains unresolved as generative AI systems are quickly implemented in various organizational settings. Despite significant advances in memory components such as RAG, vector stores, and LLM agents, these systems still have substantial memory limitations. Gen AI workflows rarely store or reflect on the full context in which decisions are made. This leads to repeated errors and a general lack of clarity. This paper introduces Contextual Memory Intelligence (CMI) as a new foundational paradigm for building intelligent systems. It repositions memory as an adaptive infrastructu...",
      "pdf_url": "https://arxiv.org/pdf/2506.05370v1.pdf",
      "relevance_score": 84,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2506.05370v1_paper.pdf"
    },
    {
      "arxiv_id": "2401.05612v2",
      "title": "Designing for Appropriate Reliance: The Roles of AI Uncertainty Presentation, Initial User Decision, and User Demographics in AI-Assisted Decision-Making",
      "authors": [
        "Shiye Cao",
        "Anqi Liu",
        "Chien-Ming Huang"
      ],
      "published": "2024-01-11T01:28:43Z",
      "categories": "",
      "summary": "Appropriate reliance is critical to achieving synergistic human-AI collaboration. For instance, when users over-rely on AI assistance, their human-AI team performance is bounded by the model's capability. This work studies how the presentation of model uncertainty may steer users' decision-making toward fostering appropriate reliance. Our results demonstrate that showing the calibrated model uncertainty alone is inadequate. Rather, calibrating model uncertainty and presenting it in a frequency format allow users to adjust their reliance accordingly and help reduce the effect of confirmation bi...",
      "pdf_url": "https://arxiv.org/pdf/2401.05612v2.pdf",
      "relevance_score": 82,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2401.05612v2_paper.pdf"
    },
    {
      "arxiv_id": "2304.08804v4",
      "title": "AI Reliance and Decision Quality: Fundamentals, Interdependence, and the Effects of Interventions",
      "authors": [
        "Jakob Schoeffer",
        "Johannes Jakubik",
        "Michael Voessing",
        "Niklas Kuehl",
        "Gerhard Satzger"
      ],
      "published": "2023-04-18T08:08:05Z",
      "categories": "",
      "summary": "In AI-assisted decision-making, a central promise of having a human-in-the-loop is that they should be able to complement the AI system by overriding its wrong recommendations. In practice, however, we often see that humans cannot assess the correctness of AI recommendations and, as a result, adhere to wrong or override correct advice. Different ways of relying on AI recommendations have immediate, yet distinct, implications for decision quality. Unfortunately, reliance and decision quality are often inappropriately conflated in the current literature on AI-assisted decision-making. In this wo...",
      "pdf_url": "https://arxiv.org/pdf/2304.08804v4.pdf",
      "relevance_score": 82,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2304.08804v4_paper.pdf"
    },
    {
      "arxiv_id": "2411.00998v1",
      "title": "Automation Bias in AI-Assisted Medical Decision-Making under Time Pressure in Computational Pathology",
      "authors": [
        "Emely Rosbach",
        "Jonathan Ganz",
        "Jonas Ammeling",
        "Andreas Riener",
        "Marc Aubreville"
      ],
      "published": "2024-11-01T19:46:55Z",
      "categories": "",
      "summary": "Artificial intelligence (AI)-based clinical decision support systems (CDSS) promise to enhance diagnostic accuracy and efficiency in computational pathology. However, human-AI collaboration might introduce automation bias, where users uncritically follow automated cues. This bias may worsen when time pressure strains practitioners' cognitive resources. We quantified automation bias by measuring the adoption of negative system consultations and examined the role of time pressure in a web-based experiment, where trained pathology experts (n=28) estimated tumor cell percentages. Our results indic...",
      "pdf_url": "https://arxiv.org/pdf/2411.00998v1.pdf",
      "relevance_score": 80,
      "dimension": "h_human_ai_collaboration",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_h_collaboration/pdfs/2411.00998v1_paper.pdf"
    }
  ],
  "dimension": "h_human_ai_collaboration",
  "downloaded_papers": 41
}