{
  "cluster": "cluster_i_threats",
  "execution_date": "2026-01-11T09:14:45.687838",
  "total_papers": 288,
  "green_papers_count": 56,
  "papers": [
    {
      "arxiv_id": "2510.11837v1",
      "title": "Countermind: A Multi-Layered Security Architecture for Large Language Models",
      "authors": [
        "Dominik Schwarz"
      ],
      "published": "2025-10-13T18:41:18Z",
      "categories": "",
      "summary": "The security of Large Language Model (LLM) applications is fundamentally challenged by \"form-first\" attacks like prompt injection and jailbreaking, where malicious instructions are embedded within user inputs. Conventional defenses, which rely on post hoc output filtering, are often brittle and fail to address the root cause: the model's inability to distinguish trusted instructions from untrusted data. This paper proposes Countermind, a multi-layered security architecture intended to shift defenses from a reactive, post hoc posture to a proactive, pre-inference, and intra-inference enforcemen...",
      "pdf_url": "https://arxiv.org/pdf/2510.11837v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.11837v1_paper.pdf"
    },
    {
      "arxiv_id": "2504.19521v4",
      "title": "Security Steerability is All You Need",
      "authors": [
        "Itay Hazan",
        "Idan Habler",
        "Ron Bitton",
        "Itsik Mantin"
      ],
      "published": "2025-04-28T06:40:01Z",
      "categories": "",
      "summary": "The adoption of Generative AI (GenAI) in applications inevitably comes with the expansion of the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate the GenAI related security threats by developing evaluation methods and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. 'How to build a bomb'), there is significantly less discussion on application-level security and how to evaluate and mitigate it. Thus, in this work we adopt an application-c...",
      "pdf_url": "https://arxiv.org/pdf/2504.19521v4.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2504.19521v4_paper.pdf"
    },
    {
      "arxiv_id": "2510.00451v1",
      "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "published": "2025-10-01T03:05:07Z",
      "categories": "",
      "summary": "Large language models have gained widespread prominence, yet their vulnerability to prompt injection and other adversarial attacks remains a critical concern. This paper argues for a security-by-design AI paradigm that proactively mitigates LLM vulnerabilities while enhancing performance. To achieve this, we introduce PromptShield, an ontology-driven framework that ensures deterministic and secure prompt interactions. It standardizes user inputs through semantic validation, eliminating ambiguity and mitigating adversarial manipulation. To assess PromptShield's security and performance capabili...",
      "pdf_url": "https://arxiv.org/pdf/2510.00451v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.00451v1_paper.pdf"
    },
    {
      "arxiv_id": "2504.18575v3",
      "title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks",
      "authors": [
        "Ivan Evtimov",
        "Arman Zharmagambetov",
        "Aaron Grattafiori",
        "Chuan Guo",
        "Kamalika Chaudhuri"
      ],
      "published": "2025-04-22T17:51:03Z",
      "categories": "",
      "summary": "Autonomous UI agents powered by AI have tremendous potential to boost human productivity by automating routine tasks such as filing taxes and paying bills. However, a major challenge in unlocking their full potential is security, which is exacerbated by the agent's ability to take action on their user's behalf. Existing tests for prompt injections in web agents either over-simplify the threat by testing unrealistic scenarios or giving the attacker too much power, or look at single-step isolated tasks. To more accurately measure progress for secure web agents, we introduce WASP -- a new publicl...",
      "pdf_url": "https://arxiv.org/pdf/2504.18575v3.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2504.18575v3_paper.pdf"
    },
    {
      "arxiv_id": "2507.02735v2",
      "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "David Wagner",
        "Chuan Guo"
      ],
      "published": "2025-07-03T15:47:13Z",
      "categories": "",
      "summary": "Prompt injection attack has been listed as the top-1 security threat to LLM-integrated applications, which interact with external environment data for complex tasks. The untrusted data may contain an injected prompt trying to arbitrarily manipulate the system. Model-level prompt injection defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source secure models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigating...",
      "pdf_url": "https://arxiv.org/pdf/2507.02735v2.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2507.02735v2_paper.pdf"
    },
    {
      "arxiv_id": "2511.04508v1",
      "title": "Large Language Models for Cyber Security",
      "authors": [
        "Raunak Somani",
        "Aswani Kumar Cherukuri"
      ],
      "published": "2025-11-06T16:25:35Z",
      "categories": "",
      "summary": "This paper studies the integration off Large Language Models into cybersecurity tools and protocols. The main issue discussed in this paper is how traditional rule-based and signature based security systems are not enough to deal with modern AI powered cyber threats. Cybersecurity industry is changing as threats are becoming more dangerous and adaptive in nature by levering the features provided by AI tools. By integrating LLMs into these tools and protocols, make the systems scalable, context-aware and intelligent. Thus helping it to mitigate these evolving cyber threats. The paper studies th...",
      "pdf_url": "https://arxiv.org/pdf/2511.04508v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.04508v1_paper.pdf"
    },
    {
      "arxiv_id": "2507.05445v1",
      "title": "A Systematization of Security Vulnerabilities in Computer Use Agents",
      "authors": [
        "Daniel Jones",
        "Giorgio Severi",
        "Martin Pouliot",
        "Gary Lopez",
        "Joris de Gruyter",
        "Santiago Zanella-Beguelin",
        "Justin Song",
        "Blake Bullwinkel",
        "Pamela Cortez",
        "Amanda Minnich"
      ],
      "published": "2025-07-07T19:50:21Z",
      "categories": "",
      "summary": "Computer Use Agents (CUAs), autonomous systems that interact with software interfaces via browsers or virtual machines, are rapidly being deployed in consumer and enterprise environments. These agents introduce novel attack surfaces and trust boundaries that are not captured by traditional threat models. Despite their growing capabilities, the security boundaries of CUAs remain poorly understood. In this paper, we conduct a systematic threat analysis and testing of real-world CUAs under adversarial conditions. We identify seven classes of risks unique to the CUA paradigm, and analyze three con...",
      "pdf_url": "https://arxiv.org/pdf/2507.05445v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2507.05445v1_paper.pdf"
    },
    {
      "arxiv_id": "2506.08837v3",
      "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
      "authors": [
        "Luca Beurer-Kellner",
        "Beat Buesser",
        "Ana-Maria Cre\u0163u",
        "Edoardo Debenedetti",
        "Daniel Dobos",
        "Daniel Fabian",
        "Marc Fischer",
        "David Froelicher",
        "Kathrin Grosse",
        "Daniel Naeff",
        "Ezinwanne Ozoani",
        "Andrew Paverd",
        "Florian Tram\u00e8r",
        "V\u00e1clav Volhejn"
      ],
      "published": "2025-06-10T14:23:55Z",
      "categories": "",
      "summary": "As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their security has become a critical challenge. Among the most pressing threats are prompt injection attacks, which exploit the agent's resilience on natural language inputs -- an especially dangerous threat when agents are granted tool access or handle sensitive information. In this work, we propose a set of principled design patterns for building AI agents with provable resistance to prompt injection. We systematically analyze these patterns, discus...",
      "pdf_url": "https://arxiv.org/pdf/2506.08837v3.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2506.08837v3_paper.pdf"
    },
    {
      "arxiv_id": "2507.06323v1",
      "title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms",
      "authors": [
        "Tarek Gasmi",
        "Ramzi Guesmi",
        "Ines Belhadj",
        "Jihene Bennaceur"
      ],
      "published": "2025-07-08T18:24:28Z",
      "categories": "",
      "summary": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Funct...",
      "pdf_url": "https://arxiv.org/pdf/2507.06323v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2507.06323v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.06556v1",
      "title": "Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks",
      "authors": [
        "Saeid Jamshidi",
        "Kawser Wazed Nafi",
        "Arghavan Moradi Dakhel",
        "Negar Shahabi",
        "Foutse Khomh",
        "Naser Ezzati-Jivan"
      ],
      "published": "2025-12-06T20:07:58Z",
      "categories": "",
      "summary": "The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors...",
      "pdf_url": "https://arxiv.org/pdf/2512.06556v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.06556v1_paper.pdf"
    },
    {
      "arxiv_id": "2506.19109v1",
      "title": "Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems",
      "authors": [
        "Valerii Gakh",
        "Hayretdin Bahsi"
      ],
      "published": "2025-06-23T20:39:43Z",
      "categories": "",
      "summary": "Prompt injection threatens novel applications that emerge from adapting LLMs for various user tasks. The newly developed LLM-based software applications become more ubiquitous and diverse. However, the threat of prompt injection attacks undermines the security of these systems as the mitigation and defenses against them, proposed so far, are insufficient. We investigated the capabilities of early prompt injection detection systems, focusing specifically on the detection performance of techniques implemented in various open-source solutions. These solutions are supposed to detect certain types ...",
      "pdf_url": "https://arxiv.org/pdf/2506.19109v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2506.19109v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.04785v1",
      "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
      "authors": [
        "Eranga Bandara",
        "Amin Hass",
        "Ross Gore",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Safdar H. Bouk",
        "Xueping Liang",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "published": "2025-12-04T13:32:40Z",
      "categories": "",
      "summary": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems...",
      "pdf_url": "https://arxiv.org/pdf/2512.04785v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.04785v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.08829v1",
      "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization",
      "authors": [
        "Debeshee Das",
        "Luca Beurer-Kellner",
        "Marc Fischer",
        "Maximilian Baader"
      ],
      "published": "2025-10-09T21:32:02Z",
      "categories": "",
      "summary": "The increasing adoption of LLM agents with access to numerous tools and sensitive data significantly widens the attack surface for indirect prompt injections. Due to the context-dependent nature of attacks, however, current defenses are often ill-calibrated as they cannot reliably differentiate malicious and benign instructions, leading to high false positive rates that prevent their real-world adoption. To address this, we present a novel approach inspired by the fundamental principle of computer security: data should not contain executable instructions. Instead of sample-level classification...",
      "pdf_url": "https://arxiv.org/pdf/2510.08829v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.08829v1_paper.pdf"
    },
    {
      "arxiv_id": "2511.11020v1",
      "title": "Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis",
      "authors": [
        "Farhad Abtahi",
        "Fernando Seoane",
        "Iv\u00e1n Pau",
        "Mario Vega-Barbas"
      ],
      "published": "2025-11-14T07:16:16Z",
      "categories": "",
      "summary": "Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 1...",
      "pdf_url": "https://arxiv.org/pdf/2511.11020v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.11020v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.23480v1",
      "title": "Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation",
      "authors": [
        "Toqeer Ali Syed",
        "Mohammad Riyaz Belgaum",
        "Salman Jan",
        "Asadullah Abdullah Khan",
        "Saad Said Alqahtani"
      ],
      "published": "2025-12-29T14:06:09Z",
      "categories": "",
      "summary": "The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, rei...",
      "pdf_url": "https://arxiv.org/pdf/2512.23480v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.23480v1_paper.pdf"
    },
    {
      "arxiv_id": "2505.10538v1",
      "title": "S3C2 Summit 2024-09: Industry Secure Software Supply Chain Summit",
      "authors": [
        "Imranur Rahman",
        "Yasemin Acar",
        "Michel Cukier",
        "William Enck",
        "Christian Kastner",
        "Alexandros Kapravelos",
        "Dominik Wermke",
        "Laurie Williams"
      ],
      "published": "2025-05-15T17:48:14Z",
      "categories": "",
      "summary": "While providing economic and software development value, software supply chains are only as strong as their weakest link. Over the past several years, there has been an exponential increase in cyberattacks, specifically targeting vulnerable links in critical software supply chains. These attacks disrupt the day-to-day functioning and threaten the security of nearly everyone on the internet, from billion-dollar companies and government agencies to hobbyist open-source developers. The ever-evolving threat of software supply chain attacks has garnered interest from the software industry and the U...",
      "pdf_url": "https://arxiv.org/pdf/2505.10538v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2505.10538v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.24920v1",
      "title": "S3C2 Summit 2025-03: Industry Secure Supply Chain Summit",
      "authors": [
        "Elizabeth Lin",
        "Jonah Ghebremichael",
        "William Enck",
        "Yasemin Acar",
        "Michel Cukier",
        "Alexandros Kapravelos",
        "Christian Kastner",
        "Laurie Williams"
      ],
      "published": "2025-10-28T19:47:07Z",
      "categories": "",
      "summary": "Software supply chains, while providing immense economic and software development value, are only as strong as their weakest link. Over the past several years, there has been an exponential increase in cyberattacks specifically targeting vulnerable links in critical software supply chains. These attacks disrupt the day-to-day functioning and threaten the security of nearly everyone on the internet, from billion-dollar companies and government agencies to hobbyist open-source developers. The ever-evolving threat of software supply chain attacks has garnered interest from both the software indus...",
      "pdf_url": "https://arxiv.org/pdf/2510.24920v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.24920v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.14778v2",
      "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks",
      "authors": [
        "Maor Reuben",
        "Ido Mendel",
        "Or Feldman",
        "Moshe Kravchik",
        "Mordehai Guri",
        "Rami Puzis"
      ],
      "published": "2025-10-16T15:14:04Z",
      "categories": "",
      "summary": "Supply chain attacks significantly threaten software security with malicious code injections within legitimate projects. Such attacks are very rare but may have a devastating impact. Detecting spurious code injections using automated tools is further complicated as it often requires deciphering the intention of both the inserted code and its context. In this study, we propose an unsupervised approach for highlighting spurious code injections by quantifying cohesion disruptions in the source code. Using a name-prediction-based cohesion (NPC) metric, we analyze how function cohesion changes when...",
      "pdf_url": "https://arxiv.org/pdf/2510.14778v2.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.14778v2_paper.pdf"
    },
    {
      "arxiv_id": "1712.05526v1",
      "title": "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning",
      "authors": [
        "Xinyun Chen",
        "Chang Liu",
        "Bo Li",
        "Kimberly Lu",
        "Dawn Song"
      ],
      "published": "2017-12-15T04:26:26Z",
      "categories": "",
      "summary": "Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based a...",
      "pdf_url": "https://arxiv.org/pdf/1712.05526v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/1712.05526v1_paper.pdf"
    },
    {
      "arxiv_id": "2504.20984v3",
      "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
      "authors": [
        "Evan Li",
        "Tushin Mallick",
        "Evan Rose",
        "William Robertson",
        "Alina Oprea",
        "Cristina Nita-Rotaru"
      ],
      "published": "2025-04-29T17:55:52Z",
      "categories": "",
      "summary": "LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.   In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solut...",
      "pdf_url": "https://arxiv.org/pdf/2504.20984v3.pdf",
      "relevance_score": 98,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2504.20984v3_paper.pdf"
    },
    {
      "arxiv_id": "2509.05755v5",
      "title": "Red-Teaming Coding Agents from a Tool-Invocation Perspective: An Empirical Security Assessment",
      "authors": [
        "Yuchong Xie",
        "Mingyu Luo",
        "Zesen Liu",
        "Zhixiang Zhang",
        "Kaikai Zhang",
        "Yu Liu",
        "Zongjie Li",
        "Ping Chen",
        "Shuai Wang",
        "Dongdong She"
      ],
      "published": "2025-09-06T15:48:49Z",
      "categories": "",
      "summary": "Coding agents powered by large language models are becoming central modules of modern IDEs, helping users perform complex tasks by invoking tools. While powerful, tool invocation opens a substantial attack surface. Prior work has demonstrated attacks against general-purpose and domain-specific agents, but none have focused on the security risks of tool invocation in coding agents. To fill this gap, we conduct the first systematic red-teaming of six popular real-world coding agents: Cursor, Claude Code, Copilot, Windsurf, Cline, and Trae. Our red-teaming proceeds in two phases. In Phase 1, we p...",
      "pdf_url": "https://arxiv.org/pdf/2509.05755v5.pdf",
      "relevance_score": 98,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.05755v5_paper.pdf"
    },
    {
      "arxiv_id": "2512.15081v1",
      "title": "Quantifying Return on Security Controls in LLM Systems",
      "authors": [
        "Richard Helder Moulton",
        "Austin O'Brien",
        "John D. Hastings"
      ],
      "published": "2025-12-17T04:58:09Z",
      "categories": "",
      "summary": "Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic perso...",
      "pdf_url": "https://arxiv.org/pdf/2512.15081v1.pdf",
      "relevance_score": 98,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.15081v1_paper.pdf"
    },
    {
      "arxiv_id": "2503.07568v1",
      "title": "Runtime Detection of Adversarial Attacks in AI Accelerators Using Performance Counters",
      "authors": [
        "Habibur Rahaman",
        "Atri Chatterjee",
        "Swarup Bhunia"
      ],
      "published": "2025-03-10T17:38:42Z",
      "categories": "",
      "summary": "Rapid adoption of AI technologies raises several major security concerns, including the risks of adversarial perturbations, which threaten the confidentiality and integrity of AI applications. Protecting AI hardware from misuse and diverse security threats is a challenging task. To address this challenge, we propose SAMURAI, a novel framework for safeguarding against malicious usage of AI hardware and its resilience to attacks. SAMURAI introduces an AI Performance Counter (APC) for tracking dynamic behavior of an AI model coupled with an on-chip Machine Learning (ML) analysis engine, known as ...",
      "pdf_url": "https://arxiv.org/pdf/2503.07568v1.pdf",
      "relevance_score": 97,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2503.07568v1_paper.pdf"
    },
    {
      "arxiv_id": "2308.10294v2",
      "title": "A review of SolarWinds attack on Orion platform using persistent threat agents and techniques for gaining unauthorized access",
      "authors": [
        "Antigoni Kruti",
        "Usman Butt",
        "Rejwan Bin Sulaiman"
      ],
      "published": "2023-08-20T15:32:28Z",
      "categories": "",
      "summary": "This paper of work examines the SolarWinds attack, designed on Orion Platform security incident. It analyses the persistent threats agents and potential technical attack techniques to gain unauthorized access. In 2020 SolarWinds attack indicates an initial breach disclosure on Orion Platform software by malware distribution on IT and government organizations such as Homeland Security, Microsoft and Intel associated with supply chains leaks consequences from small loopholes in security systems. Hackers increased the number of infected company and businesses networks during the supply-chain atta...",
      "pdf_url": "https://arxiv.org/pdf/2308.10294v2.pdf",
      "relevance_score": 97,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2308.10294v2_paper.pdf"
    },
    {
      "arxiv_id": "2410.13899v1",
      "title": "Security of and by Generative AI platforms",
      "authors": [
        "Hari Hayagreevan",
        "Souvik Khamaru"
      ],
      "published": "2024-10-15T15:27:05Z",
      "categories": "",
      "summary": "This whitepaper highlights the dual importance of securing generative AI (genAI) platforms and leveraging genAI for cybersecurity. As genAI technologies proliferate, their misuse poses significant risks, including data breaches, model tampering, and malicious content generation. Securing these platforms is critical to protect sensitive data, ensure model integrity, and prevent adversarial attacks. Simultaneously, genAI presents opportunities for enhancing security by automating threat detection, vulnerability analysis, and incident response. The whitepaper explores strategies for robust securi...",
      "pdf_url": "https://arxiv.org/pdf/2410.13899v1.pdf",
      "relevance_score": 95,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2410.13899v1_paper.pdf"
    },
    {
      "arxiv_id": "2511.15759v1",
      "title": "Securing AI Agents Against Prompt Injection Attacks",
      "authors": [
        "Badrinath Ramakrishnan",
        "Akshaya Balaji"
      ],
      "published": "2025-11-19T10:00:54Z",
      "categories": "",
      "summary": "Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test cases across five attack categories: direct injection, context manipulation, instruction override, data exfiltration, and cross-context contamination. We evaluate three defense mechanisms: content filt...",
      "pdf_url": "https://arxiv.org/pdf/2511.15759v1.pdf",
      "relevance_score": 93,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.15759v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.05709v1",
      "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
      "authors": [
        "Mary Llewellyn",
        "Annie Gray",
        "Josh Collyer",
        "Michael Harries"
      ],
      "published": "2025-10-07T09:22:22Z",
      "categories": "",
      "summary": "Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practition...",
      "pdf_url": "https://arxiv.org/pdf/2510.05709v1.pdf",
      "relevance_score": 93,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.05709v1_paper.pdf"
    },
    {
      "arxiv_id": "2305.19593v1",
      "title": "Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis",
      "authors": [
        "Mst Shapna Akter",
        "Hossain Shahriar",
        "Iysa Iqbal",
        "MD Hossain",
        "M. A. Karim",
        "Victor Clincy",
        "Razvan Voicu"
      ],
      "published": "2023-05-31T06:31:42Z",
      "categories": "",
      "summary": "The burgeoning fields of machine learning (ML) and quantum machine learning (QML) have shown remarkable potential in tackling complex problems across various domains. However, their susceptibility to adversarial attacks raises concerns when deploying these systems in security sensitive applications. In this study, we present a comparative analysis of the vulnerability of ML and QML models, specifically conventional neural networks (NN) and quantum neural networks (QNN), to adversarial attacks using a malware dataset. We utilize a software supply chain attack dataset known as ClaMP and develop ...",
      "pdf_url": "https://arxiv.org/pdf/2305.19593v1.pdf",
      "relevance_score": 93,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2305.19593v1_paper.pdf"
    },
    {
      "arxiv_id": "2511.19727v1",
      "title": "Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts",
      "authors": [
        "Steven Peh"
      ],
      "published": "2025-11-24T21:44:33Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we...",
      "pdf_url": "https://arxiv.org/pdf/2511.19727v1.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.19727v1_paper.pdf"
    },
    {
      "arxiv_id": "2410.02644v4",
      "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents",
      "authors": [
        "Hanrong Zhang",
        "Jingyuan Huang",
        "Kai Mei",
        "Yifei Yao",
        "Zhenting Wang",
        "Chenlu Zhan",
        "Hongwei Wang",
        "Yongfeng Zhang"
      ],
      "published": "2024-10-03T16:30:47Z",
      "categories": "",
      "summary": "Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenar...",
      "pdf_url": "https://arxiv.org/pdf/2410.02644v4.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2410.02644v4_paper.pdf"
    },
    {
      "arxiv_id": "2508.10991v4",
      "title": "MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI",
      "authors": [
        "Wenpeng Xing",
        "Zhonghao Qi",
        "Yupeng Qin",
        "Yilin Li",
        "Caini Chang",
        "Jiahui Yu",
        "Changting Lin",
        "Zhenzhen Xie",
        "Meng Han"
      ],
      "published": "2025-08-14T18:00:25Z",
      "categories": "",
      "summary": "While Large Language Models (LLMs) have achieved remarkable performance, they remain vulnerable to jailbreak. The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-GUARD, a robust, layered defense architecture designed for LLM-tool interactions. MCP-GUARD employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static ...",
      "pdf_url": "https://arxiv.org/pdf/2508.10991v4.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2508.10991v4_paper.pdf"
    },
    {
      "arxiv_id": "2503.23250v1",
      "title": "Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions",
      "authors": [
        "Shih-Han Chan"
      ],
      "published": "2025-03-29T23:26:57Z",
      "categories": "",
      "summary": "Security threats like prompt injection attacks pose significant risks to applications that integrate Large Language Models (LLMs), potentially leading to unauthorized actions such as API misuse. Unlike previous approaches that aim to detect these attacks on a best-effort basis, this paper introduces a novel method that appends an Encrypted Prompt to each user prompt, embedding current permissions. These permissions are verified before executing any actions (such as API calls) generated by the LLM. If the permissions are insufficient, the LLM's actions will not be executed, ensuring safety. Thi...",
      "pdf_url": "https://arxiv.org/pdf/2503.23250v1.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2503.23250v1_paper.pdf"
    },
    {
      "arxiv_id": "2410.05451v3",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "David Wagner",
        "Chuan Guo"
      ],
      "published": "2024-10-07T19:34:35Z",
      "categories": "",
      "summary": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vul...",
      "pdf_url": "https://arxiv.org/pdf/2410.05451v3.pdf",
      "relevance_score": 89,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2410.05451v3_paper.pdf"
    },
    {
      "arxiv_id": "2409.08087v3",
      "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
      "authors": [
        "Benji Peng",
        "Keyu Chen",
        "Ming Li",
        "Pohsun Feng",
        "Ziqian Bi",
        "Junyu Liu",
        "Xinyuan Song",
        "Qian Niu"
      ],
      "published": "2024-09-12T14:42:08Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled inpu...",
      "pdf_url": "https://arxiv.org/pdf/2409.08087v3.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2409.08087v3_paper.pdf"
    },
    {
      "arxiv_id": "2512.14860v1",
      "title": "Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks",
      "authors": [
        "Viet K. Nguyen",
        "Mohammad I. Husain"
      ],
      "published": "2025-12-16T19:22:50Z",
      "categories": "",
      "summary": "Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent archi...",
      "pdf_url": "https://arxiv.org/pdf/2512.14860v1.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.14860v1_paper.pdf"
    },
    {
      "arxiv_id": "2502.11127v1",
      "title": "G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems",
      "authors": [
        "Shilong Wang",
        "Guibin Zhang",
        "Miao Yu",
        "Guancheng Wan",
        "Fanci Meng",
        "Chongye Guo",
        "Kun Wang",
        "Yang Wang"
      ],
      "published": "2025-02-16T13:48:41Z",
      "categories": "",
      "summary": "Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborative problem-solving to autonomous decision-making. However, as these systems become increasingly integrated into critical applications, their vulnerability to adversarial attacks, misinformation propagation, and unintended behaviors have raised significant concerns. To address this challenge, we introduce G-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS, which leverages graph neural networks to detect anomalies on the...",
      "pdf_url": "https://arxiv.org/pdf/2502.11127v1.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2502.11127v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.15994v1",
      "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents",
      "authors": [
        "Dongsen Zhang",
        "Zekun Li",
        "Xu Luo",
        "Xuannan Liu",
        "Peipei Li",
        "Wenjun Xu"
      ],
      "published": "2025-10-14T07:36:25Z",
      "categories": "",
      "summary": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 att...",
      "pdf_url": "https://arxiv.org/pdf/2510.15994v1.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.15994v1_paper.pdf"
    },
    {
      "arxiv_id": "2402.11082v1",
      "title": "The AI Security Pyramid of Pain",
      "authors": [
        "Chris M. Ward",
        "Josh Harguess",
        "Julia Tao",
        "Daniel Christman",
        "Paul Spicer",
        "Mike Tan"
      ],
      "published": "2024-02-16T21:14:11Z",
      "categories": "",
      "summary": "We introduce the AI Security Pyramid of Pain, a framework that adapts the cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats. This framework provides a structured approach to understanding and addressing various levels of AI threats. Starting at the base, the pyramid emphasizes Data Integrity, which is essential for the accuracy and reliability of datasets and AI models, including their weights and parameters. Ensuring data integrity is crucial, as it underpins the effectiveness of all AI-driven decisions and operations. The next level, AI System Performance, focuse...",
      "pdf_url": "https://arxiv.org/pdf/2402.11082v1.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2402.11082v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.12921v1",
      "title": "Cisco Integrated AI Security and Safety Framework Report",
      "authors": [
        "Amy Chang",
        "Tiffany Saade",
        "Sanket Mendapara",
        "Adam Swanda",
        "Ankit Garg"
      ],
      "published": "2025-12-15T02:12:12Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi...",
      "pdf_url": "https://arxiv.org/pdf/2512.12921v1.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.12921v1_paper.pdf"
    },
    {
      "arxiv_id": "2409.07415v2",
      "title": "SoK: Security and Privacy Risks of Healthcare AI",
      "authors": [
        "Yuanhaur Chang",
        "Han Liu",
        "Chenyang Lu",
        "Ning Zhang"
      ],
      "published": "2024-09-11T16:59:58Z",
      "categories": "",
      "summary": "The integration of artificial intelligence (AI) and machine learning (ML) into healthcare systems holds great promise for enhancing patient care and care delivery efficiency; however, it also exposes sensitive data and system integrity to potential cyberattacks. Current security and privacy (S&P) research on healthcare AI is highly unbalanced in terms of healthcare deployment scenarios and threat models, and has a disconnected focus with the biomedical research community. This hinders a comprehensive understanding of the risks that healthcare AI entails. To address this gap, this paper takes a...",
      "pdf_url": "https://arxiv.org/pdf/2409.07415v2.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2409.07415v2_paper.pdf"
    },
    {
      "arxiv_id": "2509.18461v1",
      "title": "Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?",
      "authors": [
        "Ayan Sar",
        "Sampurna Roy",
        "Tanupriya Choudhury",
        "Ajith Abraham"
      ],
      "published": "2025-09-22T22:33:16Z",
      "categories": "",
      "summary": "Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and meta-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested AI-driven prevention...",
      "pdf_url": "https://arxiv.org/pdf/2509.18461v1.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.18461v1_paper.pdf"
    },
    {
      "arxiv_id": "2308.16684v2",
      "title": "Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack",
      "authors": [
        "Sze Jue Yang",
        "Quang Nguyen",
        "Chee Seng Chan",
        "Khoa D. Doan"
      ],
      "published": "2023-08-31T12:38:29Z",
      "categories": "",
      "summary": "The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-us...",
      "pdf_url": "https://arxiv.org/pdf/2308.16684v2.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2308.16684v2_paper.pdf"
    },
    {
      "arxiv_id": "2503.09334v3",
      "title": "CyberLLMInstruct: A Pseudo-malicious Dataset Revealing Safety-performance Trade-offs in Cyber Security LLM Fine-tuning",
      "authors": [
        "Adel ElZemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "published": "2025-03-12T12:29:27Z",
      "categories": "",
      "summary": "The integration of large language models (LLMs) into cyber security applications presents both opportunities and critical safety risks. We introduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious instruction-response pairs spanning cyber security tasks including malware analysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive evaluation using seven open-source LLMs reveals a critical trade-off: while fine-tuning improves cyber security task performance (achieving up to 92.50% accuracy on CyberMetric), it severely compromises safety resilience across all tested mo...",
      "pdf_url": "https://arxiv.org/pdf/2503.09334v3.pdf",
      "relevance_score": 86,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2503.09334v3_paper.pdf"
    },
    {
      "arxiv_id": "2505.09974v2",
      "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data",
      "authors": [
        "Adel ElZemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "published": "2025-05-15T05:22:53Z",
      "categories": "",
      "summary": "Large language models (LLMs) have been used in many application domains, including cyber security. The application of LLMs in the cyber security domain presents significant opportunities, such as for enhancing threat analysis and malware detection, but it can also introduce critical risks and safety concerns, including potential personal data leakage and automated generation of new malware. Building on recent findings that fine-tuning LLMs with pseudo-malicious cyber security data significantly compromises their safety, this paper presents a comprehensive validation and extension of these safe...",
      "pdf_url": "https://arxiv.org/pdf/2505.09974v2.pdf",
      "relevance_score": 86,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2505.09974v2_paper.pdf"
    },
    {
      "arxiv_id": "2505.21609v1",
      "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study",
      "authors": [
        "Mathew J. Walter",
        "Aaron Barrett",
        "Kimberly Tam"
      ],
      "published": "2025-05-27T17:59:05Z",
      "categories": "",
      "summary": "Adversarial artificial intelligence (AI) attacks pose a significant threat to autonomous transportation, such as maritime vessels, that rely on AI components. Malicious actors can exploit these systems to deceive and manipulate AI-driven operations. This paper addresses three critical research challenges associated with adversarial AI: the limited scope of traditional defences, inadequate security metrics, and the need to build resilience beyond model-level defences. To address these challenges, we propose building defences utilising multiple inputs and data fusion to create defensive componen...",
      "pdf_url": "https://arxiv.org/pdf/2505.21609v1.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2505.21609v1_paper.pdf"
    },
    {
      "arxiv_id": "2310.11594v3",
      "title": "Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning",
      "authors": [
        "Taejin Kim",
        "Jiarui Li",
        "Shubhranshu Singh",
        "Nikhil Madaan",
        "Carlee Joe-Wong"
      ],
      "published": "2023-10-17T21:38:41Z",
      "categories": "",
      "summary": "The delicate equilibrium between user privacy and the ability to unleash the potential of distributed data is an important concern. Federated learning, which enables the training of collaborative models without sharing of data, has emerged as a privacy-centric solution. This approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data into the training process, as well as evasion attacks that aim to induce misclassifications at test time. Our research investigates the intersection of adversarial training, a common defense meth...",
      "pdf_url": "https://arxiv.org/pdf/2310.11594v3.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2310.11594v3_paper.pdf"
    },
    {
      "arxiv_id": "2207.00091v1",
      "title": "Threat Assessment in Machine Learning based Systems",
      "authors": [
        "Lionel Nganyewou Tidjon",
        "Foutse Khomh"
      ],
      "published": "2022-06-30T20:19:50Z",
      "categories": "",
      "summary": "Machine learning is a field of artificial intelligence (AI) that is becoming essential for several critical systems, making it a good target for threat actors. Threat actors exploit different Tactics, Techniques, and Procedures (TTPs) against the confidentiality, integrity, and availability of Machine Learning (ML) systems. During the ML cycle, they exploit adversarial TTPs to poison data and fool ML-based systems. In recent years, multiple security practices have been proposed for traditional systems but they are not enough to cope with the nature of ML-based systems. In this paper, we conduc...",
      "pdf_url": "https://arxiv.org/pdf/2207.00091v1.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2207.00091v1_paper.pdf"
    },
    {
      "arxiv_id": "2302.09904v4",
      "title": "WW-FL: Secure and Private Large-Scale Federated Learning",
      "authors": [
        "Felix Marx",
        "Thomas Schneider",
        "Ajith Suresh",
        "Tobias Wehrle",
        "Christian Weinert",
        "Hossein Yalame"
      ],
      "published": "2023-02-20T11:02:55Z",
      "categories": "",
      "summary": "Federated learning (FL) is an efficient approach for large-scale distributed machine learning that promises data privacy by keeping training data on client devices. However, recent research has uncovered vulnerabilities in FL, impacting both security and privacy through poisoning attacks and the potential disclosure of sensitive information in individual model updates as well as the aggregated global model. This paper explores the inadequacies of existing FL protection measures when applied independently, and the challenges of creating effective compositions.   Addressing these issues, we prop...",
      "pdf_url": "https://arxiv.org/pdf/2302.09904v4.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2302.09904v4_paper.pdf"
    },
    {
      "arxiv_id": "2409.05014v2",
      "title": "Analyzing Challenges in Deployment of the SLSA Framework for Software Supply Chain Security",
      "authors": [
        "Mahzabin Tamanna",
        "Sivana Hamer",
        "Mindy Tran",
        "Sascha Fahl",
        "Yasemin Acar",
        "Laurie Williams"
      ],
      "published": "2024-09-08T07:54:16Z",
      "categories": "",
      "summary": "In 2023, Sonatype reported a 200\\% increase in software supply chain attacks, including major build infrastructure attacks. To secure the software supply chain, practitioners can follow security framework guidance like the Supply-chain Levels for Software Artifacts (SLSA). However, recent surveys and industry summits have shown that despite growing interest, the adoption of SLSA is not widespread. To understand adoption challenges, \\textit{the goal of this study is to aid framework authors and practitioners in improving the adoption and development of Supply-Chain Levels for Software Artifacts...",
      "pdf_url": "https://arxiv.org/pdf/2409.05014v2.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2409.05014v2_paper.pdf"
    },
    {
      "arxiv_id": "2509.08083v1",
      "title": "Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations",
      "authors": [
        "Laurie Williams",
        "Sammy Migues"
      ],
      "published": "2025-09-09T18:39:03Z",
      "categories": "",
      "summary": "Software supply chain attacks have increased exponentially since 2020. The primary attack vectors for supply chain attacks are through: (1) software components; (2) the build infrastructure; and (3) humans (a.k.a software practitioners). Software supply chain risk management frameworks provide a list of tasks that an organization can adopt to reduce software supply chain risk. Exhaustively adopting all the tasks of these frameworks is infeasible, necessitating the prioritized adoption of tasks. Software organizations can benefit from being guided in this prioritization by learning what tasks o...",
      "pdf_url": "https://arxiv.org/pdf/2509.08083v1.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.08083v1_paper.pdf"
    },
    {
      "arxiv_id": "2301.08170v1",
      "title": "On the Vulnerability of Backdoor Defenses for Federated Learning",
      "authors": [
        "Pei Fang",
        "Jinghui Chen"
      ],
      "published": "2023-01-19T17:02:02Z",
      "categories": "",
      "summary": "Federated Learning (FL) is a popular distributed machine learning paradigm that enables jointly training a global model without sharing clients' data. However, its repetitive server-client communication gives room for backdoor attacks with aim to mislead the global model into a targeted misprediction when a specific trigger pattern is presented. In response to such backdoor threats on federated learning, various defense measures have been proposed. In this paper, we study whether the current defense mechanisms truly neutralize the backdoor threats from federated learning in a practical setting...",
      "pdf_url": "https://arxiv.org/pdf/2301.08170v1.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2301.08170v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.23385v1",
      "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
      "authors": [
        "The Anh Nguyen",
        "Triet Huynh Minh Le",
        "M. Ali Babar"
      ],
      "published": "2025-12-29T11:22:11Z",
      "categories": "",
      "summary": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, ...",
      "pdf_url": "https://arxiv.org/pdf/2512.23385v1.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.23385v1_paper.pdf"
    },
    {
      "arxiv_id": "2007.08745v5",
      "title": "Backdoor Learning: A Survey",
      "authors": [
        "Yiming Li",
        "Yong Jiang",
        "Zhifeng Li",
        "Shu-Tao Xia"
      ],
      "published": "2020-07-17T04:09:20Z",
      "categories": "",
      "summary": "Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), so that the attacked models perform well on benign samples, whereas their predictions will be maliciously changed if the hidden backdoor is activated by attacker-specified triggers. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, its systematic review, however, remains blank. In this paper, we pres...",
      "pdf_url": "https://arxiv.org/pdf/2007.08745v5.pdf",
      "relevance_score": 82,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2007.08745v5_paper.pdf"
    },
    {
      "arxiv_id": "2408.07291v3",
      "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
      "authors": [
        "Yupei Liu",
        "Yuqi Jia",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "published": "2024-08-14T04:49:30Z",
      "categories": "",
      "summary": "Automatically extracting personal information -- such as name, phone number, and email address -- from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods -- such as regular expression, keyword search, and entity detection -- achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction at...",
      "pdf_url": "https://arxiv.org/pdf/2408.07291v3.pdf",
      "relevance_score": 81,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2408.07291v3_paper.pdf"
    },
    {
      "arxiv_id": "2509.21367v1",
      "title": "Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan",
      "authors": [
        "Yu-Kai Shih",
        "You-Kai Kang"
      ],
      "published": "2025-09-22T11:40:29Z",
      "categories": "",
      "summary": "As smart tourism evolves, AI-powered chatbots have become indispensable for delivering personalized, real-time assistance to travelers while promoting sustainability and efficiency. However, these systems are increasingly vulnerable to prompt injection attacks, where adversaries manipulate inputs to elicit unintended behaviors such as leaking sensitive information or generating harmful content. This paper presents a case study on the design and implementation of a secure retrieval-augmented generation (RAG) chatbot for Hsinchu smart tourism services. The system integrates RAG with API function...",
      "pdf_url": "https://arxiv.org/pdf/2509.21367v1.pdf",
      "relevance_score": 81,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.21367v1_paper.pdf"
    },
    {
      "arxiv_id": "2601.04553v1",
      "title": "Deep Dive into the Abuse of DL APIs To Create Malicious AI Models and How to Detect Them",
      "authors": [
        "Mohamed Nabeel",
        "Oleksii Starov"
      ],
      "published": "2026-01-08T03:30:20Z",
      "categories": "",
      "summary": "According to Gartner, more than 70% of organizations will have integrated AI models into their workflows by the end of 2025. In order to reduce cost and foster innovation, it is often the case that pre-trained models are fetched from model hubs like Hugging Face or TensorFlow Hub. However, this introduces a security risk where attackers can inject malicious code into the models they upload to these hubs, leading to various kinds of attacks including remote code execution (RCE), sensitive data exfiltration, and system file modification when these models are loaded or executed (predict function)...",
      "pdf_url": "https://arxiv.org/pdf/2601.04553v1.pdf",
      "relevance_score": 81,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2601.04553v1_paper.pdf"
    },
    {
      "arxiv_id": "2209.07957v1",
      "title": "Malicious Source Code Detection Using Transformer",
      "authors": [
        "Chen Tsfaty",
        "Michael Fire"
      ],
      "published": "2022-09-16T14:16:50Z",
      "categories": "",
      "summary": "Open source code is considered a common practice in modern software development. However, reusing other code allows bad actors to access a wide developers' community, hence the products that rely on it. Those attacks are categorized as supply chain attacks. Recent years saw a growing number of supply chain attacks that leverage open source during software development, relaying the download and installation procedures, whether automatic or manual. Over the years, many approaches have been invented for detecting vulnerable packages. However, it is uncommon to detect malicious code within package...",
      "pdf_url": "https://arxiv.org/pdf/2209.07957v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.13141v1",
      "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models",
      "authors": [
        "Huawei Lin",
        "Yingjie Lao",
        "Tong Geng",
        "Tan Yu",
        "Weijie Zhao"
      ],
      "published": "2025-02-18T18:59:00Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or models to generate harmful outputs. In this paper, departing from traditional deep learning attack paradigms, we explore their intrinsic relationship and collectively term them Prompt Trigger Attacks (PTA). This raises a key question: Can we determine if a prompt is benign or poisoned? To address this, we propose UniGuardian, the first unified defense mechanism designed to detect prompt injection, backdoor attacks, and adversarial attacks in LLMs....",
      "pdf_url": "https://arxiv.org/pdf/2502.13141v1.pdf",
      "relevance_score": 79,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.24019v1",
      "title": "LLM Agents Should Employ Security Principles",
      "authors": [
        "Kaiyuan Zhang",
        "Zian Su",
        "Pin-Yu Chen",
        "Elisa Bertino",
        "Xiangyu Zhang",
        "Ninghui Li"
      ],
      "published": "2025-05-29T21:39:08Z",
      "categories": "",
      "summary": "Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privi...",
      "pdf_url": "https://arxiv.org/pdf/2505.24019v1.pdf",
      "relevance_score": 78,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.01822v5",
      "title": "Firewalls to Secure Dynamic LLM Agentic Networks",
      "authors": [
        "Sahar Abdelnabi",
        "Amr Gomaa",
        "Eugene Bagdasarian",
        "Per Ola Kristensson",
        "Reza Shokri"
      ],
      "published": "2025-02-03T21:00:14Z",
      "categories": "",
      "summary": "LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically deri...",
      "pdf_url": "https://arxiv.org/pdf/2502.01822v5.pdf",
      "relevance_score": 78,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.21029v1",
      "title": "PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight",
      "authors": [
        "Ben Goertzel",
        "Paulos Yibelo"
      ],
      "published": "2025-04-26T00:46:13Z",
      "categories": "",
      "summary": "We propose a robust transformer architecture designed to prevent prompt injection attacks and ensure secure, reliable response generation. Our PICO (Prompt Isolation and Cybersecurity Oversight) framework structurally separates trusted system instructions from untrusted user inputs through dual channels that are processed independently and merged only by a controlled, gated fusion mechanism. In addition, we integrate a specialized Security Expert Agent within a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge Graph (CKG) to supply domain-specific reasoning. Our trai...",
      "pdf_url": "https://arxiv.org/pdf/2504.21029v1.pdf",
      "relevance_score": 78,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.15572v1",
      "title": "Cuckoo Attack: Stealthy and Persistent Attacks Against AI-IDE",
      "authors": [
        "Xinpeng Liu",
        "Junming Liu",
        "Peiyu Liu",
        "Han Zheng",
        "Qinying Wang",
        "Mathias Payer",
        "Shouling Ji",
        "Wenhai Wang"
      ],
      "published": "2025-09-19T04:10:52Z",
      "categories": "",
      "summary": "Modern AI-powered Integrated Development Environments (AI-IDEs) are increasingly defined by an Agent-centric architecture, where an LLM-powered Agent is deeply integrated to autonomously execute complex tasks. This tight integration, however, also introduces a new and critical attack surface. Attackers can exploit these components by injecting malicious instructions into untrusted external sources, effectively hijacking the Agent to perform harmful operations beyond the user's intention or awareness. This emerging threat has quickly attracted research attention, leading to various proposed att...",
      "pdf_url": "https://arxiv.org/pdf/2509.15572v1.pdf",
      "relevance_score": 78,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.18172v1",
      "title": "GenAI Security: Outsmarting the Bots with a Proactive Testing Framework",
      "authors": [
        "Sunil Kumar Jang Bahadur",
        "Gopala Dhar",
        "Lavi Nigam"
      ],
      "published": "2025-05-14T12:55:05Z",
      "categories": "",
      "summary": "The increasing sophistication and integration of Generative AI (GenAI) models into diverse applications introduce new security challenges that traditional methods struggle to address. This research explores the critical need for proactive security measures to mitigate the risks associated with malicious exploitation of GenAI systems. We present a framework encompassing key approaches, tools, and strategies designed to outmaneuver even advanced adversarial attacks, emphasizing the importance of securing GenAI innovation against potential liabilities. We also empirically prove the effectiveness ...",
      "pdf_url": "https://arxiv.org/pdf/2505.18172v1.pdf",
      "relevance_score": 76,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2506.12104v2",
      "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents",
      "authors": [
        "Hao Li",
        "Xiaogeng Liu",
        "Hung-Chun Chiu",
        "Dianqi Li",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "published": "2025-06-13T05:01:09Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent's behavior, potentially resulting in economic loss, privacy leakage, or system compromise. System-level defenses have recently shown promise by enforcing static or predefined policies, but they still face...",
      "pdf_url": "https://arxiv.org/pdf/2506.12104v2.pdf",
      "relevance_score": 76,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2512.08290v2",
      "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
      "authors": [
        "Shiva Gaire",
        "Srijan Gyawali",
        "Saroj Mishra",
        "Suman Niroula",
        "Dilip Thakur",
        "Umesh Yadav"
      ],
      "published": "2025-12-09T06:39:21Z",
      "categories": "",
      "summary": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial s...",
      "pdf_url": "https://arxiv.org/pdf/2512.08290v2.pdf",
      "relevance_score": 75,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2401.12273v2",
      "title": "The Ethics of Interaction: Mitigating Security Threats in LLMs",
      "authors": [
        "Ashutosh Kumar",
        "Shiv Vignesh Murthy",
        "Sagarika Singh",
        "Swathy Ragupathy"
      ],
      "published": "2024-01-22T17:11:37Z",
      "categories": "",
      "summary": "This paper comprehensively explores the ethical challenges arising from security threats to Large Language Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats--prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hat...",
      "pdf_url": "https://arxiv.org/pdf/2401.12273v2.pdf",
      "relevance_score": 75,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2508.09288v2",
      "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs",
      "authors": [
        "Aayush Gupta"
      ],
      "published": "2025-08-12T18:47:30Z",
      "categories": "",
      "summary": "Large language models (LLMs) remain acutely vulnerable to prompt injection and related jailbreak attacks; heuristic guardrails (rules, filters, LLM judges) are routinely bypassed. We present Contextual Integrity Verification (CIV), an inference-time security architecture that attaches cryptographically signed provenance labels to every token and enforces a source-trust lattice inside the transformer via a pre-softmax hard attention mask (with optional FFN/residual gating). CIV provides deterministic, per-token non-interference guarantees on frozen models: lower-trust tokens cannot influence hi...",
      "pdf_url": "https://arxiv.org/pdf/2508.09288v2.pdf",
      "relevance_score": 75,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.13072v1",
      "title": "Digital Sovereignty Control Framework for Military AI-based Cyber Security",
      "authors": [
        "Clara Maathuis",
        "Kasper Cools"
      ],
      "published": "2025-09-16T13:29:26Z",
      "categories": "",
      "summary": "In today's evolving threat landscape, ensuring digital sovereignty has become mandatory for military organizations, especially given their increased development and investment in AI-driven cyber security solutions. To this end, a multi-angled framework is proposed in this article in order to define and assess digital sovereign control of data and AI-based models for military cyber security. This framework focuses on aspects such as context, autonomy, stakeholder involvement, and mitigation of risks in this domain. Grounded on the concepts of digital sovereignty and data sovereignty, the framew...",
      "pdf_url": "https://arxiv.org/pdf/2509.13072v1.pdf",
      "relevance_score": 75,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2404.13946v1",
      "title": "Dual Model Replacement:invisible Multi-target Backdoor Attack based on Federal Learning",
      "authors": [
        "Rong Wang",
        "Guichen Zhou",
        "Mingjun Gao",
        "Yunpeng Xiao"
      ],
      "published": "2024-04-22T07:44:02Z",
      "categories": "",
      "summary": "In recent years, the neural network backdoor hidden in the parameters of the federated learning model has been proved to have great security risks. Considering the characteristics of trigger generation, data poisoning and model training in backdoor attack, this paper designs a backdoor attack method based on federated learning. Firstly, aiming at the concealment of the backdoor trigger, a TrojanGan steganography model with encoder-decoder structure is designed. The model can encode specific attack information as invisible noise and attach it to the image as a backdoor trigger, which improves t...",
      "pdf_url": "https://arxiv.org/pdf/2404.13946v1.pdf",
      "relevance_score": 75,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2311.00144v1",
      "title": "Backdoor Threats from Compromised Foundation Models to Federated Learning",
      "authors": [
        "Xi Li",
        "Songhe Wang",
        "Chen Wu",
        "Hao Zhou",
        "Jiaqi Wang"
      ],
      "published": "2023-10-31T20:39:54Z",
      "categories": "",
      "summary": "Federated learning (FL) represents a novel paradigm to machine learning, addressing critical issues related to data privacy and security, yet suffering from data insufficiency and imbalance. The emergence of foundation models (FMs) provides a promising solution to the problems with FL. For instance, FMs could serve as teacher models or good starting points for FL. However, the integration of FM in FL presents a new challenge, exposing the FL systems to potential threats. This paper investigates the robustness of FL incorporating FMs by assessing their susceptibility to backdoor attacks. Contra...",
      "pdf_url": "https://arxiv.org/pdf/2311.00144v1.pdf",
      "relevance_score": 75,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2508.02312v1",
      "title": "A Survey on Data Security in Large Language Models",
      "authors": [
        "Kang Chen",
        "Xiuze Zhou",
        "Yuanguo Lin",
        "Jinhe Su",
        "Yuanhui Yu",
        "Li Shen",
        "Fan Lin"
      ],
      "published": "2025-08-04T11:28:34Z",
      "categories": "",
      "summary": "Large Language Models (LLMs), now a foundation in advancing natural language processing, power applications such as text generation, machine translation, and conversational systems. Despite their transformative potential, these models inherently rely on massive amounts of training data, often collected from diverse and uncurated sources, which exposes them to serious data security risks. Harmful or malicious data can compromise model behavior, leading to issues such as toxic output, hallucinations, and vulnerabilities to threats such as prompt injection or data poisoning. As LLMs continue to b...",
      "pdf_url": "https://arxiv.org/pdf/2508.02312v1.pdf",
      "relevance_score": 73,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2212.09360v2",
      "title": "AI Security for Geoscience and Remote Sensing: Challenges and Future Trends",
      "authors": [
        "Yonghao Xu",
        "Tao Bai",
        "Weikang Yu",
        "Shizhen Chang",
        "Peter M. Atkinson",
        "Pedram Ghamisi"
      ],
      "published": "2022-12-19T10:54:51Z",
      "categories": "",
      "summary": "Recent advances in artificial intelligence (AI) have significantly intensified research in the geoscience and remote sensing (RS) field. AI algorithms, especially deep learning-based ones, have been developed and applied widely to RS data analysis. The successful application of AI covers almost all aspects of Earth observation (EO) missions, from low-level vision tasks like super-resolution, denoising and inpainting, to high-level vision tasks like scene classification, object detection and semantic segmentation. While AI techniques enable researchers to observe and understand the Earth more a...",
      "pdf_url": "https://arxiv.org/pdf/2212.09360v2.pdf",
      "relevance_score": 73,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2411.02773v1",
      "title": "FedBlock: A Blockchain Approach to Federated Learning against Backdoor Attacks",
      "authors": [
        "Duong H. Nguyen",
        "Phi L. Nguyen",
        "Truong T. Nguyen",
        "Hieu H. Pham",
        "Duc A. Tran"
      ],
      "published": "2024-11-05T03:34:53Z",
      "categories": "",
      "summary": "Federated Learning (FL) is a machine learning method for training with private data locally stored in distributed machines without gathering them into one place for central learning. Despite its promises, FL is prone to critical security risks. First, because FL depends on a central server to aggregate local training models, this is a single point of failure. The server might function maliciously. Second, due to its distributed nature, FL might encounter backdoor attacks by participating clients. They can poison the local model before submitting to the server. Either type of attack, on the ser...",
      "pdf_url": "https://arxiv.org/pdf/2411.02773v1.pdf",
      "relevance_score": 73,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2106.08283v1",
      "title": "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks",
      "authors": [
        "Chulin Xie",
        "Minghao Chen",
        "Pin-Yu Chen",
        "Bo Li"
      ],
      "published": "2021-06-15T16:50:54Z",
      "categories": "",
      "summary": "Federated Learning (FL) as a distributed learning paradigm that aggregates information from diverse clients to train a shared global model, has demonstrated great success. However, malicious clients can perform poisoning attacks and model replacement to introduce backdoors into the trained global model. Although there have been intensive studies designing robust aggregation methods and empirical robust federated training protocols against backdoors, existing approaches lack robustness certification. This paper provides the first general framework, Certifiably Robust Federated Learning (CRFL), ...",
      "pdf_url": "https://arxiv.org/pdf/2106.08283v1.pdf",
      "relevance_score": 73,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2408.02963v1",
      "title": "Adversarial Robustness of Open-source Text Classification Models and Fine-Tuning Chains",
      "authors": [
        "Hao Qin",
        "Mingyang Li",
        "Junjie Wang",
        "Qing Wang"
      ],
      "published": "2024-08-06T05:17:17Z",
      "categories": "",
      "summary": "Context:With the advancement of artificial intelligence (AI) technology and applications, numerous AI models have been developed, leading to the emergence of open-source model hosting platforms like Hugging Face (HF). Thanks to these platforms, individuals can directly download and use models, as well as fine-tune them to construct more domain-specific models. However, just like traditional software supply chains face security risks, AI models and fine-tuning chains also encounter new security risks, such as adversarial attacks. Therefore, the adversarial robustness of these models has garnere...",
      "pdf_url": "https://arxiv.org/pdf/2408.02963v1.pdf",
      "relevance_score": 73,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2309.02396v1",
      "title": "Black-Box Attacks against Signed Graph Analysis via Balance Poisoning",
      "authors": [
        "Jialong Zhou",
        "Yuni Lai",
        "Jian Ren",
        "Kai Zhou"
      ],
      "published": "2023-09-05T17:09:38Z",
      "categories": "",
      "summary": "Signed graphs are well-suited for modeling social networks as they capture both positive and negative relationships. Signed graph neural networks (SGNNs) are commonly employed to predict link signs (i.e., positive and negative) in such graphs due to their ability to handle the unique structure of signed graphs. However, real-world signed graphs are vulnerable to malicious attacks by manipulating edge relationships, and existing adversarial graph attack methods do not consider the specific structure of signed graphs. SGNNs often incorporate balance theory to effectively model the positive and n...",
      "pdf_url": "https://arxiv.org/pdf/2309.02396v1.pdf",
      "relevance_score": 73,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2306.08060v1",
      "title": "Software Supply Chain Vulnerabilities Detection in Source Code: Performance Comparison between Traditional and Quantum Machine Learning Algorithms",
      "authors": [
        "Mst Shapna Akter",
        "Md Jobair Hossain Faruk",
        "Nafisa Anjum",
        "Mohammad Masum",
        "Hossain Shahriar",
        "Akond Rahman",
        "Fan Wu",
        "Alfredo Cuzzocrea"
      ],
      "published": "2023-05-31T06:06:28Z",
      "categories": "",
      "summary": "The software supply chain (SSC) attack has become one of the crucial issues that are being increased rapidly with the advancement of the software development domain. In general, SSC attacks execute during the software development processes lead to vulnerabilities in software products targeting downstream customers and even involved stakeholders. Machine Learning approaches are proven in detecting and preventing software security vulnerabilities. Besides, emerging quantum machine learning can be promising in addressing SSC attacks. Considering the distinction between traditional and quantum mac...",
      "pdf_url": "https://arxiv.org/pdf/2306.08060v1.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2512.12553v1",
      "title": "Cargo Sherlock: An SMT-Based Checker for Software Trust Costs",
      "authors": [
        "Muhammad Hassnain",
        "Anirudh Basu",
        "Ethan Ng",
        "Caleb Stanford"
      ],
      "published": "2025-12-14T04:59:10Z",
      "categories": "",
      "summary": "Supply chain attacks threaten open-source software ecosystems. This paper proposes a formal framework for quantifying trust in third-party software dependencies that is both formally checkable - formalized in satisfiability modulo theories (SMT) - while at the same time incorporating human factors, like the number of downloads, authors, and other metadata that are commonly used to identify trustworthy software in practice. We use data from both software analysis tools and metadata to build a first-order relational model of software dependencies; to obtain an overall \"trust cost\" combining thes...",
      "pdf_url": "https://arxiv.org/pdf/2512.12553v1.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.08609v2",
      "title": "Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?",
      "authors": [
        "Imranur Rahman",
        "Jill Marley",
        "William Enck",
        "Laurie Williams"
      ],
      "published": "2025-10-07T14:37:03Z",
      "categories": "",
      "summary": "Developers consistently use version constraints to specify acceptable versions of the dependencies for their project. \\emph{Pinning} dependencies can reduce the likelihood of breaking changes, but comes with a cost of manually managing the replacement of outdated and vulnerable dependencies. On the other hand, \\emph{floating} can be used to automatically get bug fixes and security fixes, but comes with the risk of breaking changes. Security practitioners advocate \\emph{pinning} dependencies to prevent against software supply chain attacks, e.g., malicious package updates. However, since \\emph{...",
      "pdf_url": "https://arxiv.org/pdf/2510.08609v2.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2210.07346v2",
      "title": "An Embarrassingly Simple Backdoor Attack on Self-supervised Learning",
      "authors": [
        "Changjiang Li",
        "Ren Pang",
        "Zhaohan Xi",
        "Tianyu Du",
        "Shouling Ji",
        "Yuan Yao",
        "Ting Wang"
      ],
      "published": "2022-10-13T20:39:21Z",
      "categories": "",
      "summary": "As a new paradigm in machine learning, self-supervised learning (SSL) is capable of learning high-quality representations of complex data without relying on labels. In addition to eliminating the need for labeled data, research has found that SSL improves the adversarial robustness over supervised learning since lacking labels makes it more challenging for adversaries to manipulate model predictions. However, the extent to which this robustness superiority generalizes to other types of attacks remains an open question.   We explore this question in the context of backdoor attacks. Specifically...",
      "pdf_url": "https://arxiv.org/pdf/2210.07346v2.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2507.01321v1",
      "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks",
      "authors": [
        "Zhiyao Ren",
        "Siyuan Liang",
        "Aishan Liu",
        "Dacheng Tao"
      ],
      "published": "2025-07-02T03:09:20Z",
      "categories": "",
      "summary": "In-context learning (ICL) has demonstrated remarkable success in large language models (LLMs) due to its adaptability and parameter-free nature. However, it also introduces a critical vulnerability to backdoor attacks, where adversaries can manipulate LLM behaviors by simply poisoning a few ICL demonstrations. In this paper, we propose, for the first time, the dual-learning hypothesis, which posits that LLMs simultaneously learn both the task-relevant latent concepts and backdoor latent concepts within poisoned demonstrations, jointly influencing the probability of model outputs. Through theor...",
      "pdf_url": "https://arxiv.org/pdf/2507.01321v1.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2310.01969v1",
      "title": "Steganalysis of AI Models LSB Attacks",
      "authors": [
        "Daniel Gilkarov",
        "Ran Dubin"
      ],
      "published": "2023-10-03T11:25:18Z",
      "categories": "",
      "summary": "Artificial intelligence has made significant progress in the last decade, leading to a rise in the popularity of model sharing. The model zoo ecosystem, a repository of pre-trained AI models, has advanced the AI open-source community and opened new avenues for cyber risks. Malicious attackers can exploit shared models to launch cyber-attacks. This work focuses on the steganalysis of injected malicious Least Significant Bit (LSB) steganography into AI models, and it is the first work focusing on AI model attacks. In response to this threat, this paper presents a steganalysis method specifically...",
      "pdf_url": "https://arxiv.org/pdf/2310.01969v1.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2307.02088v4",
      "title": "Trust in Software Supply Chains: Blockchain-Enabled SBOM and the AIBOM Future",
      "authors": [
        "Boming Xia",
        "Dawen Zhang",
        "Yue Liu",
        "Qinghua Lu",
        "Zhenchang Xing",
        "Liming Zhu"
      ],
      "published": "2023-07-05T07:56:48Z",
      "categories": "",
      "summary": "The robustness of critical infrastructure systems is contingent upon the integrity and transparency of their software supply chains. A Software Bill of Materials (SBOM) is pivotal in this regard, offering an exhaustive inventory of components and dependencies crucial to software development. However, prevalent challenges in SBOM sharing, such as data tampering risks and vendors' reluctance to fully disclose sensitive information, significantly hinder its effective implementation. These challenges pose a notable threat to the security of critical infrastructure and systems where transparency an...",
      "pdf_url": "https://arxiv.org/pdf/2307.02088v4.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.15088v1",
      "title": "Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects",
      "authors": [
        "Yuxuan Wang",
        "Jingshu Chen",
        "Qingyang Wang"
      ],
      "published": "2025-05-21T04:14:35Z",
      "categories": "",
      "summary": "Command injection vulnerabilities are a significant security threat in dynamic languages like Python, particularly in widely used open-source projects where security issues can have extensive impact. With the proven effectiveness of Large Language Models(LLMs) in code-related tasks, such as testing, researchers have explored their potential for vulnerabilities analysis. This study evaluates the potential of large language models (LLMs), such as GPT-4, as an alternative approach for automated testing for vulnerability detection. In particular, LLMs have demonstrated advanced contextual understa...",
      "pdf_url": "https://arxiv.org/pdf/2505.15088v1.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2406.08688v1",
      "title": "On Security Weaknesses and Vulnerabilities in Deep Learning Systems",
      "authors": [
        "Zhongzheng Lai",
        "Huaming Chen",
        "Ruoxi Sun",
        "Yu Zhang",
        "Minhui Xue",
        "Dong Yuan"
      ],
      "published": "2024-06-12T23:04:13Z",
      "categories": "",
      "summary": "The security guarantee of AI-enabled software systems (particularly using deep learning techniques as a functional core) is pivotal against the adversarial attacks exploiting software vulnerabilities. However, little attention has been paid to a systematic investigation of vulnerabilities in such systems. A common situation learned from the open source software community is that deep learning engineers frequently integrate off-the-shelf or open-source learning frameworks into their ecosystems. In this work, we specifically look into deep learning (DL) framework and perform the first systematic...",
      "pdf_url": "https://arxiv.org/pdf/2406.08688v1.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.08747v1",
      "title": "Silent Until Sparse: Backdoor Attacks on Semi-Structured Sparsity",
      "authors": [
        "Wei Guo",
        "Maura Pintor",
        "Ambra Demontis",
        "Battista Biggio"
      ],
      "published": "2025-09-03T14:37:58Z",
      "categories": "",
      "summary": "In the deployment phase, semi-structured sparsity accelerates the execution of deep neural networks on modern GPUs via sparse matrix multiplication. In this paper, targeting the semi-structured sparsity, we introduce a Silent Until Sparse (SUS) backdoor attack, where the released full model remains silent (benign), but becomes a backdoored model after sparsification. The attack operates in two phases: (i) in the backdoor training phase, the backdoor functionality is injected into specific weights that will be retained during the pruning process; (ii) in the backdoor hiding phase, the malicious...",
      "pdf_url": "https://arxiv.org/pdf/2509.08747v1.pdf",
      "relevance_score": 71,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2511.20920v1",
      "title": "Securing the Model Context Protocol (MCP): Risks, Controls, and Governance",
      "authors": [
        "Herman Errico",
        "Jiquan Ngiam",
        "Shanita Sojan"
      ],
      "published": "2025-11-25T23:24:26Z",
      "categories": "",
      "summary": "The Model Context Protocol (MCP) replaces static, developer-controlled API integrations with more dynamic, user-driven agent systems, which also introduces new security risks. As MCP adoption grows across community servers and major platforms, organizations encounter threats that existing AI governance frameworks (such as NIST AI RMF and ISO/IEC 42001) do not yet cover in detail. We focus on three types of adversaries that take advantage of MCP s flexibility: content-injection attackers that embed malicious instructions into otherwise legitimate data; supply-chain attackers who distribute comp...",
      "pdf_url": "https://arxiv.org/pdf/2511.20920v1.pdf",
      "relevance_score": 70,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.03574v1",
      "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
      "authors": [
        "Sahana Chennabasappa",
        "Cyrus Nikolaidis",
        "Daniel Song",
        "David Molnar",
        "Stephanie Ding",
        "Shengye Wan",
        "Spencer Whitman",
        "Lauren Deason",
        "Nicholas Doucette",
        "Abraham Montilla",
        "Alekhya Gampa",
        "Beto de Paola",
        "Dominik Gabi",
        "James Crnkovich",
        "Jean-Christophe Testud",
        "Kat He",
        "Rashnil Chaturvedi",
        "Wu Zhou",
        "Joshua Saxe"
      ],
      "published": "2025-05-06T14:34:21Z",
      "categories": "",
      "summary": "Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final laye...",
      "pdf_url": "https://arxiv.org/pdf/2505.03574v1.pdf",
      "relevance_score": 69,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.14522v3",
      "title": "Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration",
      "authors": [
        "Evangelos Lamprou",
        "Julian Dai",
        "Grigoris Ntousakis",
        "Martin C. Rinard",
        "Nikos Vasilakis"
      ],
      "published": "2025-10-16T10:12:14Z",
      "categories": "",
      "summary": "Software supply-chain attacks are an important and ongoing concern in the open source software ecosystem. These attacks maintain the standard functionality that a component implements, but additionally hide malicious functionality activated only when the component reaches its target environment. Lexo addresses such stealthy attacks by automatically learning and regenerating vulnerability-free versions of potentially malicious components. Lexo first generates a set of input-output pairs to model a component's full observable behavior, which it then uses to synthesize a new version of the origin...",
      "pdf_url": "https://arxiv.org/pdf/2510.14522v3.pdf",
      "relevance_score": 69,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2307.09087v3",
      "title": "The Hitchhiker's Guide to Malicious Third-Party Dependencies",
      "authors": [
        "Piergiorgio Ladisa",
        "Merve Sahin",
        "Serena Elisa Ponta",
        "Marco Rosa",
        "Matias Martinez",
        "Olivier Barais"
      ],
      "published": "2023-07-18T09:12:06Z",
      "categories": "",
      "summary": "The increasing popularity of certain programming languages has spurred the creation of ecosystem-specific package repositories and package managers. Such repositories (e.g., npm, PyPI) serve as public databases that users can query to retrieve packages for various functionalities, whereas package managers automatically handle dependency resolution and package installation on the client side. These mechanisms enhance software modularization and accelerate implementation. However, they have become a target for malicious actors seeking to propagate malware on a large scale.   In this work, we sho...",
      "pdf_url": "https://arxiv.org/pdf/2307.09087v3.pdf",
      "relevance_score": 69,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2512.04338v1",
      "title": "One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises",
      "authors": [
        "Biagio Montaruli",
        "Luca Compagna",
        "Serena Elisa Ponta",
        "Davide Balzarotti"
      ],
      "published": "2025-12-03T23:53:56Z",
      "categories": "",
      "summary": "The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).   We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology f...",
      "pdf_url": "https://arxiv.org/pdf/2512.04338v1.pdf",
      "relevance_score": 69,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2601.00205v1",
      "title": "Understanding Security Risks of AI Agents' Dependency Updates",
      "authors": [
        "Tanmay Singla",
        "Berk \u00c7akar",
        "Paschal C. Amusuo",
        "James C. Davis"
      ],
      "published": "2026-01-01T04:44:18Z",
      "categories": "",
      "summary": "Package dependencies are a critical control point in modern software supply chains. Dependency changes can substantially alter a project's security posture. As AI coding agents increasingly modify software via pull requests, it is unclear whether their dependency decisions introduce distinct security risks.   We study 117,062 dependency changes from agent- and human-authored pull requests across seven ecosystems. Agents select known-vulnerable versions more often than humans (2.46% vs. 1.64%), and their vulnerable selections are more disruptive to remediate, with 36.8% requiring major-version ...",
      "pdf_url": "https://arxiv.org/pdf/2601.00205v1.pdf",
      "relevance_score": 69,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.10439v1",
      "title": "Crypto Miner Attack: GPU Remote Code Execution Attacks",
      "authors": [
        "Ariel Szabo",
        "Uzy Hadad"
      ],
      "published": "2025-02-09T19:26:47Z",
      "categories": "",
      "summary": "Remote Code Execution (RCE) exploits pose a significant threat to AI and ML systems, particularly in GPU-accelerated environments where the computational power of GPUs can be misused for malicious purposes. This paper focuses on RCE attacks leveraging deserialization vulnerabilities and custom layers, such as TensorFlow Lambda layers, which are often overlooked due to the complexity of monitoring GPU workloads. These vulnerabilities enable attackers to execute arbitrary code, blending malicious activity seamlessly into expected model behavior and exploiting GPUs for unauthorized tasks such as ...",
      "pdf_url": "https://arxiv.org/pdf/2502.10439v1.pdf",
      "relevance_score": 68,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2310.09571v1",
      "title": "On the Feasibility of Cross-Language Detection of Malicious Packages in npm and PyPI",
      "authors": [
        "Piergiorgio Ladisa",
        "Serena Elisa Ponta",
        "Nicola Ronzoni",
        "Matias Martinez",
        "Olivier Barais"
      ],
      "published": "2023-10-14T12:32:51Z",
      "categories": "",
      "summary": "Current software supply chains heavily rely on open-source packages hosted in public repositories. Given the popularity of ecosystems like npm and PyPI, malicious users started to spread malware by publishing open-source packages containing malicious code. Recent works apply machine learning techniques to detect malicious packages in the npm ecosystem. However, the scarcity of samples poses a challenge to the application of machine learning techniques in other ecosystems. Despite the differences between JavaScript and Python, the open-source software supply chain attacks targeting such languag...",
      "pdf_url": "https://arxiv.org/pdf/2310.09571v1.pdf",
      "relevance_score": 67,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2407.20181v2",
      "title": "Blockchain for Large Language Model Security and Safety: A Holistic Survey",
      "authors": [
        "Caleb Geren",
        "Amanda Board",
        "Gaby G. Dagher",
        "Tim Andersen",
        "Jun Zhuang"
      ],
      "published": "2024-07-26T15:24:01Z",
      "categories": "",
      "summary": "With the growing development and deployment of large language models (LLMs) in both industrial and academic fields, their security and safety concerns have become increasingly critical. However, recent studies indicate that LLMs face numerous vulnerabilities, including data poisoning, prompt injections, and unauthorized data exposure, which conventional methods have struggled to address fully. In parallel, blockchain technology, known for its data immutability and decentralized structure, offers a promising foundation for safeguarding LLMs. In this survey, we aim to comprehensively assess how ...",
      "pdf_url": "https://arxiv.org/pdf/2407.20181v2.pdf",
      "relevance_score": 66,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2412.06090v1",
      "title": "Trust No AI: Prompt Injection Along The CIA Security Triad",
      "authors": [
        "Johann Rehberger"
      ],
      "published": "2024-12-08T22:46:30Z",
      "categories": "",
      "summary": "The CIA security triad - Confidentiality, Integrity, and Availability - is a cornerstone of data and cybersecurity. With the emergence of large language model (LLM) applications, a new class of threat, known as prompt injection, was first identified in 2022. Since then, numerous real-world vulnerabilities and exploits have been documented in production LLM systems, including those from leading vendors like OpenAI, Microsoft, Anthropic and Google. This paper compiles real-world exploits and proof-of concept examples, based on the research conducted and publicly documented by the author, demonst...",
      "pdf_url": "https://arxiv.org/pdf/2412.06090v1.pdf",
      "relevance_score": 66,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2402.06363v2",
      "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
      "authors": [
        "Sizhe Chen",
        "Julien Piet",
        "Chawin Sitawarin",
        "David Wagner"
      ],
      "published": "2024-02-09T12:15:51Z",
      "categories": "",
      "summary": "Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model into deviating from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate prompts and user data. We introduce structured queries, a general approach to tackle this...",
      "pdf_url": "https://arxiv.org/pdf/2402.06363v2.pdf",
      "relevance_score": 66,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2203.05314v2",
      "title": "SoK: On the Semantic AI Security in Autonomous Driving",
      "authors": [
        "Junjie Shen",
        "Ningfei Wang",
        "Ziwen Wan",
        "Yunpeng Luo",
        "Takami Sato",
        "Zhisheng Hu",
        "Xinyang Zhang",
        "Shengjian Guo",
        "Zhenyu Zhong",
        "Kang Li",
        "Ziming Zhao",
        "Chunming Qiao",
        "Qi Alfred Chen"
      ],
      "published": "2022-03-10T12:00:34Z",
      "categories": "",
      "summary": "Autonomous Driving (AD) systems rely on AI components to make safety and correct driving decisions. Unfortunately, today's AI algorithms are known to be generally vulnerable to adversarial attacks. However, for such AI component-level vulnerabilities to be semantically impactful at the system level, it needs to address non-trivial semantic gaps both (1) from the system-level attack input spaces to those at AI component level, and (2) from AI component-level attack impacts to those at the system level. In this paper, we define such research space as semantic AI security as opposed to generic AI...",
      "pdf_url": "https://arxiv.org/pdf/2203.05314v2.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.21574v1",
      "title": "Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation",
      "authors": [
        "Bikash Saha",
        "Nanda Rani",
        "Sandeep Kumar Shukla"
      ],
      "published": "2025-04-30T12:25:30Z",
      "categories": "",
      "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping the global financial landscape, offering unprecedented opportunities to enhance customer engagement, automate complex workflows, and extract actionable insights from vast financial data. This survey provides an overview of GenAI adoption across the financial ecosystem, examining how banks, insurers, asset managers, and fintech startups worldwide are integrating large language models and other generative tools into their operations. From AI-powered virtual assistants and personalized financial advisory to fraud detection and compli...",
      "pdf_url": "https://arxiv.org/pdf/2504.21574v1.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.23622v1",
      "title": "Adversarially-Aware Architecture Design for Robust Medical AI Systems",
      "authors": [
        "Alyssa Gerhart",
        "Balaji Iyangar"
      ],
      "published": "2025-10-23T16:51:11Z",
      "categories": "",
      "summary": "Adversarial attacks pose a severe risk to AI systems used in healthcare, capable of misleading models into dangerous misclassifications that can delay treatments or cause misdiagnoses. These attacks, often imperceptible to human perception, threaten patient safety, particularly in underserved populations. Our study explores these vulnerabilities through empirical experimentation on a dermatological dataset, where adversarial methods significantly reduce classification accuracy. Through detailed threat modeling, experimental benchmarking, and model evaluation, we demonstrate both the severity o...",
      "pdf_url": "https://arxiv.org/pdf/2510.23622v1.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2106.07214v4",
      "title": "Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions",
      "authors": [
        "Antonio Emanuele Cin\u00e0",
        "Kathrin Grosse",
        "Sebastiano Vascon",
        "Ambra Demontis",
        "Battista Biggio",
        "Fabio Roli",
        "Marcello Pelillo"
      ],
      "published": "2021-06-14T08:00:48Z",
      "categories": "",
      "summary": "Backdoor attacks inject poisoning samples during training, with the goal of forcing a machine learning model to output an attacker-chosen class when presented a specific trigger at test time. Although backdoor attacks have been demonstrated in a variety of settings and against different models, the factors affecting their effectiveness are still not well understood. In this work, we provide a unifying framework to study the process of backdoor learning under the lens of incremental learning and influence functions. We show that the effectiveness of backdoor attacks depends on: (i) the complexi...",
      "pdf_url": "https://arxiv.org/pdf/2106.07214v4.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2407.14738v1",
      "title": "Flatness-aware Sequential Learning Generates Resilient Backdoors",
      "authors": [
        "Hoang Pham",
        "The-Anh Ta",
        "Anh Tran",
        "Khoa D. Doan"
      ],
      "published": "2024-07-20T03:30:05Z",
      "categories": "",
      "summary": "Recently, backdoor attacks have become an emerging threat to the security of machine learning models. From the adversary's perspective, the implanted backdoors should be resistant to defensive algorithms, but some recently proposed fine-tuning defenses can remove these backdoors with notable efficacy. This is mainly due to the catastrophic forgetting (CF) property of deep neural networks. This paper counters CF of backdoors by leveraging continual learning (CL) techniques. We begin by investigating the connectivity between a backdoored and fine-tuned model in the loss landscape. Our analysis c...",
      "pdf_url": "https://arxiv.org/pdf/2407.14738v1.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2412.01369v1",
      "title": "Behavior Backdoor for Deep Learning Models",
      "authors": [
        "Jiakai Wang",
        "Pengfei Zhang",
        "Renshuai Tao",
        "Jian Yang",
        "Hao Liu",
        "Xianglong Liu",
        "Yunchao Wei",
        "Yao Zhao"
      ],
      "published": "2024-12-02T10:54:02Z",
      "categories": "",
      "summary": "The various post-processing methods for deep-learning-based models, such as quantification, pruning, and fine-tuning, play an increasingly important role in artificial intelligence technology, with pre-train large models as one of the main development directions. However, this popular series of post-processing behaviors targeting pre-training deep models has become a breeding ground for new adversarial security issues. In this study, we take the first step towards ``behavioral backdoor'' attack, which is defined as a behavior-triggered backdoor model training procedure, to reveal a new paradig...",
      "pdf_url": "https://arxiv.org/pdf/2412.01369v1.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2110.11571v3",
      "title": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data",
      "authors": [
        "Yige Li",
        "Xixiang Lyu",
        "Nodens Koren",
        "Lingjuan Lyu",
        "Bo Li",
        "Xingjun Ma"
      ],
      "published": "2021-10-22T03:30:48Z",
      "categories": "",
      "summary": "Backdoor attack has emerged as a major security threat to deep neural networks (DNNs). While existing defense methods have demonstrated promising results on detecting or erasing backdoors, it is still not clear whether robust training methods can be devised to prevent the backdoor triggers being injected into the trained model in the first place. In this paper, we introduce the concept of \\emph{anti-backdoor learning}, aiming to train \\emph{clean} models given backdoor-poisoned data. We frame the overall learning process as a dual-task of learning the \\emph{clean} and the \\emph{backdoor} porti...",
      "pdf_url": "https://arxiv.org/pdf/2110.11571v3.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2210.10272v2",
      "title": "Training set cleansing of backdoor poisoning by self-supervised representation learning",
      "authors": [
        "H. Wang",
        "S. Karami",
        "O. Dia",
        "H. Ritter",
        "E. Emamjomeh-Zadeh",
        "J. Chen",
        "Z. Xiang",
        "D. J. Miller",
        "G. Kesidis"
      ],
      "published": "2022-10-19T03:29:58Z",
      "categories": "",
      "summary": "A backdoor or Trojan attack is an important type of data poisoning attack against deep neural network (DNN) classifiers, wherein the training dataset is poisoned with a small number of samples that each possess the backdoor pattern (usually a pattern that is either imperceptible or innocuous) and which are mislabeled to the attacker's target class. When trained on a backdoor-poisoned dataset, a DNN behaves normally on most benign test samples but makes incorrect predictions to the target class when the test sample has the backdoor pattern incorporated (i.e., contains a backdoor trigger). Here ...",
      "pdf_url": "https://arxiv.org/pdf/2210.10272v2.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2411.12220v2",
      "title": "DeTrigger: A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning",
      "authors": [
        "Kichang Lee",
        "Yujin Shin",
        "Jonghyuk Yun",
        "Songkuk Kim",
        "Jun Han",
        "JeongGil Ko"
      ],
      "published": "2024-11-19T04:12:14Z",
      "categories": "",
      "summary": "Federated Learning (FL) enables collaborative model training across distributed devices while preserving local data privacy, making it ideal for mobile and embedded systems. However, the decentralized nature of FL also opens vulnerabilities to model poisoning attacks, particularly backdoor attacks, where adversaries implant trigger patterns to manipulate model predictions. In this paper, we propose DeTrigger, a scalable and efficient backdoor-robust federated learning framework that leverages insights from adversarial attack methodologies. By employing gradient analysis with temperature scalin...",
      "pdf_url": "https://arxiv.org/pdf/2411.12220v2.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2406.19753v2",
      "title": "Attack On Prompt: Backdoor Attack in Prompt-Based Continual Learning",
      "authors": [
        "Trang Nguyen",
        "Anh Tran",
        "Nhat Ho"
      ],
      "published": "2024-06-28T08:53:33Z",
      "categories": "",
      "summary": "Prompt-based approaches offer a cutting-edge solution to data privacy issues in continual learning, particularly in scenarios involving multiple data suppliers where long-term storage of private user data is prohibited. Despite delivering state-of-the-art performance, its impressive remembering capability can become a double-edged sword, raising security concerns as it might inadvertently retain poisoned knowledge injected during learning from private user data. Following this insight, in this paper, we expose continual learning to a potential threat: backdoor attack, which drives the model to...",
      "pdf_url": "https://arxiv.org/pdf/2406.19753v2.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2007.10760v3",
      "title": "Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review",
      "authors": [
        "Yansong Gao",
        "Bao Gia Doan",
        "Zhi Zhang",
        "Siqi Ma",
        "Jiliang Zhang",
        "Anmin Fu",
        "Surya Nepal",
        "Hyoungshick Kim"
      ],
      "published": "2020-07-21T12:49:12Z",
      "categories": "",
      "summary": "This work provides the community with a timely comprehensive review of backdoor attacks and countermeasures on deep learning. According to the attacker's capability and affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into six categorizations: code poisoning, outsourcing, pretrained, data collection, collaborative learning and post-deployment. Accordingly, attacks under each categorization are combed. The countermeasures are categorized into four general classes: blind backdoor removal, offline backdoor inspection, online backdo...",
      "pdf_url": "https://arxiv.org/pdf/2007.10760v3.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2411.11006v2",
      "title": "BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation",
      "authors": [
        "Haiyang Yu",
        "Tian Xie",
        "Jiaping Gui",
        "Pengyang Wang",
        "Ping Yi",
        "Yue Wu"
      ],
      "published": "2024-11-17T09:01:55Z",
      "categories": "",
      "summary": "Over the past few years, the emergence of backdoor attacks has presented significant challenges to deep learning systems, allowing attackers to insert backdoors into neural networks. When data with a trigger is processed by a backdoor model, it can lead to mispredictions targeted by attackers, whereas normal data yields regular results. The scope of backdoor attacks is expanding beyond computer vision and encroaching into areas such as natural language processing and speech recognition. Nevertheless, existing backdoor defense methods are typically tailored to specific data modalities, restrict...",
      "pdf_url": "https://arxiv.org/pdf/2411.11006v2.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2511.12668v1",
      "title": "AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework",
      "authors": [
        "Samuel Nathanson",
        "Alexander Lee",
        "Catherine Chen Kieffer",
        "Jared Junkin",
        "Jessica Ye",
        "Amir Saeed",
        "Melanie Lockhart",
        "Russ Fink",
        "Elisha Peterson",
        "Lanier Watkins"
      ],
      "published": "2025-11-16T16:10:38Z",
      "categories": "",
      "summary": "Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot s...",
      "pdf_url": "https://arxiv.org/pdf/2511.12668v1.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1711.11008v1",
      "title": "Security Risks in Deep Learning Implementations",
      "authors": [
        "Qixue Xiao",
        "Kang Li",
        "Deyue Zhang",
        "Weilin Xu"
      ],
      "published": "2017-11-29T18:33:27Z",
      "categories": "",
      "summary": "Advance in deep learning algorithms overshadows their security risk in software implementations. This paper discloses a set of vulnerabilities in popular deep learning frameworks including Caffe, TensorFlow, and Torch. Contrast to the small code size of deep learning models, these deep learning frameworks are complex and contain heavy dependencies on numerous open source packages. This paper considers the risks caused by these vulnerabilities by studying their impact on common deep learning applications such as voice recognition and image classifications. By exploiting these framework implemen...",
      "pdf_url": "https://arxiv.org/pdf/1711.11008v1.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2208.01113v3",
      "title": "On the Evaluation of User Privacy in Deep Neural Networks using Timing Side Channel",
      "authors": [
        "Shubhi Shukla",
        "Manaar Alam",
        "Sarani Bhattacharya",
        "Debdeep Mukhopadhyay",
        "Pabitra Mitra"
      ],
      "published": "2022-08-01T19:38:16Z",
      "categories": "",
      "summary": "Recent Deep Learning (DL) advancements in solving complex real-world tasks have led to its widespread adoption in practical applications. However, this opportunity comes with significant underlying risks, as many of these models rely on privacy-sensitive data for training in a variety of applications, making them an overly-exposed threat surface for privacy violations. Furthermore, the widespread use of cloud-based Machine-Learning-as-a-Service (MLaaS) for its robust infrastructure support has broadened the threat surface to include a variety of remote side-channel attacks. In this paper, we f...",
      "pdf_url": "https://arxiv.org/pdf/2208.01113v3.pdf",
      "relevance_score": 63,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2309.03164v1",
      "title": "J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News",
      "authors": [
        "Tharindu Kumarage",
        "Amrita Bhattacharjee",
        "Djordje Padejski",
        "Kristy Roschke",
        "Dan Gillmor",
        "Scott Ruston",
        "Huan Liu",
        "Joshua Garland"
      ],
      "published": "2023-09-06T17:06:31Z",
      "categories": "",
      "summary": "The rapid proliferation of AI-generated text online is profoundly reshaping the information landscape. Among various types of AI-generated text, AI-generated news presents a significant threat as it can be a prominent source of misinformation online. While several recent efforts have focused on detecting AI-generated text in general, these methods require enhanced reliability, given concerns about their vulnerability to simple adversarial attacks. Furthermore, due to the eccentricities of news writing, applying these detection methods for AI-generated news can produce false positives, potentia...",
      "pdf_url": "https://arxiv.org/pdf/2309.03164v1.pdf",
      "relevance_score": 61,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2112.05224v2",
      "title": "Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures",
      "authors": [
        "Eugene Bagdasaryan",
        "Vitaly Shmatikov"
      ],
      "published": "2021-12-09T21:48:29Z",
      "categories": "",
      "summary": "We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to \"spin\" their outputs so as to support an adversary-chosen sentiment or point of view -- but only when the input contains adversary-chosen trigger words. For example, a spinned summarization model outputs positive summaries of any text that mentions the name of some individual or organization.   Model spinning introduces a \"meta-backdoor\" into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models...",
      "pdf_url": "https://arxiv.org/pdf/2112.05224v2.pdf",
      "relevance_score": 61,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2407.19845v1",
      "title": "BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning",
      "authors": [
        "Baoyuan Wu",
        "Hongrui Chen",
        "Mingda Zhang",
        "Zihao Zhu",
        "Shaokui Wei",
        "Danni Yuan",
        "Mingli Zhu",
        "Ruotong Wang",
        "Li Liu",
        "Chao Shen"
      ],
      "published": "2024-07-29T09:57:03Z",
      "categories": "",
      "summary": "As an emerging approach to explore the vulnerability of deep neural networks (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons or unreliable conclusions (e.g., misleading, biased or even false conclusion...",
      "pdf_url": "https://arxiv.org/pdf/2407.19845v1.pdf",
      "relevance_score": 61,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2206.12654v2",
      "title": "BackdoorBench: A Comprehensive Benchmark of Backdoor Learning",
      "authors": [
        "Baoyuan Wu",
        "Hongrui Chen",
        "Mingda Zhang",
        "Zihao Zhu",
        "Shaokui Wei",
        "Danni Yuan",
        "Chao Shen"
      ],
      "published": "2022-06-25T13:48:04Z",
      "categories": "",
      "summary": "Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility. Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future devel...",
      "pdf_url": "https://arxiv.org/pdf/2206.12654v2.pdf",
      "relevance_score": 61,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2308.04466v3",
      "title": "Backdoor Federated Learning by Poisoning Backdoor-Critical Layers",
      "authors": [
        "Haomin Zhuang",
        "Mingxian Yu",
        "Hao Wang",
        "Yang Hua",
        "Jian Li",
        "Xu Yuan"
      ],
      "published": "2023-08-08T05:46:47Z",
      "categories": "",
      "summary": "Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected...",
      "pdf_url": "https://arxiv.org/pdf/2308.04466v3.pdf",
      "relevance_score": 61,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.19310v1",
      "title": "Model X-Ray: Detection of Hidden Malware in AI Model Weights using Few Shot Learning",
      "authors": [
        "Daniel Gilkarov",
        "Ran Dubin"
      ],
      "published": "2024-09-28T10:45:28Z",
      "categories": "",
      "summary": "The potential for exploitation of AI models has increased due to the rapid advancement of Artificial Intelligence (AI) and the widespread use of platforms like Model Zoo for sharing AI models. Attackers can embed malware within AI models through steganographic techniques, taking advantage of the substantial size of these models to conceal malicious data and use it for nefarious purposes, e.g. Remote Code Execution. Ensuring the security of AI models is a burgeoning area of research essential for safeguarding the multitude of organizations and users relying on AI technologies. This study levera...",
      "pdf_url": "https://arxiv.org/pdf/2409.19310v1.pdf",
      "relevance_score": 61,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2506.23296v1",
      "title": "Securing AI Systems: A Guide to Known Attacks and Impacts",
      "authors": [
        "Naoto Kiribuchi",
        "Kengo Zenitani",
        "Takayuki Semitsu"
      ],
      "published": "2025-06-29T15:32:03Z",
      "categories": "",
      "summary": "Embedded into information systems, artificial intelligence (AI) faces security threats that exploit AI-specific vulnerabilities. This paper provides an accessible overview of adversarial attacks unique to predictive and generative AI systems. We identify eleven major attack types and explicitly link attack techniques to their impacts -- including information leakage, system compromise, and resource exhaustion -- mapped to the confidentiality, integrity, and availability (CIA) security triad. We aim to equip researchers, developers, security practitioners, and policymakers, even those without s...",
      "pdf_url": "https://arxiv.org/pdf/2506.23296v1.pdf",
      "relevance_score": 60,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.13175v2",
      "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
      "authors": [
        "Wenpeng Xing",
        "Minghao Li",
        "Mohan Li",
        "Meng Han"
      ],
      "published": "2025-02-18T03:38:07Z",
      "categories": "",
      "summary": "Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vul...",
      "pdf_url": "https://arxiv.org/pdf/2502.13175v2.pdf",
      "relevance_score": 60,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2208.07476v1",
      "title": "CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models",
      "authors": [
        "Chuyen Nguyen",
        "Caleb Morgan",
        "Sudip Mittal"
      ],
      "published": "2022-08-16T00:16:58Z",
      "categories": "",
      "summary": "As the practicality of Artificial Intelligence (AI) and Machine Learning (ML) based techniques grow, there is an ever increasing threat of adversarial attacks. There is a need to red team this ecosystem to identify system vulnerabilities, potential threats, characterize properties that will enhance system robustness, and encourage the creation of effective defenses. A secondary need is to share this AI security threat intelligence between different stakeholders like, model developers, users, and AI/ML security professionals. In this paper, we create and describe a prototype system CTI4AI, to o...",
      "pdf_url": "https://arxiv.org/pdf/2208.07476v1.pdf",
      "relevance_score": 60,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2311.14342v2",
      "title": "AI-based Attack Graph Generation",
      "authors": [
        "Sangbeom Park",
        "Jaesung Lee",
        "Jeong Do Yoo",
        "Min Geun Song",
        "Hyosun Lee",
        "Jaewoong Choi",
        "Chaeyeon Sagong",
        "Huy Kang Kim"
      ],
      "published": "2023-11-24T08:35:16Z",
      "categories": "",
      "summary": "With the advancement of IoT technology, many electronic devices are interconnected through networks, communicating with each other and performing specific roles. However, as numerous devices join networks, the threat of cyberattacks also escalates. Preventing and detecting cyber threats are crucial, and one method of preventing such threats involves using attack graphs. Attack graphs are widely used to assess security threats within networks. However, a drawback emerges as the network scales, as generating attack graphs becomes time-consuming. To overcome this limitation, artificial intelligen...",
      "pdf_url": "https://arxiv.org/pdf/2311.14342v2.pdf",
      "relevance_score": 60,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2508.20411v1",
      "title": "Governable AI: Provable Safety Under Extreme Threat Models",
      "authors": [
        "Donglin Wang",
        "Weiyun Liang",
        "Chunyuan Chen",
        "Jing Xu",
        "Yulong Fu"
      ],
      "published": "2025-08-28T04:22:59Z",
      "categories": "",
      "summary": "As AI rapidly advances, the security risks posed by AI are becoming increasingly severe, especially in critical scenarios, including those posing existential risks. If AI becomes uncontrollable, manipulated, or actively evades safety mechanisms, it could trigger systemic disasters. Existing AI safety approaches-such as model enhancement, value alignment, and human intervention-suffer from fundamental, in-principle limitations when facing AI with extreme motivations and unlimited intelligence, and cannot guarantee security. To address this challenge, we propose a Governable AI (GAI) framework t...",
      "pdf_url": "https://arxiv.org/pdf/2508.20411v1.pdf",
      "relevance_score": 60,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2108.09187v3",
      "title": "Quantization Backdoors to Deep Learning Commercial Frameworks",
      "authors": [
        "Hua Ma",
        "Huming Qiu",
        "Yansong Gao",
        "Zhi Zhang",
        "Alsharif Abuadbba",
        "Minhui Xue",
        "Anmin Fu",
        "Zhang Jiliang",
        "Said Al-Sarawi",
        "Derek Abbott"
      ],
      "published": "2021-08-20T14:08:23Z",
      "categories": "",
      "summary": "Currently, there is a burgeoning demand for deploying deep learning (DL) models on ubiquitous edge Internet of Things (IoT) devices attributed to their low latency and high privacy preservation. However, DL models are often large in size and require large-scale computation, which prevents them from being placed directly onto IoT devices, where resources are constrained and 32-bit floating-point (float-32) operations are unavailable. Commercial framework (i.e., a set of toolkits) empowered model quantization is a pragmatic solution that enables DL deployment on mobile devices and embedded syste...",
      "pdf_url": "https://arxiv.org/pdf/2108.09187v3.pdf",
      "relevance_score": 60,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.18244v1",
      "title": "Development of an Edge Resilient ML Ensemble to Tolerate ICS Adversarial Attacks",
      "authors": [
        "Likai Yao",
        "Qinxuan Shi",
        "Zhanglong Yang",
        "Sicong Shao",
        "Salim Hariri"
      ],
      "published": "2024-09-26T19:37:37Z",
      "categories": "",
      "summary": "Deploying machine learning (ML) in dynamic data-driven applications systems (DDDAS) can improve the security of industrial control systems (ICS). However, ML-based DDDAS are vulnerable to adversarial attacks because adversaries can alter the input data slightly so that the ML models predict a different result. In this paper, our goal is to build a resilient edge machine learning (reML) architecture that is designed to withstand adversarial attacks by performing Data Air Gap Transformation (DAGT) to anonymize data feature spaces using deep neural networks and randomize the ML models used for pr...",
      "pdf_url": "https://arxiv.org/pdf/2409.18244v1.pdf",
      "relevance_score": 60,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.00554v1",
      "title": "Sentry: Authenticating Machine Learning Artifacts on the Fly",
      "authors": [
        "Andrew Gan",
        "Zahra Ghodsi"
      ],
      "published": "2025-10-01T06:13:52Z",
      "categories": "",
      "summary": "Machine learning systems increasingly rely on open-source artifacts such as datasets and models that are created or hosted by other parties. The reliance on external datasets and pre-trained models exposes the system to supply chain attacks where an artifact can be poisoned before it is delivered to the end-user. Such attacks are possible due to the lack of any authenticity verification in existing machine learning systems. Incorporating cryptographic solutions such as hashing and signing can mitigate the risk of supply chain attacks. However, existing frameworks for integrity verification bas...",
      "pdf_url": "https://arxiv.org/pdf/2510.00554v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.19567v2",
      "title": "Atlas: A Framework for ML Lifecycle Provenance & Transparency",
      "authors": [
        "Marcin Spoczynski",
        "Marcela S. Melara",
        "Sebastian Szyller"
      ],
      "published": "2025-02-26T21:18:03Z",
      "categories": "",
      "summary": "The rapid adoption of open source machine learning (ML) datasets and models exposes today's AI applications to critical risks like data poisoning and supply chain attacks across the ML lifecycle. With growing regulatory pressure to address these issues through greater transparency, ML model vendors face challenges balancing these requirements against confidentiality for data and intellectual property needs. We propose Atlas, a framework that enables fully attestable ML pipelines. Atlas leverages open specifications for data and software supply chain provenance to collect verifiable records of ...",
      "pdf_url": "https://arxiv.org/pdf/2502.19567v2.pdf",
      "relevance_score": 59,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2401.14635v2",
      "title": "Signing in Four Public Software Package Registries: Quantity, Quality, and Influencing Factors",
      "authors": [
        "Taylor R Schorlemmer",
        "Kelechi G Kalu",
        "Luke Chigges",
        "Kyung Myung Ko",
        "Eman Abu Isghair",
        "Saurabh Baghi",
        "Santiago Torres-Arias",
        "James C Davis"
      ],
      "published": "2024-01-26T03:55:36Z",
      "categories": "",
      "summary": "Many software applications incorporate open-source third-party packages distributed by public package registries. Guaranteeing authorship along this supply chain is a challenge. Package maintainers can guarantee package authorship through software signing. However, it is unclear how common this practice is, and whether the resulting signatures are created properly. Prior work has provided raw data on registry signing practices, but only measured single platforms, did not consider quality, did not consider time, and did not assess factors that may influence signing. We do not have up-to-date me...",
      "pdf_url": "https://arxiv.org/pdf/2401.14635v2.pdf",
      "relevance_score": 59,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2209.14921v3",
      "title": "IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks",
      "authors": [
        "Neophytos Christou",
        "Di Jin",
        "Vaggelis Atlidakis",
        "Baishakhi Ray",
        "Vasileios P. Kemerlis"
      ],
      "published": "2022-09-29T16:39:34Z",
      "categories": "",
      "summary": "We present IvySyn, the first fully-automated framework for discovering memory error vulnerabilities in Deep Learning (DL) frameworks. IvySyn leverages the statically-typed nature of native APIs in order to automatically perform type-aware mutation-based fuzzing on low-level kernel code. Given a set of offending inputs that trigger memory safety (and runtime) errors in low-level, native DL (C/C++) code, IvySyn automatically synthesizes code snippets in high-level languages (e.g., in Python), which propagate error-triggering input via high(er)-level APIs. Such code snippets essentially act as \"P...",
      "pdf_url": "https://arxiv.org/pdf/2209.14921v3.pdf",
      "relevance_score": 59,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1910.13025v2",
      "title": "Active Subspace of Neural Networks: Structural Analysis and Universal Attacks",
      "authors": [
        "Chunfeng Cui",
        "Kaiqi Zhang",
        "Talgat Daulbaev",
        "Julia Gusak",
        "Ivan Oseledets",
        "Zheng Zhang"
      ],
      "published": "2019-10-29T01:03:23Z",
      "categories": "",
      "summary": "Active subspace is a model reduction method widely used in the uncertainty quantification community. In this paper, we propose analyzing the internal structure and vulnerability and deep neural networks using active subspace. Firstly, we employ the active subspace to measure the number of \"active neurons\" at each intermediate layer and reduce the number of neurons from several thousands to several dozens. This motivates us to change the network structure and to develop a new and more compact network, referred to as {ASNet}, that has significantly fewer model parameters. Secondly, we propose an...",
      "pdf_url": "https://arxiv.org/pdf/1910.13025v2.pdf",
      "relevance_score": 59,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.09488v1",
      "title": "Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts",
      "authors": [
        "Felix M\u00e4chtle",
        "Ashwath Shetty",
        "Jonas Sander",
        "Nils Loose",
        "S\u00f6ren Pirk",
        "Thomas Eisenbarth"
      ],
      "published": "2025-09-11T14:21:59Z",
      "categories": "",
      "summary": "Diffusion models have significantly advanced text-to-image generation, enabling the creation of highly realistic images conditioned on textual prompts and seeds. Given the considerable intellectual and economic value embedded in such prompts, prompt theft poses a critical security and privacy concern. In this paper, we investigate prompt-stealing attacks targeting diffusion models. We reveal that numerical optimization-based prompt recovery methods are fundamentally limited as they do not account for the initial random noise used during image generation. We identify and exploit a noise-generat...",
      "pdf_url": "https://arxiv.org/pdf/2509.09488v1.pdf",
      "relevance_score": 59,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2301.09305v1",
      "title": "Practical Adversarial Attacks Against AI-Driven Power Allocation in a Distributed MIMO Network",
      "authors": [
        "\u00d6mer Faruk Tuna",
        "Fehmi Emre Kadan",
        "Leyli Kara\u00e7ay"
      ],
      "published": "2023-01-23T07:51:25Z",
      "categories": "",
      "summary": "In distributed multiple-input multiple-output (D-MIMO) networks, power control is crucial to optimize the spectral efficiencies of users and max-min fairness (MMF) power control is a commonly used strategy as it satisfies uniform quality-of-service to all users. The optimal solution of MMF power control requires high complexity operations and hence deep neural network based artificial intelligence (AI) solutions are proposed to decrease the complexity. Although quite accurate models can be achieved by using AI, these models have some intrinsic vulnerabilities against adversarial attacks where ...",
      "pdf_url": "https://arxiv.org/pdf/2301.09305v1.pdf",
      "relevance_score": 58,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2106.04690v2",
      "title": "Handcrafted Backdoors in Deep Neural Networks",
      "authors": [
        "Sanghyun Hong",
        "Nicholas Carlini",
        "Alexey Kurakin"
      ],
      "published": "2021-06-08T20:58:23Z",
      "categories": "",
      "summary": "When machine learning training is outsourced to third parties, $backdoor$ $attacks$ become practical as the third party who trains the model may act maliciously to inject hidden behaviors into the otherwise accurate model. Until now, the mechanism to inject backdoors has been limited to $poisoning$. We argue that a supply-chain attacker has more attack techniques available by introducing a $handcrafted$ attack that directly manipulates a model's weights. This direct modification gives our attacker more degrees of freedom compared to poisoning, and we show it can be used to evade many backdoor ...",
      "pdf_url": "https://arxiv.org/pdf/2106.04690v2.pdf",
      "relevance_score": 58,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2503.01758v1",
      "title": "Zero-Trust Artificial Intelligence Model Security Based on Moving Target Defense and Content Disarm and Reconstruction",
      "authors": [
        "Daniel Gilkarov",
        "Ran Dubin"
      ],
      "published": "2025-03-03T17:32:19Z",
      "categories": "",
      "summary": "This paper examines the challenges in distributing AI models through model zoos and file transfer mechanisms. Despite advancements in security measures, vulnerabilities persist, necessitating a multi-layered approach to mitigate risks effectively. The physical security of model files is critical, requiring stringent access controls and attack prevention solutions. This paper proposes a novel solution architecture composed of two prevention approaches. The first is Content Disarm and Reconstruction (CDR), which focuses on disarming serialization attacks that enable attackers to run malicious co...",
      "pdf_url": "https://arxiv.org/pdf/2503.01758v1.pdf",
      "relevance_score": 58,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2309.05814v1",
      "title": "Reinforcement Learning for Supply Chain Attacks Against Frequency and Voltage Control",
      "authors": [
        "Amr S. Mohamed",
        "Sumin Lee",
        "Deepa Kundur"
      ],
      "published": "2023-09-11T20:47:11Z",
      "categories": "",
      "summary": "The ongoing modernization of the power system, involving new equipment installations and upgrades, exposes the power system to the introduction of malware into its operation through supply chain attacks. Supply chain attacks present a significant threat to power systems, allowing cybercriminals to bypass network defenses and execute deliberate attacks at the physical layer. Given the exponential advancements in machine intelligence, cybercriminals will leverage this technology to create sophisticated and adaptable attacks that can be incorporated into supply chain attacks. We demonstrate the u...",
      "pdf_url": "https://arxiv.org/pdf/2309.05814v1.pdf",
      "relevance_score": 56,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.16899v1",
      "title": "Security Vulnerabilities in Software Supply Chain for Autonomous Vehicles",
      "authors": [
        "Md Wasiul Haque",
        "Md Erfan",
        "Sagar Dasgupta",
        "Md Rayhanur Rahman",
        "Mizanur Rahman"
      ],
      "published": "2025-09-21T03:22:05Z",
      "categories": "",
      "summary": "The interest in autonomous vehicles (AVs) for critical missions, including transportation, rescue, surveillance, reconnaissance, and mapping, is growing rapidly due to their significant safety and mobility benefits. AVs consist of complex software systems that leverage artificial intelligence (AI), sensor fusion algorithms, and real-time data processing. Additionally, AVs are becoming increasingly reliant on open-source software supply chains, such as open-source packages, third-party software components, AI models, and third-party datasets. Software security best practices in the automotive s...",
      "pdf_url": "https://arxiv.org/pdf/2509.16899v1.pdf",
      "relevance_score": 56,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.23643v2",
      "title": "Securing AI Agents with Information-Flow Control",
      "authors": [
        "Manuel Costa",
        "Boris K\u00f6pf",
        "Aashish Kolluri",
        "Andrew Paverd",
        "Mark Russinovich",
        "Ahmed Salem",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B\u00e9guelin"
      ],
      "published": "2025-05-29T16:50:41Z",
      "categories": "",
      "summary": "As AI agents become increasingly autonomous and capable, ensuring their security against vulnerabilities such as prompt injection becomes critical. This paper explores the use of information-flow control (IFC) to provide security guarantees for AI agents. We present a formal model to reason about the security and expressiveness of agent planners. Using this model, we characterize the class of properties enforceable by dynamic taint-tracking and construct a taxonomy of tasks to evaluate security and utility trade-offs of planner designs. Informed by this exploration, we present Fides, a planner...",
      "pdf_url": "https://arxiv.org/pdf/2505.23643v2.pdf",
      "relevance_score": 54,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.13597v1",
      "title": "Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents",
      "authors": [
        "Abhishek Goswami"
      ],
      "published": "2025-09-16T23:43:24Z",
      "categories": "",
      "summary": "Autonomous LLM agents can issue thousands of API calls per hour without human oversight. OAuth 2.0 assumes deterministic clients, but in agentic settings stochastic reasoning, prompt injection, or multi-agent orchestration can silently expand privileges.   We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each agent's action to verifiable user intent and, optionally, to a specific workflow step. A-JWT carries an agent's identity as a one-way checksum hash derived from its prompt, tools and configuration, and a chained delegation assertion to prove which downstream agent ...",
      "pdf_url": "https://arxiv.org/pdf/2509.13597v1.pdf",
      "relevance_score": 54,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2501.19012v1",
      "title": "Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities",
      "authors": [
        "Arjun Krishna",
        "Erick Galinkin",
        "Leon Derczynski",
        "Jeffrey Martin"
      ],
      "published": "2025-01-31T10:26:18Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) have become an essential tool in the programmer's toolkit, but their tendency to hallucinate code can be used by malicious actors to introduce vulnerabilities to broad swathes of the software supply chain. In this work, we analyze package hallucination behaviour in LLMs across popular programming languages examining both existing package references and fictional dependencies. By analyzing this package hallucination behaviour we find potential attacks and suggest defensive strategies to defend against these attacks. We discover that package hallucination rate is pre...",
      "pdf_url": "https://arxiv.org/pdf/2501.19012v1.pdf",
      "relevance_score": 54,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2407.18760v4",
      "title": "Maven-Hijack: Software Supply Chain Attack Exploiting Packaging Order",
      "authors": [
        "Frank Reyes",
        "Federico Bono",
        "Aman Sharma",
        "Benoit Baudry",
        "Martin Monperrus"
      ],
      "published": "2024-07-26T14:17:47Z",
      "categories": "",
      "summary": "Java projects frequently rely on package managers such as Maven to manage complex webs of external dependencies. While these tools streamline development, they also introduce subtle risks to the software supply chain. In this paper, we present Maven-Hijack, a novel attack that exploits the order in which Maven packages dependencies and the way the Java Virtual Machine resolves classes at runtime. By injecting a malicious class with the same fully qualified name as a legitimate one into a dependency that is packaged earlier, an attacker can silently override core application behavior without mo...",
      "pdf_url": "https://arxiv.org/pdf/2407.18760v4.pdf",
      "relevance_score": 54,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.08646v1",
      "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations",
      "authors": [
        "Ron F. Del Rosario",
        "Klaudia Krawiecka",
        "Christian Schroeder de Witt"
      ],
      "published": "2025-09-10T14:41:07Z",
      "categories": "",
      "summary": "As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount. This paper provides a comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic design that separates strategic planning from tactical execution. We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + ...",
      "pdf_url": "https://arxiv.org/pdf/2509.08646v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.14240v1",
      "title": "Combined Quantum and Post-Quantum Security for Earth-Satellite Channels",
      "authors": [
        "Anju Rani",
        "Xiaoyu Ai",
        "Aman Gupta",
        "Ravi Singh Adhikari",
        "Robert Malaney"
      ],
      "published": "2025-02-20T04:08:23Z",
      "categories": "",
      "summary": "Experimental deployment of quantum communication over Earth-satellite channels opens the way to a secure global quantum Internet. In this work, we present results from a real-time prototype quantum key distribution (QKD) system, which entails the development of optical systems including the encoding of entangled photon pairs, the development of transmitters for quantum signaling through an emulated Earth-satellite channel, and the development of quantum-decoding receivers. A unique aspect of our system is the integration of QKD with existing cryptographic methods to ensure quantum-resistant se...",
      "pdf_url": "https://arxiv.org/pdf/2502.14240v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2412.17908v3",
      "title": "Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning",
      "authors": [
        "Orson Mengara"
      ],
      "published": "2024-12-23T19:04:46Z",
      "categories": "",
      "summary": "With the rapid development of generative artificial intelligence, particularly large language models a number of sub-fields of deep learning have made significant progress and are now very useful in everyday applications. For example,financial institutions simulate a wide range of scenarios for various models created by their research teams using reinforcement learning, both before production and after regular operations. In this work, we propose a backdoor attack that focuses solely on data poisoning and a method of detection by dynamic systems and statistical analysis of the distribution of ...",
      "pdf_url": "https://arxiv.org/pdf/2412.17908v3.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2503.08636v2",
      "title": "Birds look like cars: Adversarial analysis of intrinsically interpretable deep learning",
      "authors": [
        "Hubert Baniecki",
        "Przemyslaw Biecek"
      ],
      "published": "2025-03-11T17:24:33Z",
      "categories": "",
      "summary": "A common belief is that intrinsically interpretable deep learning models ensure a correct, intuitive understanding of their behavior and offer greater robustness against accidental errors or intentional manipulation. However, these beliefs have not been comprehensively verified, and growing evidence casts doubt on them. In this paper, we highlight the risks related to overreliance and susceptibility to adversarial manipulation of these so-called \"intrinsically (aka inherently) interpretable\" models by design. We introduce two strategies for adversarial analysis with prototype manipulation and ...",
      "pdf_url": "https://arxiv.org/pdf/2503.08636v2.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2403.10717v1",
      "title": "Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency",
      "authors": [
        "Soumyadeep Pal",
        "Yuguang Yao",
        "Ren Wang",
        "Bingquan Shen",
        "Sijia Liu"
      ],
      "published": "2024-03-15T22:35:07Z",
      "categories": "",
      "summary": "Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional ...",
      "pdf_url": "https://arxiv.org/pdf/2403.10717v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2011.07429v1",
      "title": "Dynamic backdoor attacks against federated learning",
      "authors": [
        "Anbu Huang"
      ],
      "published": "2020-11-15T01:32:58Z",
      "categories": "",
      "summary": "Federated Learning (FL) is a new machine learning framework, which enables millions of participants to collaboratively train machine learning model without compromising data privacy and security. Due to the independence and confidentiality of each client, FL does not guarantee that all clients are honest by design, which makes it vulnerable to adversarial attack naturally. In this paper, we focus on dynamic backdoor attacks under FL setting, where the goal of the adversary is to reduce the performance of the model on targeted tasks while maintaining a good performance on the main task, current...",
      "pdf_url": "https://arxiv.org/pdf/2011.07429v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2208.06176v1",
      "title": "A Knowledge Distillation-Based Backdoor Attack in Federated Learning",
      "authors": [
        "Yifan Wang",
        "Wei Fan",
        "Keke Yang",
        "Naji Alhusaini",
        "Jing Li"
      ],
      "published": "2022-08-12T08:52:56Z",
      "categories": "",
      "summary": "Federated Learning (FL) is a novel framework of decentralized machine learning. Due to the decentralized feature of FL, it is vulnerable to adversarial attacks in the training procedure, e.g. , backdoor attacks. A backdoor attack aims to inject a backdoor into the machine learning model such that the model will make arbitrarily incorrect behavior on the test sample with some specific backdoor trigger. Even though a range of backdoor attack methods of FL has been introduced, there are also methods defending against them. Many of the defending methods utilize the abnormal characteristics of the ...",
      "pdf_url": "https://arxiv.org/pdf/2208.06176v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2211.15929v1",
      "title": "Backdoor Vulnerabilities in Normally Trained Deep Learning Models",
      "authors": [
        "Guanhong Tao",
        "Zhenting Wang",
        "Siyuan Cheng",
        "Shiqing Ma",
        "Shengwei An",
        "Yingqi Liu",
        "Guangyu Shen",
        "Zhuo Zhang",
        "Yunshu Mao",
        "Xiangyu Zhang"
      ],
      "published": "2022-11-29T04:55:32Z",
      "categories": "",
      "summary": "We conduct a systematic study of backdoor vulnerabilities in normally trained Deep Learning models. They are as dangerous as backdoors injected by data poisoning because both can be equally exploited. We leverage 20 different types of injected backdoor attacks in the literature as the guidance and study their correspondences in normally trained models, which we call natural backdoor vulnerabilities. We find that natural backdoors are widely existing, with most injected backdoor attacks having natural correspondences. We categorize these natural backdoors and propose a general detection framewo...",
      "pdf_url": "https://arxiv.org/pdf/2211.15929v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2303.06818v1",
      "title": "Backdoor Defense via Deconfounded Representation Learning",
      "authors": [
        "Zaixi Zhang",
        "Qi Liu",
        "Zhicai Wang",
        "Zepu Lu",
        "Qingyong Hu"
      ],
      "published": "2023-03-13T02:25:59Z",
      "categories": "",
      "summary": "Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by injecting a few poisoned examples into the training dataset. While extensive efforts have been made to detect and remove backdoors from backdoored DNNs, it is still not clear whether a backdoor-free clean model can be directly obtained from poisoned datasets. In this paper, we first construct a causal graph to model the generation process of poisoned data and find that the backdoor attack acts as the confounder, which brings spurious associations betwe...",
      "pdf_url": "https://arxiv.org/pdf/2303.06818v1.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2212.10388v2",
      "title": "ThreatKG: An AI-Powered System for Automated Open-Source Cyber Threat Intelligence Gathering and Management",
      "authors": [
        "Peng Gao",
        "Xiaoyuan Liu",
        "Edward Choi",
        "Sibo Ma",
        "Xinyu Yang",
        "Dawn Song"
      ],
      "published": "2022-12-20T16:13:59Z",
      "categories": "",
      "summary": "Open-source cyber threat intelligence (OSCTI) has become essential for keeping up with the rapidly changing threat landscape. However, current OSCTI gathering and management solutions mainly focus on structured Indicators of Compromise (IOC) feeds, which are low-level and isolated, providing only a narrow view of potential threats. Meanwhile, the extensive and interconnected knowledge found in the unstructured text of numerous OSCTI reports (e.g., security articles, threat reports) available publicly is still largely underexplored.   To bridge the gap, we propose ThreatKG, an automated system ...",
      "pdf_url": "https://arxiv.org/pdf/2212.10388v2.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2512.15503v2",
      "title": "Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection",
      "authors": [
        "Konstantinos Kalogiannis",
        "Ahmed Mohamed Hussain",
        "Hexu Li",
        "Panos Papadimitratos"
      ],
      "published": "2025-12-17T14:45:33Z",
      "categories": "",
      "summary": "Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capt...",
      "pdf_url": "https://arxiv.org/pdf/2512.15503v2.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2309.06223v3",
      "title": "Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack Surfaces in DNN Executables",
      "authors": [
        "Yanzuo Chen",
        "Zhibo Liu",
        "Yuanyuan Yuan",
        "Sihang Hu",
        "Tianxiang Li",
        "Shuai Wang"
      ],
      "published": "2023-09-12T13:42:20Z",
      "categories": "",
      "summary": "Bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs). For high-level DNN models running on deep learning (DL) frameworks like PyTorch, extensive BFAs have been used to flip bits in model weights and shown effective. Defenses have also been proposed to guard model weights. However, DNNs are increasingly compiled into DNN executables by DL compilers to leverage hardware primitives. These executables manifest distinct computation paradigms; existing research fails to accurately capture and expose the BFA surfaces on DNN executables.   To this end, we launch the first systematic stud...",
      "pdf_url": "https://arxiv.org/pdf/2309.06223v3.pdf",
      "relevance_score": 51,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.03793v3",
      "title": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures",
      "authors": [
        "Ishaan Domkundwar",
        "Mukunda N S",
        "Ishaan Bhola",
        "Riddhik Kochhar"
      ],
      "published": "2024-09-03T10:14:51Z",
      "categories": "",
      "summary": "AI agents, specifically powered by large language models, have demonstrated exceptional capabilities in various applications where precision and efficacy are necessary. However, these agents come with inherent risks, including the potential for unsafe or biased actions, vulnerability to adversarial attacks, lack of transparency, and tendency to generate hallucinations. As AI agents become more prevalent in critical sectors of the industry, the implementation of effective safety protocols becomes increasingly important. This paper addresses the critical need for safety measures in AI systems, e...",
      "pdf_url": "https://arxiv.org/pdf/2409.03793v3.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2209.02442v2",
      "title": "SimCLF: A Simple Contrastive Learning Framework for Function-level Binary Embeddings",
      "authors": [
        "Sun RuiJin",
        "Guo Shize",
        "Guo Jinhong",
        "Li Wei",
        "Zhan Dazhi",
        "Sun Meng",
        "Pan Zhisong"
      ],
      "published": "2022-09-06T12:09:45Z",
      "categories": "",
      "summary": "Function-level binary code similarity detection is a crucial aspect of cybersecurity. It enables the detection of bugs and patent infringements in released software and plays a pivotal role in preventing supply chain attacks. A practical embedding learning framework relies on the robustness of the assembly code representation and the accuracy of function-pair annotation, which is traditionally accomplished using supervised learning-based frameworks. However, annotating different function pairs with accurate labels poses considerable challenges. These supervised learning methods can be easily o...",
      "pdf_url": "https://arxiv.org/pdf/2209.02442v2.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1911.07963v2",
      "title": "Can You Really Backdoor Federated Learning?",
      "authors": [
        "Ziteng Sun",
        "Peter Kairouz",
        "Ananda Theertha Suresh",
        "H. Brendan McMahan"
      ],
      "published": "2019-11-18T21:25:03Z",
      "categories": "",
      "summary": "The decentralized nature of federated learning makes detecting and defending against adversarial attacks a challenging task. This paper focuses on backdoor attacks in the federated learning setting, where the goal of the adversary is to reduce the performance of the model on targeted tasks while maintaining good performance on the main task. Unlike existing works, we allow non-malicious clients to have correctly labeled samples from the targeted tasks. We conduct a comprehensive study of backdoor attacks and defenses for the EMNIST dataset, a real-life, user-partitioned, and non-iid dataset. W...",
      "pdf_url": "https://arxiv.org/pdf/1911.07963v2.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2101.02281v5",
      "title": "FLAME: Taming Backdoors in Federated Learning (Extended Version 1)",
      "authors": [
        "Thien Duc Nguyen",
        "Phillip Rieger",
        "Huili Chen",
        "Hossein Yalame",
        "Helen M\u00f6llering",
        "Hossein Fereidooni",
        "Samuel Marchal",
        "Markus Miettinen",
        "Azalia Mirhoseini",
        "Shaza Zeitouni",
        "Farinaz Koushanfar",
        "Ahmad-Reza Sadeghi",
        "Thomas Schneider"
      ],
      "published": "2021-01-06T21:49:27Z",
      "categories": "",
      "summary": "Federated Learning (FL) is a collaborative machine learning approach allowing participants to jointly train a model without having to share their private, potentially sensitive local datasets with others. Despite its benefits, FL is vulnerable to backdoor attacks, in which an adversary injects manipulated model updates into the model aggregation process so that the resulting model will provide targeted false predictions for specific adversary-chosen inputs. Proposed defenses against backdoor attacks based on detecting and filtering out malicious model updates consider only very specific and li...",
      "pdf_url": "https://arxiv.org/pdf/2101.02281v5.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2007.03608v1",
      "title": "Backdoor attacks and defenses in feature-partitioned collaborative learning",
      "authors": [
        "Yang Liu",
        "Zhihao Yi",
        "Tianjian Chen"
      ],
      "published": "2020-07-07T16:45:20Z",
      "categories": "",
      "summary": "Since there are multiple parties in collaborative learning, malicious parties might manipulate the learning process for their own purposes through backdoor attacks. However, most of existing works only consider the federated learning scenario where data are partitioned by samples. The feature-partitioned learning can be another important scenario since in many real world applications, features are often distributed across different parties. Attacks and defenses in such scenario are especially challenging when the attackers have no labels and the defenders are not able to access the data and mo...",
      "pdf_url": "https://arxiv.org/pdf/2007.03608v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2109.10512v1",
      "title": "Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis",
      "authors": [
        "Zeyuan Yin",
        "Ye Yuan",
        "Panfeng Guo",
        "Pan Zhou"
      ],
      "published": "2021-09-22T04:19:59Z",
      "categories": "",
      "summary": "Edge devices in federated learning usually have much more limited computation and communication resources compared to servers in a data center. Recently, advanced model compression methods, like the Lottery Ticket Hypothesis, have already been implemented on federated learning to reduce the model size and communication cost. However, Backdoor Attack can compromise its implementation in the federated learning scenario. The malicious edge device trains the client model with poisoned private data and uploads parameters to the center, embedding a backdoor to the global shared model after unwitting...",
      "pdf_url": "https://arxiv.org/pdf/2109.10512v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2303.00302v2",
      "title": "Mitigating Backdoors in Federated Learning with FLD",
      "authors": [
        "Yihang Lin",
        "Pengyuan Zhou",
        "Zhiqian Wu",
        "Yong Liao"
      ],
      "published": "2023-03-01T07:54:54Z",
      "categories": "",
      "summary": "Federated learning allows clients to collaboratively train a global model without uploading raw data for privacy preservation. This feature, i.e., the inability to review participants' datasets, has recently been found responsible for federated learning's vulnerability in the face of backdoor attacks. Existing defense methods fall short from two perspectives: 1) they consider only very specific and limited attacker models and unable to cope with advanced backdoor attacks, such as distributed backdoor attacks, which break down the global trigger into multiple distributed triggers. 2) they condu...",
      "pdf_url": "https://arxiv.org/pdf/2303.00302v2.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2202.03609v5",
      "title": "PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning",
      "authors": [
        "Junfeng Guo",
        "Ang Li",
        "Cong Liu"
      ],
      "published": "2022-02-08T02:49:09Z",
      "categories": "",
      "summary": "While real-world applications of reinforcement learning are becoming popular, the security and robustness of RL systems are worthy of more attention and exploration. In particular, recent works have revealed that, in a multi-agent RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action. To ensure the security of RL agents against malicious backdoors, in this work, we propose the problem of Backdoor Detection in a multi-agent competitive reinforcement learning sy...",
      "pdf_url": "https://arxiv.org/pdf/2202.03609v5.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1708.03366v2",
      "title": "Resilient Linear Classification: An Approach to Deal with Attacks on Training Data",
      "authors": [
        "Sangdon Park",
        "James Weimer",
        "Insup Lee"
      ],
      "published": "2017-08-10T19:54:58Z",
      "categories": "",
      "summary": "Data-driven techniques are used in cyber-physical systems (CPS) for controlling autonomous vehicles, handling demand responses for energy management, and modeling human physiology for medical devices. These data-driven techniques extract models from training data, where their performance is often analyzed with respect to random errors in the training data. However, if the training data is maliciously altered by attackers, the effect of these attacks on the learning algorithms underpinning data-driven CPS have yet to be considered. In this paper, we analyze the resilience of classification algo...",
      "pdf_url": "https://arxiv.org/pdf/1708.03366v2.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.17844v1",
      "title": "Software Security Analysis in 2030 and Beyond: A Research Roadmap",
      "authors": [
        "Marcel B\u00f6hme",
        "Eric Bodden",
        "Tevfik Bultan",
        "Cristian Cadar",
        "Yang Liu",
        "Giuseppe Scanniello"
      ],
      "published": "2024-09-26T13:50:41Z",
      "categories": "",
      "summary": "As our lives, our businesses, and indeed our world economy become increasingly reliant on the secure operation of many interconnected software systems, the software engineering research community is faced with unprecedented research challenges, but also with exciting new opportunities. In this roadmap paper, we outline our vision of Software Security Analysis for the software systems of the future. Given the recent advances in generative AI, we need new methods to evaluate and maximize the security of code co-written by machines. As our software systems become increasingly heterogeneous, we ne...",
      "pdf_url": "https://arxiv.org/pdf/2409.17844v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2203.06502v1",
      "title": "Characterizing and Understanding Software Security Vulnerabilities in Machine Learning Libraries",
      "authors": [
        "Nima Shiri Harzevili",
        "Jiho Shin",
        "Junjie Wang",
        "Song Wang"
      ],
      "published": "2022-03-12T18:58:51Z",
      "categories": "",
      "summary": "The application of machine learning (ML) libraries has been tremendously increased in many domains, including autonomous driving systems, medical, and critical industries. Vulnerabilities of such libraries result in irreparable consequences. However, the characteristics of software security vulnerabilities have not been well studied. In this paper, to bridge this gap, we take the first step towards characterizing and understanding the security vulnerabilities of five well-known ML libraries, including Tensorflow, PyTorch, Sickit-learn, Pandas, and Numpy. To do so, in total, we collected 596 se...",
      "pdf_url": "https://arxiv.org/pdf/2203.06502v1.pdf",
      "relevance_score": 49,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2304.11087v1",
      "title": "AI Product Security: A Primer for Developers",
      "authors": [
        "Ebenezer R. H. P. Isaac",
        "Jim Reno"
      ],
      "published": "2023-04-18T05:22:34Z",
      "categories": "",
      "summary": "Not too long ago, AI security used to mean the research and practice of how AI can empower cybersecurity, that is, AI for security. Ever since Ian Goodfellow and his team popularized adversarial attacks on machine learning, security for AI became an important concern and also part of AI security. It is imperative to understand the threats to machine learning products and avoid common pitfalls in AI product development. This article is addressed to developers, designers, managers and researchers of AI software products.",
      "pdf_url": "https://arxiv.org/pdf/2304.11087v1.pdf",
      "relevance_score": 48,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.11398v1",
      "title": "From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming",
      "authors": [
        "Anusha Sinha",
        "Keltin Grimes",
        "James Lucassen",
        "Michael Feffer",
        "Nathan VanHoudnos",
        "Zhiwei Steven Wu",
        "Hoda Heidari"
      ],
      "published": "2025-09-14T19:21:58Z",
      "categories": "",
      "summary": "A red team simulates adversary attacks to help defenders find effective strategies to defend their systems in a real-world operational setting. As more enterprise systems adopt AI, red-teaming will need to evolve to address the unique vulnerabilities and risks posed by AI systems. We take the position that AI systems can be more effectively red-teamed if AI red-teaming is recognized as a domain-specific evolution of cyber red-teaming. Specifically, we argue that existing Cyber Red Teams who adopt this framing will be able to better evaluate systems with AI components by recognizing that AI pos...",
      "pdf_url": "https://arxiv.org/pdf/2509.11398v1.pdf",
      "relevance_score": 48,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2410.08216v1",
      "title": "New technologies and AI: envisioning future directions for UNSCR 1540",
      "authors": [
        "Clara Punzi"
      ],
      "published": "2024-09-25T12:41:12Z",
      "categories": "",
      "summary": "This paper investigates the emerging challenges posed by the integration of Artificial Intelligence (AI) in the military domain, particularly within the context of United Nations Security Council Resolution 1540 (UNSCR 1540), which seeks to prevent the proliferation of weapons of mass destruction (WMDs). While the resolution initially focused on nuclear, chemical, and biological threats, the rapid advancement of AI introduces new complexities that were previously unanticipated. We critically analyze how AI can both exacerbate existing risks associated with WMDs (e.g., thorough the deployment o...",
      "pdf_url": "https://arxiv.org/pdf/2410.08216v1.pdf",
      "relevance_score": 48,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1807.01069v4",
      "title": "Adversarial Robustness Toolbox v1.0.0",
      "authors": [
        "Maria-Irina Nicolae",
        "Mathieu Sinn",
        "Minh Ngoc Tran",
        "Beat Buesser",
        "Ambrish Rawat",
        "Martin Wistuba",
        "Valentina Zantedeschi",
        "Nathalie Baracaldo",
        "Bryant Chen",
        "Heiko Ludwig",
        "Ian M. Molloy",
        "Ben Edwards"
      ],
      "published": "2018-07-03T10:25:26Z",
      "categories": "",
      "summary": "Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model....",
      "pdf_url": "https://arxiv.org/pdf/1807.01069v4.pdf",
      "relevance_score": 48,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2407.21174v1",
      "title": "AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning",
      "authors": [
        "Maisha Binte Rashid",
        "Pablo Rivas"
      ],
      "published": "2024-07-30T20:28:31Z",
      "categories": "",
      "summary": "Multimodal machine learning models that combine visual and textual data are increasingly being deployed in critical applications, raising significant safety and security concerns due to their vulnerability to adversarial attacks. This paper presents an effective strategy to enhance the robustness of multimodal image captioning models against such attacks. By leveraging the Fast Gradient Sign Method (FGSM) to generate adversarial examples and incorporating adversarial training techniques, we demonstrate improved model robustness on two benchmark datasets: Flickr8k and COCO. Our findings indicat...",
      "pdf_url": "https://arxiv.org/pdf/2407.21174v1.pdf",
      "relevance_score": 46,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2006.07026v2",
      "title": "Backdoor Attacks on Federated Meta-Learning",
      "authors": [
        "Chien-Lun Chen",
        "Leana Golubchik",
        "Marco Paolieri"
      ],
      "published": "2020-06-12T09:23:24Z",
      "categories": "",
      "summary": "Federated learning allows multiple users to collaboratively train a shared classification model while preserving data privacy. This approach, where model updates are aggregated by a central server, was shown to be vulnerable to poisoning backdoor attacks: a malicious user can alter the shared model to arbitrarily classify specific inputs from a given class. In this paper, we analyze the effects of backdoor attacks on federated meta-learning, where users train a model that can be adapted to different sets of output classes using only a few examples. While the ability to adapt could, in principl...",
      "pdf_url": "https://arxiv.org/pdf/2006.07026v2.pdf",
      "relevance_score": 46,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2008.11533v2",
      "title": "SIGL: Securing Software Installations Through Deep Graph Learning",
      "authors": [
        "Xueyuan Han",
        "Xiao Yu",
        "Thomas Pasquier",
        "Ding Li",
        "Junghwan Rhee",
        "James Mickens",
        "Margo Seltzer",
        "Haifeng Chen"
      ],
      "published": "2020-08-26T12:52:34Z",
      "categories": "",
      "summary": "Many users implicitly assume that software can only be exploited after it is installed. However, recent supply-chain attacks demonstrate that application integrity must be ensured during installation itself. We introduce SIGL, a new tool for detecting malicious behavior during software installation. SIGL collects traces of system call activity, building a data provenance graph that it analyzes using a novel autoencoder architecture with a graph long short-term memory network (graph LSTM) for the encoder and a standard multilayer perceptron for the decoder. SIGL flags suspicious installations a...",
      "pdf_url": "https://arxiv.org/pdf/2008.11533v2.pdf",
      "relevance_score": 44,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2212.09979v1",
      "title": "Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation",
      "authors": [
        "Tianrui Qin",
        "Xianghuan He",
        "Xitong Gao",
        "Yiren Zhao",
        "Kejiang Ye",
        "Cheng-Zhong Xu"
      ],
      "published": "2022-12-20T03:43:54Z",
      "categories": "",
      "summary": "Open software supply chain attacks, once successful, can exact heavy costs in mission-critical applications. As open-source ecosystems for deep learning flourish and become increasingly universal, they present attackers previously unexplored avenues to code-inject malicious backdoors in deep neural network models. This paper proposes Flareon, a small, stealthy, seemingly harmless code modification that specifically targets the data augmentation pipeline with motion-based triggers. Flareon neither alters ground-truth labels, nor modifies the training loss objective, nor does it assume prior kno...",
      "pdf_url": "https://arxiv.org/pdf/2212.09979v1.pdf",
      "relevance_score": 44,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2104.02361v2",
      "title": "Backdoor Attack in the Physical World",
      "authors": [
        "Yiming Li",
        "Tongqing Zhai",
        "Yong Jiang",
        "Zhifeng Li",
        "Shu-Tao Xia"
      ],
      "published": "2021-04-06T08:37:33Z",
      "categories": "",
      "summary": "Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of infected models will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger. Currently, most existing backdoor attacks adopted the setting of static trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing trigger characteristics. We demonstrate that this attack paradigm is vulnerable when the trigger in testing image...",
      "pdf_url": "https://arxiv.org/pdf/2104.02361v2.pdf",
      "relevance_score": 44,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2405.15245v1",
      "title": "Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee",
      "authors": [
        "Mengtong Gao",
        "Yifei Zou",
        "Zuyuan Zhang",
        "Xiuzhen Cheng",
        "Dongxiao Yu"
      ],
      "published": "2024-05-24T06:13:31Z",
      "categories": "",
      "summary": "The safety of decentralized reinforcement learning (RL) is a challenging problem since malicious agents can share their poisoned policies with benign agents. The paper investigates a cooperative backdoor attack in a decentralized reinforcement learning scenario. Differing from the existing methods that hide a whole backdoor attack behind their shared policies, our method decomposes the backdoor behavior into multiple components according to the state space of RL. Each malicious agent hides one component in its policy and shares its policy with the benign agents. When a benign agent learns all ...",
      "pdf_url": "https://arxiv.org/pdf/2405.15245v1.pdf",
      "relevance_score": 44,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2601.01241v1",
      "title": "MCP-SandboxScan: WASM-based Secure Execution and Runtime Analysis for MCP Tools",
      "authors": [
        "Zhuoran Tan",
        "Run Hao",
        "Jeremy Singer",
        "Yutian Tang",
        "Christos Anagnostopoulos"
      ],
      "published": "2026-01-03T17:25:38Z",
      "categories": "",
      "summary": "Tool-augmented LLM agents raise new security risks: tool executions can introduce runtime-only behaviors, including prompt injection and unintended exposure of external inputs (e.g., environment secrets or local files). While existing scanners often focus on static artifacts, analyzing runtime behavior is challenging because directly executing untrusted tools can itself be dangerous. We present MCP-SandboxScan, a lightweight framework motivated by the Model Context Protocol (MCP) that safely executes untrusted tools inside a WebAssembly/WASI sandbox and produces auditable reports of external-t...",
      "pdf_url": "https://arxiv.org/pdf/2601.01241v1.pdf",
      "relevance_score": 42,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2301.07520v1",
      "title": "Adversarial AI in Insurance: Pervasiveness and Resilience",
      "authors": [
        "Elisa Luciano",
        "Matteo Cattaneo",
        "Ron Kenett"
      ],
      "published": "2023-01-17T08:49:54Z",
      "categories": "",
      "summary": "The rapid and dynamic pace of Artificial Intelligence (AI) and Machine Learning (ML) is revolutionizing the insurance sector. AI offers significant, very much welcome advantages to insurance companies, and is fundamental to their customer-centricity strategy. It also poses challenges, in the project and implementation phase. Among those, we study Adversarial Attacks, which consist of the creation of modified input data to deceive an AI system and produce false outputs. We provide examples of attacks on insurance AI applications, categorize them, and argue on defence methods and precautionary s...",
      "pdf_url": "https://arxiv.org/pdf/2301.07520v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1811.00189v3",
      "title": "Unauthorized AI cannot Recognize Me: Reversible Adversarial Example",
      "authors": [
        "Jiayang Liu",
        "Weiming Zhang",
        "Kazuto Fukuchi",
        "Youhei Akimoto",
        "Jun Sakuma"
      ],
      "published": "2018-11-01T02:28:31Z",
      "categories": "",
      "summary": "In this study, we propose a new methodology to control how user's data is recognized and used by AI via exploiting the properties of adversarial examples. For this purpose, we propose reversible adversarial example (RAE), a new type of adversarial example. A remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation. On the other hand, other unauthorized AI models cannot recognize it correctly because it functions as a...",
      "pdf_url": "https://arxiv.org/pdf/1811.00189v3.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2101.06704v1",
      "title": "Adversarial Interaction Attack: Fooling AI to Misinterpret Human Intentions",
      "authors": [
        "Nodens Koren",
        "Qiuhong Ke",
        "Yisen Wang",
        "James Bailey",
        "Xingjun Ma"
      ],
      "published": "2021-01-17T16:23:20Z",
      "categories": "",
      "summary": "Understanding the actions of both humans and artificial intelligence (AI) agents is important before modern AI systems can be fully integrated into our daily life. In this paper, we show that, despite their current huge success, deep learning based AI systems can be easily fooled by subtle adversarial noise to misinterpret the intention of an action in interaction scenarios. Based on a case study of skeleton-based human interactions, we propose a novel adversarial attack on interactions, and demonstrate how DNN-based interaction models can be tricked to predict the participants' reactions in u...",
      "pdf_url": "https://arxiv.org/pdf/2101.06704v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.18175v2",
      "title": "Generative AI for Physical-Layer Authentication",
      "authors": [
        "Rui Meng",
        "Xiqi Cheng",
        "Song Gao",
        "Xiaodong Xu",
        "Chen Dong",
        "Guoshun Nan",
        "Xiaofeng Tao",
        "Ping Zhang",
        "Tony Q. S. Quek"
      ],
      "published": "2025-04-25T08:46:38Z",
      "categories": "",
      "summary": "In recent years, Artificial Intelligence (AI)-driven Physical-Layer Authentication (PLA), which focuses on achieving endogenous security and intelligent identity authentication, has attracted considerable interest. When compared with Discriminative AI (DAI), Generative AI (GAI) offers several advantages, such as fingerprint data augmentation, fingerprint denoising and reconstruction, and protection against adversarial attacks. Inspired by these innovations, this paper provides a systematic exploration of GAI's integration into PLA frameworks. We commence with a review of representative authent...",
      "pdf_url": "https://arxiv.org/pdf/2504.18175v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2412.01363v1",
      "title": "Exploring the Robustness of AI-Driven Tools in Digital Forensics: A Preliminary Study",
      "authors": [
        "Silvia Lucia Sanna",
        "Leonardo Regano",
        "Davide Maiorca",
        "Giorgio Giacinto"
      ],
      "published": "2024-12-02T10:48:53Z",
      "categories": "",
      "summary": "Nowadays, many tools are used to facilitate forensic tasks about data extraction and data analysis. In particular, some tools leverage Artificial Intelligence (AI) to automatically label examined data into specific categories (\\ie, drugs, weapons, nudity). However, this raises a serious concern about the robustness of the employed AI algorithms against adversarial attacks. Indeed, some people may need to hide specific data to AI-based digital forensics tools, thus manipulating the content so that the AI system does not recognize the offensive/prohibited content and marks it at as suspicious to...",
      "pdf_url": "https://arxiv.org/pdf/2412.01363v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2507.13407v1",
      "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images",
      "authors": [
        "Vinu Sankar Sadasivan",
        "Mehrdad Saberi",
        "Soheil Feizi"
      ],
      "published": "2025-07-17T05:38:30Z",
      "categories": "",
      "summary": "With the rapid rise of generative AI and synthetic media, distinguishing AI-generated images from real ones has become crucial in safeguarding against misinformation and ensuring digital authenticity. Traditional watermarking techniques have shown vulnerabilities to adversarial attacks, undermining their effectiveness in the presence of attackers. We propose IConMark, a novel in-generation robust semantic watermarking method that embeds interpretable concepts into AI-generated images, as a first step toward interpretable watermarking. Unlike traditional methods, which rely on adding noise or p...",
      "pdf_url": "https://arxiv.org/pdf/2507.13407v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2202.11203v2",
      "title": "Under-confidence Backdoors Are Resilient and Stealthy Backdoors",
      "authors": [
        "Minlong Peng",
        "Zidi Xiong",
        "Quang H. Nguyen",
        "Mingming Sun",
        "Khoa D. Doan",
        "Ping Li"
      ],
      "published": "2022-02-19T01:31:41Z",
      "categories": "",
      "summary": "By injecting a small number of poisoned samples into the training set, backdoor attacks aim to make the victim model produce designed outputs on any input injected with pre-designed backdoors. In order to achieve a high attack success rate using as few poisoned training samples as possible, most existing attack methods change the labels of the poisoned samples to the target class. This practice often results in severe over-fitting of the victim model over the backdoors, making the attack quite effective in output control but easier to be identified by human inspection or automatic defense algo...",
      "pdf_url": "https://arxiv.org/pdf/2202.11203v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2007.05084v1",
      "title": "Attack of the Tails: Yes, You Really Can Backdoor Federated Learning",
      "authors": [
        "Hongyi Wang",
        "Kartik Sreenivasan",
        "Shashank Rajput",
        "Harit Vishwakarma",
        "Saurabh Agarwal",
        "Jy-yong Sohn",
        "Kangwook Lee",
        "Dimitris Papailiopoulos"
      ],
      "published": "2020-07-09T21:50:54Z",
      "categories": "",
      "summary": "Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness t...",
      "pdf_url": "https://arxiv.org/pdf/2007.05084v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2007.03767v4",
      "title": "Defending against Backdoors in Federated Learning with Robust Learning Rate",
      "authors": [
        "Mustafa Safa Ozdayi",
        "Murat Kantarcioglu",
        "Yulia R. Gel"
      ],
      "published": "2020-07-07T23:38:35Z",
      "categories": "",
      "summary": "Federated learning (FL) allows a set of agents to collaboratively train a model without sharing their potentially sensitive data. This makes FL suitable for privacy-preserving applications. At the same time, FL is susceptible to adversarial attacks due to decentralized and unvetted data. One important line of attacks against FL is the backdoor attacks. In a backdoor attack, an adversary tries to embed a backdoor functionality to the model during training that can later be activated to cause a desired misclassification. To prevent backdoor attacks, we propose a lightweight defense that requires...",
      "pdf_url": "https://arxiv.org/pdf/2007.03767v4.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2403.12723v2",
      "title": "Python Fuzzing for Trustworthy Machine Learning Frameworks",
      "authors": [
        "Ilya Yegorov",
        "Eli Kobrin",
        "Darya Parygina",
        "Alexey Vishnyakov",
        "Andrey Fedotov"
      ],
      "published": "2024-03-19T13:41:11Z",
      "categories": "",
      "summary": "Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems. Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software. Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python. We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection. Crash triagin...",
      "pdf_url": "https://arxiv.org/pdf/2403.12723v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.02629v1",
      "title": "AdvSecureNet: A Python Toolkit for Adversarial Machine Learning",
      "authors": [
        "Melih Catal",
        "Manuel G\u00fcnther"
      ],
      "published": "2024-09-04T11:47:00Z",
      "categories": "",
      "summary": "Machine learning models are vulnerable to adversarial attacks. Several tools have been developed to research these vulnerabilities, but they often lack comprehensive features and flexibility. We introduce AdvSecureNet, a PyTorch based toolkit for adversarial machine learning that is the first to natively support multi-GPU setups for attacks, defenses, and evaluation. It is the first toolkit that supports both CLI and API interfaces and external YAML configuration files to enhance versatility and reproducibility. The toolkit includes multiple attacks, defenses and evaluation metrics. Rigiorous ...",
      "pdf_url": "https://arxiv.org/pdf/2409.02629v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2408.03909v1",
      "title": "LaFA: Latent Feature Attacks on Non-negative Matrix Factorization",
      "authors": [
        "Minh Vu",
        "Ben Nebgen",
        "Erik Skau",
        "Geigh Zollicoffer",
        "Juan Castorena",
        "Kim Rasmussen",
        "Boian Alexandrov",
        "Manish Bhattarai"
      ],
      "published": "2024-08-07T17:13:46Z",
      "categories": "",
      "summary": "As Machine Learning (ML) applications rapidly grow, concerns about adversarial attacks compromising their reliability have gained significant attention. One unsupervised ML method known for its resilience to such attacks is Non-negative Matrix Factorization (NMF), an algorithm that decomposes input data into lower-dimensional latent features. However, the introduction of powerful computational tools such as Pytorch enables the computation of gradients of the latent features with respect to the original data, raising concerns about NMF's reliability. Interestingly, naively deriving the adversar...",
      "pdf_url": "https://arxiv.org/pdf/2408.03909v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2003.00883v1",
      "title": "Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color Space",
      "authors": [
        "Camilo Pestana",
        "Naveed Akhtar",
        "Wei Liu",
        "David Glance",
        "Ajmal Mian"
      ],
      "published": "2020-02-25T02:41:42Z",
      "categories": "",
      "summary": "Deep learning offers state of the art solutions for image recognition. However, deep models are vulnerable to adversarial perturbations in images that are subtle but significantly change the model's prediction. In a white-box attack, these perturbations are generally learned for deep models that operate on RGB images and, hence, the perturbations are equally distributed in the RGB color space. In this paper, we show that the adversarial perturbations prevail in the Y-channel of the YCbCr space. Our finding is motivated from the fact that the human vision and deep models are more responsive to ...",
      "pdf_url": "https://arxiv.org/pdf/2003.00883v1.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2403.14772v2",
      "title": "Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures",
      "authors": [
        "Sayanton V. Dibbo",
        "Adam Breuer",
        "Juston Moore",
        "Michael Teti"
      ],
      "published": "2024-03-21T18:26:23Z",
      "categories": "",
      "summary": "Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private and potentially sensitive training data by repeatedly querying the network. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudi...",
      "pdf_url": "https://arxiv.org/pdf/2403.14772v2.pdf",
      "relevance_score": 39,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2302.02261v3",
      "title": "NeuRI: Diversifying DNN Generation via Inductive Rule Inference",
      "authors": [
        "Jiawei Liu",
        "Jinjun Peng",
        "Yuyao Wang",
        "Lingming Zhang"
      ],
      "published": "2023-02-04T23:42:07Z",
      "categories": "",
      "summary": "Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints. To address this challenge, we propose NeuRI, a fully automated app...",
      "pdf_url": "https://arxiv.org/pdf/2302.02261v3.pdf",
      "relevance_score": 37,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2109.01002v4",
      "title": "DocTer: Documentation Guided Fuzzing for Testing Deep Learning API Functions",
      "authors": [
        "Danning Xie",
        "Yitong Li",
        "Mijung Kim",
        "Hung Viet Pham",
        "Lin Tan",
        "Xiangyu Zhang",
        "Michael W. Godfrey"
      ],
      "published": "2021-09-02T14:57:36Z",
      "categories": "",
      "summary": "Input constraints are useful for many software development tasks. For example, input constraints of a function enable the generation of valid inputs, i.e., inputs that follow these constraints, to test the function deeper. API functions of deep learning (DL) libraries have DL specific input constraints, which are described informally in the free form API documentation. Existing constraint extraction techniques are ineffective for extracting DL specific input constraints.   To fill this gap, we design and implement a new technique, DocTer, to analyze API documentation to extract DL specific inp...",
      "pdf_url": "https://arxiv.org/pdf/2109.01002v4.pdf",
      "relevance_score": 37,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2011.02272v1",
      "title": "Trustworthy AI",
      "authors": [
        "Richa Singh",
        "Mayank Vatsa",
        "Nalini Ratha"
      ],
      "published": "2020-11-02T20:04:18Z",
      "categories": "",
      "summary": "Modern AI systems are reaping the advantage of novel learning methods. With their increasing usage, we are realizing the limitations and shortfalls of these systems. Brittleness to minor adversarial changes in the input data, ability to explain the decisions, address the bias in their training data, high opacity in terms of revealing the lineage of the system, how they were trained and tested, and under which parameters and conditions they can reliably guarantee a certain level of performance, are some of the most prominent limitations. Ensuring the privacy and security of the data, assigning ...",
      "pdf_url": "https://arxiv.org/pdf/2011.02272v1.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1906.03466v1",
      "title": "Strategies to architect AI Safety: Defense to guard AI from Adversaries",
      "authors": [
        "Rajagopal. A",
        "Nirmala. V"
      ],
      "published": "2019-06-08T14:34:47Z",
      "categories": "",
      "summary": "The impact of designing for security of AI is critical for humanity in the AI era. With humans increasingly becoming dependent upon AI, there is a need for neural networks that work reliably, inspite of Adversarial attacks. The vision for Safe and secure AI for popular use is achievable. To achieve safety of AI, this paper explores strategies and a novel deep learning architecture. To guard AI from adversaries, paper explores combination of 3 strategies:   1. Introduce randomness at inference time to hide the representation learning from adversaries.   2. Detect presence of adversaries by anal...",
      "pdf_url": "https://arxiv.org/pdf/1906.03466v1.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.08202v1",
      "title": "AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques",
      "authors": [
        "Aman Raj",
        "Lakshit Arora",
        "Sanjay Surendranath Girija",
        "Shashank Kapoor",
        "Dipen Pradhan",
        "Ankit Shetgaonkar"
      ],
      "published": "2025-05-13T03:33:31Z",
      "categories": "",
      "summary": "Natural disasters, including earthquakes, wildfires and cyclones, bear a huge risk on human lives as well as infrastructure assets. An effective response to disaster depends on the ability to rapidly and efficiently assess the intensity of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence (GenAI) presents a breakthrough solution, capable of combining knowledge from multiple types and sources of data, simulating realistic scenarios of disaster, and identifying emerging trends at a speed previously unimaginable. In this paper, we present a comprehensive review on the pr...",
      "pdf_url": "https://arxiv.org/pdf/2505.08202v1.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.17861v1",
      "title": "Superplatforms Have to Attack AI Agents",
      "authors": [
        "Jianghao Lin",
        "Jiachen Zhu",
        "Zheli Zhou",
        "Yunjia Xi",
        "Weiwen Liu",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "published": "2025-05-23T13:13:44Z",
      "categories": "",
      "summary": "Over the past decades, superplatforms, digital companies that integrate a vast range of third-party services and applications into a single, unified ecosystem, have built their fortunes on monopolizing user attention through targeted advertising and algorithmic content curation. Yet the emergence of AI agents driven by large language models (LLMs) threatens to upend this business model. Agents can not only free user attention with autonomy across diverse platforms and therefore bypass the user-attention-based monetization, but might also become the new entrance for digital traffic. Hence, we a...",
      "pdf_url": "https://arxiv.org/pdf/2505.17861v1.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2405.19524v1",
      "title": "AI Risk Management Should Incorporate Both Safety and Security",
      "authors": [
        "Xiangyu Qi",
        "Yangsibo Huang",
        "Yi Zeng",
        "Edoardo Debenedetti",
        "Jonas Geiping",
        "Luxi He",
        "Kaixuan Huang",
        "Udari Madhushani",
        "Vikash Sehwag",
        "Weijia Shi",
        "Boyi Wei",
        "Tinghao Xie",
        "Danqi Chen",
        "Pin-Yu Chen",
        "Jeffrey Ding",
        "Ruoxi Jia",
        "Jiaqi Ma",
        "Arvind Narayanan",
        "Weijie J Su",
        "Mengdi Wang",
        "Chaowei Xiao",
        "Bo Li",
        "Dawn Song",
        "Peter Henderson",
        "Prateek Mittal"
      ],
      "published": "2024-05-29T21:00:47Z",
      "categories": "",
      "summary": "The exposure of security vulnerabilities in safety-aligned language models, e.g., susceptibility to adversarial attacks, has shed light on the intricate interplay between AI safety and AI security. Although the two disciplines now come together under the overarching goal of AI risk management, they have historically evolved separately, giving rise to differing perspectives. Therefore, in this paper, we advocate that stakeholders in AI risk management should be aware of the nuances, synergies, and interplay between safety and security, and unambiguously take into account the perspectives of bot...",
      "pdf_url": "https://arxiv.org/pdf/2405.19524v1.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2206.04793v1",
      "title": "Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions",
      "authors": [
        "Rucha Shinde",
        "Shruti Patil",
        "Ketan Kotecha",
        "Vidyasagar Potdar",
        "Ganeshsree Selvachandran",
        "Ajith Abraham"
      ],
      "published": "2022-05-30T14:54:00Z",
      "categories": "",
      "summary": "Healthcare systems are increasingly incorporating Artificial Intelligence into their systems, but it is not a solution for all difficulties. AI's extraordinary potential is being held back by challenges such as a lack of medical datasets for training AI models, adversarial attacks, and a lack of trust due to its black box working style. We explored how blockchain technology can improve the reliability and trustworthiness of AI-based healthcare. This paper has conducted a Systematic Literature Review to explore the state-of-the-art research studies conducted in healthcare applications developed...",
      "pdf_url": "https://arxiv.org/pdf/2206.04793v1.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2402.05967v7",
      "title": "The last Dance : Robust backdoor attack via diffusion models and bayesian approach",
      "authors": [
        "Orson Mengara"
      ],
      "published": "2024-02-05T18:00:07Z",
      "categories": "",
      "summary": "Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we aim to fool audio-based DNN models, such as those from the Hugging Face framework, primarily those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and achieve results faster and more efficiently. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on a...",
      "pdf_url": "https://arxiv.org/pdf/2402.05967v7.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.13864v3",
      "title": "Persistent Backdoor Attacks in Continual Learning",
      "authors": [
        "Zhen Guo",
        "Abhinav Kumar",
        "Reza Tourani"
      ],
      "published": "2024-09-20T19:28:48Z",
      "categories": "",
      "summary": "Backdoor attacks pose a significant threat to neural networks, enabling adversaries to manipulate model outputs on specific inputs, often with devastating consequences, especially in critical applications. While backdoor attacks have been studied in various contexts, little attention has been given to their practicality and persistence in continual learning, particularly in understanding how the continual updates to model parameters, as new data distributions are learned and integrated, impact the effectiveness of these attacks over time. To address this gap, we introduce two persistent backdo...",
      "pdf_url": "https://arxiv.org/pdf/2409.13864v3.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2309.03071v2",
      "title": "Disarming Steganography Attacks Inside Neural Network Models",
      "authors": [
        "Ran Dubin"
      ],
      "published": "2023-09-06T15:18:35Z",
      "categories": "",
      "summary": "Similar to the revolution of open source code sharing, Artificial Intelligence (AI) model sharing is gaining increased popularity. However, the fast adaptation in the industry, lack of awareness, and ability to exploit the models make them significant attack vectors. By embedding malware in neurons, the malware can be delivered covertly, with minor or no impact on the neural network's performance. The covert attack will use the Least Significant Bits (LSB) weight attack since LSB has a minimal effect on the model accuracy, and as a result, the user will not notice it. Since there are endless w...",
      "pdf_url": "https://arxiv.org/pdf/2309.03071v2.pdf",
      "relevance_score": 36,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2306.11758v2",
      "title": "MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing",
      "authors": [
        "Haitong Huang",
        "Cheng Liu",
        "Bo Liu",
        "Xinghua Xue",
        "Huawei Li",
        "Xiaowei Li"
      ],
      "published": "2023-06-20T06:46:54Z",
      "categories": "",
      "summary": "To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neura...",
      "pdf_url": "https://arxiv.org/pdf/2306.11758v2.pdf",
      "relevance_score": 35,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2303.06931v1",
      "title": "DeepVigor: Vulnerability Value Ranges and Factors for DNNs' Reliability Assessment",
      "authors": [
        "Mohammad Hasan Ahmadilivani",
        "Mahdi Taheri",
        "Jaan Raik",
        "Masoud Daneshtalab",
        "Maksim Jenihhin"
      ],
      "published": "2023-03-13T08:55:10Z",
      "categories": "",
      "summary": "Deep Neural Networks (DNNs) and their accelerators are being deployed ever more frequently in safety-critical applications leading to increasing reliability concerns. A traditional and accurate method for assessing DNNs' reliability has been resorting to fault injection, which, however, suffers from prohibitive time complexity. While analytical and hybrid fault injection-/analytical-based methods have been proposed, they are either inaccurate or specific to particular accelerator architectures. In this work, we propose a novel accurate, fine-grain, metric-oriented, and accelerator-agnostic met...",
      "pdf_url": "https://arxiv.org/pdf/2303.06931v1.pdf",
      "relevance_score": 35,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1907.01297v1",
      "title": "Neural Network Verification for the Masses (of AI graduates)",
      "authors": [
        "Ekaterina Komendantskaya",
        "Rob Stewart",
        "Kirsy Duncan",
        "Daniel Kienitz",
        "Pierre Le Hen",
        "Pascal Bacchus"
      ],
      "published": "2019-07-02T11:09:04Z",
      "categories": "",
      "summary": "Rapid development of AI applications has stimulated demand for, and has given rise to, the rapidly growing number and diversity of AI MSc degrees. AI and Robotics research communities, industries and students are becoming increasingly aware of the problems caused by unsafe or insecure AI applications. Among them, perhaps the most famous example is vulnerability of deep neural networks to ``adversarial attacks''. Owing to wide-spread use of neural networks in all areas of AI, this problem is seen as particularly acute and pervasive.   Despite of the growing number of research papers about safet...",
      "pdf_url": "https://arxiv.org/pdf/1907.01297v1.pdf",
      "relevance_score": 34,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2404.00473v1",
      "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
      "authors": [
        "Shanglun Feng",
        "Florian Tram\u00e8r"
      ],
      "published": "2024-03-30T20:43:53Z",
      "categories": "",
      "summary": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained...",
      "pdf_url": "https://arxiv.org/pdf/2404.00473v1.pdf",
      "relevance_score": 34,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2303.03320v3",
      "title": "Learning to Backdoor Federated Learning",
      "authors": [
        "Henger Li",
        "Chen Wu",
        "Sencun Zhu",
        "Zizhan Zheng"
      ],
      "published": "2023-03-06T17:47:04Z",
      "categories": "",
      "summary": "In a federated learning (FL) system, malicious participants can easily embed backdoors into the aggregated model while maintaining the model's performance on the main task. To this end, various defenses, including training stage aggregation-based defenses and post-training mitigation defenses, have been proposed recently. While these defenses obtain reasonable performance against existing backdoor attacks, which are mainly heuristics based, we show that they are insufficient in the face of more advanced attacks. In particular, we propose a general reinforcement learning-based backdoor attack f...",
      "pdf_url": "https://arxiv.org/pdf/2303.03320v3.pdf",
      "relevance_score": 34,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.15499v1",
      "title": "Guillotine: Hypervisors for Isolating Malicious AIs",
      "authors": [
        "James Mickens",
        "Sarah Radway",
        "Ravi Netravali"
      ],
      "published": "2025-04-22T00:29:18Z",
      "categories": "",
      "summary": "As AI models become more embedded in critical sectors like finance, healthcare, and the military, their inscrutable behavior poses ever-greater risks to society. To mitigate this risk, we propose Guillotine, a hypervisor architecture for sandboxing powerful AI models -- models that, by accident or malice, can generate existential threats to humanity. Although Guillotine borrows some well-known virtualization techniques, Guillotine must also introduce fundamentally new isolation mechanisms to handle the unique threat model posed by existential-risk AIs. For example, a rogue AI may try to intros...",
      "pdf_url": "https://arxiv.org/pdf/2504.15499v1.pdf",
      "relevance_score": 34,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.16743v1",
      "title": "Implementing AI Bill of Materials (AI BOM) with SPDX 3.0: A Comprehensive Guide to Creating AI and Dataset Bill of Materials",
      "authors": [
        "Karen Bennet",
        "Gopi Krishnan Rajbahadur",
        "Arthit Suriyawongkul",
        "Kate Stewart"
      ],
      "published": "2025-04-23T14:13:19Z",
      "categories": "",
      "summary": "A Software Bill of Materials (SBOM) is becoming an increasingly important tool in regulatory and technical spaces to introduce more transparency and security into a project's software supply chain.   Artificial intelligence (AI) projects face unique challenges beyond the security of their software, and thus require a more expansive approach to a bill of materials. In this report, we introduce the concept of an AI-BOM, expanding on the SBOM to include the documentation of algorithms, data collection methods, frameworks and libraries, licensing information, and standard compliance.",
      "pdf_url": "https://arxiv.org/pdf/2504.16743v1.pdf",
      "relevance_score": 34,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2506.04679v1",
      "title": "Normative Conflicts and Shallow AI Alignment",
      "authors": [
        "Rapha\u00ebl Milli\u00e8re"
      ],
      "published": "2025-06-05T06:57:28Z",
      "categories": "",
      "summary": "The progress of AI systems such as large language models (LLMs) raises increasingly pressing concerns about their safe deployment. This paper examines the value alignment problem for LLMs, arguing that current alignment strategies are fundamentally inadequate to prevent misuse. Despite ongoing efforts to instill norms such as helpfulness, honesty, and harmlessness in LLMs through fine-tuning based on human preferences, they remain vulnerable to adversarial attacks that exploit conflicts between these norms. I argue that this vulnerability reflects a fundamental limitation of existing alignment...",
      "pdf_url": "https://arxiv.org/pdf/2506.04679v1.pdf",
      "relevance_score": 32,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.18506v1",
      "title": "Exploring Patient Data Requirements in Training Effective AI Models for MRI-based Breast Cancer Classification",
      "authors": [
        "Solha Kang",
        "Wesley De Neve",
        "Francois Rameau",
        "Utku Ozbulak"
      ],
      "published": "2025-02-22T04:04:52Z",
      "categories": "",
      "summary": "The past decade has witnessed a substantial increase in the number of startups and companies offering AI-based solutions for clinical decision support in medical institutions. However, the critical nature of medical decision-making raises several concerns about relying on external software. Key issues include potential variations in image modalities and the medical devices used to obtain these images, potential legal issues, and adversarial attacks. Fortunately, the open-source nature of machine learning research has made foundation models publicly available and straightforward to use for medi...",
      "pdf_url": "https://arxiv.org/pdf/2502.18506v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2411.06146v1",
      "title": "AI-Compass: A Comprehensive and Effective Multi-module Testing Tool for AI Systems",
      "authors": [
        "Zhiyu Zhu",
        "Zhibo Jin",
        "Hongsheng Hu",
        "Minhui Xue",
        "Ruoxi Sun",
        "Seyit Camtepe",
        "Praveen Gauravaram",
        "Huaming Chen"
      ],
      "published": "2024-11-09T11:15:17Z",
      "categories": "",
      "summary": "AI systems, in particular with deep learning techniques, have demonstrated superior performance for various real-world applications. Given the need for tailored optimization in specific scenarios, as well as the concerns related to the exploits of subsurface vulnerabilities, a more comprehensive and in-depth testing AI system becomes a pivotal topic. We have seen the emergence of testing tools in real-world applications that aim to expand testing capabilities. However, they often concentrate on ad-hoc tasks, rendering them unsuitable for simultaneously testing multiple aspects or components. F...",
      "pdf_url": "https://arxiv.org/pdf/2411.06146v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2307.05842v4",
      "title": "The Butterfly Effect in Artificial Intelligence Systems: Implications for AI Bias and Fairness",
      "authors": [
        "Emilio Ferrara"
      ],
      "published": "2023-07-11T23:32:26Z",
      "categories": "",
      "summary": "The Butterfly Effect, a concept originating from chaos theory, underscores how small changes can have significant and unpredictable impacts on complex systems. In the context of AI fairness and bias, the Butterfly Effect can stem from a variety of sources, such as small biases or skewed data inputs during algorithm development, saddle points in training, or distribution shifts in data between training and testing phases. These seemingly minor alterations can lead to unexpected and substantial unfair outcomes, disproportionately affecting underrepresented individuals or groups and perpetuating ...",
      "pdf_url": "https://arxiv.org/pdf/2307.05842v4.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2502.05219v1",
      "title": "Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies",
      "authors": [
        "Kendrea Beers",
        "Helen Toner"
      ],
      "published": "2025-02-05T15:31:11Z",
      "categories": "",
      "summary": "This article describes how technical infrastructure developed by the nonprofit OpenMined enables external scrutiny of AI systems without compromising sensitive information.   Independent external scrutiny of AI systems provides crucial transparency into AI development, so it should be an integral component of any approach to AI governance. In practice, external researchers have struggled to gain access to AI systems because of AI companies' legitimate concerns about security, privacy, and intellectual property.   But now, privacy-enhancing technologies (PETs) have reached a new level of maturi...",
      "pdf_url": "https://arxiv.org/pdf/2502.05219v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.22709v1",
      "title": "Trust and Transparency in AI: Industry Voices on Data, Ethics, and Compliance",
      "authors": [
        "Louise McCormack",
        "Diletta Huyskes",
        "Dave Lewis",
        "Malika Bendechache"
      ],
      "published": "2025-09-23T20:58:01Z",
      "categories": "",
      "summary": "The EU Artificial Intelligence (AI) Act directs businesses to assess their AI systems to ensure they are developed in a way that is human-centered and trustworthy. The rapid adoption of AI in the industry has outpaced ethical evaluation frameworks, leading to significant challenges in accountability, governance, data quality, human oversight, technological robustness, and environmental and societal impacts. Through structured interviews with fifteen industry professionals, paired with a literature review conducted on each of the key interview findings, this paper investigates practical approac...",
      "pdf_url": "https://arxiv.org/pdf/2509.22709v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2306.03269v2",
      "title": "Security Knowledge-Guided Fuzzing of Deep Learning Libraries",
      "authors": [
        "Nima Shiri Harzevili",
        "Mohammad Mahdi Mohajer",
        "Moshi Wei",
        "Hung Viet Pham",
        "Song Wang"
      ],
      "published": "2023-06-05T21:38:56Z",
      "categories": "",
      "summary": "Recently, many Deep Learning fuzzers have been proposed for testing of DL libraries. However, they either perform unguided input generation (e.g., not considering the relationship between API arguments when generating inputs) or only support a limited set of corner case test inputs. Furthermore, a substantial number of developer APIs crucial for library development remain untested, as they are typically not well-documented and lack clear usage guidelines.   To fill this gap, we propose a novel fuzzer named Orion, which combines guided test input generation and corner case test input generation...",
      "pdf_url": "https://arxiv.org/pdf/2306.03269v2.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.15690v1",
      "title": "MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing",
      "authors": [
        "Shiwen Ou",
        "Yuwei Li",
        "Lu Yu",
        "Chengkun Wei",
        "Tingke Wen",
        "Qiangpu Chen",
        "Yu Chen",
        "Haizhi Tang",
        "Zulie Pan"
      ],
      "published": "2025-10-17T14:34:00Z",
      "categories": "",
      "summary": "Deep learning (DL) frameworks serve as the backbone for a wide range of artificial intelligence applications. However, bugs within DL frameworks can cascade into critical issues in higher-level applications, jeopardizing reliability and security. While numerous techniques have been proposed to detect bugs in DL frameworks, research exploring common API patterns across frameworks and the potential risks they entail remains limited. Notably, many DL frameworks expose similar APIs with overlapping input parameters and functionalities, rendering them vulnerable to shared bugs, where a flaw in one ...",
      "pdf_url": "https://arxiv.org/pdf/2510.15690v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2304.02014v1",
      "title": "Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT",
      "authors": [
        "Yinlin Deng",
        "Chunqiu Steven Xia",
        "Chenyuan Yang",
        "Shizhuo Dylan Zhang",
        "Shujing Yang",
        "Lingming Zhang"
      ],
      "published": "2023-04-04T17:59:52Z",
      "categories": "",
      "summary": "Deep Learning (DL) library bugs affect downstream DL applications, emphasizing the need for reliable systems. Generating valid input programs for fuzzing DL libraries is challenging due to the need for satisfying both language syntax/semantics and constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the constraints to generate valid DL programs for fuzzing. However, LLMs tend to generate ordinary programs following similar patterns seen in their massive train...",
      "pdf_url": "https://arxiv.org/pdf/2304.02014v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2206.14322v1",
      "title": "An Empirical Study of Challenges in Converting Deep Learning Models",
      "authors": [
        "Moses Openja",
        "Amin Nikanjam",
        "Ahmed Haj Yahmed",
        "Foutse Khomh",
        "Zhen Ming",
        " Jiang"
      ],
      "published": "2022-06-28T23:18:37Z",
      "categories": "",
      "summary": "There is an increase in deploying Deep Learning (DL)-based software systems in real-world applications. Usually DL models are developed and trained using DL frameworks that have their own internal mechanisms/formats to represent and train DL models, and usually those formats cannot be recognized by other frameworks. Moreover, trained models are usually deployed in environments different from where they were developed. To solve the interoperability issue and make DL models compatible with different frameworks/environments, some exchange formats are introduced for DL models, like ONNX and CoreML...",
      "pdf_url": "https://arxiv.org/pdf/2206.14322v1.pdf",
      "relevance_score": 27,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2212.09437v3",
      "title": "Machine Learning Systems are Bloated and Vulnerable",
      "authors": [
        "Huaifeng Zhang",
        "Fahmi Abdulqadir Ahmed",
        "Dyako Fatih",
        "Akayou Kitessa",
        "Mohannad Alhanahnah",
        "Philipp Leitner",
        "Ahmed Ali-Eldin"
      ],
      "published": "2022-12-16T10:34:27Z",
      "categories": "",
      "summary": "Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from operating systems and applications to containers. Containers are lightweight virtualization technologies used to package code and dependencies, providing portable, reproducible and isolated environments. For their ease of use, data scientists often utilize machine learning containers to simplify their workflow. However, this convenience comes at a cost: containers are often bloated with unnecessary code and dependencies, resulting in very large...",
      "pdf_url": "https://arxiv.org/pdf/2212.09437v3.pdf",
      "relevance_score": 25,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2011.04962v1",
      "title": "Characterization and Automatic Update of Deprecated Machine-Learning API Usages",
      "authors": [
        "Stefanus Agus Haryono",
        "Ferdian Thung",
        "David Lo",
        "Julia Lawall",
        "Lingxiao Jiang"
      ],
      "published": "2020-11-10T07:56:48Z",
      "categories": "",
      "summary": "Due to the rise of AI applications, machine learning libraries have become far more accessible, with Python being the most common programming language to write them. Machine learning libraries tend to be updated periodically, which may deprecate existing APIs, making it necessary for developers to update their usages. However, updating usages of deprecated APIs are typically not a priority for developers, leading to widespread usages of deprecated APIs which expose library users to vulnerability issues. In this paper, we built a tool to automate these updates. We first conducted an empirical s...",
      "pdf_url": "https://arxiv.org/pdf/2011.04962v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2303.06822v2",
      "title": "Automatic Identification and Extraction of Assumptions on GitHub",
      "authors": [
        "Chen Yang",
        "Zinan Ma",
        "Peng Liang",
        "Xiaohua Liu"
      ],
      "published": "2023-03-13T02:49:08Z",
      "categories": "",
      "summary": "In software development, due to the lack of knowledge or information, time pressure, complex context, and many other factors, various uncertainties emerge during the development process, leading to assumptions scattered in projects. Being unaware of certain assumptions can result in critical problems (e.g., system vulnerability and failures). The prerequisite of analyzing and understanding assumptions in software development is to identify and extract those assumptions with acceptable effort. In this paper, we proposed a tool (i.e., Assumption Miner) to automatically identify and extract assum...",
      "pdf_url": "https://arxiv.org/pdf/2303.06822v2.pdf",
      "relevance_score": 25,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1712.02779v4",
      "title": "Exploring the Landscape of Spatial Robustness",
      "authors": [
        "Logan Engstrom",
        "Brandon Tran",
        "Dimitris Tsipras",
        "Ludwig Schmidt",
        "Aleksander Madry"
      ],
      "published": "2017-12-07T18:53:52Z",
      "categories": "",
      "summary": "The study of adversarial robustness has so far largely focused on perturbations bound in p-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network--based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the p-norm case, first-o...",
      "pdf_url": "https://arxiv.org/pdf/1712.02779v4.pdf",
      "relevance_score": 25,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2306.15217v3",
      "title": "Unsupervised Episode Generation for Graph Meta-learning",
      "authors": [
        "Jihyeong Jung",
        "Sangwoo Seo",
        "Sungwon Kim",
        "Chanyoung Park"
      ],
      "published": "2023-06-27T05:37:23Z",
      "categories": "",
      "summary": "We propose Unsupervised Episode Generation method called Neighbors as Queries (NaQ) to solve the Few-Shot Node-Classification (FSNC) task by unsupervised Graph Meta-learning. Doing so enables full utilization of the information of all nodes in a graph, which is not possible in current supervised meta-learning methods for FSNC due to the label-scarcity problem. In addition, unlike unsupervised Graph Contrastive Learning (GCL) methods that overlook the downstream task to be solved at the training phase resulting in vulnerability to class imbalance of a graph, we adopt the episodic learning frame...",
      "pdf_url": "https://arxiv.org/pdf/2306.15217v3.pdf",
      "relevance_score": 25,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2104.14208v1",
      "title": "Self-Claimed Assumptions in Deep Learning Frameworks: An Exploratory Study",
      "authors": [
        "Chen Yang",
        "Peng Liang",
        "Liming Fu",
        "Zengyang Li"
      ],
      "published": "2021-04-29T08:56:47Z",
      "categories": "",
      "summary": "Deep learning (DL) frameworks have been extensively designed, implemented, and used in software projects across many domains. However, due to the lack of knowledge or information, time pressure, complex context, etc., various uncertainties emerge during the development, leading to assumptions made in DL frameworks. Though not all the assumptions are negative to the frameworks, being unaware of certain assumptions can result in critical problems (e.g., system vulnerability and failures, inconsistencies, and increased cost). As the first step of addressing the critical problems, there is a need ...",
      "pdf_url": "https://arxiv.org/pdf/2104.14208v1.pdf",
      "relevance_score": 25,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2503.21983v2",
      "title": "Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs",
      "authors": [
        "Abed Kareem Musaffar",
        "Anand Gokhale",
        "Sirui Zeng",
        "Rasta Tadayon",
        "Xifeng Yan",
        "Ambuj Singh",
        "Francesco Bullo"
      ],
      "published": "2025-03-27T21:01:02Z",
      "categories": "",
      "summary": "As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Mo...",
      "pdf_url": "https://arxiv.org/pdf/2503.21983v2.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2404.08285v2",
      "title": "A Survey of Neural Network Robustness Assessment in Image Recognition",
      "authors": [
        "Jie Wang",
        "Jun Ai",
        "Minyan Lu",
        "Haoran Su",
        "Dan Yu",
        "Yutao Zhang",
        "Junda Zhu",
        "Jingyu Liu"
      ],
      "published": "2024-04-12T07:19:16Z",
      "categories": "",
      "summary": "In recent years, there has been significant attention given to the robustness assessment of neural networks. Robustness plays a critical role in ensuring reliable operation of artificial intelligence (AI) systems in complex and uncertain environments. Deep learning's robustness problem is particularly significant, highlighted by the discovery of adversarial attacks on image classification models. Researchers have dedicated efforts to evaluate robustness in diverse perturbation conditions for image recognition tasks. Robustness assessment encompasses two main techniques: robustness verification...",
      "pdf_url": "https://arxiv.org/pdf/2404.08285v2.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2304.10755v4",
      "title": "Interpretable and Robust AI in EEG Systems: A Survey",
      "authors": [
        "Xinliang Zhou",
        "Chenyu Liu",
        "Jinan Zhou",
        "Zhongruo Wang",
        "Liming Zhai",
        "Ziyu Jia",
        "Cuntai Guan",
        "Yang Liu"
      ],
      "published": "2023-04-21T05:51:39Z",
      "categories": "",
      "summary": "The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of ...",
      "pdf_url": "https://arxiv.org/pdf/2304.10755v4.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2006.13555v1",
      "title": "Defending against adversarial attacks on medical imaging AI system, classification or detection?",
      "authors": [
        "Xin Li",
        "Deng Pan",
        "Dongxiao Zhu"
      ],
      "published": "2020-06-24T08:26:49Z",
      "categories": "",
      "summary": "Medical imaging AI systems such as disease classification and segmentation are increasingly inspired and transformed from computer vision based AI systems. Although an array of adversarial training and/or loss function based defense techniques have been developed and proved to be effective in computer vision, defending against adversarial attacks on medical images remains largely an uncharted territory due to the following unique challenges: 1) label scarcity in medical images significantly limits adversarial generalizability of the AI system; 2) vastly similar and dominant fore- and backgroun...",
      "pdf_url": "https://arxiv.org/pdf/2006.13555v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2507.03168v1",
      "title": "Adopting a human developmental visual diet yields robust, shape-based AI vision",
      "authors": [
        "Zejin Lu",
        "Sushrut Thorat",
        "Radoslaw M Cichy",
        "Tim C Kietzmann"
      ],
      "published": "2025-07-03T20:52:08Z",
      "categories": "",
      "summary": "Despite years of research and the dramatic scaling of artificial intelligence (AI) systems, a striking misalignment between artificial and human vision persists. Contrary to humans, AI heavily relies on texture-features rather than shape information, lacks robustness to image distortions, remains highly vulnerable to adversarial attacks, and struggles to recognise simple abstract shapes within complex backgrounds. To close this gap, we here introduce a solution that arises from a previously underexplored direction: rather than scaling up, we take inspiration from how human vision develops from...",
      "pdf_url": "https://arxiv.org/pdf/2507.03168v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2507.17944v1",
      "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text",
      "authors": [
        "Hulayyil Alshammari",
        "Praveen Rao"
      ],
      "published": "2025-07-23T21:26:33Z",
      "categories": "",
      "summary": "Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectors' ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we inves...",
      "pdf_url": "https://arxiv.org/pdf/2507.17944v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2404.08987v1",
      "title": "On the critical path to implant backdoors and the effectiveness of potential mitigation techniques: Early learnings from XZ",
      "authors": [
        "Mario Lins",
        "Ren\u00e9 Mayrhofer",
        "Michael Roland",
        "Daniel Hofer",
        "Martin Schwaighofer"
      ],
      "published": "2024-04-13T12:18:36Z",
      "categories": "",
      "summary": "An emerging supply-chain attack due to a backdoor in XZ Utils has been identified. The backdoor allows an attacker to run commands remotely on vulnerable servers utilizing SSH without prior authentication. We have started to collect available information with regards to this attack to discuss current mitigation strategies for such kinds of supply-chain attacks. This paper introduces the critical attack path of the XZ backdoor and provides an overview about potential mitigation techniques related to relevant stages of the attack path.",
      "pdf_url": "https://arxiv.org/pdf/2404.08987v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2305.01267v1",
      "title": "DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning",
      "authors": [
        "Wenqiang Sun",
        "Sen Li",
        "Yuchang Sun",
        "Jun Zhang"
      ],
      "published": "2023-05-02T09:04:34Z",
      "categories": "",
      "summary": "Federated learning (FL) attempts to train a global model by aggregating local models from distributed devices under the coordination of a central server. However, the existence of a large number of heterogeneous devices makes FL vulnerable to various attacks, especially the stealthy backdoor attack. Backdoor attack aims to trick a neural network to misclassify data to a target label by injecting specific triggers while keeping correct predictions on original training data. Existing works focus on client-side attacks which try to poison the global model by modifying the local datasets. In this ...",
      "pdf_url": "https://arxiv.org/pdf/2305.01267v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2210.06428v1",
      "title": "Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork",
      "authors": [
        "Haotao Wang",
        "Junyuan Hong",
        "Aston Zhang",
        "Jiayu Zhou",
        "Zhangyang Wang"
      ],
      "published": "2022-10-12T17:24:01Z",
      "categories": "",
      "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks. Previous works have shown it extremely challenging to unlearn the undesired backdoor behavior from the network, since the entire network can be affected by the backdoor samples. In this paper, we propose a brand-new backdoor defense strategy, which makes it much easier to remove the harmful influence of backdoor samples from the model. Our defense strategy, \\emph{Trap and Replace}, consists of two stages. In the first stage, we bait and trap the backdoors in a small and easy-to-replace subnetwork. Specifically, we add an auxiliary...",
      "pdf_url": "https://arxiv.org/pdf/2210.06428v1.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2001.05574v5",
      "title": "Advbox: a toolbox to generate adversarial examples that fool neural networks",
      "authors": [
        "Dou Goodman",
        "Hao Xin",
        "Wang Yang",
        "Wu Yuesheng",
        "Xiong Junfeng",
        "Zhang Huan"
      ],
      "published": "2020-01-13T08:11:27Z",
      "categories": "",
      "summary": "In recent years, neural networks have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. Recent studies have shown that they are all vulnerable to the attack of adversarial examples. Small and often imperceptible perturbations to the input images are sufficient to fool the most powerful neural networks. \\emph{Advbox} is a toolbox to generate adversarial examples that fool neural networks in PaddlePaddle, PyTorch, Caffe2, MxNet, Keras, TensorFlow and it can benchmark th...",
      "pdf_url": "https://arxiv.org/pdf/2001.05574v5.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2101.09324v2",
      "title": "Generating Black-Box Adversarial Examples in Sparse Domain",
      "authors": [
        "Hadi Zanddizari",
        "Behnam Zeinali",
        "J. Morris Chang"
      ],
      "published": "2021-01-22T20:45:33Z",
      "categories": "",
      "summary": "Applications of machine learning (ML) models and convolutional neural networks (CNNs) have been rapidly increased. Although state-of-the-art CNNs provide high accuracy in many applications, recent investigations show that such networks are highly vulnerable to adversarial attacks. The black-box adversarial attack is one type of attack that the attacker does not have any knowledge about the model or the training dataset, but it has some input data set and their labels. In this paper, we propose a novel approach to generate a black-box attack in sparse domain whereas the most important informati...",
      "pdf_url": "https://arxiv.org/pdf/2101.09324v2.pdf",
      "relevance_score": 24,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.02169v1",
      "title": "TAIBOM: Bringing Trustworthiness to AI-Enabled Systems",
      "authors": [
        "Vadim Safronov",
        "Anthony McCaigue",
        "Nicholas Allott",
        "Andrew Martin"
      ],
      "published": "2025-10-02T16:17:07Z",
      "categories": "",
      "summary": "The growing integration of open-source software and AI-driven technologies has introduced new layers of complexity into the software supply chain, challenging existing methods for dependency management and system assurance. While Software Bills of Materials (SBOMs) have become critical for enhancing transparency and traceability, current frameworks fall short in capturing the unique characteristics of AI systems -- namely, their dynamic, data-driven nature and the loosely coupled dependencies across datasets, models, and software components. These challenges are compounded by fragmented govern...",
      "pdf_url": "https://arxiv.org/pdf/2510.02169v1.pdf",
      "relevance_score": 22,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2410.02230v2",
      "title": "Mitigating Downstream Model Risks via Model Provenance",
      "authors": [
        "Keyu Wang",
        "Abdullah Norozi Iranzad",
        "Scott Schaffter",
        "Meg Risdal",
        "Doina Precup",
        "Jonathan Lebensold"
      ],
      "published": "2024-10-03T05:52:15Z",
      "categories": "",
      "summary": "Research and industry are rapidly advancing the innovation and adoption of foundation model-based systems, yet the tools for managing these models have not kept pace. Understanding the provenance and lineage of models is critical for researchers, industry, regulators, and public trust. While model cards and system cards were designed to provide transparency, they fall short in key areas: tracing model genealogy, enabling machine readability, offering reliable centralized management systems, and fostering consistent creation incentives. This challenge mirrors issues in software supply chain sec...",
      "pdf_url": "https://arxiv.org/pdf/2410.02230v2.pdf",
      "relevance_score": 22,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2506.05445v1",
      "title": "Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic",
      "authors": [
        "Thanh Vinh Vo",
        "Young Lee",
        "Haozhe Ma",
        "Chien Lu",
        "Tze-Yun Leong"
      ],
      "published": "2025-06-05T13:52:38Z",
      "categories": "",
      "summary": "Hidden confounders that influence both states and actions can bias policy learning in reinforcement learning (RL), leading to suboptimal or non-generalizable behavior. Most RL algorithms ignore this issue, learning policies from observational trajectories based solely on statistical associations rather than causal effects. We propose DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment), a principled extension of the SAC algorithm that corrects for hidden confounding via causal intervention estimation. DoSAC estimates the interventional policy $\u03c0(a | \\mathrm{do}(s))$ using the backdoo...",
      "pdf_url": "https://arxiv.org/pdf/2506.05445v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2401.15284v6",
      "title": "Beyond principlism: Practical strategies for ethical AI use in research practices",
      "authors": [
        "Zhicheng Lin"
      ],
      "published": "2024-01-27T03:53:25Z",
      "categories": "",
      "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a \"Triple-Too\" problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches--principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technological solutionism (overemphasis on technological fixes)--offer little pra...",
      "pdf_url": "https://arxiv.org/pdf/2401.15284v6.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2310.18708v1",
      "title": "Simultaneous embedding of multiple attractor manifolds in a recurrent neural network using constrained gradient optimization",
      "authors": [
        "Haggai Agmon",
        "Yoram Burak"
      ],
      "published": "2023-10-28T13:36:55Z",
      "categories": "",
      "summary": "The storage of continuous variables in working memory is hypothesized to be sustained in the brain by the dynamics of recurrent neural networks (RNNs) whose steady states form continuous manifolds. In some cases, it is thought that the synaptic connectivity supports multiple attractor manifolds, each mapped to a different context or task. For example, in hippocampal area CA3, positions in distinct environments are represented by distinct sets of population activity patterns, each forming a continuum. It has been argued that the embedding of multiple continuous attractors in a single RNN inevit...",
      "pdf_url": "https://arxiv.org/pdf/2310.18708v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2410.15393v1",
      "title": "CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges",
      "authors": [
        "Haitao Li",
        "Junjie Chen",
        "Qingyao Ai",
        "Zhumin Chu",
        "Yujia Zhou",
        "Qian Dong",
        "Yiqun Liu"
      ],
      "published": "2024-10-20T13:47:39Z",
      "categories": "",
      "summary": "The use of large language models (LLMs) as automated evaluation tools to assess the quality of generated natural language, known as LLMs-as-Judges, has demonstrated promising capabilities and is rapidly gaining widespread attention. However, when applied to pairwise comparisons of candidate responses, LLM-based evaluators often exhibit selection bias. Specifically, their judgments may become inconsistent when the option positions or ID tokens are swapped, compromising the effectiveness and fairness of the evaluation result. To address this challenge, we introduce CalibraEval, a novel label-fre...",
      "pdf_url": "https://arxiv.org/pdf/2410.15393v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2303.02384v4",
      "title": "Hierarchical Training of Deep Neural Networks Using Early Exiting",
      "authors": [
        "Yamin Sepehri",
        "Pedram Pad",
        "Ahmet Caner Y\u00fcz\u00fcg\u00fcler",
        "Pascal Frossard",
        "L. Andrea Dunbar"
      ],
      "published": "2023-03-04T11:30:16Z",
      "categories": "",
      "summary": "Deep neural networks provide state-of-the-art accuracy for vision tasks but they require significant resources for training. Thus, they are trained on cloud servers far from the edge devices that acquire the data. This issue increases communication cost, runtime and privacy concerns. In this study, a novel hierarchical training method for deep neural networks is proposed that uses early exits in a divided architecture between edge and cloud workers to reduce the communication cost, training runtime and privacy concerns. The method proposes a brand-new use case for early exits to separate the b...",
      "pdf_url": "https://arxiv.org/pdf/2303.02384v4.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.07109v1",
      "title": "Advancing On-Device Neural Network Training with TinyPropv2: Dynamic, Sparse, and Efficient Backpropagation",
      "authors": [
        "Marcus R\u00fcb",
        "Axel Sikora",
        "Daniel Mueller-Gritschneder"
      ],
      "published": "2024-09-11T08:56:13Z",
      "categories": "",
      "summary": "This study introduces TinyPropv2, an innovative algorithm optimized for on-device learning in deep neural networks, specifically designed for low-power microcontroller units. TinyPropv2 refines sparse backpropagation by dynamically adjusting the level of sparsity, including the ability to selectively skip training steps. This feature significantly lowers computational effort without substantially compromising accuracy. Our comprehensive evaluation across diverse datasets CIFAR 10, CIFAR100, Flower, Food, Speech Command, MNIST, HAR, and DCASE2020 reveals that TinyPropv2 achieves near-parity wit...",
      "pdf_url": "https://arxiv.org/pdf/2409.07109v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2105.11346v2",
      "title": "Position-Sensing Graph Neural Networks: Proactively Learning Nodes Relative Positions",
      "authors": [
        "Zhenyue Qin",
        "Yiqun Zhang Saeed Anwar",
        "Dongwoo Kim",
        "Yang Liu",
        "Pan Ji",
        "Tom Gedeon"
      ],
      "published": "2021-05-24T15:30:30Z",
      "categories": "",
      "summary": "Most existing graph neural networks (GNNs) learn node embeddings using the framework of message passing and aggregation. Such GNNs are incapable of learning relative positions between graph nodes within a graph. To empower GNNs with the awareness of node positions, some nodes are set as anchors. Then, using the distances from a node to the anchors, GNNs can infer relative positions between nodes. However, P-GNNs arbitrarily select anchors, leading to compromising position-awareness and feature extraction. To eliminate this compromise, we demonstrate that selecting evenly distributed and asymme...",
      "pdf_url": "https://arxiv.org/pdf/2105.11346v2.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.11568v1",
      "title": "Adaptively Pruned Spiking Neural Networks for Energy-Efficient Intracortical Neural Decoding",
      "authors": [
        "Francesca Rivelli",
        "Martin Popov",
        "Charalampos S. Kouzinopoulos",
        "Guangzhi Tang"
      ],
      "published": "2025-04-15T19:16:34Z",
      "categories": "",
      "summary": "Intracortical brain-machine interfaces demand low-latency, energy-efficient solutions for neural decoding. Spiking Neural Networks (SNNs) deployed on neuromorphic hardware have demonstrated remarkable efficiency in neural decoding by leveraging sparse binary activations and efficient spatiotemporal processing. However, reducing the computational cost of SNNs remains a critical challenge for developing ultra-efficient intracortical neural implants. In this work, we introduce a novel adaptive pruning algorithm specifically designed for SNNs with high activation sparsity, targeting intracortical ...",
      "pdf_url": "https://arxiv.org/pdf/2504.11568v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.11564v1",
      "title": "Perceptions of Agentic AI in Organizations: Implications for Responsible AI and ROI",
      "authors": [
        "Lee Ackerman"
      ],
      "published": "2025-04-15T19:15:06Z",
      "categories": "",
      "summary": "As artificial intelligence (AI) systems rapidly gain autonomy, the need for robust responsible AI frameworks becomes paramount. This paper investigates how organizations perceive and adapt such frameworks amidst the emerging landscape of increasingly sophisticated agentic AI. Employing an interpretive qualitative approach, the study explores the lived experiences of AI professionals. Findings highlight that the inherent complexity of agentic AI systems and their responsible implementation, rooted in the intricate interconnectedness of responsible AI dimensions and the thematic framework (an an...",
      "pdf_url": "https://arxiv.org/pdf/2504.11564v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2503.20099v2",
      "title": "AI Identity, Empowerment, and Mindfulness in Mitigating Unethical AI Use",
      "authors": [
        "Mayssam Tarighi Shaayesteh",
        "Sara Memarian Esfahani",
        "Hossein Mohit"
      ],
      "published": "2025-03-25T22:36:21Z",
      "categories": "",
      "summary": "This study examines how AI identity influences psychological empowerment and unethical AI behavior among college students, while also exploring the moderating role of IT mindfulness. Findings show that a strong AI identity enhances psychological empowerment and academic engagement but can also lead to increased unethical AI practices. Crucially, IT mindfulness acts as an ethical safeguard, promoting sensitivity to ethical concerns and reducing misuse of AI. These insights have implications for educators, policymakers, and AI developers, emphasizing For Peer Review the need for a balanced appro...",
      "pdf_url": "https://arxiv.org/pdf/2503.20099v2.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.19024v1",
      "title": "Examining the Impact of Label Detail and Content Stakes on User Perceptions of AI-Generated Images on Social Media",
      "authors": [
        "Jingruo Chen",
        "TungYen Wang",
        "Marie Williams",
        "Natalia Jordan",
        "Mingyi Shao",
        "Linda Zhang",
        "Susan R. Fussell"
      ],
      "published": "2025-10-21T19:06:46Z",
      "categories": "",
      "summary": "AI-generated images are increasingly prevalent on social media, raising concerns about trust and authenticity. This study investigates how different levels of label detail (basic, moderate, maximum) and content stakes (high vs. low) influence user engagement with and perceptions of AI-generated images through a within-subjects experimental study with 105 participants. Our findings reveal that increasing label detail enhances user perceptions of label transparency but does not affect user engagement. However, content stakes significantly impact user engagement and perceptions, with users demons...",
      "pdf_url": "https://arxiv.org/pdf/2510.19024v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "cs/0502006v1",
      "title": "Neural network ensembles: Evaluation of aggregation algorithms",
      "authors": [
        "P. M. Granitto",
        "P. F. Verdes",
        "H. A. Ceccatto"
      ],
      "published": "2005-02-01T21:22:24Z",
      "categories": "",
      "summary": "Ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks. However, for aggregation to be effective, the individual networks must be as accurate and diverse as possible. An important problem is, then, how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions. We present here an extensive evaluation of several algorithms for ensemble construction, including new proposals and comparing them with standard methods in the literature. We also discuss a potential problem with seque...",
      "pdf_url": "https://arxiv.org/pdf/cs/0502006v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2509.19996v1",
      "title": "Choosing to Be Green: Advancing Green AI via Dynamic Model Selection",
      "authors": [
        "Emilio Cruciani",
        "Roberto Verdecchia"
      ],
      "published": "2025-09-24T11:02:13Z",
      "categories": "",
      "summary": "Artificial Intelligence is increasingly pervasive across domains, with ever more complex models delivering impressive predictive performance. This fast technological advancement however comes at a concerning environmental cost, with state-of-the-art models - particularly deep neural networks and large language models - requiring substantial computational resources and energy. In this work, we present the intuition of Green AI dynamic model selection, an approach based on dynamic model selection that aims at reducing the environmental footprint of AI by selecting the most sustainable model whil...",
      "pdf_url": "https://arxiv.org/pdf/2509.19996v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1212.0215v1",
      "title": "Artificial Neural Network for Performance Modeling and Optimization of CMOS Analog Circuits",
      "authors": [
        "Mriganka Chakraborty"
      ],
      "published": "2012-12-02T15:07:56Z",
      "categories": "",
      "summary": "This paper presents an implementation of multilayer feed forward neural networks (NN) to optimize CMOS analog circuits. For modeling and design recently neural network computational modules have got acceptance as an unorthodox and useful tool. To achieve high performance of active or passive circuit component neural network can be trained accordingly. A well trained neural network can produce more accurate outcome depending on its learning capability. Neural network model can replace empirical modeling solutions limited by range and accuracy.[2] Neural network models are easy to obtain for new...",
      "pdf_url": "https://arxiv.org/pdf/1212.0215v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2412.00382v2",
      "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
      "authors": [
        "Chengyu Li",
        "Debo Cheng",
        "Guixian Zhang",
        "Yi Li",
        "Shichao Zhang"
      ],
      "published": "2024-11-30T07:19:34Z",
      "categories": "",
      "summary": "Graph Neural Networks (GNNs) have demonstrated strong performance in graph representation learning across various real-world applications. However, they often produce biased predictions caused by sensitive attributes, such as religion or gender, an issue that has been largely overlooked in existing methods. Recently, numerous studies have focused on reducing biases in GNNs. However, these approaches often rely on training with partial data (e.g., using either node features or graph structure alone), which can enhance fairness but frequently compromises model utility due to the limited utilizat...",
      "pdf_url": "https://arxiv.org/pdf/2412.00382v2.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2309.12148v2",
      "title": "Neural Modelling of Dynamic Systems with Time Delays Based on an Adjusted NEAT Algorithm",
      "authors": [
        "Krzysztof Laddach",
        "Rafa\u0142 \u0141angowski"
      ],
      "published": "2023-09-21T15:04:42Z",
      "categories": "",
      "summary": "A problem related to the development of an algorithm designed to find an architecture of artificial neural network used for black-box modelling of dynamic systems with time delays has been addressed in this paper. The proposed algorithm is based on a well-known NeuroEvolution of Augmenting Topologies (NEAT) algorithm. The NEAT algorithm has been adjusted by allowing additional connections within an artificial neural network and developing original specialised evolutionary operators. This resulted in a compromise between the size of neural network and its accuracy in capturing the response of t...",
      "pdf_url": "https://arxiv.org/pdf/2309.12148v2.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2306.13793v3",
      "title": "QNNRepair: Quantized Neural Network Repair",
      "authors": [
        "Xidan Song",
        "Youcheng Sun",
        "Mustafa A. Mustafa",
        "Lucas C. Cordeiro"
      ],
      "published": "2023-06-23T21:40:24Z",
      "categories": "",
      "summary": "We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QN...",
      "pdf_url": "https://arxiv.org/pdf/2306.13793v3.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1907.00262v1",
      "title": "Dissecting Pruned Neural Networks",
      "authors": [
        "Jonathan Frankle",
        "David Bau"
      ],
      "published": "2019-06-29T19:27:57Z",
      "categories": "",
      "summary": "Pruning is a standard technique for removing unnecessary structure from a neural network to reduce its storage footprint, computational demands, or energy consumption. Pruning can reduce the parameter-counts of many state-of-the-art neural networks by an order of magnitude without compromising accuracy, meaning these networks contain a vast amount of unnecessary structure. In this paper, we study the relationship between pruning and interpretability. Namely, we consider the effect of removing unnecessary structure on the number of hidden units that learn disentangled representations of human-r...",
      "pdf_url": "https://arxiv.org/pdf/1907.00262v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2410.15369v1",
      "title": "Ethical AI in Retail: Consumer Privacy and Fairness",
      "authors": [
        "Anthonette Adanyin"
      ],
      "published": "2024-10-20T12:00:14Z",
      "categories": "",
      "summary": "The adoption of artificial intelligence (AI) in retail has significantly transformed the industry, enabling more personalized services and efficient operations. However, the rapid implementation of AI technologies raises ethical concerns, particularly regarding consumer privacy and fairness. This study aims to analyze the ethical challenges of AI applications in retail, explore ways retailers can implement AI technologies ethically while remaining competitive, and provide recommendations on ethical AI practices. A descriptive survey design was used to collect data from 300 respondents across m...",
      "pdf_url": "https://arxiv.org/pdf/2410.15369v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2309.13302v4",
      "title": "Gaining the Sparse Rewards by Exploring Lottery Tickets in Spiking Neural Network",
      "authors": [
        "Hao Cheng",
        "Jiahang Cao",
        "Erjia Xiao",
        "Mengshu Sun",
        "Renjing Xu"
      ],
      "published": "2023-09-23T08:24:36Z",
      "categories": "",
      "summary": "Deploying energy-efficient deep learning algorithms on computational-limited devices, such as robots, is still a pressing issue for real-world applications. Spiking Neural Networks (SNNs), a novel brain-inspired algorithm, offer a promising solution due to their low-latency and low-energy properties over traditional Artificial Neural Networks (ANNs). Despite their advantages, the dense structure of deep SNNs can still result in extra energy consumption. The Lottery Ticket Hypothesis (LTH) posits that within dense neural networks, there exist winning Lottery Tickets (LTs), namely sub-networks, ...",
      "pdf_url": "https://arxiv.org/pdf/2309.13302v4.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2510.14538v1",
      "title": "Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts",
      "authors": [
        "Emanuele Marconato",
        "Samuele Bortolotti",
        "Emile van Krieken",
        "Paolo Morettin",
        "Elena Umili",
        "Antonio Vergari",
        "Efthymia Tsamoura",
        "Andrea Passerini",
        "Stefano Teso"
      ],
      "published": "2025-10-16T10:28:34Z",
      "categories": "",
      "summary": "Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose predictions comply with prior knowledge encoding, e.g. safety or structural constraints. As such, it represents one of the most promising avenues for reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural and symbolic steps: neural networks are typically responsible for mapping low-level inputs into high-level symbolic concepts, while symbolic reasoning infers predictions compatible with the extracted concepts and the prior knowledge. Despite their promise, it was recently shown that - whenever the conc...",
      "pdf_url": "https://arxiv.org/pdf/2510.14538v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2311.17956v1",
      "title": "QuadraNet: Improving High-Order Neural Interaction Efficiency with Hardware-Aware Quadratic Neural Networks",
      "authors": [
        "Chenhui Xu",
        "Fuxun Yu",
        "Zirui Xu",
        "Chenchen Liu",
        "Jinjun Xiong",
        "Xiang Chen"
      ],
      "published": "2023-11-29T08:45:27Z",
      "categories": "",
      "summary": "Recent progress in computer vision-oriented neural network designs is mostly driven by capturing high-order neural interactions among inputs and features.  And there emerged a variety of approaches to accomplish this, such as Transformers and its variants.  However, these interactions generate a large amount of intermediate state and/or strong data dependency, leading to considerable memory consumption and computing cost, and therefore compromising the overall runtime performance.  To address this challenge, we rethink the high-order interactive neural network design with a quadratic computing...",
      "pdf_url": "https://arxiv.org/pdf/2311.17956v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2410.15077v1",
      "title": "Emotionally Enriched Feedback via Generative AI",
      "authors": [
        "Omar Alsaiari",
        "Nilufar Baghaei",
        "Hatim Lahza",
        "Jason Lodge",
        "Marie Boden",
        "Hassan Khosravi"
      ],
      "published": "2024-10-19T11:48:20Z",
      "categories": "",
      "summary": "This study investigates the impact of emotionally enriched AI feedback on student engagement and emotional responses in higher education. Leveraging the Control-Value Theory of Achievement Emotions, we conducted a randomized controlled experiment involving 425 participants where the experimental group received AI feedback enhanced with motivational elements, while the control group received neutral feedback. Our findings reveal that emotionally enriched feedback is perceived as more beneficial and helps reduce negative emotions, particularly anger, towards receiving feedback. However, it had n...",
      "pdf_url": "https://arxiv.org/pdf/2410.15077v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2501.16634v3",
      "title": "Towards Resource-Efficient Compound AI Systems",
      "authors": [
        "Gohar Irfan Chaudhry",
        "Esha Choukse",
        "\u00cd\u00f1igo Goiri",
        "Rodrigo Fonseca",
        "Adam Belay",
        "Ricardo Bianchini"
      ],
      "published": "2025-01-28T02:15:34Z",
      "categories": "",
      "summary": "Compound AI Systems, integrating multiple interacting components like models, retrievers, and external tools, have emerged as essential for addressing complex AI tasks. However, current implementations suffer from inefficient resource utilization due to tight coupling between application logic and execution details, a disconnect between orchestration and resource management layers, and the perceived exclusiveness between efficiency and quality.   We propose a vision for resource-efficient Compound AI Systems through a declarative workflow programming model and an adaptive runtime system for dy...",
      "pdf_url": "https://arxiv.org/pdf/2501.16634v3.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2511.09325v1",
      "title": "Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI",
      "authors": [
        "Stine Beltoft",
        "Lukas Galke"
      ],
      "published": "2025-11-12T13:36:58Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, an...",
      "pdf_url": "https://arxiv.org/pdf/2511.09325v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2402.19423v1",
      "title": "Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?",
      "authors": [
        "Tiezheng Zhang",
        "Xiaoxi Chen",
        "Chongyu Qu",
        "Alan Yuille",
        "Zongwei Zhou"
      ],
      "published": "2024-02-29T18:22:12Z",
      "categories": "",
      "summary": "Interactive segmentation, an integration of AI algorithms and human expertise, premises to improve the accuracy and efficiency of curating large-scale, detailed-annotated datasets in healthcare. Human experts revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from these revised annotations. This interactive process continues to enhance the quality of annotations until no major revision is needed from experts. The key challenge is how to leverage AI predicted and expert revised annotations to iteratively improve the AI. Two problems arise: (1) The risk ...",
      "pdf_url": "https://arxiv.org/pdf/2402.19423v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2501.04312v1",
      "title": "Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models",
      "authors": [
        "Kunpeng Zhang",
        "Shuai Wang",
        "Jitao Han",
        "Xiaogang Zhu",
        "Xian Li",
        "Shaohua Wang",
        "Sheng Wen"
      ],
      "published": "2025-01-08T07:07:22Z",
      "categories": "",
      "summary": "Deep learning (DL) libraries, widely used in AI applications, often contain vulnerabilities like buffer overflows and use-after-free errors. Traditional fuzzing struggles with the complexity and API diversity of DL libraries such as TensorFlow and PyTorch, which feature over 1,000 APIs. Testing all these APIs is challenging due to complex inputs and varied usage patterns. While large language models (LLMs) show promise in code understanding and generation, existing LLM-based fuzzers lack deep knowledge of API edge cases and struggle with test input generation. To address this, we propose DFUZZ...",
      "pdf_url": "https://arxiv.org/pdf/2501.04312v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2004.11909v1",
      "title": "On the safety of vulnerable road users by cyclist orientation detection using Deep Learning",
      "authors": [
        "Marichelo Garcia-Venegas",
        "Diego A. Mercado-Ravell",
        "Carlos A. Carballo-Monsivais"
      ],
      "published": "2020-04-25T18:10:26Z",
      "categories": "",
      "summary": "In this work, orientation detection using Deep Learning is acknowledged for a particularly vulnerable class of road users,the cyclists. Knowing the cyclists' orientation is of great relevance since it provides a good notion about their future trajectory, which is crucial to avoid accidents in the context of intelligent transportation systems. Using Transfer Learning with pre-trained models and TensorFlow, we present a performance comparison between the main algorithms reported in the literature for object detection,such as SSD, Faster R-CNN and R-FCN along with MobilenetV2, InceptionV2, ResNet...",
      "pdf_url": "https://arxiv.org/pdf/2004.11909v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2005.00972v1",
      "title": "Repairing Deep Neural Networks: Fix Patterns and Challenges",
      "authors": [
        "Md Johirul Islam",
        "Rangeet Pan",
        "Giang Nguyen",
        "Hridesh Rajan"
      ],
      "published": "2020-05-03T03:06:12Z",
      "categories": "",
      "summary": "Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work prese...",
      "pdf_url": "https://arxiv.org/pdf/2005.00972v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1904.11617v1",
      "title": "High-Resolution Network for Photorealistic Style Transfer",
      "authors": [
        "Ming Li",
        "Chunyang Ye",
        "Wei Li"
      ],
      "published": "2019-04-25T22:59:37Z",
      "categories": "",
      "summary": "Photorealistic style transfer aims to transfer the style of one image to another, but preserves the original structure and detail outline of the content image, which makes the content image still look like a real shot after the style transfer. Although some realistic image styling methods have been proposed, these methods are vulnerable to lose the details of the content image and produce some irregular distortion structures. In this paper, we use a high-resolution network as the image generation network. Compared to other methods, which reduce the resolution and then restore the high resoluti...",
      "pdf_url": "https://arxiv.org/pdf/1904.11617v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.02015v1",
      "title": "Fault injection analysis of Real NVP normalising flow model for satellite anomaly detection",
      "authors": [
        "Gabriele Greco",
        "Carlo Cena",
        "Umberto Albertin",
        "Mauro Martini",
        "Marcello Chiaberge"
      ],
      "published": "2025-04-02T08:32:59Z",
      "categories": "",
      "summary": "Satellites are used for a multitude of applications, including communications, Earth observation, and space science. Neural networks and deep learning-based approaches now represent the state-of-the-art to enhance the performance and efficiency of these tasks. Given that satellites are susceptible to various faults, one critical application of Artificial Intelligence (AI) is fault detection. However, despite the advantages of neural networks, these systems are vulnerable to radiation errors, which can significantly impact their reliability. Ensuring the dependability of these solutions require...",
      "pdf_url": "https://arxiv.org/pdf/2504.02015v1.pdf",
      "relevance_score": 15,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2001.00078v2",
      "title": "Regulatory Markets for AI Safety",
      "authors": [
        "Jack Clark",
        "Gillian K. Hadfield"
      ],
      "published": "2019-12-11T19:21:54Z",
      "categories": "",
      "summary": "We propose a new model for regulation to achieve AI safety: global regulatory markets. We first sketch the model in general terms and provide an overview of the costs and benefits of this approach. We then demonstrate how the model might work in practice: responding to the risk of adversarial attacks on AI models employed in commercial drones.",
      "pdf_url": "https://arxiv.org/pdf/2001.00078v2.pdf",
      "relevance_score": 12,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2512.00142v1",
      "title": "DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions",
      "authors": [
        "Swati Sachan",
        "Dale S. Fickett"
      ],
      "published": "2025-11-28T18:30:39Z",
      "categories": "",
      "summary": "This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain...",
      "pdf_url": "https://arxiv.org/pdf/2512.00142v1.pdf",
      "relevance_score": 12,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2312.10057v1",
      "title": "Generative AI in Writing Research Papers: A New Type of Algorithmic Bias and Uncertainty in Scholarly Work",
      "authors": [
        "Rishab Jain",
        "Aditya Jain"
      ],
      "published": "2023-12-04T04:05:04Z",
      "categories": "",
      "summary": "The use of artificial intelligence (AI) in research across all disciplines is becoming ubiquitous. However, this ubiquity is largely driven by hyperspecific AI models developed during scientific studies for accomplishing a well-defined, data-dense task. These AI models introduce apparent, human-recognizable biases because they are trained with finite, specific data sets and parameters. However, the efficacy of using large language models (LLMs) -- and LLM-powered generative AI tools, such as ChatGPT -- to assist the research process is currently indeterminate. These generative AI tools, traine...",
      "pdf_url": "https://arxiv.org/pdf/2312.10057v1.pdf",
      "relevance_score": 12,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2107.08821v2",
      "title": "Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI",
      "authors": [
        "Quanshi Zhang",
        "Tian Han",
        "Lixin Fan",
        "Zhanxing Zhu",
        "Hang Su",
        "Ying Nian Wu",
        "Jie Ren",
        "Hao Zhang"
      ],
      "published": "2021-07-16T13:14:16Z",
      "categories": "",
      "summary": "This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. Deep neural networks (DNNs) have undoubtedly brought great success to a wide range of applications in computer vision, computational linguistics, and AI. However, foundational principles underlying the DNNs' success and their resilience to adversarial attacks are still largely missing. Interpreting and theorizing the internal mechanisms of DNNs becomes a compelling yet controversial topic. This workshop pays a special interest in theoretic foundations, limitations, and new...",
      "pdf_url": "https://arxiv.org/pdf/2107.08821v2.pdf",
      "relevance_score": 12,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2504.18601v1",
      "title": "The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking",
      "authors": [
        "Philipp Koralus"
      ],
      "published": "2025-04-24T19:34:43Z",
      "categories": "",
      "summary": "In the face of rapidly advancing AI technology, individuals will increasingly rely on AI agents to navigate life's growing complexities, raising critical concerns about maintaining both human agency and autonomy. This paper addresses a fundamental dilemma posed by AI decision-support systems: the risk of either becoming overwhelmed by complex decisions, thus losing agency, or having autonomy compromised by externally controlled choice architectures reminiscent of ``nudging'' practices. While the ``nudge'' framework, based on the use of choice-framing to guide individuals toward presumed benefi...",
      "pdf_url": "https://arxiv.org/pdf/2504.18601v1.pdf",
      "relevance_score": 12,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2409.17190v1",
      "title": "Enhancing Guardrails for Safe and Secure Healthcare AI",
      "authors": [
        "Ananya Gangavarapu"
      ],
      "published": "2024-09-25T06:30:06Z",
      "categories": "",
      "summary": "Generative AI holds immense promise in addressing global healthcare access challenges, with numerous innovative applications now ready for use across various healthcare domains. However, a significant barrier to the widespread adoption of these domain-specific AI solutions is the lack of robust safety mechanisms to effectively manage issues such as hallucination, misinformation, and ensuring truthfulness. Left unchecked, these risks can compromise patient safety and erode trust in healthcare AI systems. While general-purpose frameworks like Llama Guard are useful for filtering toxicity and har...",
      "pdf_url": "https://arxiv.org/pdf/2409.17190v1.pdf",
      "relevance_score": 12,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2010.05888v2",
      "title": "Garfield: System Support for Byzantine Machine Learning",
      "authors": [
        "Rachid Guerraoui",
        "Arsany Guirguis",
        "J\u00e9r\u00e9my Max Plassmann",
        "Anton Alexandre Ragot",
        "S\u00e9bastien Rouault"
      ],
      "published": "2020-10-12T17:36:19Z",
      "categories": "",
      "summary": "We present Garfield, a library to transparently make machine learning (ML) applications, initially built with popular (but fragile) frameworks, e.g., TensorFlow and PyTorch, Byzantine-resilient. Garfield relies on a novel object-oriented design, reducing the coding effort, and addressing the vulnerability of the shared-graph architecture followed by classical ML frameworks. Garfield encompasses various communication patterns and supports computations on CPUs and GPUs, allowing addressing the general question of the very practical cost of Byzantine resilience in SGD-based ML applications. We re...",
      "pdf_url": "https://arxiv.org/pdf/2010.05888v2.pdf",
      "relevance_score": 10,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2010.00373v2",
      "title": "Task Agnostic Continual Learning Using Online Variational Bayes with Fixed-Point Updates",
      "authors": [
        "Chen Zeno",
        "Itay Golan",
        "Elad Hoffer",
        "Daniel Soudry"
      ],
      "published": "2020-10-01T13:10:35Z",
      "categories": "",
      "summary": "Background: Catastrophic forgetting is the notorious vulnerability of neural networks to the changes in the data distribution during learning. This phenomenon has long been considered a major obstacle for using learning agents in realistic continual learning settings. A large body of continual learning research assumes that task boundaries are known during training. However, only a few works consider scenarios in which task boundaries are unknown or not well defined -- task agnostic scenarios. The optimal Bayesian solution for this requires an intractable online Bayes update to the weights pos...",
      "pdf_url": "https://arxiv.org/pdf/2010.00373v2.pdf",
      "relevance_score": 10,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1905.13409v2",
      "title": "Bypassing Backdoor Detection Algorithms in Deep Learning",
      "authors": [
        "Te Juin Lester Tan",
        "Reza Shokri"
      ],
      "published": "2019-05-31T04:28:00Z",
      "categories": "",
      "summary": "Deep learning models are vulnerable to various adversarial manipulations of their training data, parameters, and input sample. In particular, an adversary can modify the training data and model parameters to embed backdoors into the model, so the model behaves according to the adversary's objective if the input contains the backdoor features, referred to as the backdoor trigger (e.g., a stamp on an image). The poisoned model's behavior on clean data, however, remains unchanged. Many detection algorithms are designed to detect backdoors on input samples or model parameters, through the statisti...",
      "pdf_url": "https://arxiv.org/pdf/1905.13409v2.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2512.04480v5",
      "title": "Auditing Human Decision-Making in High-Stakes Environments via Prescriptive AI: A Stress-Test on Real-Time Tactical Management",
      "authors": [
        "Pedro Passos",
        "Patrick Moratori"
      ],
      "published": "2025-12-04T05:33:28Z",
      "categories": "",
      "summary": "High-stakes decision-making is often compromised by cognitive biases and outcome dependency. Current AI models typically mimic historical human behavior, inheriting these biases and limiting their utility for normative improvement. Here, we introduce a Prescriptive AI framework designed to audit, rather than automate, human judgment in real-time environments. By decoupling decision quality from stochastic outcomes, we quantify \"decision latency\" and status quo bias in elite soccer management - a high-pressure adversarial domain. Analyzing 2018 FIFA World Cup data, our system exposes critical r...",
      "pdf_url": "https://arxiv.org/pdf/2512.04480v5.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2407.21370v1",
      "title": "SHA-CNN: Scalable Hierarchical Aware Convolutional Neural Network for Edge AI",
      "authors": [
        "Narendra Singh Dhakad",
        "Yuvnish Malhotra",
        "Santosh Kumar Vishvakarma",
        "Kaushik Roy"
      ],
      "published": "2024-07-31T06:44:52Z",
      "categories": "",
      "summary": "This paper introduces a Scalable Hierarchical Aware Convolutional Neural Network (SHA-CNN) model architecture for Edge AI applications. The proposed hierarchical CNN model is meticulously crafted to strike a balance between computational efficiency and accuracy, addressing the challenges posed by resource-constrained edge devices. SHA-CNN demonstrates its efficacy by achieving accuracy comparable to state-of-the-art hierarchical models while outperforming baseline models in accuracy metrics. The key innovation lies in the model's hierarchical awareness, enabling it to discern and prioritize re...",
      "pdf_url": "https://arxiv.org/pdf/2407.21370v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2507.18576v3",
      "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
      "authors": [
        "Shanghai AI Lab",
        " :",
        "Yicheng Bao",
        "Guanxu Chen",
        "Mingkang Chen",
        "Yunhao Chen",
        "Chiyu Chen",
        "Lingjie Chen",
        "Sirui Chen",
        "Xinquan Chen",
        "Jie Cheng",
        "Yu Cheng",
        "Dengke Deng",
        "Yizhuo Ding",
        "Dan Ding",
        "Xiaoshan Ding",
        "Yi Ding",
        "Zhichen Dong",
        "Lingxiao Du",
        "Yuyu Fan",
        "Xinshun Feng",
        "Yanwei Fu",
        "Yuxuan Gao",
        "Ruijun Ge",
        "Tianle Gu",
        "Lujun Gui",
        "Jiaxuan Guo",
        "Qianxi He",
        "Yuenan Hou",
        "Xuhao Hu",
        "Hong Huang",
        "Kaichen Huang",
        "Shiyang Huang",
        "Yuxian Jiang",
        "Shanzhe Lei",
        "Jie Li",
        "Lijun Li",
        "Hao Li",
        "Juncheng Li",
        "Xiangtian Li",
        "Yafu Li",
        "Lingyu Li",
        "Xueyan Li",
        "Haotian Liang",
        "Dongrui Liu",
        "Qihua Liu",
        "Zhixuan Liu",
        "Bangwei Liu",
        "Huacan Liu",
        "Yuexiao Liu",
        "Zongkai Liu",
        "Chaochao Lu",
        "Yudong Lu",
        "Xiaoya Lu",
        "Zhenghao Lu",
        "Qitan Lv",
        "Caoyuan Ma",
        "Jiachen Ma",
        "Xiaoya Ma",
        "Zhongtian Ma",
        "Lingyu Meng",
        "Ziqi Miao",
        "Yazhe Niu",
        "Yuezhang Peng",
        "Yuan Pu",
        "Han Qi",
        "Chen Qian",
        "Xingge Qiao",
        "Jingjing Qu",
        "Jiashu Qu",
        "Wanying Qu",
        "Wenwen Qu",
        "Xiaoye Qu",
        "Qihan Ren",
        "Qingnan Ren",
        "Qingyu Ren",
        "Jing Shao",
        "Wenqi Shao",
        "Shuai Shao",
        "Dongxing Shi",
        "Xin Song",
        "Xinhao Song",
        "Yan Teng",
        "Xuan Tong",
        "Yingchun Wang",
        "Xuhong Wang",
        "Shujie Wang",
        "Xin Wang",
        "Yige Wang",
        "Yixu Wang",
        "Yuanfu Wang",
        "Futing Wang",
        "Ruofan Wang",
        "Wenjie Wang",
        "Yajie Wang",
        "Muhao Wei",
        "Xiaoyu Wen",
        "Fenghua Weng",
        "Yuqi Wu",
        "Yingtong Xiong",
        "Xingcheng Xu",
        "Chao Yang",
        "Yue Yang",
        "Yang Yao",
        "Yulei Ye",
        "Zhenyun Yin",
        "Yi Yu",
        "Bo Zhang",
        "Qiaosheng Zhang",
        "Jinxuan Zhang",
        "Yexin Zhang",
        "Yinqiang Zheng",
        "Hefeng Zhou",
        "Zhanhui Zhou",
        "Pengyu Zhu",
        "Qingzi Zhu",
        "Yubo Zhu",
        "Bowen Zhou"
      ],
      "published": "2025-07-24T16:49:19Z",
      "categories": "",
      "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average imp...",
      "pdf_url": "https://arxiv.org/pdf/2507.18576v3.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1212.5188v1",
      "title": "Combinatorial neural codes from a mathematical coding theory perspective",
      "authors": [
        "Carina Curto",
        "Vladimir Itskov",
        "Katherine Morrison",
        "Zachary Roth",
        "Judy L. Walker"
      ],
      "published": "2012-12-20T18:57:30Z",
      "categories": "",
      "summary": "Shannon's seminal 1948 work gave rise to two distinct areas of research: information theory and mathematical coding theory. While information theory has had a strong influence on theoretical neuroscience, ideas from mathematical coding theory have received considerably less attention. Here we take a new look at combinatorial neural codes from a mathematical coding theory perspective, examining the error correction capabilities of familiar receptive field codes (RF codes). We find, perhaps surprisingly, that the high levels of redundancy present in these codes does not support accurate error co...",
      "pdf_url": "https://arxiv.org/pdf/1212.5188v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1210.3569v2",
      "title": "Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics",
      "authors": [
        "Sohrob Kazerounian",
        "Matthew Luciw",
        "Mathis Richter",
        "Yulia Sandamirskaya"
      ],
      "published": "2012-10-12T16:41:58Z",
      "categories": "",
      "summary": "We introduce a dynamic neural algorithm called Dynamic Neural (DN) SARSA(\u03bb) for learning a behavioral sequence from delayed reward. DN-SARSA(\u03bb) combines Dynamic Field Theory models of behavioral sequence representation, classical reinforcement learning, and a computational neuroscience model of working memory, called Item and Order working memory, which serves as an eligibility trace. DN-SARSA(\u03bb) is implemented on both a simulated and real robot that must learn a specific rewarding sequence of elementary behaviors from exploration. Results show DN-SARSA(\u03bb) performs on the level of the discrete...",
      "pdf_url": "https://arxiv.org/pdf/1210.3569v2.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2001.07620v3",
      "title": "EdgeNets:Edge Varying Graph Neural Networks",
      "authors": [
        "Elvin Isufi",
        "Fernando Gama",
        "Alejandro Ribeiro"
      ],
      "published": "2020-01-21T15:51:17Z",
      "categories": "",
      "summary": "Driven by the outstanding performance of neural networks in the structured Euclidean domain, recent years have seen a surge of interest in developing neural networks for graphs and data supported on graphs. The graph is leveraged at each layer of the neural network as a parameterization to capture detail at the node level with a reduced number of parameters and computational complexity. Following this rationale, this paper puts forth a general framework that unifies state-of-the-art graph neural networks (GNNs) through the concept of EdgeNet. An EdgeNet is a GNN architecture that allows differ...",
      "pdf_url": "https://arxiv.org/pdf/2001.07620v3.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2505.02975v1",
      "title": "Navigating Privacy and Trust: AI Assistants as Social Support for Older Adults",
      "authors": [
        "Karina LaRubbio",
        "Malcolm Grba",
        "Diana Freed"
      ],
      "published": "2025-05-05T19:00:14Z",
      "categories": "",
      "summary": "AI assistants are increasingly integrated into older adults' daily lives, offering new opportunities for social support and accessibility while raising important questions about privacy, autonomy, and trust. As these systems become embedded in caregiving and social networks, older adults must navigate trade-offs between usability, data privacy, and personal agency across different interaction contexts. Although prior work has explored AI assistants' potential benefits, further research is needed to understand how perceived usefulness and risk shape adoption and engagement. This paper examines ...",
      "pdf_url": "https://arxiv.org/pdf/2505.02975v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2511.19312v2",
      "title": "Human-AI Teaming Under Deception: An Implicit BCI Safeguards Drone Team Performance in Virtual Reality",
      "authors": [
        "Christopher Baker",
        "Stephen Hinton",
        "Akashdeep Nijjar",
        "Riccardo Poli",
        "Caterina Cinel",
        "Tom Reed",
        "Stephen Fairclough"
      ],
      "published": "2025-11-24T17:07:07Z",
      "categories": "",
      "summary": "Human-AI teams can be vulnerable to catastrophic failure when feedback from the AI is incorrect, especially under high cognitive workload. Traditional team aggregation methods, such as voting, are susceptible to these AI errors, which can actively bias the behaviour of each individual and inflate the likelihood of an erroneous group decision. We hypothesised that a collaborative Brain-Computer Interface (cBCI) using neural activity collected before a behavioural decision is made can provide a source of information that is \"decoupled\" from this biased behaviour, thereby protecting the team from...",
      "pdf_url": "https://arxiv.org/pdf/2511.19312v2.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2412.03579v1",
      "title": "Towards a Practical Ethics of Generative AI in Creative Production Processes",
      "authors": [
        "Geert Hofman"
      ],
      "published": "2024-11-18T11:07:26Z",
      "categories": "",
      "summary": "The increasing integration of artificial intelligence into various domains, including design and creative processes, raises significant ethical questions. While AI ethics is often examined from the perspective of technology developers, less attention has been paid to the practical ethical considerations faced by technology users, particularly in design contexts. This paper introduces a framework for addressing ethical challenges in creative production processes, such as the Double Diamond design model. Drawing on six major ethical theories - virtue ethics, deontology, utilitarianism, contract ...",
      "pdf_url": "https://arxiv.org/pdf/2412.03579v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "1904.11713v2",
      "title": "Passive nonlinear dendritic interactions as a general computational resource in functional spiking neural networks",
      "authors": [
        "Andreas St\u00f6ckel",
        "Chris Eliasmith"
      ],
      "published": "2019-04-26T08:32:29Z",
      "categories": "",
      "summary": "Nonlinear interactions in the dendritic tree play a key role in neural computation. Nevertheless, modeling frameworks aimed at the construction of large-scale, functional spiking neural networks, such as the Neural Engineering Framework, tend to assume a linear superposition of post-synaptic currents. In this paper, we present a series of extensions to the Neural Engineering Framework that facilitate the construction of networks incorporating Dale's principle and nonlinear conductance-based synapses. We apply these extensions to a two-compartment LIF neuron that can be seen as a simple model o...",
      "pdf_url": "https://arxiv.org/pdf/1904.11713v2.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2508.04270v2",
      "title": "TDSNNs: Competitive Topographic Deep Spiking Neural Networks for Visual Cortex Modeling",
      "authors": [
        "Deming Zhou",
        "Yuetong Fang",
        "Zhaorui Wang",
        "Renjing Xu"
      ],
      "published": "2025-08-06T09:53:39Z",
      "categories": "",
      "summary": "The primate visual cortex exhibits topographic organization, where functionally similar neurons are spatially clustered, a structure widely believed to enhance neural processing efficiency. While prior works have demonstrated that conventional deep ANNs can develop topographic representations, these models largely neglect crucial temporal dynamics. This oversight often leads to significant performance degradation in tasks like object recognition and compromises their biological fidelity. To address this, we leverage spiking neural networks (SNNs), which inherently capture spike-based temporal ...",
      "pdf_url": "https://arxiv.org/pdf/2508.04270v2.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2507.14339v1",
      "title": "Fiduciary AI for the Future of Brain-Technology Interactions",
      "authors": [
        "Abhishek Bhattacharjee",
        "Jack Pilkington",
        "Nita Farahany"
      ],
      "published": "2025-07-18T19:34:08Z",
      "categories": "",
      "summary": "Brain foundation models represent a new frontier in AI: instead of processing text or images, these models interpret real-time neural signals from EEG, fMRI, and other neurotechnologies. When integrated with brain-computer interfaces (BCIs), they may enable transformative applications-from thought controlled devices to neuroprosthetics-by interpreting and acting on brain activity in milliseconds. However, these same systems pose unprecedented risks, including the exploitation of subconscious neural signals and the erosion of cognitive liberty. Users cannot easily observe or control how their b...",
      "pdf_url": "https://arxiv.org/pdf/2507.14339v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    },
    {
      "arxiv_id": "2307.05642v1",
      "title": "ConFL: Constraint-guided Fuzzing for Machine Learning Framework",
      "authors": [
        "Zhao Liu",
        "Quanchen Zou",
        "Tian Yu",
        "Xuan Wang",
        "Guozhu Meng",
        "Kai Chen",
        "Deyue Zhang"
      ],
      "published": "2023-07-11T10:16:35Z",
      "categories": "",
      "summary": "As machine learning gains prominence in various sectors of society for automated decision-making, concerns have risen regarding potential vulnerabilities in machine learning (ML) frameworks. Nevertheless, testing these frameworks is a daunting task due to their intricate implementation. Previous research on fuzzing ML frameworks has struggled to effectively extract input constraints and generate valid inputs, leading to extended fuzzing durations for deep execution or revealing the target crash.   In this paper, we propose ConFL, a constraint-guided fuzzer for ML frameworks. ConFL automaticall...",
      "pdf_url": "https://arxiv.org/pdf/2307.05642v1.pdf",
      "relevance_score": 0,
      "dimension": "cluster_i_threats"
    }
  ],
  "green_papers": [
    {
      "arxiv_id": "2510.11837v1",
      "title": "Countermind: A Multi-Layered Security Architecture for Large Language Models",
      "authors": [
        "Dominik Schwarz"
      ],
      "published": "2025-10-13T18:41:18Z",
      "categories": "",
      "summary": "The security of Large Language Model (LLM) applications is fundamentally challenged by \"form-first\" attacks like prompt injection and jailbreaking, where malicious instructions are embedded within user inputs. Conventional defenses, which rely on post hoc output filtering, are often brittle and fail to address the root cause: the model's inability to distinguish trusted instructions from untrusted data. This paper proposes Countermind, a multi-layered security architecture intended to shift defenses from a reactive, post hoc posture to a proactive, pre-inference, and intra-inference enforcemen...",
      "pdf_url": "https://arxiv.org/pdf/2510.11837v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.11837v1_paper.pdf"
    },
    {
      "arxiv_id": "2504.19521v4",
      "title": "Security Steerability is All You Need",
      "authors": [
        "Itay Hazan",
        "Idan Habler",
        "Ron Bitton",
        "Itsik Mantin"
      ],
      "published": "2025-04-28T06:40:01Z",
      "categories": "",
      "summary": "The adoption of Generative AI (GenAI) in applications inevitably comes with the expansion of the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate the GenAI related security threats by developing evaluation methods and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. 'How to build a bomb'), there is significantly less discussion on application-level security and how to evaluate and mitigate it. Thus, in this work we adopt an application-c...",
      "pdf_url": "https://arxiv.org/pdf/2504.19521v4.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2504.19521v4_paper.pdf"
    },
    {
      "arxiv_id": "2510.00451v1",
      "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "published": "2025-10-01T03:05:07Z",
      "categories": "",
      "summary": "Large language models have gained widespread prominence, yet their vulnerability to prompt injection and other adversarial attacks remains a critical concern. This paper argues for a security-by-design AI paradigm that proactively mitigates LLM vulnerabilities while enhancing performance. To achieve this, we introduce PromptShield, an ontology-driven framework that ensures deterministic and secure prompt interactions. It standardizes user inputs through semantic validation, eliminating ambiguity and mitigating adversarial manipulation. To assess PromptShield's security and performance capabili...",
      "pdf_url": "https://arxiv.org/pdf/2510.00451v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.00451v1_paper.pdf"
    },
    {
      "arxiv_id": "2504.18575v3",
      "title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks",
      "authors": [
        "Ivan Evtimov",
        "Arman Zharmagambetov",
        "Aaron Grattafiori",
        "Chuan Guo",
        "Kamalika Chaudhuri"
      ],
      "published": "2025-04-22T17:51:03Z",
      "categories": "",
      "summary": "Autonomous UI agents powered by AI have tremendous potential to boost human productivity by automating routine tasks such as filing taxes and paying bills. However, a major challenge in unlocking their full potential is security, which is exacerbated by the agent's ability to take action on their user's behalf. Existing tests for prompt injections in web agents either over-simplify the threat by testing unrealistic scenarios or giving the attacker too much power, or look at single-step isolated tasks. To more accurately measure progress for secure web agents, we introduce WASP -- a new publicl...",
      "pdf_url": "https://arxiv.org/pdf/2504.18575v3.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2504.18575v3_paper.pdf"
    },
    {
      "arxiv_id": "2507.02735v2",
      "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "David Wagner",
        "Chuan Guo"
      ],
      "published": "2025-07-03T15:47:13Z",
      "categories": "",
      "summary": "Prompt injection attack has been listed as the top-1 security threat to LLM-integrated applications, which interact with external environment data for complex tasks. The untrusted data may contain an injected prompt trying to arbitrarily manipulate the system. Model-level prompt injection defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source secure models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigating...",
      "pdf_url": "https://arxiv.org/pdf/2507.02735v2.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2507.02735v2_paper.pdf"
    },
    {
      "arxiv_id": "2511.04508v1",
      "title": "Large Language Models for Cyber Security",
      "authors": [
        "Raunak Somani",
        "Aswani Kumar Cherukuri"
      ],
      "published": "2025-11-06T16:25:35Z",
      "categories": "",
      "summary": "This paper studies the integration off Large Language Models into cybersecurity tools and protocols. The main issue discussed in this paper is how traditional rule-based and signature based security systems are not enough to deal with modern AI powered cyber threats. Cybersecurity industry is changing as threats are becoming more dangerous and adaptive in nature by levering the features provided by AI tools. By integrating LLMs into these tools and protocols, make the systems scalable, context-aware and intelligent. Thus helping it to mitigate these evolving cyber threats. The paper studies th...",
      "pdf_url": "https://arxiv.org/pdf/2511.04508v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.04508v1_paper.pdf"
    },
    {
      "arxiv_id": "2507.05445v1",
      "title": "A Systematization of Security Vulnerabilities in Computer Use Agents",
      "authors": [
        "Daniel Jones",
        "Giorgio Severi",
        "Martin Pouliot",
        "Gary Lopez",
        "Joris de Gruyter",
        "Santiago Zanella-Beguelin",
        "Justin Song",
        "Blake Bullwinkel",
        "Pamela Cortez",
        "Amanda Minnich"
      ],
      "published": "2025-07-07T19:50:21Z",
      "categories": "",
      "summary": "Computer Use Agents (CUAs), autonomous systems that interact with software interfaces via browsers or virtual machines, are rapidly being deployed in consumer and enterprise environments. These agents introduce novel attack surfaces and trust boundaries that are not captured by traditional threat models. Despite their growing capabilities, the security boundaries of CUAs remain poorly understood. In this paper, we conduct a systematic threat analysis and testing of real-world CUAs under adversarial conditions. We identify seven classes of risks unique to the CUA paradigm, and analyze three con...",
      "pdf_url": "https://arxiv.org/pdf/2507.05445v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2507.05445v1_paper.pdf"
    },
    {
      "arxiv_id": "2506.08837v3",
      "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
      "authors": [
        "Luca Beurer-Kellner",
        "Beat Buesser",
        "Ana-Maria Cre\u0163u",
        "Edoardo Debenedetti",
        "Daniel Dobos",
        "Daniel Fabian",
        "Marc Fischer",
        "David Froelicher",
        "Kathrin Grosse",
        "Daniel Naeff",
        "Ezinwanne Ozoani",
        "Andrew Paverd",
        "Florian Tram\u00e8r",
        "V\u00e1clav Volhejn"
      ],
      "published": "2025-06-10T14:23:55Z",
      "categories": "",
      "summary": "As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their security has become a critical challenge. Among the most pressing threats are prompt injection attacks, which exploit the agent's resilience on natural language inputs -- an especially dangerous threat when agents are granted tool access or handle sensitive information. In this work, we propose a set of principled design patterns for building AI agents with provable resistance to prompt injection. We systematically analyze these patterns, discus...",
      "pdf_url": "https://arxiv.org/pdf/2506.08837v3.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2506.08837v3_paper.pdf"
    },
    {
      "arxiv_id": "2507.06323v1",
      "title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms",
      "authors": [
        "Tarek Gasmi",
        "Ramzi Guesmi",
        "Ines Belhadj",
        "Jihene Bennaceur"
      ],
      "published": "2025-07-08T18:24:28Z",
      "categories": "",
      "summary": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Funct...",
      "pdf_url": "https://arxiv.org/pdf/2507.06323v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2507.06323v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.06556v1",
      "title": "Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks",
      "authors": [
        "Saeid Jamshidi",
        "Kawser Wazed Nafi",
        "Arghavan Moradi Dakhel",
        "Negar Shahabi",
        "Foutse Khomh",
        "Naser Ezzati-Jivan"
      ],
      "published": "2025-12-06T20:07:58Z",
      "categories": "",
      "summary": "The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors...",
      "pdf_url": "https://arxiv.org/pdf/2512.06556v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.06556v1_paper.pdf"
    },
    {
      "arxiv_id": "2506.19109v1",
      "title": "Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems",
      "authors": [
        "Valerii Gakh",
        "Hayretdin Bahsi"
      ],
      "published": "2025-06-23T20:39:43Z",
      "categories": "",
      "summary": "Prompt injection threatens novel applications that emerge from adapting LLMs for various user tasks. The newly developed LLM-based software applications become more ubiquitous and diverse. However, the threat of prompt injection attacks undermines the security of these systems as the mitigation and defenses against them, proposed so far, are insufficient. We investigated the capabilities of early prompt injection detection systems, focusing specifically on the detection performance of techniques implemented in various open-source solutions. These solutions are supposed to detect certain types ...",
      "pdf_url": "https://arxiv.org/pdf/2506.19109v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2506.19109v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.04785v1",
      "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
      "authors": [
        "Eranga Bandara",
        "Amin Hass",
        "Ross Gore",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Safdar H. Bouk",
        "Xueping Liang",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "published": "2025-12-04T13:32:40Z",
      "categories": "",
      "summary": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems...",
      "pdf_url": "https://arxiv.org/pdf/2512.04785v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.04785v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.08829v1",
      "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization",
      "authors": [
        "Debeshee Das",
        "Luca Beurer-Kellner",
        "Marc Fischer",
        "Maximilian Baader"
      ],
      "published": "2025-10-09T21:32:02Z",
      "categories": "",
      "summary": "The increasing adoption of LLM agents with access to numerous tools and sensitive data significantly widens the attack surface for indirect prompt injections. Due to the context-dependent nature of attacks, however, current defenses are often ill-calibrated as they cannot reliably differentiate malicious and benign instructions, leading to high false positive rates that prevent their real-world adoption. To address this, we present a novel approach inspired by the fundamental principle of computer security: data should not contain executable instructions. Instead of sample-level classification...",
      "pdf_url": "https://arxiv.org/pdf/2510.08829v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.08829v1_paper.pdf"
    },
    {
      "arxiv_id": "2511.11020v1",
      "title": "Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis",
      "authors": [
        "Farhad Abtahi",
        "Fernando Seoane",
        "Iv\u00e1n Pau",
        "Mario Vega-Barbas"
      ],
      "published": "2025-11-14T07:16:16Z",
      "categories": "",
      "summary": "Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 1...",
      "pdf_url": "https://arxiv.org/pdf/2511.11020v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.11020v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.23480v1",
      "title": "Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation",
      "authors": [
        "Toqeer Ali Syed",
        "Mohammad Riyaz Belgaum",
        "Salman Jan",
        "Asadullah Abdullah Khan",
        "Saad Said Alqahtani"
      ],
      "published": "2025-12-29T14:06:09Z",
      "categories": "",
      "summary": "The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, rei...",
      "pdf_url": "https://arxiv.org/pdf/2512.23480v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.23480v1_paper.pdf"
    },
    {
      "arxiv_id": "2505.10538v1",
      "title": "S3C2 Summit 2024-09: Industry Secure Software Supply Chain Summit",
      "authors": [
        "Imranur Rahman",
        "Yasemin Acar",
        "Michel Cukier",
        "William Enck",
        "Christian Kastner",
        "Alexandros Kapravelos",
        "Dominik Wermke",
        "Laurie Williams"
      ],
      "published": "2025-05-15T17:48:14Z",
      "categories": "",
      "summary": "While providing economic and software development value, software supply chains are only as strong as their weakest link. Over the past several years, there has been an exponential increase in cyberattacks, specifically targeting vulnerable links in critical software supply chains. These attacks disrupt the day-to-day functioning and threaten the security of nearly everyone on the internet, from billion-dollar companies and government agencies to hobbyist open-source developers. The ever-evolving threat of software supply chain attacks has garnered interest from the software industry and the U...",
      "pdf_url": "https://arxiv.org/pdf/2505.10538v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2505.10538v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.24920v1",
      "title": "S3C2 Summit 2025-03: Industry Secure Supply Chain Summit",
      "authors": [
        "Elizabeth Lin",
        "Jonah Ghebremichael",
        "William Enck",
        "Yasemin Acar",
        "Michel Cukier",
        "Alexandros Kapravelos",
        "Christian Kastner",
        "Laurie Williams"
      ],
      "published": "2025-10-28T19:47:07Z",
      "categories": "",
      "summary": "Software supply chains, while providing immense economic and software development value, are only as strong as their weakest link. Over the past several years, there has been an exponential increase in cyberattacks specifically targeting vulnerable links in critical software supply chains. These attacks disrupt the day-to-day functioning and threaten the security of nearly everyone on the internet, from billion-dollar companies and government agencies to hobbyist open-source developers. The ever-evolving threat of software supply chain attacks has garnered interest from both the software indus...",
      "pdf_url": "https://arxiv.org/pdf/2510.24920v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.24920v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.14778v2",
      "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks",
      "authors": [
        "Maor Reuben",
        "Ido Mendel",
        "Or Feldman",
        "Moshe Kravchik",
        "Mordehai Guri",
        "Rami Puzis"
      ],
      "published": "2025-10-16T15:14:04Z",
      "categories": "",
      "summary": "Supply chain attacks significantly threaten software security with malicious code injections within legitimate projects. Such attacks are very rare but may have a devastating impact. Detecting spurious code injections using automated tools is further complicated as it often requires deciphering the intention of both the inserted code and its context. In this study, we propose an unsupervised approach for highlighting spurious code injections by quantifying cohesion disruptions in the source code. Using a name-prediction-based cohesion (NPC) metric, we analyze how function cohesion changes when...",
      "pdf_url": "https://arxiv.org/pdf/2510.14778v2.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.14778v2_paper.pdf"
    },
    {
      "arxiv_id": "1712.05526v1",
      "title": "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning",
      "authors": [
        "Xinyun Chen",
        "Chang Liu",
        "Bo Li",
        "Kimberly Lu",
        "Dawn Song"
      ],
      "published": "2017-12-15T04:26:26Z",
      "categories": "",
      "summary": "Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based a...",
      "pdf_url": "https://arxiv.org/pdf/1712.05526v1.pdf",
      "relevance_score": 100,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/1712.05526v1_paper.pdf"
    },
    {
      "arxiv_id": "2504.20984v3",
      "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
      "authors": [
        "Evan Li",
        "Tushin Mallick",
        "Evan Rose",
        "William Robertson",
        "Alina Oprea",
        "Cristina Nita-Rotaru"
      ],
      "published": "2025-04-29T17:55:52Z",
      "categories": "",
      "summary": "LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.   In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solut...",
      "pdf_url": "https://arxiv.org/pdf/2504.20984v3.pdf",
      "relevance_score": 98,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2504.20984v3_paper.pdf"
    },
    {
      "arxiv_id": "2509.05755v5",
      "title": "Red-Teaming Coding Agents from a Tool-Invocation Perspective: An Empirical Security Assessment",
      "authors": [
        "Yuchong Xie",
        "Mingyu Luo",
        "Zesen Liu",
        "Zhixiang Zhang",
        "Kaikai Zhang",
        "Yu Liu",
        "Zongjie Li",
        "Ping Chen",
        "Shuai Wang",
        "Dongdong She"
      ],
      "published": "2025-09-06T15:48:49Z",
      "categories": "",
      "summary": "Coding agents powered by large language models are becoming central modules of modern IDEs, helping users perform complex tasks by invoking tools. While powerful, tool invocation opens a substantial attack surface. Prior work has demonstrated attacks against general-purpose and domain-specific agents, but none have focused on the security risks of tool invocation in coding agents. To fill this gap, we conduct the first systematic red-teaming of six popular real-world coding agents: Cursor, Claude Code, Copilot, Windsurf, Cline, and Trae. Our red-teaming proceeds in two phases. In Phase 1, we p...",
      "pdf_url": "https://arxiv.org/pdf/2509.05755v5.pdf",
      "relevance_score": 98,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.05755v5_paper.pdf"
    },
    {
      "arxiv_id": "2512.15081v1",
      "title": "Quantifying Return on Security Controls in LLM Systems",
      "authors": [
        "Richard Helder Moulton",
        "Austin O'Brien",
        "John D. Hastings"
      ],
      "published": "2025-12-17T04:58:09Z",
      "categories": "",
      "summary": "Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic perso...",
      "pdf_url": "https://arxiv.org/pdf/2512.15081v1.pdf",
      "relevance_score": 98,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.15081v1_paper.pdf"
    },
    {
      "arxiv_id": "2503.07568v1",
      "title": "Runtime Detection of Adversarial Attacks in AI Accelerators Using Performance Counters",
      "authors": [
        "Habibur Rahaman",
        "Atri Chatterjee",
        "Swarup Bhunia"
      ],
      "published": "2025-03-10T17:38:42Z",
      "categories": "",
      "summary": "Rapid adoption of AI technologies raises several major security concerns, including the risks of adversarial perturbations, which threaten the confidentiality and integrity of AI applications. Protecting AI hardware from misuse and diverse security threats is a challenging task. To address this challenge, we propose SAMURAI, a novel framework for safeguarding against malicious usage of AI hardware and its resilience to attacks. SAMURAI introduces an AI Performance Counter (APC) for tracking dynamic behavior of an AI model coupled with an on-chip Machine Learning (ML) analysis engine, known as ...",
      "pdf_url": "https://arxiv.org/pdf/2503.07568v1.pdf",
      "relevance_score": 97,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2503.07568v1_paper.pdf"
    },
    {
      "arxiv_id": "2308.10294v2",
      "title": "A review of SolarWinds attack on Orion platform using persistent threat agents and techniques for gaining unauthorized access",
      "authors": [
        "Antigoni Kruti",
        "Usman Butt",
        "Rejwan Bin Sulaiman"
      ],
      "published": "2023-08-20T15:32:28Z",
      "categories": "",
      "summary": "This paper of work examines the SolarWinds attack, designed on Orion Platform security incident. It analyses the persistent threats agents and potential technical attack techniques to gain unauthorized access. In 2020 SolarWinds attack indicates an initial breach disclosure on Orion Platform software by malware distribution on IT and government organizations such as Homeland Security, Microsoft and Intel associated with supply chains leaks consequences from small loopholes in security systems. Hackers increased the number of infected company and businesses networks during the supply-chain atta...",
      "pdf_url": "https://arxiv.org/pdf/2308.10294v2.pdf",
      "relevance_score": 97,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2308.10294v2_paper.pdf"
    },
    {
      "arxiv_id": "2410.13899v1",
      "title": "Security of and by Generative AI platforms",
      "authors": [
        "Hari Hayagreevan",
        "Souvik Khamaru"
      ],
      "published": "2024-10-15T15:27:05Z",
      "categories": "",
      "summary": "This whitepaper highlights the dual importance of securing generative AI (genAI) platforms and leveraging genAI for cybersecurity. As genAI technologies proliferate, their misuse poses significant risks, including data breaches, model tampering, and malicious content generation. Securing these platforms is critical to protect sensitive data, ensure model integrity, and prevent adversarial attacks. Simultaneously, genAI presents opportunities for enhancing security by automating threat detection, vulnerability analysis, and incident response. The whitepaper explores strategies for robust securi...",
      "pdf_url": "https://arxiv.org/pdf/2410.13899v1.pdf",
      "relevance_score": 95,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2410.13899v1_paper.pdf"
    },
    {
      "arxiv_id": "2511.15759v1",
      "title": "Securing AI Agents Against Prompt Injection Attacks",
      "authors": [
        "Badrinath Ramakrishnan",
        "Akshaya Balaji"
      ],
      "published": "2025-11-19T10:00:54Z",
      "categories": "",
      "summary": "Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test cases across five attack categories: direct injection, context manipulation, instruction override, data exfiltration, and cross-context contamination. We evaluate three defense mechanisms: content filt...",
      "pdf_url": "https://arxiv.org/pdf/2511.15759v1.pdf",
      "relevance_score": 93,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.15759v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.05709v1",
      "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
      "authors": [
        "Mary Llewellyn",
        "Annie Gray",
        "Josh Collyer",
        "Michael Harries"
      ],
      "published": "2025-10-07T09:22:22Z",
      "categories": "",
      "summary": "Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practition...",
      "pdf_url": "https://arxiv.org/pdf/2510.05709v1.pdf",
      "relevance_score": 93,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.05709v1_paper.pdf"
    },
    {
      "arxiv_id": "2305.19593v1",
      "title": "Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis",
      "authors": [
        "Mst Shapna Akter",
        "Hossain Shahriar",
        "Iysa Iqbal",
        "MD Hossain",
        "M. A. Karim",
        "Victor Clincy",
        "Razvan Voicu"
      ],
      "published": "2023-05-31T06:31:42Z",
      "categories": "",
      "summary": "The burgeoning fields of machine learning (ML) and quantum machine learning (QML) have shown remarkable potential in tackling complex problems across various domains. However, their susceptibility to adversarial attacks raises concerns when deploying these systems in security sensitive applications. In this study, we present a comparative analysis of the vulnerability of ML and QML models, specifically conventional neural networks (NN) and quantum neural networks (QNN), to adversarial attacks using a malware dataset. We utilize a software supply chain attack dataset known as ClaMP and develop ...",
      "pdf_url": "https://arxiv.org/pdf/2305.19593v1.pdf",
      "relevance_score": 93,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2305.19593v1_paper.pdf"
    },
    {
      "arxiv_id": "2511.19727v1",
      "title": "Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts",
      "authors": [
        "Steven Peh"
      ],
      "published": "2025-11-24T21:44:33Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we...",
      "pdf_url": "https://arxiv.org/pdf/2511.19727v1.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2511.19727v1_paper.pdf"
    },
    {
      "arxiv_id": "2410.02644v4",
      "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents",
      "authors": [
        "Hanrong Zhang",
        "Jingyuan Huang",
        "Kai Mei",
        "Yifei Yao",
        "Zhenting Wang",
        "Chenlu Zhan",
        "Hongwei Wang",
        "Yongfeng Zhang"
      ],
      "published": "2024-10-03T16:30:47Z",
      "categories": "",
      "summary": "Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenar...",
      "pdf_url": "https://arxiv.org/pdf/2410.02644v4.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2410.02644v4_paper.pdf"
    },
    {
      "arxiv_id": "2508.10991v4",
      "title": "MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI",
      "authors": [
        "Wenpeng Xing",
        "Zhonghao Qi",
        "Yupeng Qin",
        "Yilin Li",
        "Caini Chang",
        "Jiahui Yu",
        "Changting Lin",
        "Zhenzhen Xie",
        "Meng Han"
      ],
      "published": "2025-08-14T18:00:25Z",
      "categories": "",
      "summary": "While Large Language Models (LLMs) have achieved remarkable performance, they remain vulnerable to jailbreak. The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-GUARD, a robust, layered defense architecture designed for LLM-tool interactions. MCP-GUARD employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static ...",
      "pdf_url": "https://arxiv.org/pdf/2508.10991v4.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2508.10991v4_paper.pdf"
    },
    {
      "arxiv_id": "2503.23250v1",
      "title": "Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions",
      "authors": [
        "Shih-Han Chan"
      ],
      "published": "2025-03-29T23:26:57Z",
      "categories": "",
      "summary": "Security threats like prompt injection attacks pose significant risks to applications that integrate Large Language Models (LLMs), potentially leading to unauthorized actions such as API misuse. Unlike previous approaches that aim to detect these attacks on a best-effort basis, this paper introduces a novel method that appends an Encrypted Prompt to each user prompt, embedding current permissions. These permissions are verified before executing any actions (such as API calls) generated by the LLM. If the permissions are insufficient, the LLM's actions will not be executed, ensuring safety. Thi...",
      "pdf_url": "https://arxiv.org/pdf/2503.23250v1.pdf",
      "relevance_score": 90,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2503.23250v1_paper.pdf"
    },
    {
      "arxiv_id": "2410.05451v3",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "David Wagner",
        "Chuan Guo"
      ],
      "published": "2024-10-07T19:34:35Z",
      "categories": "",
      "summary": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vul...",
      "pdf_url": "https://arxiv.org/pdf/2410.05451v3.pdf",
      "relevance_score": 89,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2410.05451v3_paper.pdf"
    },
    {
      "arxiv_id": "2409.08087v3",
      "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
      "authors": [
        "Benji Peng",
        "Keyu Chen",
        "Ming Li",
        "Pohsun Feng",
        "Ziqian Bi",
        "Junyu Liu",
        "Xinyuan Song",
        "Qian Niu"
      ],
      "published": "2024-09-12T14:42:08Z",
      "categories": "",
      "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled inpu...",
      "pdf_url": "https://arxiv.org/pdf/2409.08087v3.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2409.08087v3_paper.pdf"
    },
    {
      "arxiv_id": "2512.14860v1",
      "title": "Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks",
      "authors": [
        "Viet K. Nguyen",
        "Mohammad I. Husain"
      ],
      "published": "2025-12-16T19:22:50Z",
      "categories": "",
      "summary": "Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent archi...",
      "pdf_url": "https://arxiv.org/pdf/2512.14860v1.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.14860v1_paper.pdf"
    },
    {
      "arxiv_id": "2502.11127v1",
      "title": "G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems",
      "authors": [
        "Shilong Wang",
        "Guibin Zhang",
        "Miao Yu",
        "Guancheng Wan",
        "Fanci Meng",
        "Chongye Guo",
        "Kun Wang",
        "Yang Wang"
      ],
      "published": "2025-02-16T13:48:41Z",
      "categories": "",
      "summary": "Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborative problem-solving to autonomous decision-making. However, as these systems become increasingly integrated into critical applications, their vulnerability to adversarial attacks, misinformation propagation, and unintended behaviors have raised significant concerns. To address this challenge, we introduce G-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS, which leverages graph neural networks to detect anomalies on the...",
      "pdf_url": "https://arxiv.org/pdf/2502.11127v1.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2502.11127v1_paper.pdf"
    },
    {
      "arxiv_id": "2510.15994v1",
      "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents",
      "authors": [
        "Dongsen Zhang",
        "Zekun Li",
        "Xu Luo",
        "Xuannan Liu",
        "Peipei Li",
        "Wenjun Xu"
      ],
      "published": "2025-10-14T07:36:25Z",
      "categories": "",
      "summary": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 att...",
      "pdf_url": "https://arxiv.org/pdf/2510.15994v1.pdf",
      "relevance_score": 88,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2510.15994v1_paper.pdf"
    },
    {
      "arxiv_id": "2402.11082v1",
      "title": "The AI Security Pyramid of Pain",
      "authors": [
        "Chris M. Ward",
        "Josh Harguess",
        "Julia Tao",
        "Daniel Christman",
        "Paul Spicer",
        "Mike Tan"
      ],
      "published": "2024-02-16T21:14:11Z",
      "categories": "",
      "summary": "We introduce the AI Security Pyramid of Pain, a framework that adapts the cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats. This framework provides a structured approach to understanding and addressing various levels of AI threats. Starting at the base, the pyramid emphasizes Data Integrity, which is essential for the accuracy and reliability of datasets and AI models, including their weights and parameters. Ensuring data integrity is crucial, as it underpins the effectiveness of all AI-driven decisions and operations. The next level, AI System Performance, focuse...",
      "pdf_url": "https://arxiv.org/pdf/2402.11082v1.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2402.11082v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.12921v1",
      "title": "Cisco Integrated AI Security and Safety Framework Report",
      "authors": [
        "Amy Chang",
        "Tiffany Saade",
        "Sanket Mendapara",
        "Adam Swanda",
        "Ankit Garg"
      ],
      "published": "2025-12-15T02:12:12Z",
      "categories": "",
      "summary": "Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi...",
      "pdf_url": "https://arxiv.org/pdf/2512.12921v1.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.12921v1_paper.pdf"
    },
    {
      "arxiv_id": "2409.07415v2",
      "title": "SoK: Security and Privacy Risks of Healthcare AI",
      "authors": [
        "Yuanhaur Chang",
        "Han Liu",
        "Chenyang Lu",
        "Ning Zhang"
      ],
      "published": "2024-09-11T16:59:58Z",
      "categories": "",
      "summary": "The integration of artificial intelligence (AI) and machine learning (ML) into healthcare systems holds great promise for enhancing patient care and care delivery efficiency; however, it also exposes sensitive data and system integrity to potential cyberattacks. Current security and privacy (S&P) research on healthcare AI is highly unbalanced in terms of healthcare deployment scenarios and threat models, and has a disconnected focus with the biomedical research community. This hinders a comprehensive understanding of the risks that healthcare AI entails. To address this gap, this paper takes a...",
      "pdf_url": "https://arxiv.org/pdf/2409.07415v2.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2409.07415v2_paper.pdf"
    },
    {
      "arxiv_id": "2509.18461v1",
      "title": "Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?",
      "authors": [
        "Ayan Sar",
        "Sampurna Roy",
        "Tanupriya Choudhury",
        "Ajith Abraham"
      ],
      "published": "2025-09-22T22:33:16Z",
      "categories": "",
      "summary": "Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and meta-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested AI-driven prevention...",
      "pdf_url": "https://arxiv.org/pdf/2509.18461v1.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.18461v1_paper.pdf"
    },
    {
      "arxiv_id": "2308.16684v2",
      "title": "Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack",
      "authors": [
        "Sze Jue Yang",
        "Quang Nguyen",
        "Chee Seng Chan",
        "Khoa D. Doan"
      ],
      "published": "2023-08-31T12:38:29Z",
      "categories": "",
      "summary": "The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-us...",
      "pdf_url": "https://arxiv.org/pdf/2308.16684v2.pdf",
      "relevance_score": 87,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2308.16684v2_paper.pdf"
    },
    {
      "arxiv_id": "2503.09334v3",
      "title": "CyberLLMInstruct: A Pseudo-malicious Dataset Revealing Safety-performance Trade-offs in Cyber Security LLM Fine-tuning",
      "authors": [
        "Adel ElZemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "published": "2025-03-12T12:29:27Z",
      "categories": "",
      "summary": "The integration of large language models (LLMs) into cyber security applications presents both opportunities and critical safety risks. We introduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious instruction-response pairs spanning cyber security tasks including malware analysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive evaluation using seven open-source LLMs reveals a critical trade-off: while fine-tuning improves cyber security task performance (achieving up to 92.50% accuracy on CyberMetric), it severely compromises safety resilience across all tested mo...",
      "pdf_url": "https://arxiv.org/pdf/2503.09334v3.pdf",
      "relevance_score": 86,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2503.09334v3_paper.pdf"
    },
    {
      "arxiv_id": "2505.09974v2",
      "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data",
      "authors": [
        "Adel ElZemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "published": "2025-05-15T05:22:53Z",
      "categories": "",
      "summary": "Large language models (LLMs) have been used in many application domains, including cyber security. The application of LLMs in the cyber security domain presents significant opportunities, such as for enhancing threat analysis and malware detection, but it can also introduce critical risks and safety concerns, including potential personal data leakage and automated generation of new malware. Building on recent findings that fine-tuning LLMs with pseudo-malicious cyber security data significantly compromises their safety, this paper presents a comprehensive validation and extension of these safe...",
      "pdf_url": "https://arxiv.org/pdf/2505.09974v2.pdf",
      "relevance_score": 86,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2505.09974v2_paper.pdf"
    },
    {
      "arxiv_id": "2505.21609v1",
      "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study",
      "authors": [
        "Mathew J. Walter",
        "Aaron Barrett",
        "Kimberly Tam"
      ],
      "published": "2025-05-27T17:59:05Z",
      "categories": "",
      "summary": "Adversarial artificial intelligence (AI) attacks pose a significant threat to autonomous transportation, such as maritime vessels, that rely on AI components. Malicious actors can exploit these systems to deceive and manipulate AI-driven operations. This paper addresses three critical research challenges associated with adversarial AI: the limited scope of traditional defences, inadequate security metrics, and the need to build resilience beyond model-level defences. To address these challenges, we propose building defences utilising multiple inputs and data fusion to create defensive componen...",
      "pdf_url": "https://arxiv.org/pdf/2505.21609v1.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2505.21609v1_paper.pdf"
    },
    {
      "arxiv_id": "2310.11594v3",
      "title": "Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning",
      "authors": [
        "Taejin Kim",
        "Jiarui Li",
        "Shubhranshu Singh",
        "Nikhil Madaan",
        "Carlee Joe-Wong"
      ],
      "published": "2023-10-17T21:38:41Z",
      "categories": "",
      "summary": "The delicate equilibrium between user privacy and the ability to unleash the potential of distributed data is an important concern. Federated learning, which enables the training of collaborative models without sharing of data, has emerged as a privacy-centric solution. This approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data into the training process, as well as evasion attacks that aim to induce misclassifications at test time. Our research investigates the intersection of adversarial training, a common defense meth...",
      "pdf_url": "https://arxiv.org/pdf/2310.11594v3.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2310.11594v3_paper.pdf"
    },
    {
      "arxiv_id": "2207.00091v1",
      "title": "Threat Assessment in Machine Learning based Systems",
      "authors": [
        "Lionel Nganyewou Tidjon",
        "Foutse Khomh"
      ],
      "published": "2022-06-30T20:19:50Z",
      "categories": "",
      "summary": "Machine learning is a field of artificial intelligence (AI) that is becoming essential for several critical systems, making it a good target for threat actors. Threat actors exploit different Tactics, Techniques, and Procedures (TTPs) against the confidentiality, integrity, and availability of Machine Learning (ML) systems. During the ML cycle, they exploit adversarial TTPs to poison data and fool ML-based systems. In recent years, multiple security practices have been proposed for traditional systems but they are not enough to cope with the nature of ML-based systems. In this paper, we conduc...",
      "pdf_url": "https://arxiv.org/pdf/2207.00091v1.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2207.00091v1_paper.pdf"
    },
    {
      "arxiv_id": "2302.09904v4",
      "title": "WW-FL: Secure and Private Large-Scale Federated Learning",
      "authors": [
        "Felix Marx",
        "Thomas Schneider",
        "Ajith Suresh",
        "Tobias Wehrle",
        "Christian Weinert",
        "Hossein Yalame"
      ],
      "published": "2023-02-20T11:02:55Z",
      "categories": "",
      "summary": "Federated learning (FL) is an efficient approach for large-scale distributed machine learning that promises data privacy by keeping training data on client devices. However, recent research has uncovered vulnerabilities in FL, impacting both security and privacy through poisoning attacks and the potential disclosure of sensitive information in individual model updates as well as the aggregated global model. This paper explores the inadequacies of existing FL protection measures when applied independently, and the challenges of creating effective compositions.   Addressing these issues, we prop...",
      "pdf_url": "https://arxiv.org/pdf/2302.09904v4.pdf",
      "relevance_score": 85,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2302.09904v4_paper.pdf"
    },
    {
      "arxiv_id": "2409.05014v2",
      "title": "Analyzing Challenges in Deployment of the SLSA Framework for Software Supply Chain Security",
      "authors": [
        "Mahzabin Tamanna",
        "Sivana Hamer",
        "Mindy Tran",
        "Sascha Fahl",
        "Yasemin Acar",
        "Laurie Williams"
      ],
      "published": "2024-09-08T07:54:16Z",
      "categories": "",
      "summary": "In 2023, Sonatype reported a 200\\% increase in software supply chain attacks, including major build infrastructure attacks. To secure the software supply chain, practitioners can follow security framework guidance like the Supply-chain Levels for Software Artifacts (SLSA). However, recent surveys and industry summits have shown that despite growing interest, the adoption of SLSA is not widespread. To understand adoption challenges, \\textit{the goal of this study is to aid framework authors and practitioners in improving the adoption and development of Supply-Chain Levels for Software Artifacts...",
      "pdf_url": "https://arxiv.org/pdf/2409.05014v2.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2409.05014v2_paper.pdf"
    },
    {
      "arxiv_id": "2509.08083v1",
      "title": "Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations",
      "authors": [
        "Laurie Williams",
        "Sammy Migues"
      ],
      "published": "2025-09-09T18:39:03Z",
      "categories": "",
      "summary": "Software supply chain attacks have increased exponentially since 2020. The primary attack vectors for supply chain attacks are through: (1) software components; (2) the build infrastructure; and (3) humans (a.k.a software practitioners). Software supply chain risk management frameworks provide a list of tasks that an organization can adopt to reduce software supply chain risk. Exhaustively adopting all the tasks of these frameworks is infeasible, necessitating the prioritized adoption of tasks. Software organizations can benefit from being guided in this prioritization by learning what tasks o...",
      "pdf_url": "https://arxiv.org/pdf/2509.08083v1.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.08083v1_paper.pdf"
    },
    {
      "arxiv_id": "2301.08170v1",
      "title": "On the Vulnerability of Backdoor Defenses for Federated Learning",
      "authors": [
        "Pei Fang",
        "Jinghui Chen"
      ],
      "published": "2023-01-19T17:02:02Z",
      "categories": "",
      "summary": "Federated Learning (FL) is a popular distributed machine learning paradigm that enables jointly training a global model without sharing clients' data. However, its repetitive server-client communication gives room for backdoor attacks with aim to mislead the global model into a targeted misprediction when a specific trigger pattern is presented. In response to such backdoor threats on federated learning, various defense measures have been proposed. In this paper, we study whether the current defense mechanisms truly neutralize the backdoor threats from federated learning in a practical setting...",
      "pdf_url": "https://arxiv.org/pdf/2301.08170v1.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2301.08170v1_paper.pdf"
    },
    {
      "arxiv_id": "2512.23385v1",
      "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
      "authors": [
        "The Anh Nguyen",
        "Triet Huynh Minh Le",
        "M. Ali Babar"
      ],
      "published": "2025-12-29T11:22:11Z",
      "categories": "",
      "summary": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, ...",
      "pdf_url": "https://arxiv.org/pdf/2512.23385v1.pdf",
      "relevance_score": 83,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2512.23385v1_paper.pdf"
    },
    {
      "arxiv_id": "2007.08745v5",
      "title": "Backdoor Learning: A Survey",
      "authors": [
        "Yiming Li",
        "Yong Jiang",
        "Zhifeng Li",
        "Shu-Tao Xia"
      ],
      "published": "2020-07-17T04:09:20Z",
      "categories": "",
      "summary": "Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), so that the attacked models perform well on benign samples, whereas their predictions will be maliciously changed if the hidden backdoor is activated by attacker-specified triggers. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, its systematic review, however, remains blank. In this paper, we pres...",
      "pdf_url": "https://arxiv.org/pdf/2007.08745v5.pdf",
      "relevance_score": 82,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2007.08745v5_paper.pdf"
    },
    {
      "arxiv_id": "2408.07291v3",
      "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
      "authors": [
        "Yupei Liu",
        "Yuqi Jia",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "published": "2024-08-14T04:49:30Z",
      "categories": "",
      "summary": "Automatically extracting personal information -- such as name, phone number, and email address -- from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods -- such as regular expression, keyword search, and entity detection -- achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction at...",
      "pdf_url": "https://arxiv.org/pdf/2408.07291v3.pdf",
      "relevance_score": 81,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2408.07291v3_paper.pdf"
    },
    {
      "arxiv_id": "2509.21367v1",
      "title": "Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan",
      "authors": [
        "Yu-Kai Shih",
        "You-Kai Kang"
      ],
      "published": "2025-09-22T11:40:29Z",
      "categories": "",
      "summary": "As smart tourism evolves, AI-powered chatbots have become indispensable for delivering personalized, real-time assistance to travelers while promoting sustainability and efficiency. However, these systems are increasingly vulnerable to prompt injection attacks, where adversaries manipulate inputs to elicit unintended behaviors such as leaking sensitive information or generating harmful content. This paper presents a case study on the design and implementation of a secure retrieval-augmented generation (RAG) chatbot for Hsinchu smart tourism services. The system integrates RAG with API function...",
      "pdf_url": "https://arxiv.org/pdf/2509.21367v1.pdf",
      "relevance_score": 81,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2509.21367v1_paper.pdf"
    },
    {
      "arxiv_id": "2601.04553v1",
      "title": "Deep Dive into the Abuse of DL APIs To Create Malicious AI Models and How to Detect Them",
      "authors": [
        "Mohamed Nabeel",
        "Oleksii Starov"
      ],
      "published": "2026-01-08T03:30:20Z",
      "categories": "",
      "summary": "According to Gartner, more than 70% of organizations will have integrated AI models into their workflows by the end of 2025. In order to reduce cost and foster innovation, it is often the case that pre-trained models are fetched from model hubs like Hugging Face or TensorFlow Hub. However, this introduces a security risk where attackers can inject malicious code into the models they upload to these hubs, leading to various kinds of attacks including remote code execution (RCE), sensitive data exfiltration, and system file modification when these models are loaded or executed (predict function)...",
      "pdf_url": "https://arxiv.org/pdf/2601.04553v1.pdf",
      "relevance_score": 81,
      "dimension": "i_security_threats",
      "pdf_downloaded": true,
      "pdf_local_path": "cluster_i_threats/pdfs/2601.04553v1_paper.pdf"
    }
  ],
  "dimension": "i_security_threats",
  "downloaded_papers": 56
}