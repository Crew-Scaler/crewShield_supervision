{
  "arxiv_id": "2512.10687v1",
  "title": "Challenges of Evaluating LLM Safety for User Welfare",
  "authors": [
    "Manon Kempermann",
    "Sai Suresh Macharla Vasu",
    "Mahalakshmi Raveenthiran",
    "Theo Farrell",
    "Ingmar Weber"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.10687v1",
  "relevance_score": 83.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "vulnerability management"
  ],
  "summary": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-triv..."
}