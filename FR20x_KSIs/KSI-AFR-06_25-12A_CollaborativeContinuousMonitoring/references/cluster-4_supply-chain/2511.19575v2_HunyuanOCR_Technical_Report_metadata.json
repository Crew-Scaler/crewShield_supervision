{
  "arxiv_id": "2511.19575v2",
  "title": "HunyuanOCR Technical Report",
  "authors": [
    " Hunyuan Vision Team",
    "Pengyuan Lyu",
    "Xingyu Wan",
    "Gengluo Li",
    "Shangpin Peng"
  ],
  "affiliation": "See author list",
  "published_date": "2025-11",
  "url": "https://arxiv.org/abs/2511.19575v2",
  "relevance_score": 90.0,
  "estimated_pages": 12,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) an..."
}