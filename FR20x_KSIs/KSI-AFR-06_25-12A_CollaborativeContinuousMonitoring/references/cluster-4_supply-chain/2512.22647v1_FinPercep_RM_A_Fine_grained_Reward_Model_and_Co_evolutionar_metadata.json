{
  "arxiv_id": "2512.22647v1",
  "title": "FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution",
  "authors": [
    "Yidi Liu",
    "Zihao Fan",
    "Jie Huang",
    "Jie Xiao",
    "Dong Li"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.22647v1",
  "relevance_score": 80.0,
  "estimated_pages": 12,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity a..."
}