{
  "arxiv_id": "2512.20732v1",
  "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
  "authors": [
    "Saeed Mohammadzadeh",
    "Erfan Hamdi",
    "Joel Shor",
    "Emma Lejeune"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20732v1",
  "relevance_score": 81.2,
  "estimated_pages": 12,
  "summary": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provide"
}