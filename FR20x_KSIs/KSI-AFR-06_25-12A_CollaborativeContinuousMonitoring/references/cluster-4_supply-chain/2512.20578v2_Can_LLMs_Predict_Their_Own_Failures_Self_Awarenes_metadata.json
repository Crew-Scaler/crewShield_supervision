{
  "arxiv_id": "2512.20578v2",
  "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
  "authors": [
    "Amirhosein Ghasemabadi",
    "Di Niu"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20578v2",
  "relevance_score": 81.2,
  "estimated_pages": 10,
  "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inf"
}