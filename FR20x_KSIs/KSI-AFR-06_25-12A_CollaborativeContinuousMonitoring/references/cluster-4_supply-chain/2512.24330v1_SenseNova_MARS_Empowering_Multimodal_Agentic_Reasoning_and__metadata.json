{
  "arxiv_id": "2512.24330v1",
  "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
  "authors": [
    "Yong Xien Chng",
    "Tao Hu",
    "Wenwen Tong",
    "Xueheng Li",
    "Jiandong Chen"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.24330v1",
  "relevance_score": 80.0,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "model monitoring",
    "autonomous systems"
  ],
  "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce S..."
}