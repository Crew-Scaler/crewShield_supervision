{
  "arxiv_id": "2512.21288v1",
  "title": "Model Merging via Multi-Teacher Knowledge Distillation",
  "authors": [
    "Seyed Arshan Dalili",
    "Mehrdad Mahdavi"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.21288v1",
  "relevance_score": 82.4,
  "estimated_pages": 12,
  "summary": "Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributio"
}