{
  "arxiv_id": "2512.21999v1",
  "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
  "authors": [
    "Jiayu Hu",
    "Beibei Li",
    "Jiangwei Xia",
    "Yanjun Qin",
    "Bing Ji"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.21999v1",
  "relevance_score": 90.0,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness"
  ],
  "summary": "While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inheren..."
}