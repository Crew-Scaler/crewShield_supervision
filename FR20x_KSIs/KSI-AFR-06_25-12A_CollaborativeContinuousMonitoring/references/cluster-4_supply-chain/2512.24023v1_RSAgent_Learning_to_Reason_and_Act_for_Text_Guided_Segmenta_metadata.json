{
  "arxiv_id": "2512.24023v1",
  "title": "RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations",
  "authors": [
    "Xingqi He",
    "Yujie Zhang",
    "Shuyong Gao",
    "Wenjie Li",
    "Lingyi Hong"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.24023v1",
  "relevance_score": 90.0,
  "estimated_pages": 10,
  "key_topics": [
    "model monitoring",
    "autonomous systems"
  ],
  "summary": "Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for s..."
}