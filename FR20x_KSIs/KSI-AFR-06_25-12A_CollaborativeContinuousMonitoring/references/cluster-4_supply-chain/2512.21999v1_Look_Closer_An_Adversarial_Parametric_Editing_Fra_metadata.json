{
  "arxiv_id": "2512.21999v1",
  "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
  "authors": [
    "Jiayu Hu",
    "Beibei Li",
    "Jiangwei Xia",
    "Yanjun Qin",
    "Bing Ji"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.21999v1",
  "relevance_score": 91.2,
  "estimated_pages": 12,
  "summary": "While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding ca"
}