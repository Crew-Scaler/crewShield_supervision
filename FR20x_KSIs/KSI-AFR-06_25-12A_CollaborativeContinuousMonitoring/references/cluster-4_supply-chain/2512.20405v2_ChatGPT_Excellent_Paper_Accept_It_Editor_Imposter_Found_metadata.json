{
  "arxiv_id": "2512.20405v2",
  "title": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected",
  "authors": [
    "Kanchon Gharami",
    "Sanjiv Kumar Sarkar",
    "Yongxin Liu",
    "Shafika Showkat Moni"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20405v2",
  "relevance_score": 83.0,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "vulnerability management"
  ],
  "summary": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical ..."
}