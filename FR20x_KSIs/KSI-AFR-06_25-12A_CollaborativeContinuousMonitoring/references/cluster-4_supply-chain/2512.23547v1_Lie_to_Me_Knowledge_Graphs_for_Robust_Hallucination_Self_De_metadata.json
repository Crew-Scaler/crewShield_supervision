{
  "arxiv_id": "2512.23547v1",
  "title": "Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs",
  "authors": [
    "Sahil Kale",
    "Antonio Luca Alfeo"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23547v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations,..."
}