{
  "arxiv_id": "2512.13655v2",
  "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
  "authors": [
    "Richard J. Young"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.13655v2",
  "relevance_score": 82.4,
  "estimated_pages": 10,
  "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative eff"
}