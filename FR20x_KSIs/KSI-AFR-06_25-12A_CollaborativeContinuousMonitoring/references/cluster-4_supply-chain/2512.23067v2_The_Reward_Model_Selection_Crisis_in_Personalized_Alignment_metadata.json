{
  "arxiv_id": "2512.23067v2",
  "title": "The Reward Model Selection Crisis in Personalized Alignment",
  "authors": [
    "Fady Rezk",
    "Yuangang Pan",
    "Chuan-Sheng Foo",
    "Xun Xu",
    "Nancy Chen"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23067v2",
  "relevance_score": 80.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "model monitoring"
  ],
  "summary": "Personalized alignment from preference data has focused primarily on improving personal reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation such as reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but al..."
}