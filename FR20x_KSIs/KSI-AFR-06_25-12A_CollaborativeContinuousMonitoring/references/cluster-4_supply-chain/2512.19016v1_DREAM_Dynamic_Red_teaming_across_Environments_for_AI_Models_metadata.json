{
  "arxiv_id": "2512.19016v1",
  "title": "DREAM: Dynamic Red-teaming across Environments for AI Models",
  "authors": [
    "Liming Lu",
    "Xiang Gu",
    "Junyu Huang",
    "Jiawei Du",
    "Yunhuai Liu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.19016v1",
  "relevance_score": 81.5,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "vulnerability management",
    "autonomous systems"
  ],
  "summary": "Large Language Models (LLMs) are increasingly used in agentic systems, where their interactions with diverse tools and environments create complex, multi-stage safety challenges. However, existing benchmarks mostly rely on static, single-turn assessments that miss vulnerabilities from adaptive, long-chain attacks. To fill this gap, we introduce DREAM, a framework for systematic evaluation of LLM agents against dynamic, multi-stage attacks. At its core, DREAM uses a Cross-Environment Adversarial ..."
}