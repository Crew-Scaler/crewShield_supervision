{
  "arxiv_id": "2512.23065v3",
  "title": "TabiBERT: A Large-Scale ModernBERT Foundation Model and A Unified Benchmark for Turkish",
  "authors": [
    "Melik\u015fah T\u00fcrker",
    "A. Ebrar K\u0131z\u0131lo\u011flu",
    "Onur G\u00fcng\u00f6r",
    "Susan \u00dcsk\u00fcdarl\u0131"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23065v3",
  "relevance_score": 81.2,
  "estimated_pages": 12,
  "summary": "Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch, incorporating such"
}