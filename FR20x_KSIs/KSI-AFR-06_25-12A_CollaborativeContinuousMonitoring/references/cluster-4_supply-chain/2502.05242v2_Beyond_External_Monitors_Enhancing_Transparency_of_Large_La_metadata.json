{
  "arxiv_id": "2502.05242v2",
  "title": "Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring",
  "authors": [
    "Guanxu Chen",
    "Dongrui Liu",
    "Tao Luo",
    "Lijie Hu",
    "Jing Shao"
  ],
  "affiliation": "See author list",
  "published_date": "2025-02",
  "url": "https://arxiv.org/abs/2502.05242v2",
  "relevance_score": 83.0,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring",
    "AI governance",
    "model monitoring"
  ],
  "summary": "Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easie..."
}