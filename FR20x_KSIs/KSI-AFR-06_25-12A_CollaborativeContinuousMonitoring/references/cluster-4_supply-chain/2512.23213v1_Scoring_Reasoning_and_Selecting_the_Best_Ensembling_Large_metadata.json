{
  "arxiv_id": "2512.23213v1",
  "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process",
  "authors": [
    "Zhijun Chen",
    "Zeyu Ji",
    "Qianren Mao",
    "Junhang Cheng",
    "Bangjie Qin"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23213v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [],
  "summary": "We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerg..."
}