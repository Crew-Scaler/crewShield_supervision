{
  "arxiv_id": "2512.24562v1",
  "title": "HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering",
  "authors": [
    "Chaodong Tong",
    "Qi Zhang",
    "Jiayang Gao",
    "Lei Jiang",
    "Yanbing Liu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.24562v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods often aim to accurately capture a single type of uncertainty while overlooking the complementarity among different sources, particularly between token-level probability uncertainty and the uncertainty..."
}