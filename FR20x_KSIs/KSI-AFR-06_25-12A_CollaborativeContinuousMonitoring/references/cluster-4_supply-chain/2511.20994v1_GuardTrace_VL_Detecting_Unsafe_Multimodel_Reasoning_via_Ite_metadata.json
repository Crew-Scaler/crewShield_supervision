{
  "arxiv_id": "2511.20994v1",
  "title": "GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision",
  "authors": [
    "Yuxiao Xiang",
    "Junchi Chen",
    "Zhenchao Jin",
    "Changtao Miao",
    "Haojie Yuan"
  ],
  "affiliation": "See author list",
  "published_date": "2025-11",
  "url": "https://arxiv.org/abs/2511.20994v1",
  "relevance_score": 83.0,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "AI governance"
  ],
  "summary": "Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use o..."
}