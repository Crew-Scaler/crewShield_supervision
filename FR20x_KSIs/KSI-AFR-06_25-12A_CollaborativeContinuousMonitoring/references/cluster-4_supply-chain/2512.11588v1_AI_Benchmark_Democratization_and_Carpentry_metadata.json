{
  "arxiv_id": "2512.11588v1",
  "title": "AI Benchmark Democratization and Carpentry",
  "authors": [
    "Gregor von Laszewski",
    "Wesley Brewer",
    "Jeyan Thiyagalingam",
    "Juri Papay",
    "Armstrong Foundjem"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.11588v1",
  "relevance_score": 93.0,
  "estimated_pages": 12,
  "key_topics": [
    "AI governance",
    "model monitoring"
  ],
  "summary": "Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.\n  Beyond traditional static benchmarks, continuous ..."
}