{
  "arxiv_id": "2512.22628v1",
  "title": "M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation",
  "authors": [
    "Fanglin Xu",
    "Wei Zhang",
    "Jian Yang",
    "Guo Chen",
    "Aishan Liu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.22628v1",
  "relevance_score": 90.0,
  "estimated_pages": 10,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in ..."
}