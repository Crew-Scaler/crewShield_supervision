{
  "arxiv_id": "2512.20578v2",
  "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
  "authors": [
    "Amirhosein Ghasemabadi",
    "Di Niu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20578v2",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring"
  ],
  "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perf..."
}