{
  "arxiv_id": "2512.20949v1",
  "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
  "authors": [
    "Shize Liang",
    "Hongzhi Wang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20949v1",
  "relevance_score": 91.5,
  "estimated_pages": 10,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods th..."
}