{
  "arxiv_id": "2502.05242v2",
  "title": "Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring",
  "authors": [
    "Guanxu Chen",
    "Dongrui Liu",
    "Tao Luo",
    "Lijie Hu",
    "Jing Shao"
  ],
  "published_date": "2025-02",
  "url": "https://arxiv.org/abs/2502.05242v2",
  "relevance_score": 83.6,
  "estimated_pages": 10,
  "summary": "Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. How"
}