{
  "arxiv_id": "2512.20661v1",
  "title": "From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers",
  "authors": [
    "Yawei Liu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20661v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an A..."
}