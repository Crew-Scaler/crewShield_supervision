{
  "arxiv_id": "2512.20661v1",
  "title": "From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers",
  "authors": [
    "Yawei Liu"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20661v1",
  "relevance_score": 81.2,
  "estimated_pages": 10,
  "summary": "Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-rel"
}