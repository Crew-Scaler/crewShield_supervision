{
  "arxiv_id": "2512.24867v2",
  "title": "Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements",
  "authors": [
    "Yiming Liang",
    "Yizhi Li",
    "Yantao Du",
    "Ge Zhang",
    "Jiayi Zhou"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.24867v2",
  "relevance_score": 93.6,
  "estimated_pages": 12,
  "summary": "Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annot"
}