{
  "arxiv_id": "2512.24618v2",
  "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
  "authors": [
    "Junru Lu",
    "Jiarui Qin",
    "Lingfeng Qiao",
    "Yinghui Li",
    "Xinyi Dai"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.24618v2",
  "relevance_score": 80.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "model monitoring",
    "autonomous systems"
  ],
  "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented voc..."
}