{
  "arxiv_id": "2512.24562v1",
  "title": "HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering",
  "authors": [
    "Chaodong Tong",
    "Qi Zhang",
    "Jiayang Gao",
    "Lei Jiang",
    "Yanbing Liu"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.24562v1",
  "relevance_score": 81.2,
  "estimated_pages": 10,
  "summary": "Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods often aim to accurately capture a single type of uncertainty while overlooking the complementarity a"
}