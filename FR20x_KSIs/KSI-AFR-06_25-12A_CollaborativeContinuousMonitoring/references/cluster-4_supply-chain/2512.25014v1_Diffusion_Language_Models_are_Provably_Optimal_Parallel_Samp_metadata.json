{
  "arxiv_id": "2512.25014v1",
  "title": "Diffusion Language Models are Provably Optimal Parallel Samplers",
  "authors": [
    "Haozhe Jiang",
    "Nika Haghtalab",
    "Lijie Chen"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.25014v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [],
  "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of ..."
}