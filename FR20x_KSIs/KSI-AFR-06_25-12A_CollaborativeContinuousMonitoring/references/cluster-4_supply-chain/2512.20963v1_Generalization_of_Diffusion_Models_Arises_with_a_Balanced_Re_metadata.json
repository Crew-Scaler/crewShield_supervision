{
  "arxiv_id": "2512.20963v1",
  "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
  "authors": [
    "Zekai Zhang",
    "Xiao Li",
    "Xiang Li",
    "Lianghe Shi",
    "Meng Wu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20963v1",
  "relevance_score": 81.5,
  "estimated_pages": 10,
  "key_topics": [],
  "summary": "Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"sp..."
}