{
  "arxiv_id": "2511.19569v1",
  "title": "An Invariant Latent Space Perspective on Language Model Inversion",
  "authors": [
    "Wentao Ye",
    "Jiaqi Hu",
    "Haobo Wang",
    "Xinpeng Ti",
    "Zhiqing Xiao"
  ],
  "published_date": "2025-11",
  "url": "https://arxiv.org/abs/2511.19569v1",
  "relevance_score": 92.4,
  "estimated_pages": 10,
  "summary": "Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings s"
}