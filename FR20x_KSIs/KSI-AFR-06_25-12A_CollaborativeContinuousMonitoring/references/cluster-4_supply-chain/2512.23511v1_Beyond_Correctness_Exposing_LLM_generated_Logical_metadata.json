{
  "arxiv_id": "2512.23511v1",
  "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving",
  "authors": [
    "Xinyi Zheng",
    "Ningke Li",
    "Xiaokun Luan",
    "Kailong Wang",
    "Ling Shi"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23511v1",
  "relevance_score": 82.4,
  "estimated_pages": 10,
  "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and ru"
}