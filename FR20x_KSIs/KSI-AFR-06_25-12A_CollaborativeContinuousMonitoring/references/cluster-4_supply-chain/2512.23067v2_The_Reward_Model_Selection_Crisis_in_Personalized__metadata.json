{
  "arxiv_id": "2512.23067v2",
  "title": "The Reward Model Selection Crisis in Personalized Alignment",
  "authors": [
    "Fady Rezk",
    "Yuangang Pan",
    "Chuan-Sheng Foo",
    "Xun Xu",
    "Nancy Chen"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23067v2",
  "relevance_score": 81.2,
  "estimated_pages": 12,
  "summary": "Personalized alignment from preference data has focused primarily on improving personal reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation such as reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a"
}