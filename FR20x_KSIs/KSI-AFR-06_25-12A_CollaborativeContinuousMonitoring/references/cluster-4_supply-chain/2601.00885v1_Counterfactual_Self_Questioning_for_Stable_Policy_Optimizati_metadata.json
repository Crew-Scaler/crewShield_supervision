{
  "arxiv_id": "2601.00885v1",
  "title": "Counterfactual Self-Questioning for Stable Policy Optimization in Language Models",
  "authors": [
    "Mandar Parab"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2601.00885v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [],
  "summary": "Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produc..."
}