{
  "arxiv_id": "2512.23025v1",
  "title": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models",
  "authors": [
    "Wenxuan Xu",
    "Arvind Pillai",
    "Subigya Nepal",
    "Amanda C Collins",
    "Daniel M Mackin"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23025v1",
  "relevance_score": 81.2,
  "estimated_pages": 10,
  "summary": "Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with lan"
}