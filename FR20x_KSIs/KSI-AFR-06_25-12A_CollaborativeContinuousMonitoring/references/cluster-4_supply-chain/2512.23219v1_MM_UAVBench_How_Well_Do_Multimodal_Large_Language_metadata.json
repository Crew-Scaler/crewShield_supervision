{
  "arxiv_id": "2512.23219v1",
  "title": "MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?",
  "authors": [
    "Shiqi Dai",
    "Zizhi Ma",
    "Zhicong Luo",
    "Xuesong Yang",
    "Yibin Huang"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23219v1",
  "relevance_score": 81.2,
  "estimated_pages": 10,
  "summary": "While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as local"
}