{
  "arxiv_id": "2601.00828v1",
  "title": "Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis",
  "authors": [
    "Yin Li"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2601.00828v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [],
  "summary": "Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major..."
}