{
  "arxiv_id": "2512.12008v1",
  "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning",
  "authors": [
    "Minghui Liu",
    "Aadi Palnitkar",
    "Tahseen Rabbani",
    "Hyunwoo Jae",
    "Kyle Rui Sang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.12008v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their..."
}