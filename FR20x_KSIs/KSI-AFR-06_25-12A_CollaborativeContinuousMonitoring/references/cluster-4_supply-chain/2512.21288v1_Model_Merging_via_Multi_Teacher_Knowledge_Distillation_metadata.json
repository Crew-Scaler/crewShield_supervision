{
  "arxiv_id": "2512.21288v1",
  "title": "Model Merging via Multi-Teacher Knowledge Distillation",
  "authors": [
    "Seyed Arshan Dalili",
    "Mehrdad Mahdavi"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.21288v1",
  "relevance_score": 81.5,
  "estimated_pages": 12,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics t..."
}