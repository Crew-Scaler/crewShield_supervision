{
  "arxiv_id": "2601.00874v1",
  "title": "LLMize: A Framework for Large Language Model-Based Numerical Optimization",
  "authors": [
    "M. Rizki Oktavian"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2601.00874v1",
  "relevance_score": 90.0,
  "estimated_pages": 10,
  "key_topics": [],
  "summary": "Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over success..."
}