{
  "arxiv_id": "2512.24776v1",
  "title": "Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models",
  "authors": [
    "\u00c1kos Prucs",
    "M\u00e1rton Csutora",
    "M\u00e1ty\u00e1s Antal",
    "M\u00e1rk Marosi"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.24776v1",
  "relevance_score": 90.0,
  "estimated_pages": 10,
  "key_topics": [
    "model monitoring"
  ],
  "summary": "Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a tes..."
}