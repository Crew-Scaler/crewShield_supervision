{
  "arxiv_id": "2512.16250v1",
  "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
  "authors": [
    "Sanjoy Chowdhury",
    "Karren D. Yang",
    "Xudong Liu",
    "Fartash Faghri",
    "Pavan Kumar Anasosalu Vasu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.16250v1",
  "relevance_score": 80.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "autonomous systems"
  ],
  "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a bench..."
}