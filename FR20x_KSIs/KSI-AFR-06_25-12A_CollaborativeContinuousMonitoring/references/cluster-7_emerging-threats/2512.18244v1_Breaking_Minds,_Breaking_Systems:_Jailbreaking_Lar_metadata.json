{
  "arxiv_id": "2512.18244v1",
  "title": "Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation",
  "authors": [
    "Zehao Liu",
    "Xi Lin"
  ],
  "affiliation": "Various",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.18244v1",
  "relevance_score": 80,
  "key_topics": [
    "compliance-reporting",
    "threat-analysis",
    "prompt-injection-security"
  ],
  "summary": "Large Language Models (LLMs) have gained considerable popularity and protected by increasingly sophisticated safety mechanisms. However, jailbreak attacks continue to pose a critical security threat by inducing models to generate policy-violating beh..."
}