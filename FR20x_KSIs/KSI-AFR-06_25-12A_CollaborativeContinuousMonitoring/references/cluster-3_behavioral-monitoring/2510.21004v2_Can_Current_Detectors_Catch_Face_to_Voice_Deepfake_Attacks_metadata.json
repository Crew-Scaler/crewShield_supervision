{
  "arxiv_id": "2510.21004v2",
  "title": "Can Current Detectors Catch Face-to-Voice Deepfake Attacks?",
  "authors": [
    "Nguyen Linh Bao Nguyen",
    "Alsharif Abuadbba",
    "Kristen Moore",
    "Tingmin Wu"
  ],
  "affiliation": "See author list",
  "published_date": "2025-10",
  "url": "https://arxiv.org/abs/2510.21004v2",
  "relevance_score": 96.0,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "vulnerability management"
  ],
  "summary": "The rapid advancement of generative models has enabled the creation of increasingly stealthy synthetic voices, commonly referred to as audio deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly alarming capability: generating a victim's voice from a single facial image, without requiring any voice sample. By exploiting correlations between facial and vocal features, FOICE produces synthetic voices realistic enough to bypass industry-standard authentication systems, inclu..."
}