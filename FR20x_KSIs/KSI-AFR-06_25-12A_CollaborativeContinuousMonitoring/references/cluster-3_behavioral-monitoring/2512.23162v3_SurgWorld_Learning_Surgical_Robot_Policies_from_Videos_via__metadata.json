{
  "arxiv_id": "2512.23162v3",
  "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
  "authors": [
    "Yufan He",
    "Pengfei Guo",
    "Mengya Xu",
    "Zhaoshuo Li",
    "Andriy Myronenko"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23162v3",
  "relevance_score": 91.5,
  "estimated_pages": 12,
  "key_topics": [
    "autonomous systems"
  ],
  "summary": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action label..."
}