{
  "arxiv_id": "2512.22508v1",
  "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals",
  "authors": [
    "Lucky Susanto",
    "Anasta Pranawijayana",
    "Cortino Sukotjo",
    "Soni Prasad",
    "Derry Wijaya"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.22508v1",
  "relevance_score": 93.6,
  "estimated_pages": 10,
  "summary": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. "
}