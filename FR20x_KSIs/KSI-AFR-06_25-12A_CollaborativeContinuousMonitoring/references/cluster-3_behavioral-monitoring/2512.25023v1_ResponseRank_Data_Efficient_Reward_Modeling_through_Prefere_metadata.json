{
  "arxiv_id": "2512.25023v1",
  "title": "ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning",
  "authors": [
    "Timo Kaufmann",
    "Yannick Metz",
    "Daniel Keim",
    "Eyke H\u00fcllermeier"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.25023v1",
  "relevance_score": 91.5,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness"
  ],
  "summary": "Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ..."
}