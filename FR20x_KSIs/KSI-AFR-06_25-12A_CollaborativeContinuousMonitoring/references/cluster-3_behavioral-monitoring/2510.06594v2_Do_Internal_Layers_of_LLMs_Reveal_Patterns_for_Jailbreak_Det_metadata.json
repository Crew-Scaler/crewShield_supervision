{
  "arxiv_id": "2510.06594v2",
  "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?",
  "authors": [
    "Sri Durga Sai Sowmya Kadali",
    "Evangelos E. Papalexakis"
  ],
  "affiliation": "See author list",
  "published_date": "2025-10",
  "url": "https://arxiv.org/abs/2510.06594v2",
  "relevance_score": 87.5,
  "estimated_pages": 8,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness"
  ],
  "summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern with the increasing prevalence and accessibility of conversational LLMs. Adversarial users often exploit these models through carefully engineered prompts to elicit restricted or sensitive outputs, a strategy widely referred to as jailbreaking. While numerous defense mechanisms have been proposed, attackers continuously develop novel prompting techniques, and no existing model can be considered fully resistant. In this s..."
}