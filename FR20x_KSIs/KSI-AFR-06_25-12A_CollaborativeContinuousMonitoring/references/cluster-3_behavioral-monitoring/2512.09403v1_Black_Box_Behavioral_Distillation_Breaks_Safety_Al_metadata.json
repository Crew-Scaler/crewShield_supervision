{
  "arxiv_id": "2512.09403v1",
  "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
  "authors": [
    "Sohely Jahan",
    "Ruimin Sun"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.09403v1",
  "relevance_score": 86.0,
  "estimated_pages": 12,
  "summary": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored.\n  We present a black-box distillation attack that replicates th"
}