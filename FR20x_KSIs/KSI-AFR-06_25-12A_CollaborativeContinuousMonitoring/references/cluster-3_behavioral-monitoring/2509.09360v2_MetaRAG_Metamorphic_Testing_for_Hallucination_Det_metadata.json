{
  "arxiv_id": "2509.09360v2",
  "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems",
  "authors": [
    "Channdeth Sok",
    "David Luz",
    "Yacine Haddam"
  ],
  "published_date": "2025-09",
  "url": "https://arxiv.org/abs/2509.09360v2",
  "relevance_score": 91.2,
  "estimated_pages": 12,
  "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise applications, yet their reliability remains limited by hallucinations, i.e., confident but factually incorrect information. Existing detection approaches, such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not address the unique challenges of Retrieval-Augmented Generation (RAG) systems, where responses must"
}