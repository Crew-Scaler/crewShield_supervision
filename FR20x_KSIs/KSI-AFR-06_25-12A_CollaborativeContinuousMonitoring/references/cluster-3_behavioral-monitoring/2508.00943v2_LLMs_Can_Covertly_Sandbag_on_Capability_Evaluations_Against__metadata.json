{
  "arxiv_id": "2508.00943v2",
  "title": "LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring",
  "authors": [
    "Chloe Li",
    "Mary Phuong",
    "Noah Y. Siegel"
  ],
  "affiliation": "See author list",
  "published_date": "2025-07",
  "url": "https://arxiv.org/abs/2508.00943v2",
  "relevance_score": 84.5,
  "estimated_pages": 10,
  "key_topics": [
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is sandbagging - the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT mon..."
}