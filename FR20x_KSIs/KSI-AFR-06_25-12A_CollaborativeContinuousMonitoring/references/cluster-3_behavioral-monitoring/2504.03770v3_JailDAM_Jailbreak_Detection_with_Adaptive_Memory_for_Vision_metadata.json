{
  "arxiv_id": "2504.03770v3",
  "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model",
  "authors": [
    "Yi Nian",
    "Shenzhe Zhu",
    "Yuehan Qin",
    "Li Li",
    "Ziyi Wang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-04",
  "url": "https://arxiv.org/abs/2504.03770v3",
  "relevance_score": 94.5,
  "estimated_pages": 10,
  "key_topics": [
    "adversarial robustness",
    "vulnerability management",
    "model monitoring"
  ],
  "summary": "Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on ..."
}