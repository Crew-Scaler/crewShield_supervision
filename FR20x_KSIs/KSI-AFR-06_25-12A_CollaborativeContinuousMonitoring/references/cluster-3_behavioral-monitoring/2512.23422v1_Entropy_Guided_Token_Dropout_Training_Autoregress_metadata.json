{
  "arxiv_id": "2512.23422v1",
  "title": "Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data",
  "authors": [
    "Jiapeng Wang",
    "Yiwen Hu",
    "Yanzipeng Gao",
    "Haoyu Wang",
    "Shuo Wang"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23422v1",
  "relevance_score": 92.4,
  "estimated_pages": 10,
  "summary": "As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an i"
}