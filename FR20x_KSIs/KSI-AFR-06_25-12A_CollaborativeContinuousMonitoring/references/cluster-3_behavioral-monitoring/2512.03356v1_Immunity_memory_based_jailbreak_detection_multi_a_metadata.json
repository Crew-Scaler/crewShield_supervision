{
  "arxiv_id": "2512.03356v1",
  "title": "Immunity memory-based jailbreak detection: multi-agent adaptive guard for large language models",
  "authors": [
    "Jun Leng",
    "Litian Zhang",
    "Xi Zhang"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.03356v1",
  "relevance_score": 83.6,
  "estimated_pages": 12,
  "summary": "Large language models (LLMs) have become foundational in AI systems, yet they remain vulnerable to adversarial jailbreak attacks. These attacks involve carefully crafted prompts that bypass safety guardrails and induce models to produce harmful content. Detecting such malicious input queries is therefore critical for maintaining LLM safety. Existing methods for jailbreak detection typically involv"
}