{
  "arxiv_id": "2512.21506v1",
  "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding",
  "authors": [
    "Aiwei Zhang",
    "Arvind Pillai",
    "Andrew Campbell",
    "Nicholas C. Jacobson"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.21506v1",
  "relevance_score": 84.5,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring"
  ],
  "summary": "As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavi..."
}