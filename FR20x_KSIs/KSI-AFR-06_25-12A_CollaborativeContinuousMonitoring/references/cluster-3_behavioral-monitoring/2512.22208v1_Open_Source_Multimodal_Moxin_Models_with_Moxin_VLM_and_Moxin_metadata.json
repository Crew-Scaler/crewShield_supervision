{
  "arxiv_id": "2512.22208v1",
  "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA",
  "authors": [
    "Pu Zhao",
    "Xuan Shen",
    "Zhenglun Kong",
    "Yixin Shen",
    "Sung-En Chang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.22208v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [
    "AI governance",
    "model monitoring"
  ],
  "summary": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and dep..."
}