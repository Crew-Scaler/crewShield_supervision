{
  "arxiv_id": "2512.08093v2",
  "title": "Training LLMs for Honesty via Confessions",
  "authors": [
    "Manas Joglekar",
    "Jeremy Chen",
    "Gabriel Wu",
    "Jason Yosinski",
    "Jasmine Wang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.08093v2",
  "relevance_score": 91.5,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "AI governance"
  ],
  "summary": "Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.\n  In this work we propose a method for eliciting an honest expression of an..."
}