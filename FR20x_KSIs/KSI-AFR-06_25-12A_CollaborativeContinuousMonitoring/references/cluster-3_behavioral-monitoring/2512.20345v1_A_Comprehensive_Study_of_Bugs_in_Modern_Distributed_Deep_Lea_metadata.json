{
  "arxiv_id": "2512.20345v1",
  "title": "A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems",
  "authors": [
    "Xiaoxue Ma",
    "Wanwei Zhan",
    "Jiale Chen",
    "Yishu Li",
    "Jacky Keung"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20345v1",
  "relevance_score": 90.0,
  "estimated_pages": 10,
  "key_topics": [
    "supply chain",
    "model monitoring"
  ],
  "summary": "In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized ..."
}