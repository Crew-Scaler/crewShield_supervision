{
  "arxiv_id": "2512.21583v1",
  "title": "A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning",
  "authors": [
    "Zelin Zang",
    "Wenyi Gu",
    "Siqi Ma",
    "Dan Yang",
    "Yue Shen"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.21583v1",
  "relevance_score": 90.0,
  "estimated_pages": 8,
  "summary": "With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment "
}