{
  "arxiv_id": "2510.02962v1",
  "title": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking",
  "authors": [
    "Jingqi Zhang",
    "Ruibo Chen",
    "Yingqing Yang",
    "Peihua Mai",
    "Heng Huang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-10",
  "url": "https://arxiv.org/abs/2510.02962v1",
  "relevance_score": 93.0,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "Large Language Models (LLMs) are increasingly fine-tuned on smaller, domain-specific datasets to improve downstream performance. These datasets often contain proprietary or copyrighted material, raising the need for reliable safeguards against unauthorized use. Existing membership inference attacks (MIAs) and dataset-inference methods typically require access to internal signals such as logits, while current black-box approaches often rely on handcrafted prompts or a clean reference dataset for ..."
}