{
  "arxiv_id": "2504.12344v1",
  "title": "Propaganda via AI? A Study on Semantic Backdoors in Large Language Models",
  "authors": [
    "Nay Myat Min",
    "Long H. Pham",
    "Yige Li",
    "Jun Sun"
  ],
  "affiliation": "See author list",
  "published_date": "2025-04",
  "url": "https://arxiv.org/abs/2504.12344v1",
  "relevance_score": 86.0,
  "estimated_pages": 12,
  "key_topics": [
    "anomaly detection",
    "adversarial robustness",
    "AI governance",
    "vulnerability management",
    "model monitoring"
  ],
  "summary": "Large language models (LLMs) demonstrate remarkable performance across myriad language tasks, yet they remain vulnerable to backdoor attacks, where adversaries implant hidden triggers that systematically manipulate model outputs. Traditional defenses focus on explicit token-level anomalies and therefore overlook semantic backdoors-covert triggers embedded at the conceptual level (e.g., ideological stances or cultural references) that rely on meaning-based cues rather than lexical oddities. We fi..."
}