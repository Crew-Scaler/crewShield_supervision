{
  "arxiv_id": "2512.21506v1",
  "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding",
  "authors": [
    "Aiwei Zhang",
    "Arvind Pillai",
    "Andrew Campbell",
    "Nicholas C. Jacobson"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.21506v1",
  "relevance_score": 83.6,
  "estimated_pages": 12,
  "summary": "As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionT"
}