{
  "arxiv_id": "2509.09112v2",
  "title": "Character-Level Perturbations Disrupt LLM Watermarks",
  "authors": [
    "Zhaoxi Zhang",
    "Xiaomei Zhang",
    "Yanjun Zhang",
    "He Zhang",
    "Shirui Pan"
  ],
  "affiliation": "See author list",
  "published_date": "2025-09",
  "url": "https://arxiv.org/abs/2509.09112v2",
  "relevance_score": 97.5,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "vulnerability management",
    "model monitoring"
  ],
  "summary": "Large Language Model (LLM) watermarking embeds detectable signals into generated text for copyright protection, misuse prevention, and content detection. While prior studies evaluate robustness using watermark removal attacks, these methods are often suboptimal, creating the misconception that effective removal requires large perturbations or powerful adversaries.\n  To bridge the gap, we first formalize the system model for LLM watermark, and characterize two realistic threat models constrained ..."
}