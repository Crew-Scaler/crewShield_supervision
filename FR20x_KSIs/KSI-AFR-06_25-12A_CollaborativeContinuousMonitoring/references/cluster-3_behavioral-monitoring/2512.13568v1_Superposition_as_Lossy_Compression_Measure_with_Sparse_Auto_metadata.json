{
  "arxiv_id": "2512.13568v1",
  "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability",
  "authors": [
    "Leonard Bereska",
    "Zoe Tzifa-Kratira",
    "Reza Samavi",
    "Efstratios Gavves"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.13568v1",
  "relevance_score": 93.0,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "vulnerability management",
    "model monitoring"
  ],
  "summary": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective f..."
}