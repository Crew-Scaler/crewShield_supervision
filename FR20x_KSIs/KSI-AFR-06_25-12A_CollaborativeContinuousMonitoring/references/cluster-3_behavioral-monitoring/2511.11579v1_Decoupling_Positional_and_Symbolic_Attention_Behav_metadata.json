{
  "arxiv_id": "2511.11579v1",
  "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers",
  "authors": [
    "Felipe Urrutia",
    "Jorge Salas",
    "Alexander Kozachinskiy",
    "Cristian Buc Calderon",
    "Hector Pasten"
  ],
  "published_date": "2025-10",
  "url": "https://arxiv.org/abs/2511.11579v1",
  "relevance_score": 82.4,
  "estimated_pages": 12,
  "summary": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued tha"
}