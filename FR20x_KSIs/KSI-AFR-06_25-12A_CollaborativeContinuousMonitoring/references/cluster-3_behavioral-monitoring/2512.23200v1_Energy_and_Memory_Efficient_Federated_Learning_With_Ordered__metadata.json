{
  "arxiv_id": "2512.23200v1",
  "title": "Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing",
  "authors": [
    "Ziru Niu",
    "Hai Dong",
    "A. K. Qin",
    "Tao Gu",
    "Pengcheng Zhang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23200v1",
  "relevance_score": 90.0,
  "estimated_pages": 12,
  "key_topics": [],
  "summary": "Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and reduces communication overhead. However, the limited computational power, memory, and bandwidth of IoT edge devices pose significant challenges to the efficiency and scalability of FL, especially when tr..."
}