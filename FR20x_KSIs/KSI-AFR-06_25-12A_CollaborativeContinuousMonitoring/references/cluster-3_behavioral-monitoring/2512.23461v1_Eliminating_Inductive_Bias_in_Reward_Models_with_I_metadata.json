{
  "arxiv_id": "2512.23461v1",
  "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance",
  "authors": [
    "Zhuo Li",
    "Pengyu Cheng",
    "Zhechao Yu",
    "Feifei Tong",
    "Anningzhe Gao"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23461v1",
  "relevance_score": 90.0,
  "estimated_pages": 12,
  "summary": "Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, lea"
}