{
  "arxiv_id": "2512.20328v1",
  "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
  "authors": [
    "Antonio Vitale",
    "Khai-Nguyen Nguyen",
    "Denys Poshyvanyk",
    "Rocco Oliveto",
    "Simone Scalabrino"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20328v1",
  "relevance_score": 81.5,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "AI governance"
  ],
  "summary": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software enginee..."
}