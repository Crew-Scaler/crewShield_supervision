{
  "arxiv_id": "2510.06594v2",
  "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?",
  "authors": [
    "Sri Durga Sai Sowmya Kadali",
    "Evangelos E. Papalexakis"
  ],
  "published_date": "2025-10",
  "url": "https://arxiv.org/abs/2510.06594v2",
  "relevance_score": 84.8,
  "estimated_pages": 8,
  "summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern with the increasing prevalence and accessibility of conversational LLMs. Adversarial users often exploit these models through carefully engineered prompts to elicit restricted or sensitive outputs, a strategy widely referred to as jailbreaking. While numerous defense mechanisms have been proposed, attackers continuously de"
}