{
  "arxiv_id": "2512.23365v1",
  "title": "SpatialMosaic: A Multiview VLM Dataset for Partial Visibility",
  "authors": [
    "Kanghee Lee",
    "Injae Lee",
    "Minseok Kwak",
    "Kwonyoung Ryu",
    "Jungi Hong"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23365v1",
  "relevance_score": 90.0,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness"
  ],
  "summary": "The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3..."
}