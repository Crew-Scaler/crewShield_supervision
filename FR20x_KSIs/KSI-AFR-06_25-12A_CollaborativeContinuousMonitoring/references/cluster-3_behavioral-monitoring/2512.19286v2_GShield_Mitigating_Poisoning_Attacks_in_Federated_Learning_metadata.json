{
  "arxiv_id": "2512.19286v2",
  "title": "GShield: Mitigating Poisoning Attacks in Federated Learning",
  "authors": [
    "Sameera K. M.",
    "Serena Nicolazzo",
    "Antonino Nocera",
    "Vinod P.",
    "Rafidha Rehiman K. A"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.19286v2",
  "relevance_score": 97.5,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "vulnerability management",
    "model monitoring"
  ],
  "summary": "Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we pres..."
}