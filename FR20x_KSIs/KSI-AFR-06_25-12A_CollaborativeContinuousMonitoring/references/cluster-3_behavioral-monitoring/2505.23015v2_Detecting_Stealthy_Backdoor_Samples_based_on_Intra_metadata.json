{
  "arxiv_id": "2505.23015v2",
  "title": "Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models",
  "authors": [
    "Jinwen Chen",
    "Hainan Zhang",
    "Fei Sun",
    "Qinnan Zhang",
    "Sijia Wen"
  ],
  "published_date": "2025-05",
  "url": "https://arxiv.org/abs/2505.23015v2",
  "relevance_score": 82.4,
  "estimated_pages": 10,
  "summary": "Stealthy data poisoning during fine-tuning can backdoor large language models (LLMs), threatening downstream safety. Existing detectors either use classifier-style probability signals--ill-suited to generation--or rely on rewriting, which can degrade quality and even introduce new triggers. We address the practical need to efficiently remove poisoned examples before or during fine-tuning. We obser"
}