{
  "arxiv_id": "2512.22519v1",
  "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding",
  "authors": [
    "Khoa Vo",
    "Taisei Hanyu",
    "Yuki Ikebe",
    "Trong Thang Pham",
    "Nhat Chung"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.22519v1",
  "relevance_score": 83.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness"
  ],
  "summary": "Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance.\n  To address ..."
}