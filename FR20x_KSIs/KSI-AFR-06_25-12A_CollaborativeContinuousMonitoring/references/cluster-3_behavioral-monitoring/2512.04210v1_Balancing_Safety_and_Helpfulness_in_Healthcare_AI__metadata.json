{
  "arxiv_id": "2512.04210v1",
  "title": "Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment",
  "authors": [
    "Huy Nghiem",
    "Swetasudha Panda",
    "Devashish Khatwani",
    "Huy V. Nguyen",
    "Krishnaram Kenthapadi"
  ],
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.04210v1",
  "relevance_score": 83.6,
  "estimated_pages": 10,
  "summary": "Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to r"
}