{
  "arxiv_id": "2505.16567v3",
  "title": "Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning",
  "authors": [
    "Thibaud Gloaguen",
    "Mark Vero",
    "Robin Staab",
    "Martin Vechev"
  ],
  "published_date": "2025-05",
  "url": "https://arxiv.org/abs/2505.16567v3",
  "relevance_score": 93.6,
  "estimated_pages": 12,
  "summary": "Finetuning open-weight Large Language Models (LLMs) is standard practice for achieving task-specific performance improvements. Until now, finetuning has been regarded as a controlled and secure process in which training on benign datasets leads to predictable behaviors. In this paper, we demonstrate, for the first time, that an adversary can create compromised LLMs that are performant and benign, "
}