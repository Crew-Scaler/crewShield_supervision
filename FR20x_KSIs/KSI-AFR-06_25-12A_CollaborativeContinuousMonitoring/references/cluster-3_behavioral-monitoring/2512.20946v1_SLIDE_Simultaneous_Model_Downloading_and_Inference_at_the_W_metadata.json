{
  "arxiv_id": "2512.20946v1",
  "title": "SLIDE: Simultaneous Model Downloading and Inference at the Wireless Network Edge",
  "authors": [
    "Guanqiao Qu",
    "Tao Li",
    "Qian Chen",
    "Xianhao Chen",
    "Sheng Zhou"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20946v1",
  "relevance_score": 81.5,
  "estimated_pages": 10,
  "key_topics": [],
  "summary": "To support on-device inference, the next-generation mobile networks are expected to support real-time model downloading services to mobile users. However, powerful AI models typically have large model sizes, resulting in excessive end-to-end (E2E) downloading-and-inference (DAI) latency. To address this issue, we propose a simultaneous model downloading and inference (SLIDE) framework, which allows users to perform inference with downloaded layers while simultaneously receiving the remaining lay..."
}