{
  "arxiv_id": "2511.11579v1",
  "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers",
  "authors": [
    "Felipe Urrutia",
    "Jorge Salas",
    "Alexander Kozachinskiy",
    "Cristian Buc Calderon",
    "Hector Pasten"
  ],
  "affiliation": "See author list",
  "published_date": "2025-10",
  "url": "https://arxiv.org/abs/2511.11579v1",
  "relevance_score": 80.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic informati..."
}