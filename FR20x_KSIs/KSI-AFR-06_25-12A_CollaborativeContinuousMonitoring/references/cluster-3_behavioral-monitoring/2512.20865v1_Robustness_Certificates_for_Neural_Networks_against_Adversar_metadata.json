{
  "arxiv_id": "2512.20865v1",
  "title": "Robustness Certificates for Neural Networks against Adversarial Attacks",
  "authors": [
    "Sara Taheri",
    "Mahalakshmi Sabanayagam",
    "Debarghya Ghoshdastidar",
    "Majid Zamani"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.20865v1",
  "relevance_score": 96.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "The increasing use of machine learning in safety-critical domains amplifies the risk of adversarial threats, especially data poisoning attacks that corrupt training data to degrade performance or induce unsafe behavior. Most existing defenses lack formal guarantees or rely on restrictive assumptions about the model class, attack type, extent of poisoning, or point-wise certification, limiting their practical reliability. This paper introduces a principled formal robustness certification framewor..."
}