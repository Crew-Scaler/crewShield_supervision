{
  "arxiv_id": "2512.09403v1",
  "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
  "authors": [
    "Sohely Jahan",
    "Ruimin Sun"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.09403v1",
  "relevance_score": 90.0,
  "estimated_pages": 12,
  "key_topics": [
    "anomaly detection",
    "behavioral monitoring",
    "adversarial robustness",
    "vulnerability management"
  ],
  "summary": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored.\n  We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuin..."
}