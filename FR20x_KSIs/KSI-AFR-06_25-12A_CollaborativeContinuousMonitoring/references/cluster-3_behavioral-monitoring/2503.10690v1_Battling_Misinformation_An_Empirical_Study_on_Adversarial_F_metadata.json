{
  "arxiv_id": "2503.10690v1",
  "title": "Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models",
  "authors": [
    "Shahnewaz Karim Sakib",
    "Anindya Bijoy Das",
    "Shibbir Ahmed"
  ],
  "affiliation": "See author list",
  "published_date": "2025-03",
  "url": "https://arxiv.org/abs/2503.10690v1",
  "relevance_score": 94.5,
  "estimated_pages": 10,
  "key_topics": [
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "Adversarial factuality refers to the deliberate insertion of misinformation into input prompts by an adversary, characterized by varying levels of expressed confidence. In this study, we systematically evaluate the performance of several open-source large language models (LLMs) when exposed to such adversarial inputs. Three tiers of adversarial confidence are considered: strongly confident, moderately confident, and limited confidence. Our analysis encompasses eight LLMs: LLaMA 3.1 (8B), Phi 3 (..."
}