{
  "arxiv_id": "2504.03770v3",
  "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model",
  "authors": [
    "Yi Nian",
    "Shenzhe Zhu",
    "Yuehan Qin",
    "Li Li",
    "Ziyi Wang"
  ],
  "published_date": "2025-04",
  "url": "https://arxiv.org/abs/2504.03770v3",
  "relevance_score": 91.2,
  "estimated_pages": 10,
  "summary": "Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deploy"
}