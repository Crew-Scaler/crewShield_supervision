{
  "arxiv_id": "2508.17361v2",
  "title": "Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias",
  "authors": [
    "Shir Bernstein",
    "David Beste",
    "Daniel Ayzenshteyn",
    "Lea Schonherr",
    "Yisroel Mirsky"
  ],
  "affiliation": "See author list",
  "published_date": "2025-08",
  "url": "https://arxiv.org/abs/2508.17361v2",
  "relevance_score": 93.0,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "vulnerability management"
  ],
  "summary": "Large Language Models (LLMs) are increasingly trusted to perform automated code review and static analysis at scale, supporting tasks such as vulnerability detection, summarization, and refactoring. In this paper, we identify and exploit a critical vulnerability in LLM-based code analysis: an abstraction bias that causes models to overgeneralize familiar programming patterns and overlook small, meaningful bugs. Adversaries can exploit this blind spot to hijack the control flow of the LLM's inter..."
}