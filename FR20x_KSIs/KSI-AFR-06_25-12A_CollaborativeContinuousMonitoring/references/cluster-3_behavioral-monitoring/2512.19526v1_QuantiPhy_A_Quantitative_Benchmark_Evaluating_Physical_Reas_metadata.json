{
  "arxiv_id": "2512.19526v1",
  "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
  "authors": [
    "Li Puyin",
    "Tiange Xiang",
    "Ella Mao",
    "Shirley Wei",
    "Xinye Chen"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.19526v1",
  "relevance_score": 91.5,
  "estimated_pages": 12,
  "key_topics": [
    "model monitoring",
    "autonomous systems"
  ],
  "summary": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively mea..."
}