{
  "arxiv_id": "2512.22880v1",
  "title": "Fundamental Novel Consistency Theory: $H$-Consistency Bounds",
  "authors": [
    "Yutao Zhong"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.22880v1",
  "relevance_score": 81.5,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "In machine learning, the loss functions optimized during training often differ from the target loss that defines task performance due to computational intractability or lack of differentiability. We present an in-depth study of the target loss estimation error relative to the surrogate loss estimation error. Our analysis leads to $H$-consistency bounds, which are guarantees accounting for the hypothesis set $H$. These bounds offer stronger guarantees than Bayes-consistency or $H$-calibration and..."
}