{
  "arxiv_id": "2512.11573v1",
  "title": "Visualizing token importance for black-box language models",
  "authors": [
    "Paulius Rauba",
    "Qiyao Wei",
    "Mihaela van der Schaar"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.11573v1",
  "relevance_score": 80.0,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring",
    "AI governance"
  ],
  "summary": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input to..."
}