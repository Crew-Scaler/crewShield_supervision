{
  "arxiv_id": "2512.18028v1",
  "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
  "authors": [
    "Tin Stribor Sohn",
    "Maximilian Dillitzer",
    "Jason J. Corso",
    "Eric Sax"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.18028v1",
  "relevance_score": 94.5,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring",
    "autonomous systems"
  ],
  "summary": "Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core..."
}