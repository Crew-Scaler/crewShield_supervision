{
  "arxiv_id": "2501.08454v2",
  "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack",
  "authors": [
    "Sagiv Antebi",
    "Edan Habler",
    "Asaf Shabtai",
    "Yuval Elovici"
  ],
  "affiliation": "See author list",
  "published_date": "2025-01",
  "url": "https://arxiv.org/abs/2501.08454v2",
  "relevance_score": 83.0,
  "estimated_pages": 10,
  "key_topics": [
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "Large language models (LLMs) have become essential tools for digital task assistance. Their training relies heavily on the collection of vast amounts of data, which may include copyright-protected or sensitive information. Recent studies on detecting pretraining data in LLMs have primarily focused on sentence- or paragraph-level membership inference attacks (MIAs), usually involving probability analysis of the target model's predicted tokens. However, these methods often exhibit poor accuracy, f..."
}