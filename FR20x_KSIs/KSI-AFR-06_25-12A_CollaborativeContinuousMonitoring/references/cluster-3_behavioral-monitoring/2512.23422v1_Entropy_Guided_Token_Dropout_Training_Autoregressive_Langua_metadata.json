{
  "arxiv_id": "2512.23422v1",
  "title": "Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data",
  "authors": [
    "Jiapeng Wang",
    "Yiwen Hu",
    "Yanzipeng Gao",
    "Haoyu Wang",
    "Shuo Wang"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.23422v1",
  "relevance_score": 91.5,
  "estimated_pages": 10,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "vulnerability management",
    "model monitoring"
  ],
  "summary": "As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to domin..."
}