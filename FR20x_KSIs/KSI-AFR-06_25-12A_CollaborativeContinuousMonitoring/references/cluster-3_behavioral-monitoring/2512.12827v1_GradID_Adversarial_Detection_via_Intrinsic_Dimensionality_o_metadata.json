{
  "arxiv_id": "2512.12827v1",
  "title": "GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients",
  "authors": [
    "Mohammad Mahdi Razmjoo",
    "Mohammad Mahdi Sharifian",
    "Saeed Bagheri Shouraki"
  ],
  "affiliation": "See author list",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.12827v1",
  "relevance_score": 87.5,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness",
    "vulnerability management",
    "model monitoring",
    "autonomous systems"
  ],
  "summary": "Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of..."
}