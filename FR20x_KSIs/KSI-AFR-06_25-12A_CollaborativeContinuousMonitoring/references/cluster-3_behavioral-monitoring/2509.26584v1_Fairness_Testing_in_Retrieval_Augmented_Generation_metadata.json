{
  "arxiv_id": "2509.26584v1",
  "title": "Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models",
  "authors": [
    "Matheus Vinicius da Silva de Oliveira",
    "Jonathan de Andrade Silva",
    "Awdren de Lima Fontao"
  ],
  "published_date": "2025-09",
  "url": "https://arxiv.org/abs/2509.26584v1",
  "relevance_score": 91.2,
  "estimated_pages": 12,
  "summary": "Large Language Models (LLMs) are widely used across multiple domains but continue to raise concerns regarding security and fairness. Beyond known attack vectors such as data poisoning and prompt injection, LLMs are also vulnerable to fairness bugs. These refer to unintended behaviors influenced by sensitive demographic cues (e.g., race or sexual orientation) that should not affect outcomes. Anothe"
}