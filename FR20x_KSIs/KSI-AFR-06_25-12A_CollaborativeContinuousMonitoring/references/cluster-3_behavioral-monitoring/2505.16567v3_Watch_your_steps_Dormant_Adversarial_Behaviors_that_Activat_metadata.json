{
  "arxiv_id": "2505.16567v3",
  "title": "Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning",
  "authors": [
    "Thibaud Gloaguen",
    "Mark Vero",
    "Robin Staab",
    "Martin Vechev"
  ],
  "affiliation": "See author list",
  "published_date": "2025-05",
  "url": "https://arxiv.org/abs/2505.16567v3",
  "relevance_score": 93.0,
  "estimated_pages": 12,
  "key_topics": [
    "behavioral monitoring",
    "adversarial robustness",
    "model monitoring"
  ],
  "summary": "Finetuning open-weight Large Language Models (LLMs) is standard practice for achieving task-specific performance improvements. Until now, finetuning has been regarded as a controlled and secure process in which training on benign datasets leads to predictable behaviors. In this paper, we demonstrate, for the first time, that an adversary can create compromised LLMs that are performant and benign, yet exhibit adversarial behaviors once finetuned by downstream users. To this end, we propose an att..."
}