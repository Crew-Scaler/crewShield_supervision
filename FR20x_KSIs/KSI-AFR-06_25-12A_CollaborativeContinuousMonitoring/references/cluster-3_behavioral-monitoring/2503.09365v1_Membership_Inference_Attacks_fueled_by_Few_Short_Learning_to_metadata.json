{
  "arxiv_id": "2503.09365v1",
  "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
  "authors": [
    "Daniel Jim\u00e9nez-L\u00f3pez",
    "Nuria Rodr\u00edguez-Barroso",
    "M. Victoria Luz\u00f3n",
    "Francisco Herrera"
  ],
  "affiliation": "See author list",
  "published_date": "2025-03",
  "url": "https://arxiv.org/abs/2503.09365v1",
  "relevance_score": 81.5,
  "estimated_pages": 12,
  "key_topics": [
    "adversarial robustness"
  ],
  "summary": "Deep learning models have an intrinsic privacy issue as they memorize parts of their training data, creating a privacy leakage. Membership Inference Attacks (MIA) exploit it to obtain confidential information about the data used for training, aiming to steal information. They can be repurposed as a measurement of data integrity by inferring whether it was used to train a machine learning model. While state-of-the-art attacks achieve a significant privacy leakage, their requirements are not feasi..."
}