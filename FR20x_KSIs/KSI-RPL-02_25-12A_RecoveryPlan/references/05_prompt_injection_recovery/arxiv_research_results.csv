query_name|arxiv_id|authors|title|year|quality_score|summary|arxiv_url|pdf_downloaded
prompt injection recovery procedures LLM|2512.12583v1|Safwan Shaheer; G. M. Refatul Islam; Mohammad Rafid Hamid|Detecting Prompt Injection Attacks Against Application Using Classifiers|2025|130|Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curat...|https://arxiv.org/abs/2512.12583v1|Yes
prompt injection recovery procedures LLM|2511.10720v1|Runpeng Geng; Yanting Wang; Chenlong Yin|PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization|2025|130|Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an at...|https://arxiv.org/abs/2511.10720v1|Yes
prompt injection recovery procedures LLM|2505.11701v2|Shaghayegh Abedi; Amin Jalali|DMN-Guided Prompting: A Framework for Controlling LLM Behavior|2025|130|Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effec...|https://arxiv.org/abs/2505.11701v2|Yes
prompt injection recovery procedures LLM|2507.22171v2|Zheng Zhang; Peilin Zhao; Deheng Ye|Enhancing Jailbreak Attacks on LLMs via Persona Prompts|2025|130|Jailbreak attacks aim to exploit large language models (LLMs) by inducing them to generate harmful content, thereby revealing their vulnerabilities. U...|https://arxiv.org/abs/2507.22171v2|Yes
prompt injection recovery procedures LLM|2509.14285v4|S M Asif Hossain; Ruksat Khan Shayoni; Mohd Ruhul Ameen|A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks|2025|130|Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inpu...|https://arxiv.org/abs/2509.14285v4|Yes
prompt injection recovery procedures LLM|2504.11168v3|William Hackett; Lewis Birch; Stefan Trawicki|Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks against Prompt Injection and Jailbreak Detection Systems|2025|130|Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable...|https://arxiv.org/abs/2504.11168v3|Yes
prompt injection recovery procedures LLM|2511.12295v1|Hasini Jayathilaka|Privacy-Preserving Prompt Injection Detection for LLMs Using Federated Learning and Embedding-Based NLP Classification|2025|130|Prompt injection attacks are an emerging threat to large language models (LLMs), enabling malicious users to manipulate outputs through carefully desi...|https://arxiv.org/abs/2511.12295v1|Yes
prompt injection recovery procedures LLM|2512.09321v3|Reachal Wang; Yuqi Jia; Neil Zhenqiang Gong|ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data|2025|130|Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended tas...|https://arxiv.org/abs/2512.09321v3|Yes
prompt injection recovery procedures LLM|2505.14368v1|Jiawen Wang; Pritha Gupta; Ivan Habernal|Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs|2025|130|Recent studies demonstrate that Large Language Models (LLMs) are vulnerable to different prompt-based attacks, generating harmful content or sensitive...|https://arxiv.org/abs/2505.14368v1|Yes
prompt injection recovery procedures LLM|2507.02735v2|Sizhe Chen; Arman Zharmagambetov; David Wagner|Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks|2025|130|Prompt injection attack has been listed as the top-1 security threat to LLM-integrated applications, which interact with external environment data for...|https://arxiv.org/abs/2507.02735v2|Yes
prompt injection recovery procedures LLM|2509.22830v1|Hwan Chang; Yonghyun Jun; Hwanhee Lee|ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents|2025|130|The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversa...|https://arxiv.org/abs/2509.22830v1|Yes
prompt injection recovery procedures LLM|2504.18333v1|Narek Maloyan; Dmitry Namiot|Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections|2025|130|LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a fra...|https://arxiv.org/abs/2504.18333v1|Yes
prompt injection recovery procedures LLM|2509.25448v2|Yuepeng Hu; Zhengyuan Jiang; Mengyuan Li|Fingerprinting LLMs via Prompt Injection|2025|130|Large language models (LLMs) are often modified after release through post-processing such as post-training or quantization, which makes it challengin...|https://arxiv.org/abs/2509.25448v2|Yes
prompt injection recovery procedures LLM|2505.04806v2|Chetan Pathade|Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs|2025|130|Large Language Models (LLMs) are increasingly integrated into consumer and enterprise applications. Despite their capabilities, they remain susceptibl...|https://arxiv.org/abs/2505.04806v2|Yes
prompt injection recovery procedures LLM|2509.05831v3|Ishaan Verma; Arsheya Yadav|Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization|2025|130|Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injectio...|https://arxiv.org/abs/2509.05831v3|Yes
document poisoning detection LLM|2510.26213v2|Hengrui Kang; Zhuangcheng Gu; Zhiyuan Zhao|OmniDocLayout: Towards Diverse Document Layout Generation via Coarse-to-Fine LLM Learning|2025|130|Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its g...|https://arxiv.org/abs/2510.26213v2|Yes
document poisoning detection LLM|2506.08726v2|Nelvin Tan; Zian Seng; Liang Zhang|Improved LLM Agents for Financial Document Question Answering|2025|130|Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numer...|https://arxiv.org/abs/2506.08726v2|Yes
document poisoning detection LLM|2502.14182v1|Pengfei He; Yue Xing; Han Xu|Multi-Faceted Studies on Data Poisoning can Advance LLM Development|2025|130|The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages...|https://arxiv.org/abs/2502.14182v1|Yes
document poisoning detection LLM|2510.07192v1|Alexandra Souly; Javier Rando; Ed Chapman|Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples|2025|130|Poisoning attacks can compromise the safety of large language models (LLMs) by injecting malicious documents into their training data. Existing work h...|https://arxiv.org/abs/2510.07192v1|Yes
document poisoning detection LLM|2508.11021v1|Zisheng Liang; Kidus Zewde; Rudra Pratap Singh|Can Multi-modal (reasoning) LLMs detect document manipulation?|2025|130|Document fraud poses a significant threat to industries reliant on secure and verifiable documentation, necessitating robust detection mechanisms. Thi...|https://arxiv.org/abs/2508.11021v1|Yes
document poisoning detection LLM|2511.02600v1|Patrick Karlsen; Even Eilertsen|On The Dangers of Poisoned LLMs In Security Automation|2025|130|"This paper investigates some of the risks introduced by ""LLM poisoning,"" the intentional or unintentional introduction of malicious or biased data dur..."|https://arxiv.org/abs/2511.02600v1|Yes
document poisoning detection LLM|2510.08931v1|Ashish Kattamuri; Harshwardhan Fartale; Arpita Vats|RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation|2025|130|Data contamination poses a significant challenge to reliable LLM evaluation, where models may achieve high performance by memorizing training data rat...|https://arxiv.org/abs/2510.08931v1|Yes
document poisoning detection LLM|2507.06262v1|Haoqi He; Xiaokai Lin; Jiancai Chen|Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method|2025|130|Data poisoning attacks pose significant threats to machine learning models by introducing malicious data into the training process, thereby degrading ...|https://arxiv.org/abs/2507.06262v1|Yes
document poisoning detection LLM|2505.17601v5|Jiawei Kong; Hao Fang; Xiaochen Yang|Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs|2025|130|Recent studies have widely investigated backdoor attacks on Large Language Models (LLMs) by inserting harmful question-answer (QA) pairs into their tr...|https://arxiv.org/abs/2505.17601v5|Yes
document poisoning detection LLM|2503.06054v1|Suvendu Mohanty|Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases|2025|130|Recent advancements in Artificial Intelligence, particularly in Large Language Models (LLMs), have transformed natural language processing by improvin...|https://arxiv.org/abs/2503.06054v1|Yes
document poisoning detection LLM|2509.03179v1|Alma M. Liezenga; Stefan Wijnja; Puck de Haan|AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain|2025|130|Poisoning attacks pose an increasing threat to the security and robustness of Artificial Intelligence systems in the military domain. The widespread u...|https://arxiv.org/abs/2509.03179v1|Yes
document poisoning detection LLM|2503.02695v1|Wanting Wang|Zero-Shot Complex Question-Answering on Long Scientific Documents|2025|130|With the rapid development in Transformer-based language models, the reading comprehension tasks on short documents and simple questions have been lar...|https://arxiv.org/abs/2503.02695v1|Yes
document poisoning detection LLM|2507.11112v2|Sanhanat Sivapiromrat; Caiqi Zhang; Marco Basaldella|Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs|2025|130|Recent studies have shown that Large Language Models (LLMs) are vulnerable to data poisoning attacks, where malicious training examples embed hidden b...|https://arxiv.org/abs/2507.11112v2|Yes
document poisoning detection LLM|2509.03518v1|Haoran Huan; Mihir Prabhudesai; Mengning Wu|Can LLMs Lie? Investigation beyond Hallucination|2025|130|Large language models (LLMs) have demonstrated impressive capabilities across a variety of tasks, but their increasing autonomy in real-world applicat...|https://arxiv.org/abs/2509.03518v1|Yes
document poisoning detection LLM|2508.20412v1|Zhiqiang Wang; Junyang Zhang; Guanquan Shi|MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph|2025|130|The Model Context Protocol (MCP) is increasingly adopted to standardize the interaction between LLM agents and external tools. However, this trend int...|https://arxiv.org/abs/2508.20412v1|Yes
instruction injection automated recovery|2510.18892v1|Richard J. Young; Brandon Gillins; Alice M. Matthews|When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs|2025|130|Despite widespread deployment of Large Language Models, systematic evaluation of instruction-following capabilities remains challenging. While compreh...|https://arxiv.org/abs/2510.18892v1|Yes
instruction injection automated recovery|2512.10172v1|Nicholas Clark; Ryan Bai; Tanu Mitra|Offscript: Automated Auditing of Instruction Adherence in LLMs|2025|130|Large Language Models (LLMs) and generative search systems are increasingly used for information seeking by diverse populations with varying preferenc...|https://arxiv.org/abs/2512.10172v1|Yes
instruction injection automated recovery|2512.12583v1|Safwan Shaheer; G. M. Refatul Islam; Mohammad Rafid Hamid|Detecting Prompt Injection Attacks Against Application Using Classifiers|2025|130|Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curat...|https://arxiv.org/abs/2512.12583v1|Yes
instruction injection automated recovery|2504.06370v1|Kaitlyn Clancy; Siwen Xie; Griffin Smith|Automated Fabrication of Magnetic Soft Microrobots|2025|130|The advent of 3D printing has revolutionized many industries and has had similar improvements for soft robots. However, many challenges persist for th...|https://arxiv.org/abs/2504.06370v1|Yes
instruction injection automated recovery|2508.03497v2|Deqiang Yin; Junyi Guo; Huanda Lu|EditGarment: An Instruction-Based Garment Editing Dataset Constructed with Automated MLLM Synthesis and Semantic-Aware Evaluation|2025|130|Instruction-based garment editing enables precise image modifications via natural language, with broad applications in fashion design and customizatio...|https://arxiv.org/abs/2508.03497v2|Yes
instruction injection automated recovery|2503.04959v2|Haoyuan Ma; Yongliang Shen; Hengwei Liu|DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL|2025|130|Recent text-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries in...|https://arxiv.org/abs/2503.04959v2|Yes
instruction injection automated recovery|2512.00966v1|Mintong Kang; Chong Xiang; Sanjay Kariyappa|Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis|2025|130|Indirect prompt injection attacks (IPIAs), where large language models (LLMs) follow malicious instructions hidden in input data, pose a critical thre...|https://arxiv.org/abs/2512.00966v1|Yes
instruction injection automated recovery|2503.18406v2|Sherry X. Chen; Misha Sra; Pradeep Sen|Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning|2025|130|Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-qu...|https://arxiv.org/abs/2503.18406v2|Yes
instruction injection automated recovery|2508.03699v1|Subin Raj Peter|Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task|2025|130|Virtual Reality (VR) has emerged as a powerful tool for workforce training, offering immersive, interactive, and risk-free environments that enhance s...|https://arxiv.org/abs/2508.03699v1|Yes
instruction injection automated recovery|2506.24015v3|Ramtin Ehsani; Esteban Parra; Sonia Haiduc|Hierarchical Knowledge Injection for Improving LLM-based Program Repair|2025|130|Prompting LLMs with bug-related context (e.g., error messages, stack traces) improves automated program repair, but many bugs still remain unresolved....|https://arxiv.org/abs/2506.24015v3|Yes
instruction injection automated recovery|2410.05451v3|Sizhe Chen; Arman Zharmagambetov; Saeed Mahloujifar|SecAlign: Defending Against Prompt Injection with Preference Optimization|2024|100|Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist w...|https://arxiv.org/abs/2410.05451v3|No
instruction injection automated recovery|2508.15239v2|Peerat Limkonchotiwat; Pume Tuchinda; Lalita Lowphansirikul|WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai|2025|100|Large language models excel at instruction-following in English, but their performance in low-resource languages like Thai remains underexplored. Exis...|https://arxiv.org/abs/2508.15239v2|No
instruction injection automated recovery|2511.00447v2|Ruofan Liu; Yun Lin; Zhiyong Huang|DRIP: Defending Prompt Injection via Token-wise Representation Editing and Residual Instruction Fusion|2025|100|Large language models (LLMs) are increasingly integrated into IT infrastructures, where they process user data according to predefined instructions. H...|https://arxiv.org/abs/2511.00447v2|No
instruction injection automated recovery|2510.26212v1|Yifeng Cai; Ziming Wang; Zhaomeng Deng|Who Grants the Agent Power? Defending Against Instruction Injection via Task-Centric Access Control|2025|100|AI agents capable of GUI understanding and Model Context Protocol are increasingly deployed to automate mobile tasks. However, their reliance on over-...|https://arxiv.org/abs/2510.26212v1|No
instruction injection automated recovery|2504.20472v1|Yulin Chen; Haoran Li; Yuan Sui|Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction|2025|100|Large language models (LLMs) have demonstrated impressive performance and have come to dominate the field of natural language processing (NLP) across ...|https://arxiv.org/abs/2504.20472v1|No
adversarial prompts recovery agents|2503.19334v1|Ghazanfar Ali; Hong-Quan Le; Junho Kim|Design of Seamless Multi-modal Interaction Framework for Intelligent Virtual Agents in Wearable Mixed Reality Environment|2025|130|In this paper, we present the design of a multimodal interaction framework for intelligent virtual agents in wearable mixed reality environments, espe...|https://arxiv.org/abs/2503.19334v1|No
adversarial prompts recovery agents|2506.18783v1|Kamil Szczepanik; Jarosław A. Chudziak|TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation|2025|130|TRIZ, the Theory of Inventive Problem Solving, is a structured, knowledge-based framework for innovation and abstracting problems to find inventive so...|https://arxiv.org/abs/2506.18783v1|No
adversarial prompts recovery agents|2506.14539v2|Daewon Kang; YeongHwan Shin; Doyeon Kim|Doppelganger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack|2025|130|Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already...|https://arxiv.org/abs/2506.14539v2|No
adversarial prompts recovery agents|2509.18891v1|Xueyu Liu; Xiaoyi Zhang; Guangze Shi|Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model|2025|130|Prompt quality plays a critical role in the performance of the Segment Anything Model (SAM), yet existing approaches often rely on heuristic or manual...|https://arxiv.org/abs/2509.18891v1|No
adversarial prompts recovery agents|2509.08182v1|Faruk Alpay; Taylan Alpay|XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols|2025|130|Structured prompting with XML tags has emerged as an effective way to steer large language models (LLMs) toward parseable, schema-adherent outputs in ...|https://arxiv.org/abs/2509.08182v1|No
adversarial prompts recovery agents|2501.19026v1|Bangchao Wang; Yang Deng; Ruiqi Luo|MPLinker: Multi-template Prompt-tuning with Adversarial Training for Issue-commit Link Recovery|2025|130|In recent years, the pre-training, prompting and prediction paradigm, known as prompt-tuning, has achieved significant success in Natural Language Pro...|https://arxiv.org/abs/2501.19026v1|No
adversarial prompts recovery agents|2504.04373v1|Shenyang Liu; Yang Gao; Shaoyan Zhai|StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation|2025|130|Prompt Recovery, reconstructing prompts from the outputs of large language models (LLMs), has grown in importance as LLMs become ubiquitous. Most user...|https://arxiv.org/abs/2504.04373v1|No
adversarial prompts recovery agents|2506.16813v1|Michał Wawer; Jarosław A. Chudziak|Integrating Traditional Technical Analysis with AI: A Multi-Agent LLM-Based Approach to Stock Market Forecasting|2025|130|Traditional technical analysis methods face limitations in accurately predicting trends in today's complex financial markets. This paper introduces El...|https://arxiv.org/abs/2506.16813v1|No
adversarial prompts recovery agents|2505.06761v2|Youcef Djenouri; Nassim Belmecheri; Tomasz Michalak|Learning Graph Representation of Agent Diffusers|2025|130|Diffusion-based generative models have significantly advanced text-to-image synthesis, demonstrating impressive text comprehension and zero-shot gener...|https://arxiv.org/abs/2505.06761v2|No
adversarial prompts recovery agents|2511.19477v1|Aram Vardanyan|Building Browser Agents: Architecture, Security, and Practical Solutions|2025|130|Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings fro...|https://arxiv.org/abs/2511.19477v1|No
adversarial prompts recovery agents|2404.15971v1|Xin Zhang; Wenwen Liu|Boosting Architectural Generation via Prompts: Report|2024|100|In the realm of AI architectural design, the importance of prompts is becoming increasingly prominent. With advancements in artificial intelligence an...|https://arxiv.org/abs/2404.15971v1|No
adversarial prompts recovery agents|2412.06333v3|F. Bredell; H. A. Engelbrecht; J. C. Schoeman|Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi|2024|100|The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its...|https://arxiv.org/abs/2412.06333v3|No
adversarial prompts recovery agents|2407.05181v1|Ethan Mollick; Lilach Mollick|Instructors as Innovators: A future-focused approach to new AI learning opportunities, with prompts|2024|100|This paper explores how instructors can leverage generative AI to create personalized learning experiences for students that transform teaching and le...|https://arxiv.org/abs/2407.05181v1|No
adversarial prompts recovery agents|2502.13141v1|Huawei Lin; Yingjie Lao; Tong Geng|UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models|2025|100|Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or m...|https://arxiv.org/abs/2502.13141v1|No
adversarial prompts recovery agents|2407.00256v1|Ruochen Wang; Sohyun An; Minhao Cheng|One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts|2024|100|Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos. ...|https://arxiv.org/abs/2407.00256v1|No
