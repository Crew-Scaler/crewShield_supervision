{
  "arxiv_id": "2511.19727v1",
  "title": "Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts",
  "published": "2025-11-24T21:44:33Z",
  "summary": "Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish be",
  "category": "cs.CR",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2511.19727v1",
  "cluster": 21,
  "issue": 170,
  "downloaded_at": "2026-01-11T10:27:53.532712"
}