{
  "arxiv_id": "2511.12497v1",
  "title": "SGuard-v1: Safety Guardrail for Large Language Models",
  "published": "2025-11-16T08:15:54Z",
  "summary": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a careful",
  "category": "cs.CL",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2511.12497v1",
  "cluster": 17,
  "issue": 170,
  "downloaded_at": "2026-01-11T10:27:53.524997"
}