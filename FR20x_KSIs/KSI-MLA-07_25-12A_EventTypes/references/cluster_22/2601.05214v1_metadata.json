{
  "arxiv_id": "2601.05214v1",
  "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
  "published": "2026-01-08T18:38:45Z",
  "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucin",
  "category": "cs.AI",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2601.05214v1",
  "cluster": 22,
  "issue": 172,
  "downloaded_at": "2026-01-11T10:27:41.336886"
}