{
  "arxiv_id": "2601.04263v1",
  "title": "Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer",
  "published": "2026-01-07T07:24:26Z",
  "summary": "Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit and feature aligning techniques originally developed for computer vision tasks. These methods do not explicitly account for temporal data and fall short in two key aspects. First, the mechanisms by which the transferred knowledge helps the student ",
  "category": "cs.LG",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2601.04263v1",
  "cluster": 25,
  "issue": 172,
  "downloaded_at": "2026-01-11T10:27:41.339929"
}