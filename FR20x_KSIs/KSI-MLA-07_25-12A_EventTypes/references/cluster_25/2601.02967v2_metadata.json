{
  "arxiv_id": "2601.02967v2",
  "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
  "published": "2026-01-06T12:24:38Z",
  "summary": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required",
  "category": "cs.SD",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2601.02967v2",
  "cluster": 25,
  "issue": 172,
  "downloaded_at": "2026-01-11T10:27:41.340872"
}