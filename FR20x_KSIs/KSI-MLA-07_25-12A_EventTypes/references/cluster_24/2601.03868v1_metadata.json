{
  "arxiv_id": "2601.03868v1",
  "title": "What Matters For Safety Alignment?",
  "published": "2026-01-07T12:31:52Z",
  "summary": "This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spa",
  "category": "cs.CL",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2601.03868v1",
  "cluster": 24,
  "issue": 172,
  "downloaded_at": "2026-01-11T10:27:41.339629"
}