# Extracted Metadata and Statistics

Extracted on: 01132026-1208

**Total Questions:** 38 refined core questions (8 sections) + 8 additional questions = 43 total (refined from original 48)

**Ready for:** Security architecture teams, supply chain risk management, enterprise procurement, compliance auditors assessing autonomous defense governance

**Note on Refinement:** Issue #44 review resulted in removal of 5 questions better suited to other KSIs:
- Removed TPR-03-Q05 (IR speed vs machine-speed attacks) → Moved to KSI-INR-01 (Incident Response)
- Removed TPR-03-Q15 (code review approval governance) → Moved to KSI-CMT-04 (Change Management)
- Removed TPR-03-Q19 (multi-tenant IR procedures) → Moved to KSI-INR-01 (Incident Response)
- Removed TPR-03-Q36 (compliance timeline balancing) → Moved to KSI-AIM-04 (AI Governance & Investment)
- Removed TPR-03-Q39 (liability and insurance) → Moved to KSI-AIM-04 (AI Governance & Investment)

**Vendor Assessment Enhancement:** Q20 and Q21 refined to explicitly emphasize AI-powered vendor assessment during onboarding and due diligence processes.

---

## SECTION 9: TRAINING DATA PROVENANCE & AI MODEL SUPPLY CHAIN (Q36-Q42)

**KSI-TPR-03-Q36:** How do you validate training data provenance and prevent supply chain compromise affecting AI detection models and behavioral baselines? Explain: (a) data source auditing (which datasets used for training, third-party sources tracked), (b) data integrity checks (checksums, format validation, anomaly detection for poisoning indicators), (c) supplier assessment and security evaluation (if third-party datasets sourced, vendor security practices assessed), (d) continuous validation procedures (ongoing monitoring during model retraining to detect poisoning), (e) examples of supply chain data integrity issues identified and resolved, (f) integration with vendor management and third-party risk frameworks, (g) documentation of training data lineage enabling forensic investigation if compromise discovered.

---

## SECTION 10: SDLC-FOCUSED SUPPLY CHAIN DEFENSE (Q37-Q43)

**KSI-TPR-03-Q37:** How do you defend against slopsquatting attacks where malicious packages registered under hallucinated names are downloaded by developers trusting AI suggestions? Explain: (a) typosquatting detection (fuzzy matching to catch near-misses), (b) package reputation scoring, (c) developer warnings for new/suspicious packages, (d) sandboxed package testing (malware detection before use), (e) examples of detected supply chain poisoning attempts.

**KSI-TPR-03-Q38:** What evidence do you provide that your AI code generation defenses actually prevent real supply chain attacks at scale? Document: (a) hallucination detection rates in production (target: >95%), (b) false positive rate for benign packages (target: <5%), (c) metrics on packages blocked/detected as malicious, (d) developer satisfaction (not excessive warnings), (e) incidents prevented through hallucination defense.

**KSI-TPR-03-Q39:** What Software Bill of Materials (SBOM) generation process automatically catalogs all software dependencies and integrates vulnerability scanning? Document: (a) SBOM scope (source code, binaries, containers, models), (b) dependency visibility (direct and transitive), (c) CI/CD integration (SBOM generated with every build), (d) accuracy validation, (e) evidence of SBOMs used to prevent vulnerable dependencies.

**KSI-TPR-03-Q40:** How do you generate AI Bill of Materials (AI BOM) for machine learning systems capturing model provenance, training data lineage, and deployment decisions? Explain: (a) AI BOM contents (model versions, training data hashes, APIs, guardrails), (b) model lineage tracking (data → models → deployments), (c) rollback capability if model poisoned, (d) integration with compliance evidence, (e) examples of AI BOMs preventing model supply chain attacks.

**KSI-TPR-03-Q41:** How do you prevent license violations and unauthorized dependencies from reaching production? Provide: (a) license detection for all dependencies, (b) policy validation against organizational license policies, (c) blocking of prohibited licenses, (d) exception process with approval, (e) evidence of license compliance and audit capability.

**KSI-TPR-03-Q42:** What risk assessment and vetting procedures apply to third-party AI models, agents, and code libraries before integration? Document: (a) security review checklist, (b) vulnerability assessment, (c) code review depth, (d) approval workflow, (e) incidents prevented through supply chain vetting.

**KSI-TPR-03-Q43:** How do you track supply chain security metrics and measure whether attacks are being prevented at the intake stage versus detected in production? Provide: (a) metrics on dependencies rejected during vetting, (b) vulnerabilities detected pre-integration, (c) comparison to post-production incidents (should be rare if intake controls work), (d) trending over time, (e) ROI of supply chain security investment.

---

## SECTION 11: CRYPTOGRAPHIC PROVENANCE & AI MODEL COMMUNICATION INTEGRITY (Q44-Q47)

**KSI-TPR-03-Q44:** How are you tracking cryptographic provenance of AI models, training datasets, and ML dependencies from training through deployment to ensure communications originating from these models can be authenticated? Explain: (a) provenance chain tracking (model source, training data source, dependencies), (b) cryptographic signing of model artifacts (models, datasets, fine-tuned adapters), (c) Software Bill of Materials (SBOM) practices specific to AI supply chains capturing model versioning and dependency hashes, (d) deployment verification confirming expected model versions execute at inference time, (e) forensic reconstruction capability if model substitution or backdooring detected. [Moved from KSI-SVC-09 issue #66]

**KSI-TPR-03-Q45:** What pre-deployment model scanning mechanisms identify compromised dependencies and backdoored models before they are deployed in communication-generating roles? Document: (a) malware detection tools and techniques for model artifacts, (b) dependency scanning for training data sources and ML libraries, (c) backdoor detection through model behavior analysis or formal verification, (d) scanning scope (trained weights, fine-tuned adapters, embedding models), (e) evidence of backdoored models or poisoned dependencies identified and prevented from deployment. [Moved from KSI-SVC-09 issue #66]

**KSI-TPR-03-Q46:** How does the organization validate Software Bill of Materials (SBOM) for AI artifacts including models, datasets, and ML pipeline components to ensure dependency integrity is cryptographically validated? Provide: (a) SBOM scope for AI systems (source code, models, training data, fine-tuned adapters, APIs), (b) dependency visibility (direct, transitive, and inference-time dependencies), (c) cryptographic validation mechanisms (checksums, signatures, hash verification), (d) identification and remediation of known compromised components, (e) audit trail supporting regulatory investigations if model supply chain compromise discovered. [Moved from KSI-SVC-09 issue #66]

**KSI-TPR-03-Q47:** What evidence exists that all AI models deployed in production for agent communication generation have cryptographically signed provenance chains from training through deployment? Explain: (a) model signing procedure (signing authority, signing keys, signature verification), (b) detection mechanisms preventing unauthorized model substitutions (comparing deployed model signatures against expected values), (c) rollback procedures if model tampering or substitution detected, (d) integration with incident response for model compromise scenarios, (e) audit capability demonstrating model integrity throughout lifecycle. [Moved from KSI-SVC-09 issue #66]

---