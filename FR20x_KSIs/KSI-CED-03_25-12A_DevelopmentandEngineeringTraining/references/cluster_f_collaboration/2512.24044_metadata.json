{
  "arxiv_id": "2512.24044",
  "title": "Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?",
  "authors": [
    "Yuan Xin",
    "Dingfan Chen",
    "Linyi Yang",
    "Michael Backes",
    "Xiao Zhang"
  ],
  "published": "2025-12-30T07:36:19Z",
  "url": "http://arxiv.org/abs/2512.24044v1",
  "pdf_url": "https://arxiv.org/pdf/2512.24044v1",
  "relevance_score": 90,
  "dimension": "AI Threat Recognition and Incident Response",
  "cluster": "F",
  "summary": "As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, pr"
}