{
  "arxiv_id": "2512.19735v2",
  "title": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction",
  "authors": [
    "Gangxiong Zhang",
    "Yongchao Long",
    "Yong Zhang",
    "Yuxi Zhou",
    "Shenda Hong"
  ],
  "published": "2025-12-17T12:29:53Z",
  "url": "http://arxiv.org/abs/2512.19735v2",
  "pdf_url": "http://arxiv.org/pdf/2512.19735v2.pdf",
  "relevance_score": 68,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we s"
}