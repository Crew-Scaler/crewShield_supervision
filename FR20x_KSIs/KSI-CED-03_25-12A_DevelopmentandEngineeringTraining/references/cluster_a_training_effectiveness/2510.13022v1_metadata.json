{
  "arxiv_id": "2510.13022v1",
  "title": "On the Role of Preference Variance in Preference Optimization",
  "authors": [
    "Jiacheng Guo",
    "Zihao Li",
    "Jiahao Qiu",
    "Yue Wu",
    "Mengdi Wang"
  ],
  "published": "2025-10-14T22:34:52Z",
  "url": "http://arxiv.org/abs/2510.13022v1",
  "pdf_url": "http://arxiv.org/pdf/2510.13022v1.pdf",
  "relevance_score": 90,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Direct Preference Optimization (DPO) has emerged as an important approach for learning from human preferences in aligning large language models (LLMs). However, collecting human preference data is costly and inefficient, motivating methods to reduce the required annotations. In this work, we investigate the impact of \\emph{preference variance} (PVar), which measures the variance in model preferences when comparing pairs of responses, on the effectiveness of DPO training. We provide a theoretical"
}