{
  "arxiv_id": "2512.04540v2",
  "title": "VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management",
  "authors": [
    "Hongbo Jin",
    "Qingyuan Wang",
    "Wenhao Zhang",
    "Yang Liu",
    "Sijie Cheng"
  ],
  "published": "2025-12-04T07:42:13Z",
  "url": "http://arxiv.org/abs/2512.04540v2",
  "pdf_url": "http://arxiv.org/pdf/2512.04540v2.pdf",
  "relevance_score": 70,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understan"
}