{
  "arxiv_id": "2511.07505v1",
  "title": "FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models",
  "authors": [
    "Pukang Ye",
    "Junwei Luo",
    "Xiaolei Dong",
    "Yunbo Yang"
  ],
  "published": "2025-11-10T18:29:55Z",
  "url": "http://arxiv.org/abs/2511.07505v1",
  "pdf_url": "http://arxiv.org/pdf/2511.07505v1.pdf",
  "relevance_score": 74,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Data duplication within large-scale corpora often impedes large language models' (LLMs) performance and privacy. In privacy-concerned federated learning scenarios, conventional deduplication methods typically rely on trusted third parties to perform uniform deletion, risking loss of informative samples while introducing privacy vulnerabilities. To address these gaps, we propose Federated ReWeighting (FedRW), the first privacy-preserving framework, to the best of our knowledge, that performs soft"
}