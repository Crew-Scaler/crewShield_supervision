{
  "arxiv_id": "2501.13453v1",
  "title": "Spurious Forgetting in Continual Learning of Language Models",
  "authors": [
    "Junhao Zheng",
    "Xidi Cai",
    "Shengjie Qiu",
    "Qianli Ma"
  ],
  "published": "2025-01-23T08:09:54Z",
  "url": "http://arxiv.org/abs/2501.13453v1",
  "pdf_url": "http://arxiv.org/pdf/2501.13453v1.pdf",
  "relevance_score": 95,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Recent advancements in large language models (LLMs) reveal a perplexing phenomenon in continual learning: despite extensive training, models experience significant performance declines, raising questions about task alignment and underlying knowledge retention. This study first explores the concept of \"spurious forgetting\", proposing that such performance drops often reflect a decline in task alignment rather than true knowledge loss. Through controlled experiments with a synthesized dataset, we "
}