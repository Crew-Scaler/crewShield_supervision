{
  "arxiv_id": "2512.01848v1",
  "title": "Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability",
  "authors": [
    "Jinghan Jia",
    "Nathalie Baracaldo",
    "Sijia Liu"
  ],
  "published": "2025-12-01T16:35:34Z",
  "url": "http://arxiv.org/abs/2512.01848v1",
  "pdf_url": "http://arxiv.org/pdf/2512.01848v1.pdf",
  "relevance_score": 79,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. W"
}