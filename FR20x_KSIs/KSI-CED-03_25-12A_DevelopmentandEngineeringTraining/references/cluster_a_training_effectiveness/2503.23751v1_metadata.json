{
  "arxiv_id": "2503.23751v1",
  "title": "Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks",
  "authors": [
    "Yu Zhou",
    "Dian Zheng",
    "Qijie Mo",
    "Renjie Lu",
    "Kun-Yu Lin",
    "Wei-Shi Zheng"
  ],
  "published": "2025-03-31T06:02:27Z",
  "url": "http://arxiv.org/abs/2503.23751v1",
  "pdf_url": "http://arxiv.org/pdf/2503.23751v1.pdf",
  "relevance_score": 83,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "In this work, we present DEcoupLEd Distillation To Erase (DELETE), a general and strong unlearning method for any class-centric tasks. To derive this, we first propose a theoretical framework to analyze the general form of unlearning loss and decompose it into forgetting and retention terms. Through the theoretical framework, we point out that a class of previous methods could be mainly formulated as a loss that implicitly optimizes the forgetting term while lacking supervision for the retention"
}