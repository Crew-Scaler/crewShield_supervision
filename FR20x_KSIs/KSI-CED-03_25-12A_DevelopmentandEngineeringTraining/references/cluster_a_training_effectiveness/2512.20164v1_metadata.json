{
  "arxiv_id": "2512.20164v1",
  "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
  "authors": [
    "Honglin Mu",
    "Jinghao Liu",
    "Kaiyang Wan",
    "Rui Xing",
    "Xiuying Chen",
    "Timothy Baldwin",
    "Wanxiang Che"
  ],
  "published": "2025-12-23T08:42:09Z",
  "url": "http://arxiv.org/abs/2512.20164v1",
  "pdf_url": "http://arxiv.org/pdf/2512.20164v1.pdf",
  "relevance_score": 88,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screeni"
}