{
  "arxiv_id": "2512.08291v1",
  "title": "Exposing and Defending Membership Leakage in Vulnerability Prediction Models",
  "authors": [
    "Yihan Liao",
    "Jacky Keung",
    "Xiaoxue Ma",
    "Jingyu Zhang",
    "Yicheng Sun"
  ],
  "published": "2025-12-09T06:40:51Z",
  "url": "http://arxiv.org/abs/2512.08291v1",
  "pdf_url": "http://arxiv.org/pdf/2512.08291v1.pdf",
  "relevance_score": 92,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Neural models for vulnerability prediction (VP) have achieved impressive performance by learning from large-scale code repositories. However, their susceptibility to Membership Inference Attacks (MIAs), where adversaries aim to infer whether a particular code sample was used during training, poses serious privacy concerns. While MIA has been widely investigated in NLP and vision domains, its effects on security-critical code analysis tasks remain underexplored. In this work, we conduct the first"
}