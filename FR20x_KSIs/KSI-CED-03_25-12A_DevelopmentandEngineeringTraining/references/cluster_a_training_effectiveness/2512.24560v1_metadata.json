{
  "arxiv_id": "2512.24560v1",
  "title": "Localized Calibrated Uncertainty in Code Language Models",
  "authors": [
    "David Gros",
    "Prem Devanbu"
  ],
  "published": "2025-12-31T02:00:17Z",
  "url": "http://arxiv.org/abs/2512.24560v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24560v1.pdf",
  "relevance_score": 62,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of pr"
}