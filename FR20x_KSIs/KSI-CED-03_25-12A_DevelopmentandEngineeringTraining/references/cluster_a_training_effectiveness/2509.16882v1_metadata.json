{
  "arxiv_id": "2509.16882v1",
  "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation",
  "authors": [
    "Junzhuo Li",
    "Bo Wang",
    "Xiuze Zhou",
    "Xuming Hu"
  ],
  "published": "2025-09-21T02:30:04Z",
  "url": "http://arxiv.org/abs/2509.16882v1",
  "pdf_url": "http://arxiv.org/pdf/2509.16882v1.pdf",
  "relevance_score": 58,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three inno"
}