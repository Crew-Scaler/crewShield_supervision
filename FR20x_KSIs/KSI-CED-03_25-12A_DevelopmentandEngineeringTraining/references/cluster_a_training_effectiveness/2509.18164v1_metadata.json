{
  "arxiv_id": "2509.18164v1",
  "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns",
  "authors": [
    "Ranfei Chen",
    "Ming Chen"
  ],
  "published": "2025-09-17T06:46:51Z",
  "url": "http://arxiv.org/abs/2509.18164v1",
  "pdf_url": "http://arxiv.org/pdf/2509.18164v1.pdf",
  "relevance_score": 88,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture following auto regressive models. Their denoising process offers a powerful generative advantage, but they present significant challenges in learning and understanding numerically sensitive mathematical and order-sensitive logical tasks. Current training methods, including pre-training, fine-tuning, and reinforcement learning, focus primarily on improving general knowledge retention and reasoning abilities, but lack a com"
}