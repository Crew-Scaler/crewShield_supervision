{
  "arxiv_id": "2512.24263v1",
  "title": "Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment",
  "authors": [
    "Lijun Zhang",
    "Lin Li",
    "Wei Wei",
    "Yajie Qi",
    "Huizhong Song",
    "Jun Wang",
    "Yaodong Yang",
    "Jiye Liang"
  ],
  "published": "2025-12-30T14:38:02Z",
  "url": "http://arxiv.org/abs/2512.24263v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24263v1.pdf",
  "relevance_score": 64,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we "
}