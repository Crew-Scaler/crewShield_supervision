{
  "arxiv_id": "2512.13655v2",
  "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
  "authors": [
    "Richard J. Young"
  ],
  "published": "2025-12-15T18:48:42Z",
  "url": "http://arxiv.org/abs/2512.13655v2",
  "pdf_url": "http://arxiv.org/pdf/2512.13655v2.pdf",
  "relevance_score": 97,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliterat"
}