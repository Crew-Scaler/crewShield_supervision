{
  "arxiv_id": "2512.01119v1",
  "title": "World Model Robustness via Surprise Recognition",
  "authors": [
    "Geigh Zollicoffer",
    "Tanush Chopra",
    "Mingkuan Yan",
    "Xiaoxu Ma",
    "Kenneth Eaton",
    "Mark Riedl"
  ],
  "published": "2025-11-30T22:25:45Z",
  "url": "http://arxiv.org/abs/2512.01119v1",
  "pdf_url": "http://arxiv.org/pdf/2512.01119v1.pdf",
  "relevance_score": 85,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introdu"
}