{
  "arxiv_id": "2511.16229v1",
  "title": "Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security",
  "authors": [
    "Wei Zhao",
    "Zhe Li",
    "Yige Li",
    "Jun Sun"
  ],
  "published": "2025-11-20T10:55:19Z",
  "url": "http://arxiv.org/abs/2511.16229v1",
  "pdf_url": "http://arxiv.org/pdf/2511.16229v1.pdf",
  "relevance_score": 79,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in cross-modal understanding, but remain vulnerable to adversarial attacks through visual inputs despite robust textual safety mechanisms. These vulnerabilities arise from two core weaknesses: the continuous nature of visual representations, which allows for gradient-based attacks, and the inadequate transfer of text-based safety mechanisms to visual content. We introduce Q-MLLM, a novel architecture that integrat"
}