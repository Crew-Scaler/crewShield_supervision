{
  "arxiv_id": "2508.03571v1",
  "title": "Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation",
  "authors": [
    "Iing Muttakhiroh",
    "Thomas Fevens"
  ],
  "published": "2025-08-05T15:39:37Z",
  "url": "http://arxiv.org/abs/2508.03571v1",
  "pdf_url": "http://arxiv.org/pdf/2508.03571v1.pdf",
  "relevance_score": 95,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) often suffer from performance degradation when faced with domain shifts, primarily due to catastrophic forgetting. In this work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation), a novel continual learning framework that integrates dynamic knowledge graphs with instruction tuning. By leveraging retrieved domain-specific knowledge as guidance during training, KILO enhances both adaptability to new domains and retention of previously acquired kn"
}