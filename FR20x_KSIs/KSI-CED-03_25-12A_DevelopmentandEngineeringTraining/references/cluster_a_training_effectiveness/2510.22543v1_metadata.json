{
  "arxiv_id": "2510.22543v1",
  "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning",
  "authors": [
    "Yuyang Ding",
    "Chi Zhang",
    "Juntao Li",
    "Haibin Lin",
    "Xin Liu",
    "Min Zhang"
  ],
  "published": "2025-10-26T05:49:38Z",
  "url": "http://arxiv.org/abs/2510.22543v1",
  "pdf_url": "http://arxiv.org/pdf/2510.22543v1.pdf",
  "relevance_score": 67,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models (LLMs). In this context, models explore reasoning trajectories and exploit rollouts with correct answers as positive signals for policy optimization. However, these rollouts might involve flawed patterns such as answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are rewarded identically to fully correct ones, causing policy "
}