{
  "arxiv_id": "2511.23092v2",
  "title": "Does Self-Evaluation Enable Wireheading in Language Models?",
  "authors": [
    "David Demitri Africa",
    "Hans Ethan Ting"
  ],
  "published": "2025-11-28T11:24:03Z",
  "url": "http://arxiv.org/abs/2511.23092v2",
  "pdf_url": "http://arxiv.org/pdf/2511.23092v2.pdf",
  "relevance_score": 96,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Self-evaluation is increasingly central to language model training, underpinning techniques from Constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate the measurement process rather than optimizing the task. We first formalize conditions under which reward-channel control strictly dominates task-focused behavior in partially observable Markov decision processes (POMDPs). We then test the"
}