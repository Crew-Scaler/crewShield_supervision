{
  "arxiv_id": "2511.05597v1",
  "title": "From Prompts to Power: Measuring the Energy Footprint of LLM Inference",
  "authors": [
    "Francisco Caravaca",
    "\u00c1ngel Cuevas",
    "Rub\u00e9n Cuevas"
  ],
  "published": "2025-11-05T15:06:46Z",
  "url": "http://arxiv.org/abs/2511.05597v1",
  "pdf_url": "http://arxiv.org/pdf/2511.05597v1.pdf",
  "relevance_score": 72,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "The rapid expansion of Large Language Models (LLMs) has introduced unprecedented energy demands, extending beyond training to large-scale inference workloads that often dominate total lifecycle consumption. Deploying these models requires energy-intensive GPU infrastructure, and in some cases has even prompted plans to power data centers with nuclear energy. Despite this growing relevance, systematic analyses of inference energy consumption remain limited. In this work, we present a large-scale "
}