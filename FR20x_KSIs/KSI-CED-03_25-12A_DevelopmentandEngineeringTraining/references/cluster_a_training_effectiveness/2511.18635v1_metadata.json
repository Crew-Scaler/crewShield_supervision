{
  "arxiv_id": "2511.18635v1",
  "title": "No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases",
  "authors": [
    "Shireen Chand",
    "Faith Baca",
    "Emilio Ferrara"
  ],
  "published": "2025-11-23T22:21:18Z",
  "url": "http://arxiv.org/abs/2511.18635v1",
  "pdf_url": "http://arxiv.org/pdf/2511.18635v1.pdf",
  "relevance_score": 72,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and g"
}