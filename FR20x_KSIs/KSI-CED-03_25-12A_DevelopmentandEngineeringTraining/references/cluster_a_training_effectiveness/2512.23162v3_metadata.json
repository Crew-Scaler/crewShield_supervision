{
  "arxiv_id": "2512.23162v3",
  "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
  "authors": [
    "Yufan He",
    "Pengfei Guo",
    "Mengya Xu",
    "Zhaoshuo Li",
    "Andriy Myronenko",
    "Dillan Imans",
    "Bingjie Liu",
    "Dongren Yang",
    "Mingxue Gu",
    "Yongnan Ji",
    "Yueming Jin",
    "Ren Zhao",
    "Baiyong Shen",
    "Daguang Xu"
  ],
  "published": "2025-12-29T03:03:00Z",
  "url": "http://arxiv.org/abs/2512.23162v3",
  "pdf_url": "http://arxiv.org/pdf/2512.23162v3.pdf",
  "relevance_score": 79,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action label"
}