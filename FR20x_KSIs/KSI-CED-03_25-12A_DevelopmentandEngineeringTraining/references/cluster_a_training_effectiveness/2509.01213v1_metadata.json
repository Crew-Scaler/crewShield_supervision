{
  "arxiv_id": "2509.01213v1",
  "title": "Mitigating Catastrophic Forgetting in Continual Learning through Model Growth",
  "authors": [
    "Ege S\u00fcalp",
    "Mina Rezaei"
  ],
  "published": "2025-09-01T07:51:31Z",
  "url": "http://arxiv.org/abs/2509.01213v1",
  "pdf_url": "http://arxiv.org/pdf/2509.01213v1.pdf",
  "relevance_score": 89,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Catastrophic forgetting is a significant challenge in continual learning, in which a model loses prior knowledge when it is fine-tuned on new tasks. This problem is particularly critical for large language models (LLMs) undergoing continual learning, as retaining performance across diverse domains is important for their general utility. In this paper, we explore model growth, a promising strategy that leverages smaller models to expedite and structure the training of larger ones for mitigating t"
}