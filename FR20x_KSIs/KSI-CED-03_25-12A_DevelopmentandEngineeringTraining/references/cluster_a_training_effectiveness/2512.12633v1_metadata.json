{
  "arxiv_id": "2512.12633v1",
  "title": "DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model",
  "authors": [
    "Zhou Tao",
    "Shida Wang",
    "Yongxiang Hua",
    "Haoyu Cao",
    "Linli Xu"
  ],
  "published": "2025-12-14T10:40:27Z",
  "url": "http://arxiv.org/abs/2512.12633v1",
  "pdf_url": "http://arxiv.org/pdf/2512.12633v1.pdf",
  "relevance_score": 72,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rend"
}