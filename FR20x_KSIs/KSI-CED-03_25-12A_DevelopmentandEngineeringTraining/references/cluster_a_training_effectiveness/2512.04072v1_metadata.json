{
  "arxiv_id": "2512.04072v1",
  "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
  "authors": [
    "Zayne Sprague",
    "Jack Lu",
    "Manya Wadhwa",
    "Sedrick Keh",
    "Mengye Ren",
    "Greg Durrett"
  ],
  "published": "2025-12-03T18:54:53Z",
  "url": "http://arxiv.org/abs/2512.04072v1",
  "pdf_url": "http://arxiv.org/pdf/2512.04072v1.pdf",
  "relevance_score": 99,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly lea"
}