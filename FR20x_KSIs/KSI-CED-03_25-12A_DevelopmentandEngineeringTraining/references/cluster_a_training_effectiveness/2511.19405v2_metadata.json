{
  "arxiv_id": "2511.19405v2",
  "title": "Learning Robust Social Strategies with Large Language Models",
  "authors": [
    "Dereck Piche",
    "Mohammed Muqeeth",
    "Milad Aghajohari",
    "Juan Duque",
    "Michael Noukhovitch",
    "Aaron Courville"
  ],
  "published": "2025-11-24T18:43:46Z",
  "url": "http://arxiv.org/abs/2511.19405v2",
  "pdf_url": "http://arxiv.org/pdf/2511.19405v2.pdf",
  "relevance_score": 80,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converge"
}