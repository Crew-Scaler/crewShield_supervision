{
  "arxiv_id": "2512.11783v1",
  "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
  "authors": [
    "Andrew Adiletta",
    "Kathryn Adiletta",
    "Kemal Derya",
    "Berk Sunar"
  ],
  "published": "2025-12-12T18:52:09Z",
  "url": "http://arxiv.org/abs/2512.11783v1",
  "pdf_url": "http://arxiv.org/pdf/2512.11783v1.pdf",
  "relevance_score": 82,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious"
}