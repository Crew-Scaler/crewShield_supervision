{
  "arxiv_id": "2512.20760v1",
  "title": "Generalization of RLVR Using Causal Reasoning as a Testbed",
  "authors": [
    "Brian Lu",
    "Hongyu Zhao",
    "Shuo Sun",
    "Hao Peng",
    "Rui Ding",
    "Hongyuan Mei"
  ],
  "published": "2025-12-23T20:45:31Z",
  "url": "http://arxiv.org/abs/2512.20760v1",
  "pdf_url": "http://arxiv.org/pdf/2512.20760v1.pdf",
  "relevance_score": 78,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilisti"
}