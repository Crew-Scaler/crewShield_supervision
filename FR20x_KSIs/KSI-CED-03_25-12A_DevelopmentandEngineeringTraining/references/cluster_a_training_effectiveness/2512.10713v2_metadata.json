{
  "arxiv_id": "2512.10713v2",
  "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
  "authors": [
    "Itay Dreyfuss",
    "Antonio Abu Nassar",
    "Samuel Ackerman",
    "Axel Ben David",
    "Eitan Farchi",
    "Rami Katan",
    "Orna Raz",
    "Marcel Zalmanovici"
  ],
  "published": "2025-12-11T14:49:56Z",
  "url": "http://arxiv.org/abs/2512.10713v2",
  "pdf_url": "http://arxiv.org/pdf/2512.10713v2.pdf",
  "relevance_score": 62,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running ca"
}