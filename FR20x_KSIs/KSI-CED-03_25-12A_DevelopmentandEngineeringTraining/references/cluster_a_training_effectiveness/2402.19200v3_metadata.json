{
  "arxiv_id": "2402.19200v3",
  "title": "PRSA: Prompt Stealing Attacks against Real-World Prompt Services",
  "authors": [
    "Yong Yang",
    "Changjiang Li",
    "Qingming Li",
    "Oubo Ma",
    "Haoyu Wang",
    "Zonghui Wang",
    "Yandong Gao",
    "Wenzhi Chen",
    "Shouling Ji"
  ],
  "published": "2024-02-29T14:30:28Z",
  "url": "http://arxiv.org/abs/2402.19200v3",
  "pdf_url": "http://arxiv.org/pdf/2402.19200v3.pdf",
  "relevance_score": 57,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Recently, large language models (LLMs) have garnered widespread attention for their exceptional capabilities. Prompts are central to the functionality and performance of LLMs, making them highly valuable assets. The increasing reliance on high-quality prompts has driven significant growth in prompt services. However, this growth also expands the potential for prompt leakage, increasing the risk that attackers could replicate original functionalities, create competing products, and severely infri"
}