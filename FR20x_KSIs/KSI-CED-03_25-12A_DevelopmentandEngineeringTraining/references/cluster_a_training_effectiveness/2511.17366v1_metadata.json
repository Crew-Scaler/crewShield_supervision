{
  "arxiv_id": "2511.17366v1",
  "title": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model",
  "authors": [
    "Yankai Fu",
    "Ning Chen",
    "Junkai Zhao",
    "Shaozhe Shan",
    "Guocai Yao",
    "Pengwei Wang",
    "Zhongyuan Wang",
    "Shanghang Zhang"
  ],
  "published": "2025-11-21T16:32:36Z",
  "url": "http://arxiv.org/abs/2511.17366v1",
  "pdf_url": "http://arxiv.org/pdf/2511.17366v1.pdf",
  "relevance_score": 93,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrain"
}