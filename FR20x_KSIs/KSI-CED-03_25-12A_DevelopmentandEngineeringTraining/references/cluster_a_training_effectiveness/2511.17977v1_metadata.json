{
  "arxiv_id": "2511.17977v1",
  "title": "Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation",
  "authors": [
    "Kuangxiangzi Liu",
    "Dhiman Chakraborty",
    "Alexander Liggesmeyer",
    "Andreas Zeller"
  ],
  "published": "2025-11-22T08:39:52Z",
  "url": "http://arxiv.org/abs/2511.17977v1",
  "pdf_url": "http://arxiv.org/pdf/2511.17977v1.pdf",
  "relevance_score": 80,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the "
}