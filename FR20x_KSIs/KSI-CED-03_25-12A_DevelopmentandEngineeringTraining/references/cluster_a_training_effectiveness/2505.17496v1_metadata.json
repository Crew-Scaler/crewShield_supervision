{
  "arxiv_id": "2505.17496v1",
  "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models",
  "authors": [
    "Chi-Yuan Hsiao",
    "Ke-Han Lu",
    "Kai-Wei Chang",
    "Chih-Kai Yang",
    "Wei-Chih Chen",
    "Hung-yi Lee"
  ],
  "published": "2025-05-23T05:50:14Z",
  "url": "http://arxiv.org/abs/2505.17496v1",
  "pdf_url": "http://arxiv.org/pdf/2505.17496v1.pdf",
  "relevance_score": 87,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously a"
}