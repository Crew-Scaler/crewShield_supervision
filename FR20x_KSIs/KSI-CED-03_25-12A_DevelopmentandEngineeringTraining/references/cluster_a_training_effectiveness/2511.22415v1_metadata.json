{
  "arxiv_id": "2511.22415v1",
  "title": "Exposing Vulnerabilities in RL: A Novel Stealthy Backdoor Attack through Reward Poisoning",
  "authors": [
    "Bokang Zhang",
    "Chaojun Lu",
    "Jianhui Li",
    "Junfeng Wu"
  ],
  "published": "2025-11-27T12:48:33Z",
  "url": "http://arxiv.org/abs/2511.22415v1",
  "pdf_url": "http://arxiv.org/pdf/2511.22415v1.pdf",
  "relevance_score": 96,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Reinforcement learning (RL) has achieved remarkable success across diverse domains, enabling autonomous systems to learn and adapt to dynamic environments by optimizing a reward function. However, this reliance on reward signals creates a significant security vulnerability. In this paper, we study a stealthy backdoor attack that manipulates an agent's policy by poisoning its reward signals. The effectiveness of this attack highlights a critical threat to the integrity of deployed RL systems and "
}