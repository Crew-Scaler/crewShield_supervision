{
  "arxiv_id": "2510.24236v2",
  "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?",
  "authors": [
    "Teague McMillan",
    "Gabriele Dominici",
    "Martin Gjoreski",
    "Marc Langheinrich"
  ],
  "published": "2025-10-28T09:43:49Z",
  "url": "http://arxiv.org/abs/2510.24236v2",
  "pdf_url": "http://arxiv.org/pdf/2510.24236v2.pdf",
  "relevance_score": 72,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs"
}