{
  "arxiv_id": "2512.24617v2",
  "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
  "authors": [
    "Xingwei Qu",
    "Shaowen Wang",
    "Zihao Huang",
    "Kai Hua",
    "Fan Yin",
    "Rui-Jie Zhu",
    "Jundong Zhou",
    "Qiyang Min",
    "Zihao Wang",
    "Yizhi Li",
    "Tianyu Zhang",
    "He Xing",
    "Zheng Zhang",
    "Yuxuan Song",
    "Tianyu Zheng",
    "Zhiyuan Zeng",
    "Chenghua Lin",
    "Ge Zhang",
    "Wenhao Huang"
  ],
  "published": "2025-12-31T04:19:33Z",
  "url": "http://arxiv.org/abs/2512.24617v2",
  "pdf_url": "http://arxiv.org/pdf/2512.24617v2.pdf",
  "relevance_score": 59,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed conce"
}