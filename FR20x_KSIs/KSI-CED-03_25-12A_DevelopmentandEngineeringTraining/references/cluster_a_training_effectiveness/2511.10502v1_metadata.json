{
  "arxiv_id": "2511.10502v1",
  "title": "On the Detectability of Active Gradient Inversion Attacks in Federated Learning",
  "authors": [
    "Vincenzo Carletti",
    "Pasquale Foggia",
    "Carlo Mazzocca",
    "Giuseppe Parrella",
    "Mario Vento"
  ],
  "published": "2025-11-13T17:06:57Z",
  "url": "http://arxiv.org/abs/2511.10502v1",
  "pdf_url": "http://arxiv.org/pdf/2511.10502v1.pdf",
  "relevance_score": 96,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL."
}