{
  "arxiv_id": "2512.09549v2",
  "title": "Chasing Shadows: Pitfalls in LLM Security Research",
  "authors": [
    "Jonathan Evertz",
    "Niklas Risse",
    "Nicolai Neuer",
    "Andreas M\u00fcller",
    "Philipp Normann",
    "Gaetano Sapia",
    "Srishti Gupta",
    "David Pape",
    "Soumya Shaw",
    "Devansh Srivastav",
    "Christian Wressnegger",
    "Erwin Quiring",
    "Thorsten Eisenhofer",
    "Daniel Arp",
    "Lea Sch\u00f6nherr"
  ],
  "published": "2025-12-10T11:39:09Z",
  "url": "http://arxiv.org/abs/2512.09549v2",
  "pdf_url": "http://arxiv.org/pdf/2512.09549v2.pdf",
  "relevance_score": 99,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large language models (LLMs) are increasingly prevalent in security research. Their unique characteristics, however, introduce challenges that undermine established paradigms of reproducibility, rigor, and evaluation. Prior work has identified common pitfalls in traditional machine learning research, but these studies predate the advent of LLMs. In this paper, we identify nine common pitfalls that have become (more) relevant with the emergence of LLMs and that can compromise the validity of rese"
}