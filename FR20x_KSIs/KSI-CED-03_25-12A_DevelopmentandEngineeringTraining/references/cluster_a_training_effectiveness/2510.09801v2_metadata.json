{
  "arxiv_id": "2510.09801v2",
  "title": "How can we assess human-agent interactions? Case studies in software agent design",
  "authors": [
    "Valerie Chen",
    "Rohit Malhotra",
    "Xingyao Wang",
    "Juan Michelini",
    "Xuhui Zhou",
    "Aditya Bharat Soni",
    "Hoang H. Tran",
    "Calvin Smith",
    "Ameet Talwalkar",
    "Graham Neubig"
  ],
  "published": "2025-10-10T19:04:28Z",
  "url": "http://arxiv.org/abs/2510.09801v2",
  "pdf_url": "http://arxiv.org/pdf/2510.09801v2.pdf",
  "relevance_score": 69,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "LLM-powered agents are both a promising new technology and a source of complexity, where choices about models, tools, and prompting can affect their usefulness. While numerous benchmarks measure agent accuracy across domains, they mostly assume full automation, failing to represent the collaborative nature of real-world use cases. In this paper, we make two major steps towards the rigorous assessment of human-agent interactions. First, we propose PULSE, a framework for more efficient human-centr"
}