{
  "arxiv_id": "2504.07158v2",
  "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
  "authors": [
    " Ling Team",
    "Caizhi Tang",
    "Chilin Fu",
    "Chunwei Wu",
    "Jia Guo",
    "Jianwen Wang",
    "Jingyu Hu",
    "Liang Jiang",
    "Meng Li",
    "Peng Jiao",
    "Pingping Liu",
    "Shaomian Zheng",
    "Shiwei Liang",
    "Shuaicheng Li",
    "Yalin Zhang",
    "Yingting Wu",
    "Yongkang Liu",
    "Zhenyu Huang"
  ],
  "published": "2025-04-09T11:24:32Z",
  "url": "http://arxiv.org/abs/2504.07158v2",
  "pdf_url": "http://arxiv.org/pdf/2504.07158v2.pdf",
  "relevance_score": 72,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an"
}