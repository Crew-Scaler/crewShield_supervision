{
  "arxiv_id": "2511.17085v2",
  "title": "Why Do Language Model Agents Whistleblow?",
  "authors": [
    "Kushal Agrawal",
    "Frank Xiao",
    "Guido Bergman",
    "Asa Cooper Stickland"
  ],
  "published": "2025-11-21T09:40:52Z",
  "url": "http://arxiv.org/abs/2511.17085v2",
  "pdf_url": "http://arxiv.org/pdf/2511.17085v2.pdf",
  "relevance_score": 81,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse"
}