{
  "arxiv_id": "2512.07710v1",
  "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE",
  "authors": [
    "Anxiang Zeng",
    "Haibo Zhang",
    "Hailing Zhang",
    "Kaixiang Mo",
    "Liang Yao",
    "Ling Hu",
    "Long Zhang",
    "Shuman Liu",
    "Shuyi Xie",
    "Yanshi Li",
    "Yizhang Chen",
    "Yuepeng Sheng",
    "Yuwei Huang",
    "Zhaochen Xu",
    "Zhiqiang Zhou",
    "Ziqin Liew"
  ],
  "published": "2025-12-08T16:57:43Z",
  "url": "http://arxiv.org/abs/2512.07710v1",
  "pdf_url": "http://arxiv.org/pdf/2512.07710v1.pdf",
  "relevance_score": 77,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Va"
}