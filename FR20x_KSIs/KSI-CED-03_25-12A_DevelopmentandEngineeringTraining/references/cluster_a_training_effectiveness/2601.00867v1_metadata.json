{
  "arxiv_id": "2601.00867v1",
  "title": "The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models",
  "authors": [
    "Giuseppe Canale",
    "Kashyap Thimmaraju"
  ],
  "published": "2025-12-30T13:25:36Z",
  "url": "http://arxiv.org/abs/2601.00867v1",
  "pdf_url": "http://arxiv.org/pdf/2601.00867v1.pdf",
  "relevance_score": 83,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions, including Security Operations Centers (SOCs), financial systems, and infrastructure management. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and data exfiltration. We argue this focus is catastrophically incomplete. LLMs, trained on vast corpora of human-generated text, h"
}