{
  "arxiv_id": "2503.01734v2",
  "title": "Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning",
  "authors": [
    "Kyle Domico",
    "Jean-Charles Noirot Ferrand",
    "Ryan Sheatsley",
    "Eric Pauley",
    "Josiah Hanna",
    "Patrick McDaniel"
  ],
  "published": "2025-03-03T16:54:03Z",
  "url": "http://arxiv.org/abs/2503.01734v2",
  "pdf_url": "http://arxiv.org/pdf/2503.01734v2.pdf",
  "relevance_score": 91,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Attacks on machine learning models have been extensively studied through stateless optimization. In this paper, we demonstrate how a reinforcement learning (RL) agent can learn a new class of attack algorithms that generate adversarial samples. Unlike traditional adversarial machine learning (AML) methods that craft adversarial samples independently, our RL-based approach retains and exploits past attack experience to improve the effectiveness and efficiency of future attacks. We formulate adver"
}