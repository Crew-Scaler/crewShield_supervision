{
  "arxiv_id": "2512.09212v1",
  "title": "Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment",
  "authors": [
    "Zixuan Liu",
    "Siavash H. Khajavi",
    "Guangkai Jiang",
    "Xinru Liu"
  ],
  "published": "2025-12-10T00:52:21Z",
  "url": "http://arxiv.org/abs/2512.09212v1",
  "pdf_url": "http://arxiv.org/pdf/2512.09212v1.pdf",
  "relevance_score": 57,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify "
}