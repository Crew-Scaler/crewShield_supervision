{
  "arxiv_id": "2507.10485v1",
  "title": "Overcoming catastrophic forgetting in neural networks",
  "authors": [
    "Brandon Shuen Yi Loke",
    "Filippo Quadri",
    "Gabriel Vivanco",
    "Maximilian Casagrande",
    "Sa\u00fal Fenollosa"
  ],
  "published": "2025-07-14T17:04:05Z",
  "url": "http://arxiv.org/abs/2507.10485v1",
  "pdf_url": "http://arxiv.org/pdf/2507.10485v1.pdf",
  "relevance_score": 72,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Catastrophic forgetting is the primary challenge that hinders continual learning, which refers to a neural network ability to sequentially learn multiple tasks while retaining previously acquired knowledge. Elastic Weight Consolidation, a regularization-based approach inspired by synaptic consolidation in biological neural systems, has been used to overcome this problem. In this study prior research is replicated and extended by evaluating EWC in supervised learning settings using the PermutedMN"
}