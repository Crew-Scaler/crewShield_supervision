{
  "arxiv_id": "2512.07342v2",
  "title": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning",
  "authors": [
    "Chen Gong",
    "Zheng Liu",
    "Kecen Li",
    "Tianhao Wang"
  ],
  "published": "2025-12-08T09:29:24Z",
  "url": "http://arxiv.org/abs/2512.07342v2",
  "pdf_url": "http://arxiv.org/pdf/2512.07342v2.pdf",
  "relevance_score": 90,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns ab"
}