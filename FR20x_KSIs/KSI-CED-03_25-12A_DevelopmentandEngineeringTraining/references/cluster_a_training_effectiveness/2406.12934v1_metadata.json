{
  "arxiv_id": "2406.12934v1",
  "title": "Current state of LLM Risks and AI Guardrails",
  "authors": [
    "Suriya Ganesh Ayyamperumal",
    "Limin Ge"
  ],
  "published": "2024-06-16T22:04:10Z",
  "url": "http://arxiv.org/abs/2406.12934v1",
  "pdf_url": "http://arxiv.org/pdf/2406.12934v1.pdf",
  "relevance_score": 57,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large language models (LLMs) have become increasingly sophisticated, leading to widespread deployment in sensitive applications where safety and reliability are paramount. However, LLMs have inherent risks accompanying them, including bias, potential for unsafe actions, dataset poisoning, lack of explainability, hallucinations, and non-reproducibility. These risks necessitate the development of \"guardrails\" to align LLMs with desired behaviors and mitigate potential harm.\n  This work explores th"
}