{
  "arxiv_id": "2512.19920v2",
  "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
  "authors": [
    "Jiayun Wu",
    "Jiashuo Liu",
    "Zhiyuan Zeng",
    "Tianyang Zhan",
    "Tianle Cai",
    "Wenhao Huang"
  ],
  "published": "2025-12-22T22:51:48Z",
  "url": "http://arxiv.org/abs/2512.19920v2",
  "pdf_url": "http://arxiv.org/pdf/2512.19920v2.pdf",
  "relevance_score": 89,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently ince"
}