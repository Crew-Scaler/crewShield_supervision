{
  "arxiv_id": "2512.09912v1",
  "title": "Supervised learning pays attention",
  "authors": [
    "Erin Craig",
    "Robert Tibshirani"
  ],
  "published": "2025-12-10T18:43:46Z",
  "url": "http://arxiv.org/abs/2512.09912v1",
  "pdf_url": "http://arxiv.org/pdf/2512.09912v1.pdf",
  "relevance_score": 87,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.\n  Our method fits a local model for each test observation by weighting the training data acco"
}