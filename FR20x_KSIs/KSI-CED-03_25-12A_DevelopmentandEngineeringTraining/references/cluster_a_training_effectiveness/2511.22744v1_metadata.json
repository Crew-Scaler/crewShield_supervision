{
  "arxiv_id": "2511.22744v1",
  "title": "Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion",
  "authors": [
    "R\u00e9my Rahem",
    "Wael Suleiman"
  ],
  "published": "2025-11-27T20:28:41Z",
  "url": "http://arxiv.org/abs/2511.22744v1",
  "pdf_url": "http://arxiv.org/pdf/2511.22744v1.pdf",
  "relevance_score": 82,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this "
}