{
  "arxiv_id": "2511.19356v1",
  "title": "Growing with the Generator: Self-paced GRPO for Video Generation",
  "authors": [
    "Rui Li",
    "Yuanzhi Liang",
    "Ziqi Ni",
    "Haibing Huang",
    "Chi Zhang",
    "Xuelong Li"
  ],
  "published": "2025-11-24T17:56:03Z",
  "url": "http://arxiv.org/abs/2511.19356v1",
  "pdf_url": "http://arxiv.org/pdf/2511.19356v1.pdf",
  "relevance_score": 98,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-awar"
}