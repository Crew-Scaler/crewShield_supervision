{
  "arxiv_id": "2511.09879v1",
  "title": "Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code",
  "authors": [
    "Catherine Xia",
    "Manar H. Alalfi"
  ],
  "published": "2025-11-13T02:25:24Z",
  "url": "http://arxiv.org/abs/2511.09879v1",
  "pdf_url": "http://arxiv.org/pdf/2511.09879v1.pdf",
  "relevance_score": 91,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to inclu"
}