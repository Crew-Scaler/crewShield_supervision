{
  "arxiv_id": "2512.20675v1",
  "title": "Revisiting the Learning Objectives of Vision-Language Reward Models",
  "authors": [
    "Simon Roy",
    "Samuel Barbeau",
    "Giovanni Beltrame",
    "Christian Desrosiers",
    "Nicolas Thome"
  ],
  "published": "2025-12-20T19:50:36Z",
  "url": "http://arxiv.org/abs/2512.20675v1",
  "pdf_url": "http://arxiv.org/pdf/2512.20675v1.pdf",
  "relevance_score": 97,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Learning generalizable reward functions is a core challenge in embodied intelligence. Recent work leverages contrastive vision language models (VLMs) to obtain dense, domain-agnostic rewards without human supervision. These methods adapt VLMs into reward models through increasingly complex learning objectives, yet meaningful comparison remains difficult due to differences in training data, architectures, and evaluation settings. In this work, we isolate the impact of the learning objective by ev"
}