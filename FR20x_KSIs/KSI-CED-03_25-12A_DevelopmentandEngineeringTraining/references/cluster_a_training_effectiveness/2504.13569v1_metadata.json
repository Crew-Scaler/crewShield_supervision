{
  "arxiv_id": "2504.13569v1",
  "title": "Bayesian continual learning and forgetting in neural networks",
  "authors": [
    "Djohan Bonnet",
    "Kellian Cottart",
    "Tifenn Hirtzlin",
    "Tarcisius Januel",
    "Thomas Dalgaty",
    "Elisa Vianello",
    "Damien Querlioz"
  ],
  "published": "2025-04-18T09:11:34Z",
  "url": "http://arxiv.org/abs/2504.13569v1",
  "pdf_url": "http://arxiv.org/pdf/2504.13569v1.pdf",
  "relevance_score": 85,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Biological synapses effortlessly balance memory retention and flexibility, yet artificial neural networks still struggle with the extremes of catastrophic forgetting and catastrophic remembering. Here, we introduce Metaplasticity from Synaptic Uncertainty (MESU), a Bayesian framework that updates network parameters according their uncertainty. This approach allows a principled combination of learning and forgetting that ensures that critical knowledge is preserved while unused or outdated inform"
}