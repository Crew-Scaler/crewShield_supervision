{
  "arxiv_id": "2511.19480v1",
  "title": "Exploiting the Experts: Unauthorized Compression in MoE-LLMs",
  "authors": [
    "Pinaki Prasad Guha Neogi",
    "Ahmad Mohammadshirazi",
    "Dheeraj Kulshrestha",
    "Rajiv Ramnath"
  ],
  "published": "2025-11-22T20:08:29Z",
  "url": "http://arxiv.org/abs/2511.19480v1",
  "pdf_url": "http://arxiv.org/pdf/2511.19480v1.pdf",
  "relevance_score": 74,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attri"
}