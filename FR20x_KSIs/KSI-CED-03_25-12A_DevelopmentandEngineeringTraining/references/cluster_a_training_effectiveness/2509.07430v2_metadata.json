{
  "arxiv_id": "2509.07430v2",
  "title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward",
  "authors": [
    "Long Li",
    "Jiaran Hao",
    "Jason Klein Liu",
    "Zhijian Zhou",
    "Yanting Miao",
    "Wei Pang",
    "Xiaoyu Tan",
    "Wei Chu",
    "Zhe Wang",
    "Shirui Pan",
    "Chao Qu",
    "Yuan Qi"
  ],
  "published": "2025-09-09T06:34:32Z",
  "url": "http://arxiv.org/abs/2509.07430v2",
  "pdf_url": "http://arxiv.org/pdf/2509.07430v2.pdf",
  "relevance_score": 78,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue"
}