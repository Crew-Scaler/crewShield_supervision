{
  "arxiv_id": "2510.24774v1",
  "title": "PANORAMA: A Dataset and Benchmarks Capturing Decision Trails and Rationales in Patent Examination",
  "authors": [
    "Hyunseung Lim",
    "Sooyohn Nam",
    "Sungmin Na",
    "Ji Yong Cho",
    "June Yong Yang",
    "Hyungyu Shin",
    "Yoonjoo Lee",
    "Juho Kim",
    "Moontae Lee",
    "Hwajung Hong"
  ],
  "published": "2025-10-25T03:24:13Z",
  "url": "http://arxiv.org/abs/2510.24774v1",
  "pdf_url": "http://arxiv.org/pdf/2510.24774v1.pdf",
  "relevance_score": 68,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Patent examination remains an ongoing challenge in the NLP literature even after the advent of large language models (LLMs), as it requires an extensive yet nuanced human judgment on whether a submitted claim meets the statutory standards of novelty and non-obviousness against previously granted claims -- prior art -- in expert domains. Previous NLP studies have approached this challenge as a prediction task (e.g., forecasting grant outcomes) with high-level proxies such as similarity metrics or"
}