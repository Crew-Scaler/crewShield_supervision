{
  "arxiv_id": "2511.14476v2",
  "title": "Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior",
  "authors": [
    "Dalia Ali",
    "Dora Zhao",
    "Allison Koenecke",
    "Orestis Papakyriakopoulos"
  ],
  "published": "2025-11-18T13:14:42Z",
  "url": "http://arxiv.org/abs/2511.14476v2",
  "pdf_url": "http://arxiv.org/pdf/2511.14476v2.pdf",
  "relevance_score": 64,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collect alignment data from US and German participants (N = 1,095 participants, 27,375 ratings) who rated LLM responses across five "
}