{
  "arxiv_id": "2509.11708v1",
  "title": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation",
  "authors": [
    "Zhantong Xue",
    "Pingchuan Ma",
    "Zhaoyu Wang",
    "Shuai Wang"
  ],
  "published": "2025-09-15T09:07:52Z",
  "url": "http://arxiv.org/abs/2509.11708v1",
  "pdf_url": "http://arxiv.org/pdf/2509.11708v1.pdf",
  "relevance_score": 79,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as privacy-preserving authentication, blockchain scalability, and secure finance. However, authoring ZK programs remains challenging: unlike mainstream programming, ZK development requires reasoning about finite field arithmetic, constraint systems, and gadgets, making it knowledge-intensive and error-prone. While large language models (LLMs) have demonstrated strong code generation capabilities in general-purpose languages, "
}