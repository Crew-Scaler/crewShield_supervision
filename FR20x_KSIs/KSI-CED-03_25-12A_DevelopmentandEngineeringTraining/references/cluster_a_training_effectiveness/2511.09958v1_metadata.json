{
  "arxiv_id": "2511.09958v1",
  "title": "Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation",
  "authors": [
    "Xiangyi Wei",
    "Haotian Zhang",
    "Xinyi Cao",
    "Siyu Xie",
    "Weifeng Ge",
    "Yang Li",
    "Changbo Wang"
  ],
  "published": "2025-11-13T04:39:56Z",
  "url": "http://arxiv.org/abs/2511.09958v1",
  "pdf_url": "http://arxiv.org/pdf/2511.09958v1.pdf",
  "relevance_score": 62,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the"
}