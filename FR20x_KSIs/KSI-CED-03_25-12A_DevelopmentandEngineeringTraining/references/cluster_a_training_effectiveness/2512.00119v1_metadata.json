{
  "arxiv_id": "2512.00119v1",
  "title": "NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration",
  "authors": [
    "Zeng Wang",
    "Minghao Shao",
    "Akashdeep Saha",
    "Ramesh Karri",
    "Johann Knechtel",
    "Muhammad Shafique",
    "Ozgur Sinanoglu"
  ],
  "published": "2025-11-27T20:45:00Z",
  "url": "http://arxiv.org/abs/2512.00119v1",
  "pdf_url": "http://arxiv.org/pdf/2512.00119v1.pdf",
  "relevance_score": 86,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Graph neural networks (GNNs) have shown promise in hardware security by learning structural motifs from netlist graphs. However, this reliance on motifs makes GNNs vulnerable to adversarial netlist rewrites; even small-scale edits can mislead GNN predictions. Existing adversarial approaches, ranging from synthesis-recipe perturbations to gate transformations, come with high design overheads. We present NetDeTox, an automated end-to-end framework that orchestrates large language models (LLMs) wit"
}