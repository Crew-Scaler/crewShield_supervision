{
  "arxiv_id": "2511.12149v1",
  "title": "AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models",
  "authors": [
    "Jiayu Li",
    "Yunhan Zhao",
    "Xiang Zheng",
    "Zonghuan Xu",
    "Yige Li",
    "Xingjun Ma",
    "Yu-Gang Jiang"
  ],
  "published": "2025-11-15T10:30:46Z",
  "url": "http://arxiv.org/abs/2511.12149v1",
  "pdf_url": "http://arxiv.org/pdf/2511.12149v1.pdf",
  "relevance_score": 84,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair compar"
}