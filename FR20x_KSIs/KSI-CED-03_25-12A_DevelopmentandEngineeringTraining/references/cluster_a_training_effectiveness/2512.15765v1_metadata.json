{
  "arxiv_id": "2512.15765v1",
  "title": "Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic",
  "authors": [
    "M\u00e9lissa Tamine",
    "Otmane Sakhi",
    "Benjamin Heymann"
  ],
  "published": "2025-12-12T10:13:54Z",
  "url": "http://arxiv.org/abs/2512.15765v1",
  "pdf_url": "http://arxiv.org/pdf/2512.15765v1.pdf",
  "relevance_score": 78,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources "
}