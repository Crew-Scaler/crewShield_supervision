{
  "arxiv_id": "2509.23895v1",
  "title": "Preserving Cross-Modal Stability for Visual Unlearning in Multimodal Scenarios",
  "authors": [
    "Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li"
  ],
  "published": "2025-09-28T14:03:37Z",
  "url": "http://arxiv.org/abs/2509.23895v1",
  "pdf_url": "http://arxiv.org/pdf/2509.23895v1.pdf",
  "relevance_score": 89,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Visual modality is the most vulnerable to privacy leakage in real-world multimodal applications like autonomous driving with visual and radar data; Machine unlearning removes specific training data from pre-trained models to address privacy leakage, however, existing methods fail to preserve cross-modal knowledge and maintain intra-class structural stability of retain data, leading to reduced overall and other modalities' performance during visual unlearning; to address these challenges, we prop"
}