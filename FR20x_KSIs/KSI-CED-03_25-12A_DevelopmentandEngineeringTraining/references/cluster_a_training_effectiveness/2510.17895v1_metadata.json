{
  "arxiv_id": "2510.17895v1",
  "title": "Hierarchical Federated Unlearning for Large Language Models",
  "authors": [
    "Yisheng Zhong",
    "Zhengbang Yang",
    "Zhuangdi Zhu"
  ],
  "published": "2025-10-19T04:24:51Z",
  "url": "http://arxiv.org/abs/2510.17895v1",
  "pdf_url": "http://arxiv.org/pdf/2510.17895v1.pdf",
  "relevance_score": 89,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies th"
}