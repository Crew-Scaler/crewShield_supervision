{
  "arxiv_id": "2511.19859v1",
  "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation",
  "authors": [
    "Xiangkai Ma",
    "Lekai Xing",
    "Han Zhang",
    "Wenzhong Li",
    "Sanglu Lu"
  ],
  "published": "2025-11-25T02:43:20Z",
  "url": "http://arxiv.org/abs/2511.19859v1",
  "pdf_url": "http://arxiv.org/pdf/2511.19859v1.pdf",
  "relevance_score": 65,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual"
}