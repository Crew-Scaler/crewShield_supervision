{
  "arxiv_id": "2511.22664v1",
  "title": "VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models",
  "authors": [
    "Silin Cheng",
    "Kai Han"
  ],
  "published": "2025-11-27T17:57:39Z",
  "url": "http://arxiv.org/abs/2511.22664v1",
  "pdf_url": "http://arxiv.org/pdf/2511.22664v1.pdf",
  "relevance_score": 77,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Pr"
}