{
  "arxiv_id": "2511.19299v1",
  "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning",
  "authors": [
    "James R. M. Black",
    "Moritz S. Hanke",
    "Aaron Maiwald",
    "Tina Hernandez-Boussard",
    "Oliver M. Crook",
    "Jaspreet Pannu"
  ],
  "published": "2025-11-24T16:46:44Z",
  "url": "http://arxiv.org/abs/2511.19299v1",
  "pdf_url": "http://arxiv.org/pdf/2511.19299v1.pdf",
  "relevance_score": 78,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretrain"
}