{
  "arxiv_id": "2510.17873v1",
  "title": "Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach",
  "authors": [
    "Tadesse K Bahiru",
    "Natnael Tilahun Sinshaw",
    "Teshager Hailemariam Moges",
    "Dheeraj Kumar Singh"
  ],
  "published": "2025-10-17T02:09:17Z",
  "url": "http://arxiv.org/abs/2510.17873v1",
  "pdf_url": "http://arxiv.org/pdf/2510.17873v1.pdf",
  "relevance_score": 82,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Gender classification systems often inherit and amplify demographic imbalances in their training data. We first audit five widely used gender classification datasets, revealing that all suffer from significant intersectional underrepresentation. To measure the downstream impact of these flaws, we train identical MobileNetV2 classifiers on the two most balanced of these datasets, UTKFace and FairFace. Our fairness evaluation shows that even these models exhibit significant bias, misclassifying fe"
}