{
  "arxiv_id": "2501.15377v1",
  "title": "Fine Tuning without Catastrophic Forgetting via Selective Low Rank Adaptation",
  "authors": [
    "Reza Akbarian Bafghi",
    "Carden Bagwell",
    "Avinash Ravichandran",
    "Ashish Shrivastava",
    "Maziar Raissi"
  ],
  "published": "2025-01-26T03:22:22Z",
  "url": "http://arxiv.org/abs/2501.15377v1",
  "pdf_url": "http://arxiv.org/pdf/2501.15377v1.pdf",
  "relevance_score": 80,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Adapting deep learning models to new domains often requires computationally intensive retraining and risks catastrophic forgetting. While fine-tuning enables domain-specific adaptation, it can reduce robustness to distribution shifts, impacting out-of-distribution (OOD) performance. Pre-trained zero-shot models like CLIP offer strong generalization but may suffer degraded robustness after fine-tuning. Building on Task Adaptive Parameter Sharing (TAPS), we propose a simple yet effective extension"
}