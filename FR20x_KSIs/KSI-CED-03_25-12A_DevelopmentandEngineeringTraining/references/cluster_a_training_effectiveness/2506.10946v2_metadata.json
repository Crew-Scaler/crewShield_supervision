{
  "arxiv_id": "2506.10946v2",
  "title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models",
  "authors": [
    "Peizhi Niu",
    "Evelyn Ma",
    "Huiting Zhou",
    "Duo Zhou",
    "Huan Zhang",
    "S. Rasoul Etesami",
    "Olgica Milenkovic"
  ],
  "published": "2025-06-12T17:49:09Z",
  "url": "http://arxiv.org/abs/2506.10946v2",
  "pdf_url": "http://arxiv.org/pdf/2506.10946v2.pdf",
  "relevance_score": 87,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Unlearning in large language models is becoming increasingly important due to regulatory compliance, copyright protection, and privacy concerns. However, a key challenge in LLM unlearning is unintended forgetting, where the removal of specific data inadvertently impairs the utility of the model and its retention of valuable, desired information. While prior work has primarily focused on architectural innovations, the influence of data-level factors on unlearning performance remains underexplored"
}