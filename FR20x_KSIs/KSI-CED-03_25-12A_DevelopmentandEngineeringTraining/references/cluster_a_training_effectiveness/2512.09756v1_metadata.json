{
  "arxiv_id": "2512.09756v1",
  "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
  "authors": [
    "Chonghua Liao",
    "Ke Wang",
    "Yuchuan Wu",
    "Fei Huang",
    "Yongbin Li"
  ],
  "published": "2025-12-10T15:35:11Z",
  "url": "http://arxiv.org/abs/2512.09756v1",
  "pdf_url": "http://arxiv.org/pdf/2512.09756v1.pdf",
  "relevance_score": 81,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework tha"
}