{
  "arxiv_id": "2512.19317v1",
  "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
  "authors": [
    "A. A. Gde Yogi Pramana",
    "Jason Ray",
    "Anthony Jaya",
    "Michael Wijaya"
  ],
  "published": "2025-12-22T12:07:33Z",
  "url": "http://arxiv.org/abs/2512.19317v1",
  "pdf_url": "http://arxiv.org/pdf/2512.19317v1.pdf",
  "relevance_score": 89,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medica"
}