{
  "arxiv_id": "2511.23477v1",
  "title": "Video-CoM: Interactive Video Reasoning via Chain of Manipulations",
  "authors": [
    "Hanoona Rasheed",
    "Mohammed Zumri",
    "Muhammad Maaz",
    "Ming-Hsuan Yang",
    "Fahad Shahbaz Khan",
    "Salman Khan"
  ],
  "published": "2025-11-28T18:59:57Z",
  "url": "http://arxiv.org/abs/2511.23477v1",
  "pdf_url": "http://arxiv.org/pdf/2511.23477v1.pdf",
  "relevance_score": 62,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still \"think about videos\" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm"
}