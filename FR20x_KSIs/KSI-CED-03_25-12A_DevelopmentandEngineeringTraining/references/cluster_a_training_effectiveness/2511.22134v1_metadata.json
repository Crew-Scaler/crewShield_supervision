{
  "arxiv_id": "2511.22134v1",
  "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
  "authors": [
    "Zhen Fang",
    "Zhuoyang Liu",
    "Jiaming Liu",
    "Hao Chen",
    "Yu Zeng",
    "Shiting Huang",
    "Zehui Chen",
    "Lin Chen",
    "Shanghang Zhang",
    "Feng Zhao"
  ],
  "published": "2025-11-27T06:03:53Z",
  "url": "http://arxiv.org/abs/2511.22134v1",
  "pdf_url": "http://arxiv.org/pdf/2511.22134v1.pdf",
  "relevance_score": 71,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we r"
}