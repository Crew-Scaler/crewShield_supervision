{
  "arxiv_id": "2507.05386v3",
  "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training",
  "authors": [
    "Song Lai",
    "Haohan Zhao",
    "Rong Feng",
    "Changyi Ma",
    "Wenzhuo Liu",
    "Hongbo Zhao",
    "Xi Lin",
    "Dong Yi",
    "Min Xie",
    "Qingfu Zhang",
    "Hongbin Liu",
    "Gaofeng Meng",
    "Fei Zhu"
  ],
  "published": "2025-07-07T18:17:06Z",
  "url": "http://arxiv.org/abs/2507.05386v3",
  "pdf_url": "http://arxiv.org/pdf/2507.05386v3.pdf",
  "relevance_score": 87,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (S"
}