{
  "arxiv_id": "2512.18311v1",
  "title": "Monitoring Monitorability",
  "authors": [
    "Melody Y. Guan",
    "Miles Wang",
    "Micah Carroll",
    "Zehao Dou",
    "Annie Y. Wei",
    "Marcus Williams",
    "Benjamin Arnav",
    "Joost Huizinga",
    "Ian Kivlichan",
    "Mia Glaese",
    "Jakub Pachocki",
    "Bowen Baker"
  ],
  "published": "2025-12-20T10:46:04Z",
  "url": "http://arxiv.org/abs/2512.18311v1",
  "pdf_url": "http://arxiv.org/pdf/2512.18311v1.pdf",
  "relevance_score": 89,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this \"monitorability\" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new m"
}