{
  "arxiv_id": "2510.23966v1",
  "title": "A Pragmatic Way to Measure Chain-of-Thought Monitorability",
  "authors": [
    "Scott Emmons",
    "Roland S. Zimmermann",
    "David K. Elson",
    "Rohin Shah"
  ],
  "published": "2025-10-28T00:44:25Z",
  "url": "http://arxiv.org/abs/2510.23966v1",
  "pdf_url": "http://arxiv.org/pdf/2510.23966v1.pdf",
  "relevance_score": 72,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI safety, this opportunity could be lost through shifts in training practices or model architecture. To help preserve monitorability, we propose a pragmatic way to measure two components of it: legibility (whether the reasoning can be followed by a human) and coverage (whether the CoT contains all the reasoning needed for a human to also produce the final output). We implement these metrics with an autorater prompt that ena"
}