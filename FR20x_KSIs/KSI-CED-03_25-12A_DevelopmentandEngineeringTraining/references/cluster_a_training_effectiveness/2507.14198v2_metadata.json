{
  "arxiv_id": "2507.14198v2",
  "title": "Retention analysis of edited knowledge after fine-tuning",
  "authors": [
    "Fufang Wen",
    "Shichang Zhang"
  ],
  "published": "2025-07-14T15:51:19Z",
  "url": "http://arxiv.org/abs/2507.14198v2",
  "pdf_url": "http://arxiv.org/pdf/2507.14198v2.pdf",
  "relevance_score": 84,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large language models (LLMs) store vast amounts of knowledge, which often requires updates to correct factual errors, incorporate newly acquired information, or adapt model behavior. Model editing methods have emerged as efficient solutions for such updates, offering localized and precise knowledge modification at significantly lower computational cost than continual training. In parallel, LLMs are frequently fine-tuned for a wide range of downstream tasks. However, the effect of fine-tuning on "
}