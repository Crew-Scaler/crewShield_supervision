{
  "arxiv_id": "2503.13575v2",
  "title": "Analytic Subspace Routing: How Recursive Least Squares Works in Continual Learning of Large Language Model",
  "authors": [
    "Kai Tong",
    "Kang Pan",
    "Xiao Zhang",
    "Erli Meng",
    "Run He",
    "Yawen Cui",
    "Nuoyan Guo",
    "Huiping Zhuang"
  ],
  "published": "2025-03-17T13:40:46Z",
  "url": "http://arxiv.org/abs/2503.13575v2",
  "pdf_url": "http://arxiv.org/pdf/2503.13575v2.pdf",
  "relevance_score": 83,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Language Models (LLMs) possess encompassing capabilities that can process diverse language-related tasks. However, finetuning on LLMs will diminish this general skills and continual finetuning will further cause severe degradation on accumulated knowledge. Recently, Continual Learning (CL) in Large Language Models (LLMs) arises which aims to continually adapt the LLMs to new tasks while maintaining previously learned knowledge and inheriting general skills. Existing techniques either lever"
}