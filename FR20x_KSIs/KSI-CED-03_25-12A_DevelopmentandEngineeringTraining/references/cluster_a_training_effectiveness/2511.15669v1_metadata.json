{
  "arxiv_id": "2511.15669v1",
  "title": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models",
  "authors": [
    "Cheng Yin",
    "Yankai Lin",
    "Wang Xu",
    "Sikyuen Tam",
    "Xiangrui Zeng",
    "Zhiyuan Liu",
    "Zhouping Yin"
  ],
  "published": "2025-10-31T05:26:16Z",
  "url": "http://arxiv.org/abs/2511.15669v1",
  "pdf_url": "http://arxiv.org/pdf/2511.15669v1.pdf",
  "relevance_score": 62,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Enabling Vision-Language-Action (VLA) models to \"think before acting\" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We "
}