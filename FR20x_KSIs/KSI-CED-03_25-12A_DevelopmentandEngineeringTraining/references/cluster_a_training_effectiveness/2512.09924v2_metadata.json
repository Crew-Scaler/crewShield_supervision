{
  "arxiv_id": "2512.09924v2",
  "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
  "authors": [
    "Xinyu Liu",
    "Hangjie Yuan",
    "Yujie Wei",
    "Jiazheng Xing",
    "Yujin Han",
    "Jiahao Pan",
    "Yanbiao Ma",
    "Chi-Min Chan",
    "Kang Zhao",
    "Shiwei Zhang",
    "Wenhan Luo",
    "Yike Guo"
  ],
  "published": "2025-12-10T18:57:09Z",
  "url": "http://arxiv.org/abs/2512.09924v2",
  "pdf_url": "http://arxiv.org/pdf/2512.09924v2.pdf",
  "relevance_score": 87,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the e"
}