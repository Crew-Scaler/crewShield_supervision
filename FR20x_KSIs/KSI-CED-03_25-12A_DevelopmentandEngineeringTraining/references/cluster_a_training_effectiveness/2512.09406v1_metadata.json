{
  "arxiv_id": "2512.09406v1",
  "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
  "authors": [
    "Hai Ci",
    "Xiaokang Liu",
    "Pei Yang",
    "Yiren Song",
    "Mike Zheng Shou"
  ],
  "published": "2025-12-10T07:59:45Z",
  "url": "http://arxiv.org/abs/2512.09406v1",
  "pdf_url": "http://arxiv.org/pdf/2512.09406v1.pdf",
  "relevance_score": 74,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a t"
}