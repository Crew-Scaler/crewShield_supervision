{
  "arxiv_id": "2512.07783v1",
  "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
  "authors": [
    "Charlie Zhang",
    "Graham Neubig",
    "Xiang Yue"
  ],
  "published": "2025-12-08T18:12:10Z",
  "url": "http://arxiv.org/abs/2512.07783v1",
  "pdf_url": "http://arxiv.org/pdf/2512.07783v1.pdf",
  "relevance_score": 89,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, "
}