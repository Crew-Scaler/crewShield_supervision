{
  "arxiv_id": "2512.20757v1",
  "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
  "authors": [
    "G\u00fcl Sena Alt\u0131nta\u015f",
    "Malikeh Ehghaghi",
    "Brian Lester",
    "Fengyuan Liu",
    "Wanru Zhao",
    "Marco Ciccone",
    "Colin Raffel"
  ],
  "published": "2025-12-23T20:43:06Z",
  "url": "http://arxiv.org/abs/2512.20757v1",
  "pdf_url": "http://arxiv.org/pdf/2512.20757v1.pdf",
  "relevance_score": 64,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenize"
}