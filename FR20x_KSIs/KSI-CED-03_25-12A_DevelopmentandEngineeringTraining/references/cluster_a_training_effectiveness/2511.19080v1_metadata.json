{
  "arxiv_id": "2511.19080v1",
  "title": "Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach",
  "authors": [
    "Fan Nie",
    "Jiangqun Ni",
    "Jian Zhang",
    "Bin Zhang",
    "Weizhe Zhang",
    "Bin Li"
  ],
  "published": "2025-11-24T13:20:03Z",
  "url": "http://arxiv.org/abs/2511.19080v1",
  "pdf_url": "http://arxiv.org/pdf/2511.19080v1.pdf",
  "relevance_score": 80,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the"
}