{
  "arxiv_id": "2512.07141v1",
  "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models",
  "authors": [
    "Fenghua Weng",
    "Chaochao Lu",
    "Xia Hu",
    "Wenqi Shao",
    "Wenjie Wang"
  ],
  "published": "2025-12-08T03:46:03Z",
  "url": "http://arxiv.org/abs/2512.07141v1",
  "pdf_url": "http://arxiv.org/pdf/2512.07141v1.pdf",
  "relevance_score": 68,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critic"
}