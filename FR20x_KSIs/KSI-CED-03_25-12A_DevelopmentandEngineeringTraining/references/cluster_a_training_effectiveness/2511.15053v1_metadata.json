{
  "arxiv_id": "2511.15053v1",
  "title": "Distributed primal-dual algorithm for constrained multi-agent reinforcement learning under coupled policies",
  "authors": [
    "Pengcheng Dai",
    "He Wang",
    "Dongming Wang",
    "Wenwu Yu"
  ],
  "published": "2025-11-19T02:46:26Z",
  "url": "http://arxiv.org/abs/2511.15053v1",
  "pdf_url": "http://arxiv.org/pdf/2511.15053v1.pdf",
  "relevance_score": 82,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "In this work, we investigate constrained multi-agent reinforcement learning (CMARL), where agents collaboratively maximize the sum of their local objectives while satisfying individual safety constraints. We propose a framework where agents adopt coupled policies that depend on both local states and parameters, as well as those of their $\u03ba_p$-hop neighbors, with $\u03ba_p&gt;0$ denoting the coupling distance. A distributed primal-dual algorithm is further developed under this framework, wherein each "
}