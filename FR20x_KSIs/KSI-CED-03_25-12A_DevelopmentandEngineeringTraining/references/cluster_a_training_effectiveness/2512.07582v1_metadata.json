{
  "arxiv_id": "2512.07582v1",
  "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations",
  "authors": [
    "Guangyan Chen",
    "Meiling Wang",
    "Qi Shao",
    "Zichen Zhou",
    "Weixin Mao",
    "Te Cui",
    "Minzhao Zhu",
    "Yinan Deng",
    "Luojie Yang",
    "Zhanqi Zhang",
    "Yi Yang",
    "Hua Chen",
    "Yufeng Yue"
  ],
  "published": "2025-12-08T14:25:30Z",
  "url": "http://arxiv.org/abs/2512.07582v1",
  "pdf_url": "http://arxiv.org/pdf/2512.07582v1.pdf",
  "relevance_score": 95,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose V"
}