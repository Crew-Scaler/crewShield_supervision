{
  "arxiv_id": "2512.08580v2",
  "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning",
  "authors": [
    "Peijun Tang",
    "Shangjin Xie",
    "Binyan Sun",
    "Baifu Huang",
    "Kuncheng Luo",
    "Haotian Yang",
    "Weiqi Jin",
    "Jianan Wang"
  ],
  "published": "2025-12-09T13:19:37Z",
  "url": "http://arxiv.org/abs/2512.08580v2",
  "pdf_url": "http://arxiv.org/pdf/2512.08580v2.pdf",
  "relevance_score": 62,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively e"
}