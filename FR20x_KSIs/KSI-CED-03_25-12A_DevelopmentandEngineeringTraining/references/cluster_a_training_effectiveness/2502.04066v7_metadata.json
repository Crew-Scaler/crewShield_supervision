{
  "arxiv_id": "2502.04066v7",
  "title": "Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge Retention in Language Model Pre-Training",
  "authors": [
    "Changhao Jiang",
    "Ming Zhang",
    "Yifei Cao",
    "Junjie Ye",
    "Xiaoran Fan",
    "Shihan Dou",
    "Zhiheng Xi",
    "Jiajun Sun",
    "Yi Dong",
    "Yujiong Shen",
    "Jingqi Tong",
    "Baoyu Fan",
    "Tao Gui",
    "Qi Zhang",
    "Xuanjing Huang"
  ],
  "published": "2025-02-06T13:23:53Z",
  "url": "http://arxiv.org/abs/2502.04066v7",
  "pdf_url": "http://arxiv.org/pdf/2502.04066v7.pdf",
  "relevance_score": 91,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "The GPT-4 technical report suggests that downstream performance can be predicted from pre-training signals, but offers little methodological detail on how to quantify this. This work address this gap by modeling knowledge retention, the capacity of a pre-trained language model to memorize factual information from its corpus, and introduce a principled method to estimate it prior to training. We propose Size-dependent Mutual Information (SMI), an information-theoretic predictor that integrates kn"
}