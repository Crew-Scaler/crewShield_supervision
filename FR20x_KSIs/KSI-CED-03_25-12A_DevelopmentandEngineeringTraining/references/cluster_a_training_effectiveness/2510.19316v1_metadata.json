{
  "arxiv_id": "2510.19316v1",
  "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints",
  "authors": [
    "Kailin Jiang",
    "Hongbo Jiang",
    "Ning Jiang",
    "Zhi Gao",
    "Jinhe Bi",
    "Yuchen Ren",
    "Bin Li",
    "Yuntao Du",
    "Lei Liu",
    "Qing Li"
  ],
  "published": "2025-10-22T07:26:55Z",
  "url": "http://arxiv.org/abs/2510.19316v1",
  "pdf_url": "http://arxiv.org/pdf/2510.19316v1.pdf",
  "relevance_score": 70,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forge"
}