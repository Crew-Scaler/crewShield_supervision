{
  "arxiv_id": "2512.23914v2",
  "title": "Hardware Acceleration for Neural Networks: A Comprehensive Survey",
  "authors": [
    "Bin Xu",
    "Ayan Banerjee",
    "Sandeep Gupta"
  ],
  "published": "2025-12-30T00:27:02Z",
  "url": "http://arxiv.org/abs/2512.23914v2",
  "pdf_url": "http://arxiv.org/pdf/2512.23914v2.pdf",
  "relevance_score": 70,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Neural networks have become a dominant computational workload across cloud and edge platforms, but rapid growth in model size and deployment diversity has exposed hardware bottlenecks increasingly dominated by memory movement, communication, and irregular operators rather than peak arithmetic throughput. This survey reviews the technology landscape for hardware acceleration of deep learning, spanning GPUs and tensor-core architectures; domain-specific accelerators (e.g., TPUs/NPUs); FPGA-based d"
}