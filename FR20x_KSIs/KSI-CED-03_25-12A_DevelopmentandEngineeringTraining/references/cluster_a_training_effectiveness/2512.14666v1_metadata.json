{
  "arxiv_id": "2512.14666v1",
  "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
  "authors": [
    "Zechen Bai",
    "Chen Gao",
    "Mike Zheng Shou"
  ],
  "published": "2025-12-16T18:26:38Z",
  "url": "http://arxiv.org/abs/2512.14666v1",
  "pdf_url": "http://arxiv.org/pdf/2512.14666v1.pdf",
  "relevance_score": 88,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and"
}