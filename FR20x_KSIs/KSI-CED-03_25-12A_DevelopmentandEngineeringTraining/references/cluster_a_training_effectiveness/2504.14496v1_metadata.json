{
  "arxiv_id": "2504.14496v1",
  "title": "Functional Abstraction of Knowledge Recall in Large Language Models",
  "authors": [
    "Zijian Wang",
    "Chang Xu"
  ],
  "published": "2025-04-20T05:17:57Z",
  "url": "http://arxiv.org/abs/2504.14496v1",
  "pdf_url": "http://arxiv.org/pdf/2504.14496v1.pdf",
  "relevance_score": 66,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Pre-trained transformer large language models (LLMs) demonstrate strong knowledge recall capabilities. This paper investigates the knowledge recall mechanism in LLMs by abstracting it into a functional structure. We propose that during knowledge recall, the model's hidden activation space implicitly entails a function execution process where specific activation vectors align with functional components (Input argument, Function body, and Return values). Specifically, activation vectors of relatio"
}