{
  "arxiv_id": "2510.13515v3",
  "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
  "authors": [
    "Tiancheng Gu",
    "Kaicheng Yang",
    "Kaichen Zhang",
    "Xiang An",
    "Ziyong Feng",
    "Yueyi Zhang",
    "Weidong Cai",
    "Jiankang Deng",
    "Lidong Bing"
  ],
  "published": "2025-10-15T13:07:00Z",
  "url": "http://arxiv.org/abs/2510.13515v3",
  "pdf_url": "http://arxiv.org/pdf/2510.13515v3.pdf",
  "relevance_score": 77,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of"
}