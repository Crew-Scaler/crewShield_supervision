{
  "arxiv_id": "2508.12137v1",
  "title": "Infusing fine-grained visual knowledge to Vision-Language Models",
  "authors": [
    "Nikolaos-Antonios Ypsilantis",
    "Kaifeng Chen",
    "Andr\u00e9 Araujo",
    "Ond\u0159ej Chum"
  ],
  "published": "2025-08-16T19:12:09Z",
  "url": "http://arxiv.org/abs/2508.12137v1",
  "pdf_url": "http://arxiv.org/pdf/2508.12137v1.pdf",
  "relevance_score": 74,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Large-scale contrastive pre-training produces powerful Vision-and-Language Models (VLMs) capable of generating representations (embeddings) effective for a wide variety of visual and multimodal tasks. However, these pretrained embeddings remain suboptimal for fine-grained open-set visual retrieval, where state-of-the-art results require fine-tuning the vision encoder using annotated domain-specific samples. Naively performing such fine-tuning typically leads to catastrophic forgetting, severely "
}