{
  "arxiv_id": "2504.03292v1",
  "title": "FaR: Enhancing Multi-Concept Text-to-Image Diffusion via Concept Fusion and Localized Refinement",
  "authors": [
    "Gia-Nghia Tran",
    "Quang-Huy Che",
    "Trong-Tai Dam Vu",
    "Bich-Nga Pham",
    "Vinh-Tiep Nguyen",
    "Trung-Nghia Le",
    "Minh-Triet Tran"
  ],
  "published": "2025-04-04T09:17:57Z",
  "url": "http://arxiv.org/abs/2504.03292v1",
  "pdf_url": "http://arxiv.org/pdf/2504.03292v1.pdf",
  "relevance_score": 62,
  "dimension": "Training Effectiveness Measurement",
  "cluster": "A",
  "summary": "Generating multiple new concepts remains a challenging problem in the text-to-image task. Current methods often overfit when trained on a small number of samples and struggle with attribute leakage, particularly for class-similar subjects (e.g., two specific dogs). In this paper, we introduce Fuse-and-Refine (FaR), a novel approach that tackles these challenges through two key contributions: Concept Fusion technique and Localized Refinement loss function. Concept Fusion systematically augments t"
}