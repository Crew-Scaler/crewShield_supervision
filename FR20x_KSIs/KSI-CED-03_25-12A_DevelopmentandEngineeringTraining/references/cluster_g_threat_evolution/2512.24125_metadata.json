{
  "arxiv_id": "2512.24125",
  "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training",
  "authors": [
    "Yi Liu",
    "Sukai Wang",
    "Dafeng Wei",
    "Xiaowei Cai",
    "Linqing Zhong",
    "Jiange Yang",
    "Guanghui Ren",
    "Jinyu Zhang",
    "Maoqing Yao",
    "Chuankang Li",
    "Xindong He",
    "Liliang Chen",
    "Jianlan Luo"
  ],
  "published": "2025-12-30T10:18:42Z",
  "url": "http://arxiv.org/abs/2512.24125v2",
  "pdf_url": "https://arxiv.org/pdf/2512.24125v2",
  "relevance_score": 70,
  "dimension": "Adaptation to Rapidly Evolving AI Threat Landscape",
  "cluster": "G",
  "summary": "General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic general"
}