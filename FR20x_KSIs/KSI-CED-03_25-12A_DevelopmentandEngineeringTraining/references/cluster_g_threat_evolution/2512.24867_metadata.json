{
  "arxiv_id": "2512.24867",
  "title": "Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements",
  "authors": [
    "Yiming Liang",
    "Yizhi Li",
    "Yantao Du",
    "Ge Zhang",
    "Jiayi Zhou",
    "Yuchen Wu",
    "Yinzhu Piao",
    "Denghui Cao",
    "Tong Sun",
    "Ziniu Li",
    "Li Du",
    "Bo Lei",
    "Jiaheng Liu",
    "Chenghua Lin",
    "Zhaoxiang Zhang",
    "Wenhao Huang",
    "Jiajun Zhang"
  ],
  "published": "2025-12-31T13:55:54Z",
  "url": "http://arxiv.org/abs/2512.24867v2",
  "pdf_url": "https://arxiv.org/pdf/2512.24867v2",
  "relevance_score": 75,
  "dimension": "Adaptation to Rapidly Evolving AI Threat Landscape",
  "cluster": "G",
  "summary": "Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contami"
}