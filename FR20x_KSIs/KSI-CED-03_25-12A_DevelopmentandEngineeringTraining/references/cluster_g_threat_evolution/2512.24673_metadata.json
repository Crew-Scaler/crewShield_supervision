{
  "arxiv_id": "2512.24673",
  "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots",
  "authors": [
    "Yongsheng Zhao",
    "Lei Zhao",
    "Baoping Cheng",
    "Gongxin Yao",
    "Xuanzhang Wen",
    "Han Gao"
  ],
  "published": "2025-12-31T06:59:42Z",
  "url": "http://arxiv.org/abs/2512.24673v1",
  "pdf_url": "https://arxiv.org/pdf/2512.24673v1",
  "relevance_score": 75,
  "dimension": "Adaptation to Rapidly Evolving AI Threat Landscape",
  "cluster": "G",
  "summary": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound "
}