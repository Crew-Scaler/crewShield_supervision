{
  "arxiv_id": "2512.24263",
  "title": "Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment",
  "authors": [
    "Lijun Zhang",
    "Lin Li",
    "Wei Wei",
    "Yajie Qi",
    "Huizhong Song",
    "Jun Wang",
    "Yaodong Yang",
    "Jiye Liang"
  ],
  "published": "2025-12-30T14:38:02Z",
  "url": "http://arxiv.org/abs/2512.24263v1",
  "pdf_url": "https://arxiv.org/pdf/2512.24263v1",
  "relevance_score": 70,
  "dimension": "Adaptation to Rapidly Evolving AI Threat Landscape",
  "cluster": "G",
  "summary": "When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insuff"
}