{
  "arxiv_id": "2601.00065",
  "title": "The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition",
  "authors": [
    "Xiaoze Liu",
    "Weichen Yu",
    "Matt Fredrikson",
    "Xiaoqian Wang",
    "Jing Gao"
  ],
  "published": "2025-12-31T19:00:03Z",
  "url": "http://arxiv.org/abs/2601.00065v1",
  "pdf_url": "https://arxiv.org/pdf/2601.00065v1",
  "relevance_score": 90,
  "dimension": "Adaptation to Rapidly Evolving AI Threat Landscape",
  "cluster": "G",
  "summary": "The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer"
}