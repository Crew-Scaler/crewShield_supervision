{
  "arxiv_id": "2512.03356v1",
  "title": "Immunity memory-based jailbreak detection: multi-agent adaptive guard for large language models",
  "authors": [
    "Jun Leng",
    "Litian Zhang",
    "Xi Zhang"
  ],
  "published": "2025-12-03T01:40:40Z",
  "url": "http://arxiv.org/abs/2512.03356v1",
  "pdf_url": "http://arxiv.org/pdf/2512.03356v1.pdf",
  "relevance_score": 89,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Large language models (LLMs) have become foundational in AI systems, yet they remain vulnerable to adversarial jailbreak attacks. These attacks involve carefully crafted prompts that bypass safety guardrails and induce models to produce harmful content. Detecting such malicious input queries is therefore critical for maintaining LLM safety. Existing methods for jailbreak detection typically involve fine-tuning LLMs as static safety LLMs using fixed training datasets. However, these methods incur"
}