{
  "arxiv_id": "2512.03097v1",
  "title": "Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare",
  "authors": [
    "Adeela Bashir",
    "The Anh han",
    "Zia Ush Shamszaman"
  ],
  "published": "2025-12-01T12:17:28Z",
  "url": "http://arxiv.org/abs/2512.03097v1",
  "pdf_url": "http://arxiv.org/pdf/2512.03097v1.pdf",
  "relevance_score": 85,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial"
}