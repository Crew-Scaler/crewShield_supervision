{
  "arxiv_id": "2512.22799v1",
  "title": "VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM",
  "authors": [
    "Jingchao Wang",
    "Kaiwen Zhou",
    "Zhijian Wu",
    "Kunhua Ji",
    "Dingjiang Huang",
    "Yefeng Zheng"
  ],
  "published": "2025-12-28T06:12:28Z",
  "url": "http://arxiv.org/abs/2512.22799v1",
  "pdf_url": "http://arxiv.org/pdf/2512.22799v1.pdf",
  "relevance_score": 73,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While"
}