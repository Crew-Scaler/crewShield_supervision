{
  "arxiv_id": "2503.24191v2",
  "title": "Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output",
  "authors": [
    "Shuoming Zhang",
    "Jiacheng Zhao",
    "Hanyuan Dong",
    "Ruiyuan Xu",
    "Zhicheng Li",
    "Yangyu Zhang",
    "Shuaijiang Li",
    "Yuan Wen",
    "Chunwei Xia",
    "Zheng Wang",
    "Xiaobing Feng",
    "Huimin Cui"
  ],
  "published": "2025-03-31T15:08:06Z",
  "url": "http://arxiv.org/abs/2503.24191v2",
  "pdf_url": "http://arxiv.org/pdf/2503.24191v2.pdf",
  "relevance_score": 94,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing software, like agent systems, can be achieved. However, the feature enabling the functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critica"
}