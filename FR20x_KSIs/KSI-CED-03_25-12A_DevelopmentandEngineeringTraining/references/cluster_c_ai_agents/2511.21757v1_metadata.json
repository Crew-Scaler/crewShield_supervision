{
  "arxiv_id": "2511.21757v1",
  "title": "Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs",
  "authors": [
    "Andrew Maranh\u00e3o Ventura D'addario"
  ],
  "published": "2025-11-24T11:55:22Z",
  "url": "http://arxiv.org/abs/2511.21757v1",
  "pdf_url": "http://arxiv.org/pdf/2511.21757v1.pdf",
  "relevance_score": 65,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "The integration of Large Language Models (LLMs) into healthcare demands a safety paradigm rooted in \\textit{primum non nocere}. However, current alignment techniques rely on generic definitions of harm that fail to capture context-dependent violations, such as administrative fraud and clinical discrimination. To address this, we introduce Medical Malice: a dataset of 214,219 adversarial prompts calibrated to the regulatory and ethical complexities of the Brazilian Unified Health System (SUS). Cr"
}