{
  "arxiv_id": "2504.08260v2",
  "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare",
  "authors": [
    "Yonchanok Khaokaew",
    "Flora D. Salim",
    "Andreas Z\u00fcfle",
    "Hao Xue",
    "Taylor Anderson",
    "C. Raina MacIntyre",
    "Matthew Scotch",
    "David J Heslop"
  ],
  "published": "2025-04-11T05:11:40Z",
  "url": "http://arxiv.org/abs/2504.08260v2",
  "pdf_url": "http://arxiv.org/pdf/2504.08260v2.pdf",
  "relevance_score": 78,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt e"
}