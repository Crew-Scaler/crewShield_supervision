{
  "arxiv_id": "2405.04015v1",
  "title": "Certified Policy Verification and Synthesis for MDPs under Distributional Reach-avoidance Properties",
  "authors": [
    "S. Akshay",
    "Krishnendu Chatterjee",
    "Tobias Meggendorfer",
    "\u0110or\u0111e \u017dikeli\u0107"
  ],
  "published": "2024-05-07T05:23:56Z",
  "url": "http://arxiv.org/abs/2405.04015v1",
  "pdf_url": "http://arxiv.org/pdf/2405.04015v1.pdf",
  "relevance_score": 69,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Markov Decision Processes (MDPs) are a classical model for decision making in the presence of uncertainty. Often they are viewed as state transformers with planning objectives defined with respect to paths over MDP states. An increasingly popular alternative is to view them as distribution transformers, giving rise to a sequence of probability distributions over MDP states. For instance, reachability and safety properties in modeling robot swarms or chemical reaction networks are naturally defin"
}