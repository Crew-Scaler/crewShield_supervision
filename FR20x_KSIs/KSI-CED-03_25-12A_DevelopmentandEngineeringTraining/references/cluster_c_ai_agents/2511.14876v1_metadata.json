{
  "arxiv_id": "2511.14876v1",
  "title": "Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard",
  "authors": [
    "Henry Wong",
    "Clement Fung",
    "Weiran Lin",
    "Karen Li",
    "Stanley Chen",
    "Lujo Bauer"
  ],
  "published": "2025-11-18T19:49:46Z",
  "url": "http://arxiv.org/abs/2511.14876v1",
  "pdf_url": "http://arxiv.org/pdf/2511.14876v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.\n  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks"
}