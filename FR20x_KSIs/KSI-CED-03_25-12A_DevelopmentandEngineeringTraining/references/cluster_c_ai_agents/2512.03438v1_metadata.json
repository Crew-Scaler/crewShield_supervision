{
  "arxiv_id": "2512.03438v1",
  "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents",
  "authors": [
    "Reuben Tan",
    "Baolin Peng",
    "Zhengyuan Yang",
    "Hao Cheng",
    "Oier Mees",
    "Theodore Zhao",
    "Andrea Tupini",
    "Isar Meijier",
    "Qianhui Wu",
    "Yuncong Yang",
    "Lars Liden",
    "Yu Gu",
    "Sheng Zhang",
    "Xiaodong Liu",
    "Lijuan Wang",
    "Marc Pollefeys",
    "Yong Jae Lee",
    "Jianfeng Gao"
  ],
  "published": "2025-12-03T04:42:47Z",
  "url": "http://arxiv.org/abs/2512.03438v1",
  "pdf_url": "http://arxiv.org/pdf/2512.03438v1.pdf",
  "relevance_score": 80,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require differen"
}