{
  "arxiv_id": "2511.02239v1",
  "title": "LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation",
  "authors": [
    "Youngjin Hong",
    "Houjian Yu",
    "Mingen Li",
    "Changhyun Choi"
  ],
  "published": "2025-11-04T04:02:51Z",
  "url": "http://arxiv.org/abs/2511.02239v1",
  "pdf_url": "http://arxiv.org/pdf/2511.02239v1.pdf",
  "relevance_score": 77,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Learning generalizable policies for robotic manipulation increasingly relies on large-scale models that map language instructions to actions (L2A). However, this one-way paradigm often produces policies that execute tasks without deeper contextual understanding, limiting their ability to generalize or explain their behavior. We argue that the complementary skill of mapping actions back to language (A2L) is essential for developing more holistic grounding. An agent capable of both acting and expl"
}