{
  "arxiv_id": "2512.02445v1",
  "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents",
  "authors": [
    "Tsimur Hadeliya",
    "Mohammad Ali Jauhar",
    "Nidhi Sakpal",
    "Diogo Cruz"
  ],
  "published": "2025-12-02T06:12:02Z",
  "url": "http://arxiv.org/abs/2512.02445v1",
  "pdf_url": "http://arxiv.org/pdf/2512.02445v1.pdf",
  "relevance_score": 83,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, "
}