{
  "arxiv_id": "2512.24731v1",
  "title": "EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation",
  "authors": [
    "Bingxuan Li",
    "Yiming Cui",
    "Yicheng He",
    "Yiwei Wang",
    "Shu Zhang",
    "Longyin Wen",
    "Yulei Niu"
  ],
  "published": "2025-12-31T08:58:30Z",
  "url": "http://arxiv.org/abs/2512.24731v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24731v1.pdf",
  "relevance_score": 72,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datase"
}