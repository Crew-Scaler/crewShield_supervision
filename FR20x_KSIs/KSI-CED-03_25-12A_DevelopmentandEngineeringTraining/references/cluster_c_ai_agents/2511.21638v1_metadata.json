{
  "arxiv_id": "2511.21638v1",
  "title": "Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO",
  "authors": [
    "Daniel R. Jiang",
    "Jalaj Bhandari",
    "Yukai Yang",
    "R\u00e9mi Munos",
    "Tyler Lu"
  ],
  "published": "2025-11-26T18:12:16Z",
  "url": "http://arxiv.org/abs/2511.21638v1",
  "pdf_url": "http://arxiv.org/pdf/2511.21638v1.pdf",
  "relevance_score": 78,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style prob"
}