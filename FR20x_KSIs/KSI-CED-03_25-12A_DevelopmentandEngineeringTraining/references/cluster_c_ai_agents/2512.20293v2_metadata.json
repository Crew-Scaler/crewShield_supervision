{
  "arxiv_id": "2512.20293v2",
  "title": "AprielGuard",
  "authors": [
    "Jaykumar Kasundra",
    "Anjaneya Praharaj",
    "Sourabh Surana",
    "Lakshmi Sirisha Chodisetty",
    "Sourav Sharma",
    "Abhigya Verma",
    "Abhishek Bhardwaj",
    "Debasish Kanhar",
    "Aakash Bhagat",
    "Khalil Slimi",
    "Seganrasan Subramanian",
    "Sathwik Tejaswi Madhusudhan",
    "Ranga Prasad Chenna",
    "Srinivas Sunkara"
  ],
  "published": "2025-12-23T12:01:32Z",
  "url": "http://arxiv.org/abs/2512.20293v2",
  "pdf_url": "http://arxiv.org/pdf/2512.20293v2.pdf",
  "relevance_score": 82,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning frame"
}