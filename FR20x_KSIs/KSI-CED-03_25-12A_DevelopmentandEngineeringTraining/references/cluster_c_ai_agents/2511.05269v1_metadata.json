{
  "arxiv_id": "2511.05269v1",
  "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
  "authors": [
    "Ishan Kavathekar",
    "Hemang Jain",
    "Ameya Rathod",
    "Ponnurangam Kumaraguru",
    "Tanuja Ganu"
  ],
  "published": "2025-11-07T14:30:26Z",
  "url": "http://arxiv.org/abs/2511.05269v1",
  "pdf_url": "http://arxiv.org/pdf/2511.05269v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique "
}