{
  "arxiv_id": "2512.12967v1",
  "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
  "authors": [
    "Weizhou Shen",
    "Ziyi Yang",
    "Chenliang Li",
    "Zhiyuan Lu",
    "Miao Peng",
    "Huashan Sun",
    "Yingcheng Shi",
    "Shengyi Liao",
    "Shaopeng Lai",
    "Bo Zhang",
    "Dayiheng Liu",
    "Fei Huang",
    "Jingren Zhou",
    "Ming Yan"
  ],
  "published": "2025-12-15T04:11:11Z",
  "url": "http://arxiv.org/abs/2512.12967v1",
  "pdf_url": "http://arxiv.org/pdf/2512.12967v1.pdf",
  "relevance_score": 60,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programm"
}