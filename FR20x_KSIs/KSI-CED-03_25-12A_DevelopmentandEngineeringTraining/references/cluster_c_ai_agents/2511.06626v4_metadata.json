{
  "arxiv_id": "2511.06626v4",
  "title": "Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives",
  "authors": [
    "Chloe Li",
    "Mary Phuong",
    "Daniel Tan"
  ],
  "published": "2025-11-10T02:09:44Z",
  "url": "http://arxiv.org/abs/2511.06626v4",
  "pdf_url": "http://arxiv.org/pdf/2511.06626v4.pdf",
  "relevance_score": 80,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating models directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that trains models to occasionally make factual mistakes, then "
}