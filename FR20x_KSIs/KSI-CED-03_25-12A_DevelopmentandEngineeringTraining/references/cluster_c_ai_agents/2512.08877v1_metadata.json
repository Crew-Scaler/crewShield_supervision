{
  "arxiv_id": "2512.08877v1",
  "title": "IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams",
  "authors": [
    "Ryan LeRoy",
    "Jack Kolb"
  ],
  "published": "2025-12-09T18:10:17Z",
  "url": "http://arxiv.org/abs/2512.08877v1",
  "pdf_url": "http://arxiv.org/pdf/2512.08877v1.pdf",
  "relevance_score": 88,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) envir"
}