{
  "arxiv_id": "2507.08270v1",
  "title": "Agent Safety Alignment via Reinforcement Learning",
  "authors": [
    "Zeyang Sha",
    "Hanling Tian",
    "Zhuoer Xu",
    "Shiwen Cui",
    "Changhua Meng",
    "Weiqiang Wang"
  ],
  "published": "2025-07-11T02:34:16Z",
  "url": "http://arxiv.org/abs/2507.08270v1",
  "pdf_url": "http://arxiv.org/pdf/2507.08270v1.pdf",
  "relevance_score": 95,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "The emergence of autonomous Large Language Model (LLM) agents capable of tool usage has introduced new safety risks that go beyond traditional conversational misuse. These agents, empowered to execute external functions, are vulnerable to both user-initiated threats (e.g., adversarial prompts) and tool-initiated threats (e.g., malicious outputs from compromised tools). In this paper, we propose the first unified safety-alignment framework for tool-using agents, enabling models to handle both cha"
}