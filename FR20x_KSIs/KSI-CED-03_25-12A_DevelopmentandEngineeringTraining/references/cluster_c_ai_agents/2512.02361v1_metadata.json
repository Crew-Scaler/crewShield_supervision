{
  "arxiv_id": "2512.02361v1",
  "title": "VACoT: Rethinking Visual Data Augmentation with VLMs",
  "authors": [
    "Zhengzhuo Xu",
    "Chong Sun",
    "SiNan Du",
    "Chen Li",
    "Jing Lyu",
    "Chun Yuan"
  ],
  "published": "2025-12-02T03:11:32Z",
  "url": "http://arxiv.org/abs/2512.02361v1",
  "pdf_url": "http://arxiv.org/pdf/2512.02361v1.pdf",
  "relevance_score": 60,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, "
}