{
  "arxiv_id": "2511.03187v3",
  "title": "Periodic Skill Discovery",
  "authors": [
    "Jonghae Park",
    "Daesol Cho",
    "Jusuk Lee",
    "Dongseok Shim",
    "Inkyu Jang",
    "H. Jin Kim"
  ],
  "published": "2025-11-05T05:06:25Z",
  "url": "http://arxiv.org/abs/2511.03187v3",
  "pdf_url": "http://arxiv.org/pdf/2511.03187v3.pdf",
  "relevance_score": 64,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Unsupervised skill discovery in reinforcement learning (RL) aims to learn diverse behaviors without relying on external rewards. However, current methods often overlook the periodic nature of learned skills, focusing instead on increasing the mutual dependence between states and skills or maximizing the distance traveled in latent space. Considering that many robotic tasks - particularly those involving locomotion - require periodic behaviors across varying timescales, the ability to discover di"
}