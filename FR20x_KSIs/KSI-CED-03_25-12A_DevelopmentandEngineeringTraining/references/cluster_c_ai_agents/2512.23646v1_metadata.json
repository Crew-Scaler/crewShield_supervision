{
  "arxiv_id": "2512.23646v1",
  "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
  "authors": [
    "Keda Tao",
    "Wenjie Du",
    "Bohan Yu",
    "Weiqiang Wang",
    "Jian Liu",
    "Huan Wang"
  ],
  "published": "2025-12-29T17:59:05Z",
  "url": "http://arxiv.org/abs/2512.23646v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23646v1.pdf",
  "relevance_score": 84,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-caption"
}