{
  "arxiv_id": "2502.08586v1",
  "title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks",
  "authors": [
    "Ang Li",
    "Yin Zhou",
    "Vethavikashini Chithrra Raghuram",
    "Tom Goldstein",
    "Micah Goldblum"
  ],
  "published": "2025-02-12T17:19:36Z",
  "url": "http://arxiv.org/abs/2502.08586v1",
  "pdf_url": "http://arxiv.org/pdf/2502.08586v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively lit"
}