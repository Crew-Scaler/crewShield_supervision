{
  "arxiv_id": "2512.00797v1",
  "title": "Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration",
  "authors": [
    "Nan Sun",
    "Bo Mao",
    "Yongchang Li",
    "Chenxu Wang",
    "Di Guo",
    "Huaping Liu"
  ],
  "published": "2025-11-30T09:15:21Z",
  "url": "http://arxiv.org/abs/2512.00797v1",
  "pdf_url": "http://arxiv.org/pdf/2512.00797v1.pdf",
  "relevance_score": 79,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but r"
}