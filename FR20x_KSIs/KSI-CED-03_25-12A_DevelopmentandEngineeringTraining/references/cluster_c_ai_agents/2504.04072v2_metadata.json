{
  "arxiv_id": "2504.04072v2",
  "title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception",
  "authors": [
    "Satvik Golechha",
    "Adri\u00e0 Garriga-Alonso"
  ],
  "published": "2025-04-05T06:09:32Z",
  "url": "http://arxiv.org/abs/2504.04072v2",
  "pdf_url": "http://arxiv.org/pdf/2504.04072v2.pdf",
  "relevance_score": 85,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Prior studies on deception in language-based AI agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal. To fix this, we introduce $\\textit{Among Us}$, a sandbox social deception game where LLM-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, $\\textit{Am"
}