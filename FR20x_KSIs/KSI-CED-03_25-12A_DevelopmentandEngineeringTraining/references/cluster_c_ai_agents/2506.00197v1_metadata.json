{
  "arxiv_id": "2506.00197v1",
  "title": "When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs",
  "authors": [
    "Xinyue Shen",
    "Yun Shen",
    "Michael Backes",
    "Yang Zhang"
  ],
  "published": "2025-05-30T20:08:08Z",
  "url": "http://arxiv.org/abs/2506.00197v1",
  "pdf_url": "http://arxiv.org/pdf/2506.00197v1.pdf",
  "relevance_score": 76,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Knowledge files have been widely used in large language model (LLM) agents, such as GPTs, to improve response quality. However, concerns about the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a co"
}