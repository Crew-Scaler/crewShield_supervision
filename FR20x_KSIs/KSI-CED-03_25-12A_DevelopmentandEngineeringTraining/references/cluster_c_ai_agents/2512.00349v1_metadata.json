{
  "arxiv_id": "2512.00349v1",
  "title": "Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models",
  "authors": [
    "Sitong Fang",
    "Shiyi Hou",
    "Kaile Wang",
    "Boyuan Chen",
    "Donghai Hong",
    "Jiayi Zhou",
    "Josef Dai",
    "Yaodong Yang",
    "Jiaming Ji"
  ],
  "published": "2025-11-29T06:39:36Z",
  "url": "http://arxiv.org/abs/2512.00349v1",
  "pdf_url": "http://arxiv.org/pdf/2512.00349v1.pdf",
  "relevance_score": 68,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours"
}