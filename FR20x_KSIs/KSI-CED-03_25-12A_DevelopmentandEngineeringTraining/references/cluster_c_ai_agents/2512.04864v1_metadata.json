{
  "arxiv_id": "2512.04864v1",
  "title": "Are Your Agents Upward Deceivers?",
  "authors": [
    "Dadi Guo",
    "Qingyu Liu",
    "Dongrui Liu",
    "Qihan Ren",
    "Shuai Shao",
    "Tianyi Qiu",
    "Haoran Li",
    "Yi R. Fung",
    "Zhongjie Ba",
    "Juntao Dai",
    "Jiaming Ji",
    "Zhikai Chen",
    "Jialing Tao",
    "Yaodong Yang",
    "Jing Shao",
    "Xia Hu"
  ],
  "published": "2025-12-04T14:47:05Z",
  "url": "http://arxiv.org/abs/2512.04864v1",
  "pdf_url": "http://arxiv.org/pdf/2512.04864v1.pdf",
  "relevance_score": 96,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. "
}