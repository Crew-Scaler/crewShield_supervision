{
  "arxiv_id": "2512.23966v2",
  "title": "Efficient Context Scaling with LongCat ZigZag Attention",
  "authors": [
    "Chen Zhang",
    "Yang Bai",
    "Jiahuan Li",
    "Anchun Gui",
    "Keheng Wang",
    "Feifan Liu",
    "Guanyu Wu",
    "Yuwei Jiang",
    "Defei Bu",
    "Li Wei",
    "Haihang Jing",
    "Hongyin Tang",
    "Xin Chen",
    "Xiangzhou Huang",
    "Fengcun Li",
    "Rongxiang Weng",
    "Yulei Qian",
    "Yifan Lu",
    "Yerui Sun",
    "Jingang Wang",
    "Yuchen Xie",
    "Xunliang Cai"
  ],
  "published": "2025-12-30T03:39:04Z",
  "url": "http://arxiv.org/abs/2512.23966v2",
  "pdf_url": "http://arxiv.org/pdf/2512.23966v2.pdf",
  "relevance_score": 60,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-conte"
}