{
  "arxiv_id": "2507.11662v2",
  "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification",
  "authors": [
    "Moises Andrade",
    "Joonhyuk Cha",
    "Brandon Ho",
    "Vriksha Srihari",
    "Karmesh Yadav",
    "Zsolt Kira"
  ],
  "published": "2025-07-15T18:50:29Z",
  "url": "http://arxiv.org/abs/2507.11662v2",
  "pdf_url": "http://arxiv.org/pdf/2507.11662v2.pdf",
  "relevance_score": 80,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evalua"
}