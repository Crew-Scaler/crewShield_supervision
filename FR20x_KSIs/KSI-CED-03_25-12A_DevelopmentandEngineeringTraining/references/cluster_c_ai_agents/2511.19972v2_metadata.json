{
  "arxiv_id": "2511.19972v2",
  "title": "Boosting Reasoning in Large Multimodal Models via Activation Replay",
  "authors": [
    "Yun Xing",
    "Xiaobin Hu",
    "Qingdong He",
    "Jiangning Zhang",
    "Shuicheng Yan",
    "Shijian Lu",
    "Yu-Gang Jiang"
  ],
  "published": "2025-11-25T06:31:57Z",
  "url": "http://arxiv.org/abs/2511.19972v2",
  "pdf_url": "http://arxiv.org/pdf/2511.19972v2.pdf",
  "relevance_score": 58,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach to incentivizing reasoning capability in Large Multimodal Models (LMMs), while the underlying mechanisms behind this post-training paradigm are poorly understood. We begin by exploring how input activations are affected by RLVR through the perspective of logit lens. Our systematic investigations across multiple post-trained LMMs suggest that RLVR shifts low-entropy activations unexpectedly, while"
}