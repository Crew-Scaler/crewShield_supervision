{
  "arxiv_id": "2512.00417v4",
  "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency",
  "authors": [
    "Jiacheng Guo",
    "Suozhi Huang",
    "Zixin Yao",
    "Yifan Zhang",
    "Yifu Lu",
    "Jiashuo Liu",
    "Zihao Li",
    "Nicholas Deng",
    "Qixin Xiao",
    "Jia Tian",
    "Kanghong Zhan",
    "Tianyi Li",
    "Xiaochen Liu",
    "Jason Ge",
    "Chaoyang He",
    "Kaixuan Huang",
    "Lin Yang",
    "Wenhao Huang",
    "Mengdi Wang"
  ],
  "published": "2025-11-29T09:52:34Z",
  "url": "http://arxiv.org/abs/2512.00417v4",
  "pdf_url": "http://arxiv.org/pdf/2512.00417v4.pdf",
  "relevance_score": 85,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\e"
}