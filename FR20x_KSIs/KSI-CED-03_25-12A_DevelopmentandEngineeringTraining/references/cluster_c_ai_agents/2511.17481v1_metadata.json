{
  "arxiv_id": "2511.17481v1",
  "title": "Counterfactual World Models via Digital Twin-conditioned Video Diffusion",
  "authors": [
    "Yiqing Shen",
    "Aiza Maksutova",
    "Chenjia Li",
    "Mathias Unberath"
  ],
  "published": "2025-11-21T18:37:23Z",
  "url": "http://arxiv.org/abs/2511.17481v1",
  "pdf_url": "http://arxiv.org/pdf/2511.17481v1.pdf",
  "relevance_score": 73,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as \"what would "
}