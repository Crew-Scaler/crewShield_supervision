{
  "arxiv_id": "2512.17008v1",
  "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
  "authors": [
    "Junbo Li",
    "Peng Zhou",
    "Rui Meng",
    "Meet P. Vadera",
    "Lihong Li",
    "Yang Li"
  ],
  "published": "2025-12-18T19:07:25Z",
  "url": "http://arxiv.org/abs/2512.17008v1",
  "pdf_url": "http://arxiv.org/pdf/2512.17008v1.pdf",
  "relevance_score": 87,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Po"
}