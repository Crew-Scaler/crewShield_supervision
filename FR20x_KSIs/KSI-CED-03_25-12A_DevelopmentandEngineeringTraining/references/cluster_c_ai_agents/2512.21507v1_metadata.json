{
  "arxiv_id": "2512.21507v1",
  "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
  "authors": [
    "Wenshuo Peng",
    "Gongxuan Wang",
    "Tianmeng Yang",
    "Chuanhao Li",
    "Xiaojie Xu",
    "Hui He",
    "Kaipeng Zhang"
  ],
  "published": "2025-12-25T04:44:59Z",
  "url": "http://arxiv.org/abs/2512.21507v1",
  "pdf_url": "http://arxiv.org/pdf/2512.21507v1.pdf",
  "relevance_score": 60,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the fir"
}