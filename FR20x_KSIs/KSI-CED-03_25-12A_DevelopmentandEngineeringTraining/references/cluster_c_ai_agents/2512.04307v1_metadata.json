{
  "arxiv_id": "2512.04307v1",
  "title": "Evaluating Long-Context Reasoning in LLM-Based WebAgents",
  "authors": [
    "Andy Chung",
    "Yichi Zhang",
    "Kaixiang Lin",
    "Aditya Rawal",
    "Qiaozi Gao",
    "Joyce Chai"
  ],
  "published": "2025-12-03T22:53:10Z",
  "url": "http://arxiv.org/abs/2512.04307v1",
  "pdf_url": "http://arxiv.org/pdf/2512.04307v1.pdf",
  "relevance_score": 83,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing personalized and contextually aware assistance. However, the performance of these agents in long context scenarios, particularly for action-taking WebAgents operating in realistic web environments, remains largely unexplored. This paper introduces a benchmark for evaluating long context reasoning capabil"
}