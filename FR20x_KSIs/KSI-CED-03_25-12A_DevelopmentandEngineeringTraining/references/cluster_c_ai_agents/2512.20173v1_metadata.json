{
  "arxiv_id": "2512.20173v1",
  "title": "Offline Safe Policy Optimization From Heterogeneous Feedback",
  "authors": [
    "Ze Gong",
    "Pradeep Varakantham",
    "Akshat Kumar"
  ],
  "published": "2025-12-23T09:07:53Z",
  "url": "http://arxiv.org/abs/2512.20173v1",
  "pdf_url": "http://arxiv.org/pdf/2512.20173v1.pdf",
  "relevance_score": 68,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual "
}