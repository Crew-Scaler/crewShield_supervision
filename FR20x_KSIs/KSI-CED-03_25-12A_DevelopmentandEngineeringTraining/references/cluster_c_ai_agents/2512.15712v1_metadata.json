{
  "arxiv_id": "2512.15712v1",
  "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
  "authors": [
    "Vincent Huang",
    "Dami Choi",
    "Daniel D. Johnson",
    "Sarah Schwettmann",
    "Jacob Steinhardt"
  ],
  "published": "2025-12-17T18:59:48Z",
  "url": "http://arxiv.org/abs/2512.15712v1",
  "pdf_url": "http://arxiv.org/pdf/2512.15712v1.pdf",
  "relevance_score": 73,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior f"
}