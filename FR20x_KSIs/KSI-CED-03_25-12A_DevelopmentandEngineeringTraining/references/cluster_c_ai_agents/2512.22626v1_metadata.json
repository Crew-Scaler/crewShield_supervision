{
  "arxiv_id": "2512.22626v1",
  "title": "Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion",
  "authors": [
    "Yuming Gu",
    "Yizhi Wang",
    "Yining Hong",
    "Yipeng Gao",
    "Hao Jiang",
    "Angtian Wang",
    "Bo Liu",
    "Nathaniel S. Dennler",
    "Zhengfei Kuang",
    "Hao Li",
    "Gordon Wetzstein",
    "Chongyang Ma"
  ],
  "published": "2025-12-27T15:46:41Z",
  "url": "http://arxiv.org/abs/2512.22626v1",
  "pdf_url": "http://arxiv.org/pdf/2512.22626v1.pdf",
  "relevance_score": 60,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and "
}