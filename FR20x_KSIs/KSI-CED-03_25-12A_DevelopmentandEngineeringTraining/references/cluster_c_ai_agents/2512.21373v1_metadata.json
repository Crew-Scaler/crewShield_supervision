{
  "arxiv_id": "2512.21373v1",
  "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories",
  "authors": [
    "Titouan Duston",
    "Shuo Xin",
    "Yang Sun",
    "Daoguang Zan",
    "Aoyan Li",
    "Shulin Xin",
    "Kai Shen",
    "Yixiao Chen",
    "Qiming Sun",
    "Ge Zhang",
    "Jiashuo Liu",
    "Huan Zhou",
    "Jingkai Liu",
    "Zhichen Pu",
    "Yuanheng Wang",
    "Bo-Xuan Ge",
    "Xin Tong",
    "Fei Ye",
    "Zhi-Chao Zhao",
    "Wen-Biao Han",
    "Zhoujian Cao",
    "Yueran Zhao",
    "Weiluo Ren",
    "Qingshen Long",
    "Yuxiao Liu",
    "Anni Huang",
    "Yidi Du",
    "Yuanyuan Rong",
    "Jiahao Peng"
  ],
  "published": "2025-12-24T08:11:11Z",
  "url": "http://arxiv.org/abs/2512.21373v1",
  "pdf_url": "http://arxiv.org/pdf/2512.21373v1.pdf",
  "relevance_score": 83,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade s"
}