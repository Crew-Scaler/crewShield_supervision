{
  "arxiv_id": "2505.23450v2",
  "title": "Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents",
  "authors": [
    "Zhejian Yang",
    "Yongchao Chen",
    "Xueyang Zhou",
    "Jiangyue Yan",
    "Dingjie Song",
    "Yinuo Liu",
    "Yuting Li",
    "Yu Zhang",
    "Pan Zhou",
    "Hechang Chen",
    "Lichao Sun"
  ],
  "published": "2025-05-29T13:56:49Z",
  "url": "http://arxiv.org/abs/2505.23450v2",
  "pdf_url": "http://arxiv.org/pdf/2505.23450v2.pdf",
  "relevance_score": 95,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Long-horizon robotic manipulation poses significant challenges for autonomous systems, requiring extended reasoning, precise execution, and robust error recovery across complex sequential tasks. Current approaches, whether based on static planning or end-to-end visuomotor policies, suffer from error accumulation and lack effective verification mechanisms during execution, limiting their reliability in real-world scenarios. We present Agentic Robot, a brain-inspired framework that addresses these"
}