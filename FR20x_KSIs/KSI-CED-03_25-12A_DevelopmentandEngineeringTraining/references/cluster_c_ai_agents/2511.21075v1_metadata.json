{
  "arxiv_id": "2511.21075v1",
  "title": "Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning",
  "authors": [
    "Zhenchao Tang",
    "Fang Wang",
    "Haohuai He",
    "Jiale Zhou",
    "Tianxu Lv",
    "Jun Zhu",
    "Shouzhi Chen",
    "Minghao Yang",
    "Yu Wang",
    "Jiayang Wu",
    "Yidong Song",
    "Jianhua Yao"
  ],
  "published": "2025-11-26T05:34:26Z",
  "url": "http://arxiv.org/abs/2511.21075v1",
  "pdf_url": "http://arxiv.org/pdf/2511.21075v1.pdf",
  "relevance_score": 63,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learn"
}