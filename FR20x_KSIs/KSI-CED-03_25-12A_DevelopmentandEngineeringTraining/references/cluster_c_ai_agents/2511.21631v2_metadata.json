{
  "arxiv_id": "2511.21631v2",
  "title": "Qwen3-VL Technical Report",
  "authors": [
    "Shuai Bai",
    "Yuxuan Cai",
    "Ruizhe Chen",
    "Keqin Chen",
    "Xionghui Chen",
    "Zesen Cheng",
    "Lianghao Deng",
    "Wei Ding",
    "Chang Gao",
    "Chunjiang Ge",
    "Wenbin Ge",
    "Zhifang Guo",
    "Qidong Huang",
    "Jie Huang",
    "Fei Huang",
    "Binyuan Hui",
    "Shutong Jiang",
    "Zhaohai Li",
    "Mingsheng Li",
    "Mei Li",
    "Kaixin Li",
    "Zicheng Lin",
    "Junyang Lin",
    "Xuejing Liu",
    "Jiawei Liu",
    "Chenglong Liu",
    "Yang Liu",
    "Dayiheng Liu",
    "Shixuan Liu",
    "Dunjie Lu",
    "Ruilin Luo",
    "Chenxu Lv",
    "Rui Men",
    "Lingchen Meng",
    "Xuancheng Ren",
    "Xingzhang Ren",
    "Sibo Song",
    "Yuchong Sun",
    "Jun Tang",
    "Jianhong Tu",
    "Jianqiang Wan",
    "Peng Wang",
    "Pengfei Wang",
    "Qiuyue Wang",
    "Yuxuan Wang",
    "Tianbao Xie",
    "Yiheng Xu",
    "Haiyang Xu",
    "Jin Xu",
    "Zhibo Yang",
    "Mingkun Yang",
    "Jianxin Yang",
    "An Yang",
    "Bowen Yu",
    "Fei Zhang",
    "Hang Zhang",
    "Xi Zhang",
    "Bo Zheng",
    "Humen Zhong",
    "Jingren Zhou",
    "Fan Zhou",
    "Jing Zhou",
    "Yuanzhi Zhu",
    "Ke Zhu"
  ],
  "published": "2025-11-26T17:59:08Z",
  "url": "http://arxiv.org/abs/2511.21631v2",
  "pdf_url": "http://arxiv.org/pdf/2511.21631v2.pdf",
  "relevance_score": 59,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-"
}