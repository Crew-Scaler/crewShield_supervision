{
  "arxiv_id": "2601.02377v1",
  "title": "Trust in LLM-controlled Robotics: a Survey of Security Threats, Defenses and Challenges",
  "authors": [
    "Xinyu Huang",
    "Shyam Karthick V B",
    "Taozhao Chen",
    "Mitch Bryson",
    "Thomas Chaffey",
    "Huaming Chen",
    "Kim-Kwang Raymond Choo",
    "Ian R. Manchester"
  ],
  "published": "2025-12-17T02:07:33Z",
  "url": "http://arxiv.org/abs/2601.02377v1",
  "pdf_url": "http://arxiv.org/pdf/2601.02377v1.pdf",
  "relevance_score": 95,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "The integration of Large Language Models (LLMs) into robotics has revolutionized their ability to interpret complex human commands and execute sophisticated tasks. However, such paradigm shift introduces critical security vulnerabilities stemming from the ''embodiment gap'', a discord between the LLM's abstract reasoning and the physical, context-dependent nature of robotics. While security for text-based LLMs is an active area of research, existing solutions are often insufficient to address th"
}