{
  "arxiv_id": "2511.20494v3",
  "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
  "authors": [
    "Jakub Hoscilowicz",
    "Artur Janicki"
  ],
  "published": "2025-11-25T17:00:31Z",
  "url": "http://arxiv.org/abs/2511.20494v3",
  "pdf_url": "http://arxiv.org/pdf/2511.20494v3.pdf",
  "relevance_score": 88,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of"
}