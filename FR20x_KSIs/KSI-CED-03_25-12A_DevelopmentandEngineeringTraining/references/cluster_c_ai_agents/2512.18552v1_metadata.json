{
  "arxiv_id": "2512.18552v1",
  "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
  "authors": [
    "Yuxiang Wei",
    "Zhiqing Sun",
    "Emily McMilin",
    "Jonas Gehring",
    "David Zhang",
    "Gabriel Synnaeve",
    "Daniel Fried",
    "Lingming Zhang",
    "Sida Wang"
  ],
  "published": "2025-12-21T00:49:40Z",
  "url": "http://arxiv.org/abs/2512.18552v1",
  "pdf_url": "http://arxiv.org/pdf/2512.18552v1.pdf",
  "relevance_score": 83,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our appro"
}