{
  "arxiv_id": "2512.10310v1",
  "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
  "authors": [
    "Duo Zheng",
    "Shijia Huang",
    "Yanyang Li",
    "Liwei Wang"
  ],
  "published": "2025-12-11T05:57:48Z",
  "url": "http://arxiv.org/abs/2512.10310v1",
  "pdf_url": "http://arxiv.org/pdf/2512.10310v1.pdf",
  "relevance_score": 78,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-e"
}