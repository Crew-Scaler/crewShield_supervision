{
  "arxiv_id": "2512.14234v1",
  "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body",
  "authors": [
    "Juze Zhang",
    "Changan Chen",
    "Xin Chen",
    "Heng Yu",
    "Tiange Xiang",
    "Ali Sartaz Khan",
    "Shrinidhi K. Lakshmikanth",
    "Ehsan Adeli"
  ],
  "published": "2025-12-16T09:41:21Z",
  "url": "http://arxiv.org/abs/2512.14234v1",
  "pdf_url": "http://arxiv.org/pdf/2512.14234v1.pdf",
  "relevance_score": 80,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or in"
}