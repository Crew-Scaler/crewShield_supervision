{
  "arxiv_id": "2512.04459v1",
  "title": "dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning",
  "authors": [
    "Yingzi Ma",
    "Yulong Cao",
    "Wenhao Ding",
    "Shuibai Zhang",
    "Yan Wang",
    "Boris Ivanovic",
    "Ming Jiang",
    "Marco Pavone",
    "Chaowei Xiao"
  ],
  "published": "2025-12-04T05:05:41Z",
  "url": "http://arxiv.org/abs/2512.04459v1",
  "pdf_url": "http://arxiv.org/pdf/2512.04459v1.pdf",
  "relevance_score": 81,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we"
}