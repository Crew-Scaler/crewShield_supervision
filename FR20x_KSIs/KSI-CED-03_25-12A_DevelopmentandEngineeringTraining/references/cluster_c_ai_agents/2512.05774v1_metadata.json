{
  "arxiv_id": "2512.05774v1",
  "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
  "authors": [
    "Ziyang Wang",
    "Honglu Zhou",
    "Shijie Wang",
    "Junnan Li",
    "Caiming Xiong",
    "Silvio Savarese",
    "Mohit Bansal",
    "Michael S. Ryoo",
    "Juan Carlos Niebles"
  ],
  "published": "2025-12-05T15:03:48Z",
  "url": "http://arxiv.org/abs/2512.05774v1",
  "pdf_url": "http://arxiv.org/pdf/2512.05774v1.pdf",
  "relevance_score": 80,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that"
}