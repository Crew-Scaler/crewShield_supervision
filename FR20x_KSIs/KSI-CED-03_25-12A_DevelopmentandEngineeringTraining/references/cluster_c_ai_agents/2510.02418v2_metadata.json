{
  "arxiv_id": "2510.02418v2",
  "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks",
  "authors": [
    "Sagnik Anupam",
    "Davis Brown",
    "Shuo Li",
    "Eric Wong",
    "Hamed Hassani",
    "Osbert Bastani"
  ],
  "published": "2025-10-02T15:22:21Z",
  "url": "http://arxiv.org/abs/2510.02418v2",
  "pdf_url": "http://arxiv.org/pdf/2510.02418v2.pdf",
  "relevance_score": 85,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-u"
}