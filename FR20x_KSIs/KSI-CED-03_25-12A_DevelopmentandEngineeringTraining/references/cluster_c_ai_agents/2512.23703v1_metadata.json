{
  "arxiv_id": "2512.23703v1",
  "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
  "authors": [
    "Huajie Tan",
    "Sixiang Chen",
    "Yijie Xu",
    "Zixiao Wang",
    "Yuheng Ji",
    "Cheng Chi",
    "Yaoxu Lyu",
    "Zhongxia Zhao",
    "Xiansheng Chen",
    "Peterson Co",
    "Shaoxuan Xie",
    "Guocai Yao",
    "Pengwei Wang",
    "Zhongyuan Wang",
    "Shanghang Zhang"
  ],
  "published": "2025-12-29T18:57:44Z",
  "url": "http://arxiv.org/abs/2512.23703v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23703v1.pdf",
  "relevance_score": 70,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often "
}