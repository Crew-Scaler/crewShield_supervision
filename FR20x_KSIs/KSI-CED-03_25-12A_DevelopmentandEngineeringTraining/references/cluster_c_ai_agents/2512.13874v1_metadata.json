{
  "arxiv_id": "2512.13874v1",
  "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
  "authors": [
    "Jitesh Jain",
    "Jialuo Li",
    "Zixian Ma",
    "Jieyu Zhang",
    "Chris Dongjoo Kim",
    "Sangho Lee",
    "Rohun Tripathi",
    "Tanmay Gupta",
    "Christopher Clark",
    "Humphrey Shi"
  ],
  "published": "2025-12-15T20:14:19Z",
  "url": "http://arxiv.org/abs/2512.13874v1",
  "pdf_url": "http://arxiv.org/pdf/2512.13874v1.pdf",
  "relevance_score": 78,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it po"
}