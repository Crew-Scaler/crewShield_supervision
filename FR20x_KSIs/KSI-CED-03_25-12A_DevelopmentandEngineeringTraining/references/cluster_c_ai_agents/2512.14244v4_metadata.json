{
  "arxiv_id": "2512.14244v4",
  "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
  "authors": [
    "Yiqing Zhou",
    "Yu Lei",
    "Shuzheng Si",
    "Qingyan Sun",
    "Wei Wang",
    "Yifei Wu",
    "Hao Wen",
    "Gang Chen",
    "Fanchao Qi",
    "Maosong Sun"
  ],
  "published": "2025-12-16T09:52:58Z",
  "url": "http://arxiv.org/abs/2512.14244v4",
  "pdf_url": "http://arxiv.org/pdf/2512.14244v4.pdf",
  "relevance_score": 97,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introd"
}