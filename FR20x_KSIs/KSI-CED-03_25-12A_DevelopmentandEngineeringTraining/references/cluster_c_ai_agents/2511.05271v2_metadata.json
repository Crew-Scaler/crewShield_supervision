{
  "arxiv_id": "2511.05271v2",
  "title": "DeepEyesV2: Toward Agentic Multimodal Model",
  "authors": [
    "Jack Hong",
    "Chenxiao Zhao",
    "ChengLin Zhu",
    "Weiheng Lu",
    "Guohai Xu",
    "Xing Yu"
  ],
  "published": "2025-11-07T14:31:20Z",
  "url": "http://arxiv.org/abs/2511.05271v2",
  "pdf_url": "http://arxiv.org/pdf/2511.05271v2.pdf",
  "relevance_score": 84,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motiv"
}