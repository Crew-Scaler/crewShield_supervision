{
  "arxiv_id": "2503.17332v4",
  "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
  "authors": [
    "Yuxuan Zhu",
    "Antony Kellermann",
    "Dylan Bowman",
    "Philip Li",
    "Akul Gupta",
    "Adarsh Danda",
    "Richard Fang",
    "Conner Jensen",
    "Eric Ihli",
    "Jason Benn",
    "Jet Geronimo",
    "Avi Dhir",
    "Sudhit Rao",
    "Kaicheng Yu",
    "Twm Stone",
    "Daniel Kang"
  ],
  "published": "2025-03-21T17:32:32Z",
  "url": "http://arxiv.org/abs/2503.17332v4",
  "pdf_url": "http://arxiv.org/pdf/2503.17332v4.pdf",
  "relevance_score": 100,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities invol"
}