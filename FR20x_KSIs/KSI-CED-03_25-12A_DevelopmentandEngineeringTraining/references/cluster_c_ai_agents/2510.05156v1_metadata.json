{
  "arxiv_id": "2510.05156v1",
  "title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation",
  "authors": [
    "Lesly Miculicich",
    "Mihir Parmar",
    "Hamid Palangi",
    "Krishnamurthy Dj Dvijotham",
    "Mirko Montanari",
    "Tomas Pfister",
    "Long T. Le"
  ],
  "published": "2025-10-03T04:11:43Z",
  "url": "http://arxiv.org/abs/2510.05156v1",
  "pdf_url": "http://arxiv.org/pdf/2510.05156v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Agent Security Architecture",
  "cluster": "C",
  "summary": "The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework tha"
}