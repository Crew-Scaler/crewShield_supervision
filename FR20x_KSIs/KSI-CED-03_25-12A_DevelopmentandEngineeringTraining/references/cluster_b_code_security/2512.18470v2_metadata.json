{
  "arxiv_id": "2512.18470v2",
  "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
  "authors": [
    "Minh V. T. Thai",
    "Tue Le",
    "Dung Nguyen Manh",
    "Huy Phan Nhat",
    "Nghi D. Q. Bui"
  ],
  "published": "2025-12-20T19:08:15Z",
  "url": "http://arxiv.org/abs/2512.18470v2",
  "pdf_url": "http://arxiv.org/pdf/2512.18470v2.pdf",
  "relevance_score": 80,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challe"
}