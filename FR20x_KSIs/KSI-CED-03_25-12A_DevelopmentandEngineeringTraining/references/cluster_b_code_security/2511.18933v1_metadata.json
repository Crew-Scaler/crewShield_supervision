{
  "arxiv_id": "2511.18933v1",
  "title": "Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations",
  "authors": [
    "Ryan Wong",
    "Hosea David Yu Fei Ng",
    "Dhananjai Sharma",
    "Glenn Jun Jie Ng",
    "Kavishvaran Srinivasan"
  ],
  "published": "2025-11-24T09:38:11Z",
  "url": "http://arxiv.org/abs/2511.18933v1",
  "pdf_url": "http://arxiv.org/pdf/2511.18933v1.pdf",
  "relevance_score": 71,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Stee"
}