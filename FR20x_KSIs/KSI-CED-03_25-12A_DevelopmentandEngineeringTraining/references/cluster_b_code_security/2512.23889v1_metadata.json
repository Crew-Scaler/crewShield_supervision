{
  "arxiv_id": "2512.23889v1",
  "title": "How Large Language Models Systematically Misrepresent American Climate Opinions",
  "authors": [
    "Sola Kim",
    "Jieshu Wang",
    "Marco A. Janssen",
    "John M. Anderies"
  ],
  "published": "2025-12-29T22:29:10Z",
  "url": "http://arxiv.org/abs/2512.23889v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23889v1.pdf",
  "relevance_score": 67,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Federal agencies and researchers increasingly use large language models to analyze and simulate public opinion. When AI mediates between the public and policymakers, accuracy across intersecting identities becomes consequential; inaccurate group-level estimates can mislead outreach, consultation, and policy design. While research examines intersectionality in LLM outputs, no study has compared these outputs against real human responses across intersecting identities. Climate policy is one such d"
}