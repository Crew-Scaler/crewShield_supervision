{
  "arxiv_id": "2512.06914v1",
  "title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions",
  "authors": [
    "Guanquan Shi",
    "Haohua Du",
    "Zhiqiang Wang",
    "Xiaoyu Liang",
    "Weiwenpei Liu",
    "Song Bian",
    "Zhenyu Guan"
  ],
  "published": "2025-12-07T16:41:02Z",
  "url": "http://arxiv.org/abs/2512.06914v1",
  "pdf_url": "http://arxiv.org/pdf/2512.06914v1.pdf",
  "relevance_score": 92,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for determi"
}