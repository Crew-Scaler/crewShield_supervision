{
  "arxiv_id": "2511.20709v1",
  "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation",
  "authors": [
    "Abhijeet Pathak",
    "Suvadra Barua",
    "Dinesh Gudimetla",
    "Rupam Patir",
    "Jiawei Guo",
    "Hongxin Hu",
    "Haipeng Cai"
  ],
  "published": "2025-11-24T22:26:14Z",
  "url": "http://arxiv.org/abs/2511.20709v1",
  "pdf_url": "http://arxiv.org/pdf/2511.20709v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental n"
}