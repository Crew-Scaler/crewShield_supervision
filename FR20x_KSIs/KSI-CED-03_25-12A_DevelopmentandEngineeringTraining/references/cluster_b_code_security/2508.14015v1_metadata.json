{
  "arxiv_id": "2508.14015v1",
  "title": "Backdooring Self-Supervised Contrastive Learning by Noisy Alignment",
  "authors": [
    "Tuo Chen",
    "Jie Gui",
    "Minjing Dong",
    "Ju Jia",
    "Lanting Fang",
    "Jian Liu"
  ],
  "published": "2025-08-19T17:25:43Z",
  "url": "http://arxiv.org/abs/2508.14015v1",
  "pdf_url": "http://arxiv.org/pdf/2508.14015v1.pdf",
  "relevance_score": 80,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Self-supervised contrastive learning (CL) effectively learns transferable representations from unlabeled data containing images or image-text pairs but suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary can inject poisoned images into pretraining datasets, causing compromised CL encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs, however, achieve limited efficacy due to their dependence on fragile implicit co-occurrence between backdoor and t"
}