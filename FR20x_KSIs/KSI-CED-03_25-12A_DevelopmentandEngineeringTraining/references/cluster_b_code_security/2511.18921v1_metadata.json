{
  "arxiv_id": "2511.18921v1",
  "title": "BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models",
  "authors": [
    "Juncheng Li",
    "Yige Li",
    "Hanxun Huang",
    "Yunhao Chen",
    "Xin Wang",
    "Yixu Wang",
    "Xingjun Ma",
    "Yu-Gang Jiang"
  ],
  "published": "2025-11-24T09:30:38Z",
  "url": "http://arxiv.org/abs/2511.18921v1",
  "pdf_url": "http://arxiv.org/pdf/2511.18921v1.pdf",
  "relevance_score": 65,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \\textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on V"
}