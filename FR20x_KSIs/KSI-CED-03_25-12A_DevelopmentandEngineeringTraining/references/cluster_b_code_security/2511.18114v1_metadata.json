{
  "arxiv_id": "2511.18114v1",
  "title": "ASTRA: Agentic Steerability and Risk Assessment Framework",
  "authors": [
    "Itay Hazan",
    "Yael Mathov",
    "Guy Shtar",
    "Ron Bitton",
    "Itsik Mantin"
  ],
  "published": "2025-11-22T16:32:29Z",
  "url": "http://arxiv.org/abs/2511.18114v1",
  "pdf_url": "http://arxiv.org/pdf/2511.18114v1.pdf",
  "relevance_score": 85,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Securing AI agents powered by Large Language Models (LLMs) represents one of the most critical challenges in AI security today. Unlike traditional software, AI agents leverage LLMs as their \"brain\" to autonomously perform actions via connected tools. This capability introduces significant risks that go far beyond those of harmful text presented in a chatbot that was the main application of LLMs. A compromised AI agent can deliberately abuse powerful tools to perform malicious actions, in many ca"
}