{
  "arxiv_id": "2512.04228v1",
  "title": "Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework",
  "authors": [
    "Peter B. Walker",
    "Hannah Davidson",
    "Aiden Foster",
    "Matthew Lienert",
    "Thomas Pardue",
    "Dale Russell"
  ],
  "published": "2025-12-03T19:50:39Z",
  "url": "http://arxiv.org/abs/2512.04228v1",
  "pdf_url": "http://arxiv.org/pdf/2512.04228v1.pdf",
  "relevance_score": 69,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \\textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This p"
}