{
  "arxiv_id": "2512.04841v1",
  "title": "SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security",
  "authors": [
    "Wei Zhao",
    "Zhe Li",
    "Jun Sun"
  ],
  "published": "2025-12-04T14:25:15Z",
  "url": "http://arxiv.org/abs/2512.04841v1",
  "pdf_url": "http://arxiv.org/pdf/2512.04841v1.pdf",
  "relevance_score": 90,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.\n  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions t"
}