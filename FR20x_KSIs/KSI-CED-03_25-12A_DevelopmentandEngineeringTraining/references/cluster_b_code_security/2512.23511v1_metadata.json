{
  "arxiv_id": "2512.23511v1",
  "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving",
  "authors": [
    "Xinyi Zheng",
    "Ningke Li",
    "Xiaokun Luan",
    "Kailong Wang",
    "Ling Shi",
    "Meng Sun",
    "Haoyu Wang"
  ],
  "published": "2025-12-29T14:48:15Z",
  "url": "http://arxiv.org/abs/2512.23511v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23511v1.pdf",
  "relevance_score": 75,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-st"
}