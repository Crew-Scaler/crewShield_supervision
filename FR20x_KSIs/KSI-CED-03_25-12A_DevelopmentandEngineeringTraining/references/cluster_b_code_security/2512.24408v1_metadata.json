{
  "arxiv_id": "2512.24408v1",
  "title": "DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model",
  "authors": [
    "Bohong Chen",
    "Haiyang Liu"
  ],
  "published": "2025-12-30T18:43:38Z",
  "url": "http://arxiv.org/abs/2512.24408v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24408v1.pdf",
  "relevance_score": 77,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Generating realistic, dyadic talking head video requires ultra-low latency. Existing chunk-based methods require full non-causal context windows, introducing significant delays. This high latency critically prevents the immediate, non-verbal feedback required for a realistic listener. To address this, we present DyStream, a flow matching-based autoregressive model that could generate video in real-time from both speaker and listener audio. Our method contains two key designs: (1) we adopt a stre"
}