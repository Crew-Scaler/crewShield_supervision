{
  "arxiv_id": "2510.02917v1",
  "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders",
  "authors": [
    "Kriz Tahimic",
    "Charibeth Cheng"
  ],
  "published": "2025-10-03T11:44:21Z",
  "url": "http://arxiv.org/abs/2510.02917v1",
  "pdf_url": "http://arxiv.org/pdf/2510.02917v1.pdf",
  "relevance_score": 98,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "As Large Language Models become integral to software development, with substantial portions of AI-suggested code entering production, understanding their internal correctness mechanisms becomes critical for safe deployment. We apply sparse autoencoders to decompose LLM representations, identifying directions that correspond to code correctness. We select predictor directions using t-statistics and steering directions through separation scores from base model representations, then analyze their m"
}