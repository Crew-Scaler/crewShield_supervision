{
  "arxiv_id": "2512.25026v1",
  "title": "Modeling Language as a Sequence of Thoughts",
  "authors": [
    "Nasim Borazjanizadeh",
    "James McClelland"
  ],
  "published": "2025-12-31T18:24:57Z",
  "url": "http://arxiv.org/abs/2512.25026v1",
  "pdf_url": "http://arxiv.org/pdf/2512.25026v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input"
}