{
  "arxiv_id": "2511.11773v1",
  "title": "On the Measure of a Model: From Intelligence to Generality",
  "authors": [
    "Ruchira Dhar",
    "Ninell Oldenburg",
    "Anders Soegaard"
  ],
  "published": "2025-11-14T09:46:48Z",
  "url": "http://arxiv.org/abs/2511.11773v1",
  "pdf_url": "http://arxiv.org/pdf/2511.11773v1.pdf",
  "relevance_score": 69,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than ab"
}