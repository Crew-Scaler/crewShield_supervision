{
  "arxiv_id": "2512.25014v1",
  "title": "Diffusion Language Models are Provably Optimal Parallel Samplers",
  "authors": [
    "Haozhe Jiang",
    "Nika Haghtalab",
    "Lijie Chen"
  ],
  "published": "2025-12-31T18:03:05Z",
  "url": "http://arxiv.org/abs/2512.25014v1",
  "pdf_url": "http://arxiv.org/pdf/2512.25014v1.pdf",
  "relevance_score": 70,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of "
}