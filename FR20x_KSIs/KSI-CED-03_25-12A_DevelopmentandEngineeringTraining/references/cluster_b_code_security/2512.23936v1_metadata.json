{
  "arxiv_id": "2512.23936v1",
  "title": "MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation",
  "authors": [
    "Yulong Zou",
    "Bo Liu",
    "Cun-Jing Zheng",
    "Yuan-ming Geng",
    "Siyue Li",
    "Qiankun Zuo",
    "Shuihua Wang",
    "Yudong Zhang",
    "Jin Hong"
  ],
  "published": "2025-12-30T01:37:04Z",
  "url": "http://arxiv.org/abs/2512.23936v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23936v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Leveraging multimodal information from Magnetic Resonance Imaging (MRI) plays a vital role in lesion segmentation, especially for brain tumors. However, in clinical practice, multimodal MRI data are often incomplete, making it challenging to fully utilize the available information. Therefore, maximizing the utilization of this incomplete multimodal information presents a crucial research challenge. We present a novel meta-guided multi-modal learning (MGML) framework that comprises two components"
}