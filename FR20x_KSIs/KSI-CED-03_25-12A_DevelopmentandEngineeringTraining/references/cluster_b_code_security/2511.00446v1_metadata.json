{
  "arxiv_id": "2511.00446v1",
  "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training",
  "authors": [
    "Xin Yao",
    "Haiyang Zhao",
    "Yimin Chen",
    "Jiawei Guo",
    "Kecheng Huang",
    "Ming Zhao"
  ],
  "published": "2025-11-01T08:25:49Z",
  "url": "http://arxiv.org/abs/2511.00446v1",
  "pdf_url": "http://arxiv.org/pdf/2511.00446v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "The Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP's training, remains underexplored. In this work, we introduce ToxicText"
}