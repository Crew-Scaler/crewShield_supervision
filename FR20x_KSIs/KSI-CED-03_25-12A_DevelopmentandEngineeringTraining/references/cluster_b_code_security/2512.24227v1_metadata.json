{
  "arxiv_id": "2512.24227v1",
  "title": "Mirage: One-Step Video Diffusion for Photorealistic and Coherent Asset Editing in Driving Scenes",
  "authors": [
    "Shuyun Wang",
    "Haiyang Sun",
    "Bing Wang",
    "Hangjun Ye",
    "Xin Yu"
  ],
  "published": "2025-12-30T13:40:23Z",
  "url": "http://arxiv.org/abs/2512.24227v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24227v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Vision-centric autonomous driving systems rely on diverse and scalable training data to achieve robust performance. While video object editing offers a promising path for data augmentation, existing methods often struggle to maintain both high visual fidelity and temporal coherence. In this work, we propose \\textbf{Mirage}, a one-step video diffusion model for photorealistic and coherent asset editing in driving scenes. Mirage builds upon a text-to-video diffusion prior to ensure temporal consis"
}