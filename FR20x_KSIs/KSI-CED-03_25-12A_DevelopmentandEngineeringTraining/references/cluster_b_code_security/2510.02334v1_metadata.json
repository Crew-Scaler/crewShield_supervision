{
  "arxiv_id": "2510.02334v1",
  "title": "Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing",
  "authors": [
    "Zhe Li",
    "Wei Zhao",
    "Yige Li",
    "Jun Sun"
  ],
  "published": "2025-09-26T12:07:47Z",
  "url": "http://arxiv.org/abs/2510.02334v1",
  "pdf_url": "http://arxiv.org/pdf/2510.02334v1.pdf",
  "relevance_score": 71,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. Diagnosing the root causes of these failures poses a critical challenge for AI safety. Existing attribution methods, particularly those based on parameter gradients, often fall short due to prohibitive noisy signals and computational complexity. In this work, we introduce a nov"
}