{
  "arxiv_id": "2512.10415v1",
  "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation",
  "authors": [
    "Devanshu Sahoo",
    "Vasudev Majhi",
    "Arjun Neekhra",
    "Yash Sinha",
    "Murari Mandal",
    "Dhruv Kumar"
  ],
  "published": "2025-12-11T08:28:33Z",
  "url": "http://arxiv.org/abs/2512.10415v1",
  "pdf_url": "http://arxiv.org/pdf/2512.10415v1.pdf",
  "relevance_score": 90,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jai"
}