{
  "arxiv_id": "2510.09269v1",
  "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects",
  "authors": [
    "Zirun Zhou",
    "Zhengyang Xiao",
    "Haochuan Xu",
    "Jing Sun",
    "Di Wang",
    "Jingfeng Zhang"
  ],
  "published": "2025-10-10T11:09:36Z",
  "url": "http://arxiv.org/abs/2510.09269v1",
  "pdf_url": "http://arxiv.org/pdf/2510.09269v1.pdf",
  "relevance_score": 70,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Recent advances in vision-language-action (VLA) models have greatly improved embodied AI, enabling robots to follow natural language instructions and perform diverse tasks. However, their reliance on uncurated training datasets raises serious security concerns. Existing backdoor attacks on VLAs mostly assume white-box access and result in task failures instead of enforcing specific actions. In this work, we reveal a more practical threat: attackers can manipulate VLAs by simply injecting physica"
}