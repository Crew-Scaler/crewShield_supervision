{
  "arxiv_id": "2504.12344v1",
  "title": "Propaganda via AI? A Study on Semantic Backdoors in Large Language Models",
  "authors": [
    "Nay Myat Min",
    "Long H. Pham",
    "Yige Li",
    "Jun Sun"
  ],
  "published": "2025-04-15T16:43:15Z",
  "url": "http://arxiv.org/abs/2504.12344v1",
  "pdf_url": "http://arxiv.org/pdf/2504.12344v1.pdf",
  "relevance_score": 69,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large language models (LLMs) demonstrate remarkable performance across myriad language tasks, yet they remain vulnerable to backdoor attacks, where adversaries implant hidden triggers that systematically manipulate model outputs. Traditional defenses focus on explicit token-level anomalies and therefore overlook semantic backdoors-covert triggers embedded at the conceptual level (e.g., ideological stances or cultural references) that rely on meaning-based cues rather than lexical oddities. We fi"
}