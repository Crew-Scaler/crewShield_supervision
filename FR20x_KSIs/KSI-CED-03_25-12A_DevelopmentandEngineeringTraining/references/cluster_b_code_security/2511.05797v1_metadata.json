{
  "arxiv_id": "2511.05797v1",
  "title": "When AI Meets the Web: Prompt Injection Risks in Third-Party AI Chatbot Plugins",
  "authors": [
    "Yigitcan Kaya",
    "Anton Landerer",
    "Stijn Pletinckx",
    "Michelle Zimmermann",
    "Christopher Kruegel",
    "Giovanni Vigna"
  ],
  "published": "2025-11-08T02:02:24Z",
  "url": "http://arxiv.org/abs/2511.05797v1",
  "pdf_url": "http://arxiv.org/pdf/2511.05797v1.pdf",
  "relevance_score": 81,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Prompt injection attacks pose a critical threat to large language models (LLMs), with prior work focusing on cutting-edge LLM applications like personal copilots. In contrast, simpler LLM applications, such as customer service chatbots, are widespread on the web, yet their security posture and exposure to such attacks remain poorly understood. These applications often rely on third-party chatbot plugins that act as intermediaries to commercial LLM APIs, offering non-expert website builders intui"
}