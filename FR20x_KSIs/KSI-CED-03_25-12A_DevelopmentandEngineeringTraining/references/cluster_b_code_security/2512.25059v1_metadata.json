{
  "arxiv_id": "2512.25059v1",
  "title": "Reliable and Resilient Collective Communication Library for LLM Training and Serving",
  "authors": [
    "Wei Wang",
    "Nengneng Yu",
    "Sixian Xiong",
    "Zaoxing Liu"
  ],
  "published": "2025-12-31T18:53:11Z",
  "url": "http://arxiv.org/abs/2512.25059v1",
  "pdf_url": "http://arxiv.org/pdf/2512.25059v1.pdf",
  "relevance_score": 71,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL perform"
}