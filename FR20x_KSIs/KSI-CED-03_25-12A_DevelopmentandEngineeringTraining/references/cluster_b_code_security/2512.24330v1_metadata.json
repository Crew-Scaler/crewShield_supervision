{
  "arxiv_id": "2512.24330v1",
  "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
  "authors": [
    "Yong Xien Chng",
    "Tao Hu",
    "Wenwen Tong",
    "Xueheng Li",
    "Jiandong Chen",
    "Haojia Yu",
    "Jiefan Lu",
    "Hewei Guo",
    "Hanming Deng",
    "Chengjun Xie",
    "Gao Huang",
    "Dahua Lin",
    "Lewei Lu"
  ],
  "published": "2025-12-30T16:31:45Z",
  "url": "http://arxiv.org/abs/2512.24330v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24330v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce S"
}