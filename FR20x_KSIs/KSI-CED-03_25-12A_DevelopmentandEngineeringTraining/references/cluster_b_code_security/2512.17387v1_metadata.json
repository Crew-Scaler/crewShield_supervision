{
  "arxiv_id": "2512.17387v1",
  "title": "CIFE: Code Instruction-Following Evaluation",
  "authors": [
    "Sravani Gunnu",
    "Shanmukha Guttula",
    "Hima Patel"
  ],
  "published": "2025-12-19T09:43:20Z",
  "url": "http://arxiv.org/abs/2512.17387v1",
  "pdf_url": "http://arxiv.org/pdf/2512.17387v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 devel"
}