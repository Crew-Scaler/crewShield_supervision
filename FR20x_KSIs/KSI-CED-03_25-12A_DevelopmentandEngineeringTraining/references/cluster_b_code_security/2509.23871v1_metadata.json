{
  "arxiv_id": "2509.23871v1",
  "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack",
  "authors": [
    "Yukun Chen",
    "Boheng Li",
    "Yu Yuan",
    "Leyi Qi",
    "Yiming Li",
    "Tianwei Zhang",
    "Zhan Qin",
    "Kui Ren"
  ],
  "published": "2025-09-28T13:24:46Z",
  "url": "http://arxiv.org/abs/2509.23871v1",
  "pdf_url": "http://arxiv.org/pdf/2509.23871v1.pdf",
  "relevance_score": 73,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Knowledge distillation (KD) is a vital technique for deploying deep neural networks (DNNs) on resource-constrained devices by transferring knowledge from large teacher models to lightweight student models. While teacher models from third-party platforms may undergo security verification (\\eg, backdoor detection), we uncover a novel and critical threat: distillation-conditional backdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into teacher models, which become activated in"
}