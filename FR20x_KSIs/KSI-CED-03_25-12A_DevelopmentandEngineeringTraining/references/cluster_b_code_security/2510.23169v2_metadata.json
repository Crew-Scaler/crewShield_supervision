{
  "arxiv_id": "2510.23169v2",
  "title": "MATCH: Task-Driven Code Evaluation through Contrastive Learning",
  "authors": [
    "Marah Ghoummaid",
    "Vladimir Tchuiev",
    "Ofek Glick",
    "Michal Moshkovitz",
    "Dotan Di Castro"
  ],
  "published": "2025-10-27T09:51:49Z",
  "url": "http://arxiv.org/abs/2510.23169v2",
  "pdf_url": "http://arxiv.org/pdf/2510.23169v2.pdf",
  "relevance_score": 87,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "AI-based code generation is increasingly prevalent, with GitHub Copilot estimated to generate 46% of the code on GitHub. Accurately evaluating how well generated code aligns with developer intent remains a critical challenge. Traditional evaluation methods, such as unit tests, are often unscalable and costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code functionality, and metrics like CodeBERTScore require reference code, which is not always available. To address the gap"
}