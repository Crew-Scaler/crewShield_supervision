{
  "arxiv_id": "2512.07730v2",
  "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination",
  "authors": [
    "Sangha Park",
    "Seungryong Yoo",
    "Jisoo Mok",
    "Sungroh Yoon"
  ],
  "published": "2025-12-08T17:20:07Z",
  "url": "http://arxiv.org/abs/2512.07730v2",
  "pdf_url": "http://arxiv.org/pdf/2512.07730v2.pdf",
  "relevance_score": 86,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual"
}