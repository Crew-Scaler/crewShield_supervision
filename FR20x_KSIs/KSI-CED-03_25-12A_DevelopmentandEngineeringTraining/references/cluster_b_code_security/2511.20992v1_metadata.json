{
  "arxiv_id": "2511.20992v1",
  "title": "Dataset Poisoning Attacks on Behavioral Cloning Policies",
  "authors": [
    "Akansha Kalra",
    "Soumil Datta",
    "Ethan Gilmore",
    "Duc La",
    "Guanhong Tao",
    "Daniel S. Brown"
  ],
  "published": "2025-11-26T02:47:33Z",
  "url": "http://arxiv.org/abs/2511.20992v1",
  "pdf_url": "http://arxiv.org/pdf/2511.20992v1.pdf",
  "relevance_score": 66,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Behavior Cloning (BC) is a popular framework for training sequential decision policies from expert demonstrations via supervised learning. As these policies are increasingly being deployed in the real world, their robustness and potential vulnerabilities are an important concern. In this work, we perform the first analysis of the efficacy of clean-label backdoor attacks on BC policies. Our backdoor attacks poison a dataset of demonstrations by injecting a visual trigger to create a spurious corr"
}