{
  "arxiv_id": "2512.08213v1",
  "title": "Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs",
  "authors": [
    "Md Nazmul Haque",
    "Elizabeth Lin",
    "Lawrence Arkoh",
    "Biruk Tadesse",
    "Bowen Xu"
  ],
  "published": "2025-12-09T03:47:31Z",
  "url": "http://arxiv.org/abs/2512.08213v1",
  "pdf_url": "http://arxiv.org/pdf/2512.08213v1.pdf",
  "relevance_score": 97,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable dep"
}