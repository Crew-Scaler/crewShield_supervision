{
  "arxiv_id": "2509.23041v2",
  "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data",
  "authors": [
    "Zi Liang",
    "Qingqing Ye",
    "Xuan Liu",
    "Yanyun Wang",
    "Jianliang Xu",
    "Haibo Hu"
  ],
  "published": "2025-09-27T01:39:41Z",
  "url": "http://arxiv.org/abs/2509.23041v2",
  "pdf_url": "http://arxiv.org/pdf/2509.23041v2.pdf",
  "relevance_score": 85,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong "
}