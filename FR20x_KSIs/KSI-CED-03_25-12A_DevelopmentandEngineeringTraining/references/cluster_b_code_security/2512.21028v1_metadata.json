{
  "arxiv_id": "2512.21028v1",
  "title": "Artificial or Just Artful? Do LLMs Bend the Rules in Programming?",
  "authors": [
    "Oussama Ben Sghaier",
    "Kevin Delcourt",
    "Houari Sahraoui"
  ],
  "published": "2025-12-24T07:51:15Z",
  "url": "http://arxiv.org/abs/2512.21028v1",
  "pdf_url": "http://arxiv.org/pdf/2512.21028v1.pdf",
  "relevance_score": 95,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, a"
}