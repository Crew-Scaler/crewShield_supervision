{
  "arxiv_id": "2512.24605v1",
  "title": "MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding",
  "authors": [
    "Panquan Yang",
    "Junfei Huang",
    "Zongzhangbao Yin",
    "Yingsong Hu",
    "Anni Xu",
    "Xinyi Luo",
    "Xueqi Sun",
    "Hai Wu",
    "Sheng Ao",
    "Zhaoxing Zhu",
    "Chenglu Wen",
    "Cheng Wang"
  ],
  "published": "2025-12-31T03:56:28Z",
  "url": "http://arxiv.org/abs/2512.24605v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24605v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-"
}