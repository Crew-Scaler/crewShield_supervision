{
  "arxiv_id": "2512.20732v1",
  "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
  "authors": [
    "Saeed Mohammadzadeh",
    "Erfan Hamdi",
    "Joel Shor",
    "Emma Lejeune"
  ],
  "published": "2025-12-23T19:40:51Z",
  "url": "http://arxiv.org/abs/2512.20732v1",
  "pdf_url": "http://arxiv.org/pdf/2512.20732v1.pdf",
  "relevance_score": 92,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathemat"
}