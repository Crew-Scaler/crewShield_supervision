{
  "arxiv_id": "2512.01174v1",
  "title": "DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks",
  "authors": [
    "Hyunjun Kim",
    "Sooyoung Ryu"
  ],
  "published": "2025-12-01T01:18:21Z",
  "url": "http://arxiv.org/abs/2512.01174v1",
  "pdf_url": "http://arxiv.org/pdf/2512.01174v1.pdf",
  "relevance_score": 69,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-base"
}