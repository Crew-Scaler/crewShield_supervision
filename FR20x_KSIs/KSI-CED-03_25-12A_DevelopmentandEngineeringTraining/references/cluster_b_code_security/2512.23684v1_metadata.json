{
  "arxiv_id": "2512.23684v1",
  "title": "Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing",
  "authors": [
    "Panagiotis Theocharopoulos",
    "Ajinkya Kulkarni",
    "Mathew Magimai. -Doss"
  ],
  "published": "2025-12-29T18:43:05Z",
  "url": "http://arxiv.org/abs/2512.23684v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23684v1.pdf",
  "relevance_score": 80,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using a"
}