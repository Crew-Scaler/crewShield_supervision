{
  "arxiv_id": "2506.04743v2",
  "title": "SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs",
  "authors": [
    "Shuhan Xu",
    "Siyuan Liang",
    "Hongling Zheng",
    "Aishan Liu",
    "Xinbiao Wang",
    "Yong Luo",
    "Fu Lin",
    "Leszek Rutkowski",
    "Dacheng Tao"
  ],
  "published": "2025-06-05T08:22:24Z",
  "url": "http://arxiv.org/abs/2506.04743v2",
  "pdf_url": "http://arxiv.org/pdf/2506.04743v2.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Visual language models (VLMs) have made significant progress in image captioning tasks, yet recent studies have found they are vulnerable to backdoor attacks. Attackers can inject undetectable perturbations into the data during inference, triggering abnormal behavior and generating malicious captions. These attacks are particularly challenging to detect and defend against due to the stealthiness and cross-modal propagation of the trigger signals. In this paper, we identify two key vulnerabilitie"
}