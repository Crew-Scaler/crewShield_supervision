{
  "arxiv_id": "2510.22620v1",
  "title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents",
  "authors": [
    "Julia Bazinska",
    "Max Mathys",
    "Francesco Casucci",
    "Mateo Rojas-Carulla",
    "Xander Davies",
    "Alexandra Souly",
    "Niklas Pfister"
  ],
  "published": "2025-10-26T10:36:42Z",
  "url": "http://arxiv.org/abs/2510.22620v1",
  "pdf_url": "http://arxiv.org/pdf/2510.22620v1.pdf",
  "relevance_score": 96,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "AI agents powered by large language models (LLMs) are being deployed at scale, yet we lack a systematic understanding of how the choice of backbone LLM affects agent security. The non-deterministic sequential nature of AI agents complicates security modeling, while the integration of traditional software with AI components entangles novel LLM vulnerabilities with conventional security risks. Existing frameworks only partially address these challenges as they either capture specific vulnerabiliti"
}