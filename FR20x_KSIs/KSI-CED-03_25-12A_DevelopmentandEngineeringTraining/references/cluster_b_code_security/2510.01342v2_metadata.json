{
  "arxiv_id": "2510.01342v2",
  "title": "Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach",
  "authors": [
    "Xiangfang Li",
    "Yu Wang",
    "Bo Li"
  ],
  "published": "2025-10-01T18:14:13Z",
  "url": "http://arxiv.org/abs/2510.01342v2",
  "pdf_url": "http://arxiv.org/pdf/2510.01342v2.pdf",
  "relevance_score": 67,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "With the rapid advancement of large language models (LLMs), ensuring their safe use becomes increasingly critical. Fine-tuning is a widely used method for adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks. However, most existing studies focus on overly simplified attack scenarios, limiting their practical relevance to real-world defense settings. To make this risk concrete, we present a three-pronged jailbreak attack and evaluate it against provider defenses under a "
}