{
  "arxiv_id": "2511.02347v1",
  "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
  "authors": [
    "Liuhao Lin",
    "Ke Li",
    "Zihan Xu",
    "Yuchen Shi",
    "Yulei Qin",
    "Yan Zhang",
    "Xing Sun",
    "Rongrong Ji"
  ],
  "published": "2025-11-04T08:11:23Z",
  "url": "http://arxiv.org/abs/2511.02347v1",
  "pdf_url": "http://arxiv.org/pdf/2511.02347v1.pdf",
  "relevance_score": 69,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research--relying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates a dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, a breakthrough benchmark that trans"
}