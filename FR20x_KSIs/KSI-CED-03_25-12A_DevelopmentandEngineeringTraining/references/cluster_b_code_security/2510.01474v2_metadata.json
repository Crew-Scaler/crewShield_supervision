{
  "arxiv_id": "2510.01474v2",
  "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance",
  "authors": [
    "Bill Marino",
    "Rosco Hunter",
    "Zubair Jamali",
    "Marinos Emmanouil Kalpakos",
    "Mudra Kashyap",
    "Isaiah Hinton",
    "Alexa Hanson",
    "Maahum Nazir",
    "Christoph Schnabl",
    "Felix Steffek",
    "Hongkai Wen",
    "Nicholas D. Lane"
  ],
  "published": "2025-10-01T21:33:33Z",
  "url": "http://arxiv.org/abs/2510.01474v2",
  "pdf_url": "http://arxiv.org/pdf/2510.01474v2.pdf",
  "relevance_score": 65,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce AIReg-Bench: the first benchmark dataset designed to test how well LLMs can assess compliance with the EU AI Act (AIA). We created this dataset through a two-step process: (1) by prompting an LLM with "
}