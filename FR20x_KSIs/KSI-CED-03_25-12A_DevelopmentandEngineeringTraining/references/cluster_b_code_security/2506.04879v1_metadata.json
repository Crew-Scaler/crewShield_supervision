{
  "arxiv_id": "2506.04879v1",
  "title": "Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking",
  "authors": [
    "Yu-Feng Chen",
    "Tzuhsuan Huang",
    "Pin-Yen Chiu",
    "Jun-Cheng Chen"
  ],
  "published": "2025-06-05T10:51:58Z",
  "url": "http://arxiv.org/abs/2506.04879v1",
  "pdf_url": "http://arxiv.org/pdf/2506.04879v1.pdf",
  "relevance_score": 75,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Diffusion models have achieved remarkable progress in both image generation and editing. However, recent studies have revealed their vulnerability to backdoor attacks, in which specific patterns embedded in the input can manipulate the model's behavior. Most existing research in this area has proposed attack frameworks focused on the image generation pipeline, leaving backdoor attacks in image editing relatively unexplored. Among the few studies targeting image editing, most utilize visible trig"
}