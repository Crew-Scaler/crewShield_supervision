{
  "arxiv_id": "2507.05113v2",
  "title": "CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset Separation",
  "authors": [
    "Binyan Xu",
    "Fan Yang",
    "Xilin Dai",
    "Di Tang",
    "Kehuan Zhang"
  ],
  "published": "2025-07-07T15:29:26Z",
  "url": "http://arxiv.org/abs/2507.05113v2",
  "pdf_url": "http://arxiv.org/pdf/2507.05113v2.pdf",
  "relevance_score": 65,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Deep Neural Networks (DNNs) are susceptible to backdoor attacks, where adversaries poison training data to implant backdoor into the victim model. Current backdoor defenses on poisoned data often suffer from high computational costs or low effectiveness against advanced attacks like clean-label and clean-image backdoors. To address them, we introduce CLIP-Guided backdoor Defense (CGD), an efficient and effective method that mitigates various backdoor attacks. CGD utilizes a publicly accessible C"
}