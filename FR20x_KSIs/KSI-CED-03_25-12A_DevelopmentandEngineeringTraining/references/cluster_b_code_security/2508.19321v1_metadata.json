{
  "arxiv_id": "2508.19321v1",
  "title": "An Investigation on Group Query Hallucination Attacks",
  "authors": [
    "Kehao Miao",
    "Xiaolong Jin"
  ],
  "published": "2025-08-26T14:30:59Z",
  "url": "http://arxiv.org/abs/2508.19321v1",
  "pdf_url": "http://arxiv.org/pdf/2508.19321v1.pdf",
  "relevance_score": 67,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "With the widespread use of large language models (LLMs), understanding their potential failure modes during user interactions is essential. In practice, users often pose multiple questions in a single conversation with LLMs. Therefore, in this study, we propose Group Query Attack, a technique that simulates this scenario by presenting groups of queries to LLMs simultaneously. We investigate how the accumulated context from consecutive prompts influences the outputs of LLMs. Specifically, we obse"
}