{
  "arxiv_id": "2512.24975v1",
  "title": "Attribution-Guided Distillation of Matryoshka Sparse Autoencoders",
  "authors": [
    "Cristina P. Martin-Linares",
    "Jonathan P. Ling"
  ],
  "published": "2025-12-31T17:12:55Z",
  "url": "http://arxiv.org/abs/2512.24975v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24975v1.pdf",
  "relevance_score": 77,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: tra"
}