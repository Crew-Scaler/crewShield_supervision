{
  "arxiv_id": "2511.20703v1",
  "title": "PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach",
  "authors": [
    "Udari Madhushani Sehwag",
    "Shayan Shabihi",
    "Alex McAvoy",
    "Vikash Sehwag",
    "Yuancheng Xu",
    "Dalton Towers",
    "Furong Huang"
  ],
  "published": "2025-11-24T18:46:44Z",
  "url": "http://arxiv.org/abs/2511.20703v1",
  "pdf_url": "http://arxiv.org/pdf/2511.20703v1.pdf",
  "relevance_score": 68,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Recent advances in Large Language Models (LLMs) have sparked concerns over their potential to acquire and misuse dangerous or high-risk capabilities, posing frontier risks. Current safety evaluations primarily test for what a model \\textit{can} do - its capabilities - without assessing what it $\\textit{would}$ do if endowed with high-risk capabilities. This leaves a critical blind spot: models may strategically conceal capabilities or rapidly acquire them, while harboring latent inclinations tow"
}