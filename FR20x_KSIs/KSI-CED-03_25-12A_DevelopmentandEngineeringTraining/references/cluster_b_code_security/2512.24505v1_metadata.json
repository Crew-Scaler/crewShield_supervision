{
  "arxiv_id": "2512.24505v1",
  "title": "Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems",
  "authors": [
    "Samuel Golladay",
    "Majid Bani-Yaghoub"
  ],
  "published": "2025-12-30T23:05:11Z",
  "url": "http://arxiv.org/abs/2512.24505v1",
  "pdf_url": "http://arxiv.org/pdf/2512.24505v1.pdf",
  "relevance_score": 70,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading L"
}