{
  "arxiv_id": "2512.16538v1",
  "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
  "authors": [
    "Xiao Li",
    "Yue Li",
    "Hao Wu",
    "Yue Zhang",
    "Yechao Zhang",
    "Fengyuan Xu",
    "Sheng Zhong"
  ],
  "published": "2025-12-18T13:49:59Z",
  "url": "http://arxiv.org/abs/2512.16538v1",
  "pdf_url": "http://arxiv.org/pdf/2512.16538v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported tec"
}