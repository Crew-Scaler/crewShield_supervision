{
  "arxiv_id": "2512.10766v1",
  "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
  "authors": [
    "Chenyu Zhang",
    "Yiwen Ma",
    "Lanjun Wang",
    "Wenhui Li",
    "Yi Tu",
    "An-An Liu"
  ],
  "published": "2025-12-06T12:38:00Z",
  "url": "http://arxiv.org/abs/2512.10766v1",
  "pdf_url": "http://arxiv.org/pdf/2512.10766v1.pdf",
  "relevance_score": 72,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defens"
}