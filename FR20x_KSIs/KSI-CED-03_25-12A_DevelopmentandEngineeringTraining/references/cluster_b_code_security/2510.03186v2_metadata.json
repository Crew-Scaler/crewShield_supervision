{
  "arxiv_id": "2510.03186v2",
  "title": "Superposition disentanglement of neural representations reveals hidden alignment",
  "authors": [
    "Andr\u00e9 Longon",
    "David Klindt",
    "Meenakshi Khosla"
  ],
  "published": "2025-10-03T17:12:40Z",
  "url": "http://arxiv.org/abs/2510.03186v2",
  "pdf_url": "http://arxiv.org/pdf/2510.03186v2.pdf",
  "relevance_score": 65,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "The superposition hypothesis states that single neurons may participate in representing multiple features in order for the neural network to represent more features than it has neurons. In neuroscience and AI, representational alignment metrics measure the extent to which different deep neural networks (DNNs) or brains represent similar information. In this work, we explore a critical question: does superposition interact with alignment metrics in any undesirable way? We hypothesize that models "
}