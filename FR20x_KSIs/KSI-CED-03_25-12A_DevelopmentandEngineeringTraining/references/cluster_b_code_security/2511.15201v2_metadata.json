{
  "arxiv_id": "2511.15201v2",
  "title": "Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval",
  "authors": [
    "Qing Wang",
    "Chong-Wah Ngo",
    "Ee-Peng Lim"
  ],
  "published": "2025-11-19T07:39:53Z",
  "url": "http://arxiv.org/abs/2511.15201v2",
  "pdf_url": "http://arxiv.org/pdf/2511.15201v2.pdf",
  "relevance_score": 60,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors"
}