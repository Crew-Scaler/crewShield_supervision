{
  "arxiv_id": "2510.16380v1",
  "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes",
  "authors": [
    "Yu Ying Chiu",
    "Michael S. Lee",
    "Rachel Calcott",
    "Brandon Handoko",
    "Paul de Font-Reaulx",
    "Paula Rodriguez",
    "Chen Bo Calvin Zhang",
    "Ziwen Han",
    "Udari Madhushani Sehwag",
    "Yash Maurya",
    "Christina Q Knight",
    "Harry R. Lloyd",
    "Florence Bacus",
    "Mantas Mazeika",
    "Bing Liu",
    "Yejin Choi",
    "Mitchell L Gordon",
    "Sydney Levine"
  ],
  "published": "2025-10-18T07:34:31Z",
  "url": "http://arxiv.org/abs/2510.16380v1",
  "pdf_url": "http://arxiv.org/pdf/2510.16380v1.pdf",
  "relevance_score": 72,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "As AI systems progress, we rely more on them to make decisions with us and for us. To ensure that such decisions are aligned with human values, it is imperative for us to understand not only what decisions they make but also how they come to those decisions. Reasoning language models, which provide both final responses and (partially transparent) intermediate thinking traces, present a timely opportunity to study AI procedural reasoning. Unlike math and code problems which often have objectively"
}