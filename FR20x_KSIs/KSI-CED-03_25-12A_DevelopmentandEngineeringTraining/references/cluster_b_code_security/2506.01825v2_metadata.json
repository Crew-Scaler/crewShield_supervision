{
  "arxiv_id": "2506.01825v2",
  "title": "Backdoors in Code Summarizers: How Bad Is It?",
  "authors": [
    "Chenyu Wang",
    "Zhou Yang",
    "Yaniv Harel",
    "David Lo"
  ],
  "published": "2025-06-02T16:07:34Z",
  "url": "http://arxiv.org/abs/2506.01825v2",
  "pdf_url": "http://arxiv.org/pdf/2506.01825v2.pdf",
  "relevance_score": 96,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Code LLMs are increasingly employed in software development. However, studies have shown that they are vulnerable to backdoor attacks: when a trigger (a specific input pattern) appears in the input, the backdoor will be activated and cause the model to generate malicious outputs. Researchers have designed various triggers and demonstrated the feasibility of implanting backdoors by poisoning a fraction of the training data. Some basic conclusions have been made, such as backdoors becoming easier "
}