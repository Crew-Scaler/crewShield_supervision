{
  "arxiv_id": "2512.00412v2",
  "title": "Red Teaming Large Reasoning Models",
  "authors": [
    "Jiawei Chen",
    "Yang Yang",
    "Chao Yu",
    "Yu Tian",
    "Zhi Cao",
    "Linghao Li",
    "Hang Su",
    "Zhaoxia Yin"
  ],
  "published": "2025-11-29T09:45:03Z",
  "url": "http://arxiv.org/abs/2512.00412v2",
  "pdf_url": "http://arxiv.org/pdf/2512.00412v2.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM ev"
}