{
  "arxiv_id": "2512.25034v1",
  "title": "Generative Classifiers Avoid Shortcut Solutions",
  "authors": [
    "Alexander C. Li",
    "Ananya Kumar",
    "Deepak Pathak"
  ],
  "published": "2025-12-31T18:31:46Z",
  "url": "http://arxiv.org/abs/2512.25034v1",
  "pdf_url": "http://arxiv.org/pdf/2512.25034v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need fo"
}