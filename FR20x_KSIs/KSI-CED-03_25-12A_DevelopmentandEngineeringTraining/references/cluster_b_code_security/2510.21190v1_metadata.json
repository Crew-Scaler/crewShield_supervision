{
  "arxiv_id": "2510.21190v1",
  "title": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning",
  "authors": [
    "Mingrui Liu",
    "Sixiao Zhang",
    "Cheng Long",
    "Kwok Yan Lam"
  ],
  "published": "2025-10-24T06:43:10Z",
  "url": "http://arxiv.org/abs/2510.21190v1",
  "pdf_url": "http://arxiv.org/pdf/2510.21190v1.pdf",
  "relevance_score": 83,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) have advanced rapidly and now encode extensive world knowledge. Despite safety fine-tuning, however, they remain susceptible to adversarial prompts that elicit harmful content. Existing jailbreak techniques fall into two categories: white-box methods (e.g., gradient-based approaches such as GCG), which require model internals and are infeasible for closed-source APIs, and black-box methods that rely on attacker LLMs to search or mutate prompts but often produce templ"
}