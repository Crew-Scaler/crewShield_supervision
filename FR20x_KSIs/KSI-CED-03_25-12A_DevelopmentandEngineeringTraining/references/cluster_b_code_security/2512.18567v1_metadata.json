{
  "arxiv_id": "2512.18567v1",
  "title": "AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software",
  "authors": [
    "Bin Wang",
    "Wenjie Yu",
    "Yilu Zhong",
    "Hao Yu",
    "Keke Lian",
    "Chaohua Lu",
    "Hongfang Zheng",
    "Dong Zhang",
    "Hui Li"
  ],
  "published": "2025-12-21T02:26:29Z",
  "url": "http://arxiv.org/abs/2512.18567v1",
  "pdf_url": "http://arxiv.org/pdf/2512.18567v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.\n  We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii"
}