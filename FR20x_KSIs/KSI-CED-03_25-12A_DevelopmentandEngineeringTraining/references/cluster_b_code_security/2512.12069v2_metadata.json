{
  "arxiv_id": "2512.12069v2",
  "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring",
  "authors": [
    "Peichun Hua",
    "Hao Li",
    "Shanghao Shi",
    "Zhiyuan Yu",
    "Ning Zhang"
  ],
  "published": "2025-12-12T22:31:38Z",
  "url": "http://arxiv.org/abs/2512.12069v2",
  "pdf_url": "http://arxiv.org/pdf/2512.12069v2.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel "
}