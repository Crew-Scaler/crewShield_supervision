{
  "arxiv_id": "2511.16209v1",
  "title": "PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization",
  "authors": [
    "Huseein Jawad",
    "Nicolas Brunel"
  ],
  "published": "2025-11-20T10:25:45Z",
  "url": "http://arxiv.org/abs/2511.16209v1",
  "pdf_url": "http://arxiv.org/pdf/2511.16209v1.pdf",
  "relevance_score": 85,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "System prompts are critical for guiding the behavior of Large Language Models (LLMs), yet they often contain proprietary logic or sensitive information, making them a prime target for extraction attacks. Adversarial queries can successfully elicit these hidden instructions, posing significant security and privacy risks. Existing defense mechanisms frequently rely on heuristics, incur substantial computational overhead, or are inapplicable to models accessed via black-box APIs. This paper introdu"
}