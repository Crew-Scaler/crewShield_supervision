{
  "arxiv_id": "2510.09507v1",
  "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
  "authors": [
    "Zixin Zhang",
    "Kanghao Chen",
    "Xingwang Lin",
    "Lutao Jiang",
    "Xu Zheng",
    "Yuanhuiyi Lyu",
    "Litao Guo",
    "Yinchuan Li",
    "Ying-Cong Chen"
  ],
  "published": "2025-10-10T16:10:45Z",
  "url": "http://arxiv.org/abs/2510.09507v1",
  "pdf_url": "http://arxiv.org/pdf/2510.09507v1.pdf",
  "relevance_score": 73,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physi"
}