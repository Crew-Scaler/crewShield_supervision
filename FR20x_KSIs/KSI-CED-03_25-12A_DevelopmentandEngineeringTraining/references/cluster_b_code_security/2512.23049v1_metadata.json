{
  "arxiv_id": "2512.23049v1",
  "title": "Accelerating Language Model Workflows with Prompt Choreography",
  "authors": [
    "TJ Bai",
    "Jason Eisner"
  ],
  "published": "2025-12-28T19:21:11Z",
  "url": "http://arxiv.org/abs/2512.23049v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23049v1.pdf",
  "relevance_score": 77,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with t"
}