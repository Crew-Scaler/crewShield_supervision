{
  "arxiv_id": "2512.23836v1",
  "title": "Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?",
  "authors": [
    "Dingmin Wang",
    "Ji Ma",
    "Shankar Kumar"
  ],
  "published": "2025-12-29T19:59:10Z",
  "url": "http://arxiv.org/abs/2512.23836v1",
  "pdf_url": "http://arxiv.org/pdf/2512.23836v1.pdf",
  "relevance_score": 78,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitti"
}