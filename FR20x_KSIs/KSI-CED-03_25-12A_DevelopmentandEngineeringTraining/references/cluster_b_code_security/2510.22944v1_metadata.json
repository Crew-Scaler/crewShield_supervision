{
  "arxiv_id": "2510.22944v1",
  "title": "Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies",
  "authors": [
    "Bin Wang",
    "YiLu Zhong",
    "MiDi Wan",
    "WenJie Yu",
    "YuanBing Ouyang",
    "Yenan Huang",
    "Hui Li"
  ],
  "published": "2025-10-27T02:59:17Z",
  "url": "http://arxiv.org/abs/2510.22944v1",
  "pdf_url": "http://arxiv.org/pdf/2510.22944v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large language models (LLMs) have become indispensable for automated code generation, yet the quality and security of their outputs remain a critical concern. Existing studies predominantly concentrate on adversarial attacks or inherent flaws within the models. However, a more prevalent yet underexplored issue concerns how the quality of a benign but poorly formulated prompt affects the security of the generated code. To investigate this, we first propose an evaluation framework for prompt quali"
}