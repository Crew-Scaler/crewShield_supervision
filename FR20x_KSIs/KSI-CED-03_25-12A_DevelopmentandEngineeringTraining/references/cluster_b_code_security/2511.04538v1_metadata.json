{
  "arxiv_id": "2511.04538v1",
  "title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting",
  "authors": [
    "Cyril Vallez",
    "Alexander Sternfeld",
    "Andrei Kucharavy",
    "Ljiljana Dolamic"
  ],
  "published": "2025-11-06T16:52:27Z",
  "url": "http://arxiv.org/abs/2511.04538v1",
  "pdf_url": "http://arxiv.org/pdf/2511.04538v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "As the role of Large Language Models (LLM)-based coding assistants in software development becomes more critical, so does the role of the bugs they generate in the overall cybersecurity landscape. While a number of LLM code security benchmarks have been proposed alongside approaches to improve the security of generated code, it remains unclear to what extent they have impacted widely used coding LLMs. Here, we show that even the latest open-weight models are vulnerable in the earliest reported v"
}