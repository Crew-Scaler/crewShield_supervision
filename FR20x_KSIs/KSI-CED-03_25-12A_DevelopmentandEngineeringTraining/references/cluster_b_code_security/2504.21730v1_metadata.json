{
  "arxiv_id": "2504.21730v1",
  "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense",
  "authors": [
    "Ting Qiao",
    "Yingjia Wang",
    "Xing Liu",
    "Sixing Wu",
    "Jianbing Li",
    "Yiming Li"
  ],
  "published": "2025-04-30T15:21:25Z",
  "url": "http://arxiv.org/abs/2504.21730v1",
  "pdf_url": "http://arxiv.org/pdf/2504.21730v1.pdf",
  "relevance_score": 63,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more "
}