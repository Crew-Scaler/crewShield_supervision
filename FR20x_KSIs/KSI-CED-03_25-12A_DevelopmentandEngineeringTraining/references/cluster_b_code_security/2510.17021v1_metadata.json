{
  "arxiv_id": "2510.17021v1",
  "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning",
  "authors": [
    "Bingqi Shang",
    "Yiwei Chen",
    "Yihua Zhang",
    "Bingquan Shen",
    "Sijia Liu"
  ],
  "published": "2025-10-19T22:00:01Z",
  "url": "http://arxiv.org/abs/2510.17021v1",
  "pdf_url": "http://arxiv.org/pdf/2510.17021v1.pdf",
  "relevance_score": 71,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large language model (LLM) unlearning has become a critical mechanism for removing undesired data, knowledge, or behaviors from pre-trained models while retaining their general utility. Yet, with the rise of open-weight LLMs, we ask: can the unlearning process itself be backdoored, appearing successful under normal conditions yet reverting to pre-unlearned behavior when a hidden trigger is activated? Drawing inspiration from classical backdoor attacks that embed triggers into training data to en"
}