{
  "arxiv_id": "2505.02824v2",
  "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
  "authors": [
    "Kuofeng Gao",
    "Yufei Zhu",
    "Yiming Li",
    "Jiawang Bai",
    "Yong Yang",
    "Zhifeng Li",
    "Shu-Tao Xia"
  ],
  "published": "2025-05-05T17:51:55Z",
  "url": "http://arxiv.org/abs/2505.02824v2",
  "pdf_url": "http://arxiv.org/pdf/2505.02824v2.pdf",
  "relevance_score": 68,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Text-to-image (T2I) diffusion models enable high-quality image generation conditioned on textual prompts. However, fine-tuning these pre-trained models for personalization raises concerns about unauthorized dataset usage. To address this issue, dataset ownership verification (DOV) has recently been proposed, which embeds watermarks into fine-tuning datasets via backdoor techniques. These watermarks remain dormant on benign samples but produce owner-specified outputs when triggered. Despite its p"
}