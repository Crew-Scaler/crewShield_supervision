{
  "arxiv_id": "2512.04111v1",
  "title": "HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding",
  "authors": [
    "Hanjun Luo",
    "Chiming Ni",
    "Jiaheng Wen",
    "Zhimu Huang",
    "Yiran Wang",
    "Bingduo Liao",
    "Sylvia Chung",
    "Yingbin Jin",
    "Xinfeng Li",
    "Wenyuan Xu",
    "XiaoFeng Wang",
    "Hanan Salam"
  ],
  "published": "2025-11-30T21:44:44Z",
  "url": "http://arxiv.org/abs/2512.04111v1",
  "pdf_url": "http://arxiv.org/pdf/2512.04111v1.pdf",
  "relevance_score": 73,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To "
}