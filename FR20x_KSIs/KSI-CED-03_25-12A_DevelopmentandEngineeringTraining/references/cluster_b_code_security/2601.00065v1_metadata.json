{
  "arxiv_id": "2601.00065v1",
  "title": "The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition",
  "authors": [
    "Xiaoze Liu",
    "Weichen Yu",
    "Matt Fredrikson",
    "Xiaoqian Wang",
    "Jing Gao"
  ],
  "published": "2025-12-31T19:00:03Z",
  "url": "http://arxiv.org/abs/2601.00065v1",
  "pdf_url": "http://arxiv.org/pdf/2601.00065v1.pdf",
  "relevance_score": 83,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "
}