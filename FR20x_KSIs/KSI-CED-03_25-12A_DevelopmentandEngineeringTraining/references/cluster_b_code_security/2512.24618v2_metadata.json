{
  "arxiv_id": "2512.24618v2",
  "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
  "authors": [
    "Junru Lu",
    "Jiarui Qin",
    "Lingfeng Qiao",
    "Yinghui Li",
    "Xinyi Dai",
    "Bo Ke",
    "Jianfeng He",
    "Ruizhi Qiao",
    "Di Yin",
    "Xing Sun",
    "Yunsheng Wu",
    "Yinsong Liu",
    "Shuangyin Liu",
    "Mingkong Tang",
    "Haodong Lin",
    "Jiayi Kuang",
    "Fanxu Meng",
    "Xiaojuan Tang",
    "Yunjia Xi",
    "Junjie Huang",
    "Haotong Yang",
    "Zhenyi Shen",
    "Yangning Li",
    "Qianwen Zhang",
    "Yifei Yu",
    "Siyu An",
    "Junnan Dong",
    "Qiufeng Wang",
    "Jie Wang",
    "Keyu Chen",
    "Wei Wen",
    "Taian Guo",
    "Zhifeng Shen",
    "Daohai Yu",
    "Jiahao Li",
    "Ke Li",
    "Zongyi Li",
    "Xiaoyu Tan"
  ],
  "published": "2025-12-31T04:25:11Z",
  "url": "http://arxiv.org/abs/2512.24618v2",
  "pdf_url": "http://arxiv.org/pdf/2512.24618v2.pdf",
  "relevance_score": 73,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented voc"
}