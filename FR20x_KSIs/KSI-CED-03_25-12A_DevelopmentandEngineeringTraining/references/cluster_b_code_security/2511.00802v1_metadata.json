{
  "arxiv_id": "2511.00802v1",
  "title": "GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents",
  "authors": [
    "Jie JW Wu",
    "Ayanda Patrick Herlihy",
    "Ahmad Saleem Mirza",
    "Ali Afoud",
    "Fatemeh Fard"
  ],
  "published": "2025-11-02T04:47:17Z",
  "url": "http://arxiv.org/abs/2511.00802v1",
  "pdf_url": "http://arxiv.org/pdf/2511.00802v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "With the software industry shifting toward a data-driven culture, online A/B testing is a key tool for evaluating new technologies. However, deploying such experiments requires substantial resources, may negatively impact users, and involves long data collection periods. To address this, \\textit{off-policy evaluation (OPE)}, or offline A/B testing, uses logged data to assess technologies and is fundamental in Reinforcement Learning, making it crucial in domains where online testing is costly or "
}