{
  "arxiv_id": "2507.18305v1",
  "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit",
  "authors": [
    "Biao Yi",
    "Zekun Fei",
    "Jianing Geng",
    "Tong Li",
    "Lihai Nie",
    "Zheli Liu",
    "Yiming Li"
  ],
  "published": "2025-07-24T11:24:35Z",
  "url": "http://arxiv.org/abs/2507.18305v1",
  "pdf_url": "http://arxiv.org/pdf/2507.18305v1.pdf",
  "relevance_score": 67,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which we term \"overthinking backdoors\". We advance this concept by proposing a novel tunable backdoor, wh"
}