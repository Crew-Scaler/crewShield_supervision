{
  "arxiv_id": "2511.10098v1",
  "title": "MTAttack: Multi-Target Backdoor Attacks against Large Vision-Language Models",
  "authors": [
    "Zihan Wang",
    "Guansong Pang",
    "Wenjun Miao",
    "Jin Zheng",
    "Xiao Bai"
  ],
  "published": "2025-11-13T09:00:21Z",
  "url": "http://arxiv.org/abs/2511.10098v1",
  "pdf_url": "http://arxiv.org/pdf/2511.10098v1.pdf",
  "relevance_score": 70,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Recent advances in Large Visual Language Models (LVLMs) have demonstrated impressive performance across various vision-language tasks by leveraging large-scale image-text pretraining and instruction tuning. However, the security vulnerabilities of LVLMs have become increasingly concerning, particularly their susceptibility to backdoor attacks. Existing backdoor attacks focus on single-target attacks, i.e., targeting a single malicious output associated with a specific trigger. In this work, we u"
}