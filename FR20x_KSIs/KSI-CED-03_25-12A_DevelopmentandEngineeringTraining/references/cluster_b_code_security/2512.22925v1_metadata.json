{
  "arxiv_id": "2512.22925v1",
  "title": "Argus: Token Aware Distributed LLM Inference Optimization",
  "authors": [
    "Panlong Wu",
    "Yifei Zhong",
    "Danyang Chen",
    "Ting Wang",
    "Fangxin Wang"
  ],
  "published": "2025-12-28T13:38:38Z",
  "url": "http://arxiv.org/abs/2512.22925v1",
  "pdf_url": "http://arxiv.org/pdf/2512.22925v1.pdf",
  "relevance_score": 68,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edg"
}