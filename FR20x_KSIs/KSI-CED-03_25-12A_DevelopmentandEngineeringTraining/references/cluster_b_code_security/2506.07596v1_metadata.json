{
  "arxiv_id": "2506.07596v1",
  "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
  "authors": [
    "Torsten Krau\u00df",
    "Hamid Dashtbani",
    "Alexandra Dmitrienko"
  ],
  "published": "2025-06-09T09:54:25Z",
  "url": "http://arxiv.org/abs/2506.07596v1",
  "pdf_url": "http://arxiv.org/pdf/2506.07596v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Machine learning is advancing rapidly, with applications bringing notable benefits, such as improvements in translation and code generation. Models like ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated into daily life. However, alongside these benefits, LLMs also introduce social risks. Malicious users can exploit LLMs by submitting harmful prompts, such as requesting instructions for illegal activities. To mitigate this, models often include a security mechanism tha"
}