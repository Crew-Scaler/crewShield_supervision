{
  "arxiv_id": "2512.25023v1",
  "title": "ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning",
  "authors": [
    "Timo Kaufmann",
    "Yannick Metz",
    "Daniel Keim",
    "Eyke H\u00fcllermeier"
  ],
  "published": "2025-12-31T18:21:52Z",
  "url": "http://arxiv.org/abs/2512.25023v1",
  "pdf_url": "http://arxiv.org/pdf/2512.25023v1.pdf",
  "relevance_score": 62,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose "
}