{
  "arxiv_id": "2510.13859v1",
  "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation",
  "authors": [
    "Ruchit Rawal",
    "Jeffrey Yang Fan Chiang",
    "Chihao Shen",
    "Jeffery Siyuan Tian",
    "Aastha Mahajan",
    "Tom Goldstein",
    "Yizheng Chen"
  ],
  "published": "2025-10-13T01:20:46Z",
  "url": "http://arxiv.org/abs/2510.13859v1",
  "pdf_url": "http://arxiv.org/pdf/2510.13859v1.pdf",
  "relevance_score": 100,
  "dimension": "AI Code Generation Security",
  "cluster": "B",
  "summary": "AI coding assistants powered by large language models (LLMs) have transformed software development, significantly boosting productivity. While existing benchmarks evaluate the correctness and security of LLM-generated code, they are typically limited to single-turn tasks that do not reflect the iterative nature of real-world development. We introduce MT-Sec, the first benchmark to systematically evaluate both correctness and security in multi-turn coding scenarios. We construct this using a synt"
}