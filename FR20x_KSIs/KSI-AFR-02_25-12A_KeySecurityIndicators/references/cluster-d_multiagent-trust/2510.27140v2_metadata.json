{
  "arxiv_id": "2510.27140v2",
  "title": "Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels",
  "authors": [
    "Chenghao Du",
    "Quanfeng Huang",
    "Tingxuan Tang",
    "Zihao Wang",
    "Adwait Nadkarni",
    "Yue Xiao"
  ],
  "published": "2025-10-31",
  "year": 2025,
  "relevance_score": 65,
  "arxiv_url": "https://arxiv.org/abs/2510.27140v2",
  "pdf_url": "https://arxiv.org/pdf/2510.27140v2.pdf",
  "cluster": "cluster-d",
  "abstract": "Large Language Models (LLMs) have transformed software development, enabling AI-powered applications known as LLM-based agents that promise to automate tasks across diverse apps and workflows. Yet, the security implications of deploying such agents in adversarial mobile environments remain poorly understood. In this paper, we present the first systematic study of security risks in mobile LLM agents. We design and evaluate a suite of adversarial case studies, ranging from opportunistic manipulati",
  "metadata_generated": "2026-01-10T18:09:06.438810"
}