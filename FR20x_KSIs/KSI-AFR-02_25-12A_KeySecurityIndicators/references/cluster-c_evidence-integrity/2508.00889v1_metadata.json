{
  "arxiv_id": "2508.00889v1",
  "title": "FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts",
  "authors": [
    "Hagyeong Shin",
    "Binoy Robin Dalal",
    "Iwona Bialynicka-Birula",
    "Navjot Matharu",
    "Ryan Muir",
    "Xingwei Yang",
    "Samuel W. K. Wong"
  ],
  "published": "2025-07-26",
  "year": 2025,
  "relevance_score": 70,
  "arxiv_url": "https://arxiv.org/abs/2508.00889v1",
  "pdf_url": "https://arxiv.org/pdf/2508.00889v1.pdf",
  "cluster": "cluster-c",
  "abstract": "Large language models (LLMs) are known to hallucinate, producing natural language outputs that are not grounded in the input, reference materials, or real-world knowledge. In enterprise applications where AI features support business decisions, such hallucinations can be particularly detrimental. LLMs that analyze and summarize contact center conversations introduce a unique set of challenges for factuality evaluation, because ground-truth labels often do not exist for analytical interpretations",
  "metadata_generated": "2026-01-10T18:09:06.432605"
}