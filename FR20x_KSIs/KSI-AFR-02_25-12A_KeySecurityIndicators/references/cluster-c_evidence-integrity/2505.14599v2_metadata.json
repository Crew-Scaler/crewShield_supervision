{
  "arxiv_id": "2505.14599v2",
  "title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
  "authors": [
    "Guangzhi Xiong",
    "Eric Xie",
    "Corey Williams",
    "Myles Kim",
    "Amir Hassan Shariatmadari",
    "Sikun Guo",
    "Stefan Bekiranov",
    "Aidong Zhang"
  ],
  "published": "2025-05-20",
  "year": 2025,
  "relevance_score": 80,
  "arxiv_url": "https://arxiv.org/abs/2505.14599v2",
  "pdf_url": "https://arxiv.org/pdf/2505.14599v2.pdf",
  "cluster": "cluster-c",
  "abstract": "Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appea",
  "metadata_generated": "2026-01-10T18:09:06.432971"
}