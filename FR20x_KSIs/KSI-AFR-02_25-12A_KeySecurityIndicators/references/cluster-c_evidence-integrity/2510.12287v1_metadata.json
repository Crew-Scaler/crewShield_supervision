{
  "arxiv_id": "2510.12287v1",
  "title": "Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector",
  "authors": [
    "Sifan Li",
    "Hongkai Chen",
    "Yujun Cai",
    "Qingwen Ye",
    "Liyang Chen",
    "Junsong Yuan",
    "Yiwei Wang"
  ],
  "published": "2025-10-14",
  "year": 2025,
  "relevance_score": 65,
  "arxiv_url": "https://arxiv.org/abs/2510.12287v1",
  "pdf_url": "https://arxiv.org/pdf/2510.12287v1.pdf",
  "cluster": "cluster-c",
  "abstract": "Vision Language Models (VLMs) have achieved impressive progress in multimodal reasoning; yet, they remain vulnerable to hallucinations, where outputs are not grounded in visual evidence. In this paper, we investigate a previously overlooked setting: logo hallucination, where models generate brand names or textual content despite logos containing no visible words. Using curated splits of pure symbols, hybrids, and text-bearing logos, as well as the challenging Hard-60 subset, we systematically me",
  "metadata_generated": "2026-01-10T18:09:06.432422"
}