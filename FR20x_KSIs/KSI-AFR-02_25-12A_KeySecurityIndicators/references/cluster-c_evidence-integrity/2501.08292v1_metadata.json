{
  "arxiv_id": "2501.08292v1",
  "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
  "authors": [
    "Abhilasha Ravichander",
    "Shrusti Ghela",
    "David Wadden",
    "Yejin Choi"
  ],
  "published": "2025-01-14",
  "year": 2025,
  "relevance_score": 75,
  "arxiv_url": "https://arxiv.org/abs/2501.08292v1",
  "pdf_url": "https://arxiv.org/pdf/2501.08292v1.pdf",
  "cluster": "cluster-c",
  "abstract": "Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generativ",
  "metadata_generated": "2026-01-10T18:09:06.431879"
}