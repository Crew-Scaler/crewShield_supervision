{
  "arxiv_id": "2512.22416v1",
  "title": "Hallucination Detection and Evaluation of Large Language Model",
  "authors": [
    "Chenggong Zhang",
    "Haopeng Wang"
  ],
  "published": "2025-12-27",
  "year": 2025,
  "relevance_score": 90,
  "arxiv_url": "https://arxiv.org/abs/2512.22416v1",
  "pdf_url": "https://arxiv.org/pdf/2512.22416v1.pdf",
  "cluster": "cluster-c",
  "abstract": "Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while",
  "metadata_generated": "2026-01-10T18:09:06.432247"
}