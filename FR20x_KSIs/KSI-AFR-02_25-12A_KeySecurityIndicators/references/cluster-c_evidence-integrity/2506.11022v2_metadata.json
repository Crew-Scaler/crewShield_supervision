{
  "arxiv_id": "2506.11022v2",
  "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox",
  "authors": [
    "Shivani Shukla",
    "Himanshu Joshi",
    "Romilla Syed"
  ],
  "published": "2025-05-19",
  "year": 2025,
  "relevance_score": 60,
  "arxiv_url": "https://arxiv.org/abs/2506.11022v2",
  "pdf_url": "https://arxiv.org/pdf/2506.11022v2.pdf",
  "cluster": "cluster-c",
  "abstract": "The rapid adoption of Large Language Models(LLMs) for code generation has transformed software development, yet little attention has been given to how security vulnerabilities evolve through iterative LLM feedback. This paper analyzes security degradation in AI-generated code through a controlled experiment with 400 code samples across 40 rounds of \"improvements\" using four distinct prompting strategies. Our findings show a 37.6% increase in critical vulnerabilities after just five iterations, w",
  "metadata_generated": "2026-01-10T18:09:06.431188"
}