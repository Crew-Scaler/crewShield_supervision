{
  "arxiv_id": "2510.06265v2",
  "title": "Large Language Models Hallucination: A Comprehensive Survey",
  "authors": [
    "Aisha Alansari",
    "Hamzah Luqman"
  ],
  "published": "2025-10-05",
  "year": 2025,
  "relevance_score": 70,
  "arxiv_url": "https://arxiv.org/abs/2510.06265v2",
  "pdf_url": "https://arxiv.org/pdf/2510.06265v2.pdf",
  "cluster": "cluster-c",
  "abstract": "Large language models (LLMs) have transformed natural language processing, achieving remarkable performance across diverse tasks. However, their impressive fluency often comes at the cost of producing false or fabricated information, a phenomenon known as hallucination. Hallucination refers to the generation of content by an LLM that is fluent and syntactically correct but factually inaccurate or unsupported by external evidence. Hallucinations undermine the reliability and trustworthiness of LL",
  "metadata_generated": "2026-01-10T18:09:06.431667"
}