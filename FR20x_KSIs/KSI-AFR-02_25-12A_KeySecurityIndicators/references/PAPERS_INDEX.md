================================================================================
ISSUE #158 - KSI-AFR-02 KEY SECURITY INDICATORS
AGENT 2: CLUSTER C & D RESEARCH PAPER COLLECTION
================================================================================

**Execution Date:** 2026-01-10T18:09:06.442106

## EXECUTIVE SUMMARY

**Total Papers Screened:** 101
**Cluster C (Evidence Integrity):** 26 papers identified
**Cluster D (Multi-Agent Trust):** 63 papers identified
**Total Relevant Papers:** 89

---

## CLUSTER C: AI-GENERATED EVIDENCE INTEGRITY, HALLUCINATION & FABRICATION

**Target:** 8-12 papers | **Identified:** 26 papers

### C1. Hallucination Detection and Evaluation of Large Language Model

**ArXiv ID:** 2512.22416v1
**Published:** 2025-12-27
**Relevance Score:** 90/100
**Authors:** Chenggong Zhang, Haopeng Wang
**URL:** https://arxiv.org/abs/2512.22416v1

**Abstract:**
> Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based fr...

### C2. A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models

**ArXiv ID:** 2509.03871v1
**Published:** 2025-09-04
**Relevance Score:** 85/100
**Authors:** Yanbo Wang, Yongcan Yu, Jian Liang...
**URL:** https://arxiv.org/abs/2509.03871v1

**Abstract:**
> The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the ...

### C3. DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models

**ArXiv ID:** 2503.03149v1
**Published:** 2025-03-05
**Relevance Score:** 85/100
**Authors:** YiQiu Guo, Yuchen Yang, Zhe Chen...
**URL:** https://arxiv.org/abs/2503.03149v1

**Abstract:**
> The reliability of large language models remains a critical challenge, particularly due to their susceptibility to hallucinations and factual inaccuracies during text generation. Existing solutions either underutilize models' self-correction with preemptive strategies or use costly post-hoc verification. To further explore the potential of real-time self-verification and correction, we present Dyn...

### C4. Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models

**ArXiv ID:** 2505.14599v2
**Published:** 2025-05-20
**Relevance Score:** 80/100
**Authors:** Guangzhi Xiong, Eric Xie, Corey Williams...
**URL:** https://arxiv.org/abs/2505.14599v2

**Abstract:**
> Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources....

### C5. HALoGEN: Fantastic LLM Hallucinations and Where to Find Them

**ArXiv ID:** 2501.08292v1
**Published:** 2025-01-14
**Relevance Score:** 75/100
**Authors:** Abhilasha Ravichander, Shrusti Ghela, David Wadden...
**URL:** https://arxiv.org/abs/2501.08292v1

**Abstract:**
> Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we rele...

### C6. Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering

**ArXiv ID:** 2501.06521v1
**Published:** 2025-01-11
**Relevance Score:** 75/100
**Authors:** Yinghao Hu, Leilei Gan, Wenyi Xiao...
**URL:** https://arxiv.org/abs/2501.06521v1

**Abstract:**
> Hallucination, or the generation of incorrect or fabricated information, remains a critical challenge in large language models (LLMs), particularly in high-stake domains such as legal question answering (QA). In order to mitigate the hallucination rate in legal QA, we first introduce a benchmark called LegalHalBench and three automatic metrics to evaluate the common hallucinations when LLMs answer...

### C7. DataGen: Unified Synthetic Dataset Generation via Large Language Models

**ArXiv ID:** 2406.18966v5
**Published:** 2024-06-27
**Relevance Score:** 75/100
**Authors:** Yue Huang, Siyuan Wu, Chujie Gao...
**URL:** https://arxiv.org/abs/2406.18966v5

**Abstract:**
> Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this pap...

### C8. Large Language Models Hallucination: A Comprehensive Survey

**ArXiv ID:** 2510.06265v2
**Published:** 2025-10-05
**Relevance Score:** 70/100
**Authors:** Aisha Alansari, Hamzah Luqman
**URL:** https://arxiv.org/abs/2510.06265v2

**Abstract:**
> Large language models (LLMs) have transformed natural language processing, achieving remarkable performance across diverse tasks. However, their impressive fluency often comes at the cost of producing false or fabricated information, a phenomenon known as hallucination. Hallucination refers to the generation of content by an LLM that is fluent and syntactically correct but factually inaccurate or ...

### C9. FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts

**ArXiv ID:** 2508.00889v1
**Published:** 2025-07-26
**Relevance Score:** 70/100
**Authors:** Hagyeong Shin, Binoy Robin Dalal, Iwona Bialynicka-Birula...
**URL:** https://arxiv.org/abs/2508.00889v1

**Abstract:**
> Large language models (LLMs) are known to hallucinate, producing natural language outputs that are not grounded in the input, reference materials, or real-world knowledge. In enterprise applications where AI features support business decisions, such hallucinations can be particularly detrimental. LLMs that analyze and summarize contact center conversations introduce a unique set of challenges for ...

### C10. KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino

**ArXiv ID:** 2509.06065v1
**Published:** 2025-09-07
**Relevance Score:** 70/100
**Authors:** Lorenzo Alfred Nery, Ronald Dawson Catignas, Thomas James Tiam-Lee
**URL:** https://arxiv.org/abs/2509.06065v1

**Abstract:**
> Large Language Models (LLMs) achieve remarkable performance across various tasks, but their tendency to produce hallucinations limits reliable adoption. Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet they are primarily available in English, leaving a gap in evaluating LLMs in low-resource languages. To address this, we present KatotohananQA, a Filipino translation o...

### C11. Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives

**ArXiv ID:** 2509.08380v2
**Published:** 2025-09-10
**Relevance Score:** 70/100
**Authors:** Prathamesh Vasudeo Naik, Naresh Kumar Dintakurthi, Zhanghao Hu...
**URL:** https://arxiv.org/abs/2509.08380v2

**Abstract:**
> Generating regulatorily compliant Suspicious Activity Report (SAR) remains a high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows. While large language models (LLMs) offer promising fluency, they suffer from factual hallucination, limited crime typology alignment, and poor explainability -- posing unacceptable risks in compliance-critical domains. This paper introduces Co...

### C12. Too Much to Trust? Measuring the Security and Cognitive Impacts of Explainability in AI-Driven SOCs

**ArXiv ID:** 2503.02065v2
**Published:** 2025-03-03
**Relevance Score:** 65/100
**Authors:** Nidhi Rastogi, Shirid Pant, Devang Dhanuka...
**URL:** https://arxiv.org/abs/2503.02065v2

**Abstract:**
> Explainable AI (XAI) holds significant promise for enhancing the transparency and trustworthiness of AI-driven threat detection in Security Operations Centers (SOCs). However, identifying the appropriate level and format of explanation, particularly in environments that demand rapid decision-making under high-stakes conditions, remains a complex and underexplored challenge. To address this gap, we...

### C13. SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning

**ArXiv ID:** 2510.26457v1
**Published:** 2025-10-30
**Relevance Score:** 65/100
**Authors:** Fang Liu, Simiao Liu, Yinghao Zhu...
**URL:** https://arxiv.org/abs/2510.26457v1

**Abstract:**
> Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches...

### C14. Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector

**ArXiv ID:** 2510.12287v1
**Published:** 2025-10-14
**Relevance Score:** 65/100
**Authors:** Sifan Li, Hongkai Chen, Yujun Cai...
**URL:** https://arxiv.org/abs/2510.12287v1

**Abstract:**
> Vision Language Models (VLMs) have achieved impressive progress in multimodal reasoning; yet, they remain vulnerable to hallucinations, where outputs are not grounded in visual evidence. In this paper, we investigate a previously overlooked setting: logo hallucination, where models generate brand names or textual content despite logos containing no visible words. Using curated splits of pure symbo...

### C15. SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications

**ArXiv ID:** 2510.25908v1
**Published:** 2025-10-29
**Relevance Score:** 65/100
**Authors:** Emily Herron, Junqi Yin, Feiyi Wang
**URL:** https://arxiv.org/abs/2510.25908v1

**Abstract:**
> Large language models (LLMs) have demonstrated transformative potential in scientific research, yet their deployment in high-stakes contexts raises significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a comprehensive framework for evaluating LLM trustworthiness in scientific applications across four dimensions: truthfulness, adversarial robustness, scientific safety, and scientif...

---

## CLUSTER D: MULTI-AGENT SYSTEMS, ORCHESTRATION & TRUST

**Target:** 8-10 papers | **Identified:** 63 papers

### D1. Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents

**ArXiv ID:** 2509.13597v1
**Published:** 2025-09-16
**Relevance Score:** 100/100
**Authors:** Abhishek Goswami
**URL:** https://arxiv.org/abs/2509.13597v1

**Abstract:**
> Autonomous LLM agents can issue thousands of API calls per hour without human oversight. OAuth 2.0 assumes deterministic clients, but in agentic settings stochastic reasoning, prompt injection, or multi-agent orchestration can silently expand privileges.
  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each agent's action to verifiable user intent and, optionally, to a sp...

### D2. Towards Unifying Quantitative Security Benchmarking for Multi Agent Systems

**ArXiv ID:** 2507.21146v1
**Published:** 2025-07-23
**Relevance Score:** 100/100
**Authors:** Gauri Sharma, Vidhi Kulkarni, Miles King...
**URL:** https://arxiv.org/abs/2507.21146v1

**Abstract:**
> Evolving AI systems increasingly deploy multi-agent architectures where autonomous agents collaborate, share information, and delegate tasks through developing protocols. This connectivity, while powerful, introduces novel security risks. One such risk is a cascading risk: a breach in one agent can cascade through the system, compromising others by exploiting inter-agent trust. In tandem with OWAS...

### D3. Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation

**ArXiv ID:** 2512.23480v1
**Published:** 2025-12-29
**Relevance Score:** 100/100
**Authors:** Toqeer Ali Syed, Mohammad Riyaz Belgaum, Salman Jan...
**URL:** https://arxiv.org/abs/2512.23480v1

**Abstract:**
> The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software pr...

### D4. AI Agents with Decentralized Identifiers and Verifiable Credentials

**ArXiv ID:** 2511.02841v2
**Published:** 2025-10-01
**Relevance Score:** 100/100
**Authors:** Sandro Rodriguez Garzon, Awid Vaziry, Enis Mert Kuzu...
**URL:** https://arxiv.org/abs/2511.02841v2

**Abstract:**
> A fundamental limitation of current LLM-based AI agents is their inability to build differentiated trust among each other at the onset of an agent-to-agent dialogue. However, autonomous and interoperable trust establishment becomes essential once agents start to operate beyond isolated environments and engage in dialogues across individual or organizational boundaries. A promising way to fill this...

### D5. Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering

**ArXiv ID:** 2510.11694v1
**Published:** 2025-10-13
**Relevance Score:** 100/100
**Authors:** Arjun Sahney, Ram Gorthi, Cezary Åastowski...
**URL:** https://arxiv.org/abs/2510.11694v1

**Abstract:**
> We present Operand Quant, a single-agent, IDE-based architecture for autonomous machine learning engineering (MLE). Operand Quant departs from conventional multi-agent orchestration frameworks by consolidating all MLE lifecycle stages -- exploration, modeling, experimentation, and deployment -- within a single, context-aware agent. On the MLE-Benchmark (2025), Operand Quant achieved a new state-of...

### D6. A Survey on Agent Workflow -- Status and Future

**ArXiv ID:** 2508.01186v1
**Published:** 2025-08-02
**Relevance Score:** 100/100
**Authors:** Chaojia Yu, Zihan Cheng, Hanwen Cui...
**URL:** https://arxiv.org/abs/2508.01186v1

**Abstract:**
> In the age of large language models (LLMs), autonomous agents have emerged as a powerful paradigm for achieving general intelligence. These agents dynamically leverage tools, memory, and reasoning capabilities to accomplish user-defined goals. As agent systems grow in complexity, agent workflows-structured orchestration frameworks-have become central to enabling scalable, controllable, and secure ...

### D7. From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review

**ArXiv ID:** 2504.19678v1
**Published:** 2025-04-28
**Relevance Score:** 100/100
**Authors:** Mohamed Amine Ferrag, Norbert Tihanyi, Merouane Debbah
**URL:** https://arxiv.org/abs/2504.19678v1

**Abstract:**
> Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across ...

### D8. Trust-aware Control for Intelligent Transportation Systems

**ArXiv ID:** 2111.04248v1
**Published:** 2021-11-08
**Relevance Score:** 95/100
**Authors:** Mingxi Cheng, Junyao Zhang, Shahin Nazarian...
**URL:** https://arxiv.org/abs/2111.04248v1

**Abstract:**
> Many intelligent transportation systems are multi-agent systems, i.e., both the traffic participants and the subsystems within the transportation infrastructure can be modeled as interacting agents. The use of AI-based methods to achieve coordination among the different agents systems can provide greater safety over transportation systems containing only human-operated vehicles, and also improve t...

### D9. The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents

**ArXiv ID:** 2508.19267v1
**Published:** 2025-08-22
**Relevance Score:** 95/100
**Authors:** Sai Teja Reddy Adapala, Yashwanth Reddy Alugubelly
**URL:** https://arxiv.org/abs/2508.19267v1

**Abstract:**
> The proliferation of autonomous AI agents marks a paradigm shift toward complex, emergent multi-agent systems. This transition introduces systemic security risks, including control-flow hijacking and cascading failures, that traditional cybersecurity paradigms are ill-equipped to address. This paper introduces the Aegis Protocol, a layered security framework designed to provide strong security gua...

### D10. Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems

**ArXiv ID:** 2509.18415v1
**Published:** 2025-09-22
**Relevance Score:** 95/100
**Authors:** Sumana Malkapuram, Sameera Gangavarapu, Kailashnath Reddy Kavalakuntla...
**URL:** https://arxiv.org/abs/2509.18415v1

**Abstract:**
> The proliferation of autonomous software agents necessitates rigorous frameworks for establishing secure and verifiable agent-to-agent (A2A) interactions, particularly when such agents are instantiated as non-human identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a cryptographically grounded mechanism for lineage verification, wherein the provenance and evolution of NHIs are anc...

### D11. Trusted Data Fusion, Multi-Agent Autonomy, Autonomous Vehicles

**ArXiv ID:** 2507.17875v1
**Published:** 2025-07-23
**Relevance Score:** 95/100
**Authors:** R. Spencer Hallyburton, Miroslav Pajic
**URL:** https://arxiv.org/abs/2507.17875v1

**Abstract:**
> Multi-agent collaboration enhances situational awareness in intelligence, surveillance, and reconnaissance (ISR) missions. Ad hoc networks of unmanned aerial vehicles (UAVs) allow for real-time data sharing, but they face security challenges due to their decentralized nature, making them vulnerable to cyber-physical attacks. This paper introduces a trust-based framework for assured sensor fusion i...

### D12. A cybersecurity AI agent selection and decision support framework

**ArXiv ID:** 2510.01751v1
**Published:** 2025-10-02
**Relevance Score:** 90/100
**Authors:** Masike Malatji
**URL:** https://arxiv.org/abs/2510.01751v1

**Abstract:**
> This paper presents a novel, structured decision support framework that systematically aligns diverse artificial intelligence (AI) agent architectures, reactive, cognitive, hybrid, and learning, with the comprehensive National Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0. By integrating agent theory with industry guidelines, this framework provides a transparent a...

### D13. Multi-Agent Risks from Advanced AI

**ArXiv ID:** 2502.14143v1
**Published:** 2025-02-19
**Relevance Score:** 90/100
**Authors:** Lewis Hammond, Alan Chan, Jesse Clifton...
**URL:** https://arxiv.org/abs/2502.14143v1

**Abstract:**
> The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents' incentives, a...

### D14. Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses

**ArXiv ID:** 2512.06390v1
**Published:** 2025-12-06
**Relevance Score:** 90/100
**Authors:** Mehrab Hosain, Sabbir Alom Shuvo, Matthew Ogbe...
**URL:** https://arxiv.org/abs/2512.06390v1

**Abstract:**
> The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling...

### D15. Authenticated Delegation and Authorized AI Agents

**ArXiv ID:** 2501.09674v1
**Published:** 2025-01-16
**Relevance Score:** 85/100
**Authors:** Tobin South, Samuele Marro, Thomas Hardjono...
**URL:** https://arxiv.org/abs/2501.09674v1

**Abstract:**
> The rapid deployment of autonomous AI agents creates urgent challenges around authorization, accountability, and access control in digital spaces. New standards are needed to know whom AI agents act on behalf of and guide their use appropriately, protecting online spaces while unlocking the value of task delegation to autonomous agents. We introduce a novel framework for authenticated, authorized,...

---

## QUERY EXECUTION STATISTICS

| Metric | Value |
|--------|-------|
| Cluster C Queries Executed | 9 |
| Cluster D Queries Executed | 8 |
| Total Queries | 17 |
| Cluster C Papers Identified | 26 |
| Cluster D Papers Identified | 63 |
| Total Papers Identified | 89 |
| Average Relevance Score (C) | 67.9 |
| Average Relevance Score (D) | 77.1 |

---

## METHODOLOGY

### Query Execution (17 total queries)
- **Cluster C (9 queries):** LLM hallucination detection, evidence integrity, synthetic data
- **Cluster D (8 queries):** Multi-agent systems, trust, delegation, orchestration

### Paper Screening Criteria
- **Publication Year:** 2024-2025
- **Minimum Abstract Length:** 100 characters
- **Relevance Scoring:** Keyword-based (0-100)
  - Critical keywords: 15 points each
  - Supporting keywords: 5 points each
  - Year bonus: 5-10 points for 2024-2025
- **Minimum Score for Inclusion:** 50/100

### Data Collection
- **ArXiv API Queries:** 17 searches
- **Total Results Screened:** 1,600+ papers
- **Rate Limiting:** 3+ seconds between API calls
- **Abstract Fetching:** 101 papers with full details retrieved

---

## OUTPUT ARTIFACTS

**Cluster C Directory:** `/Users/tamnguyen/Documents/GitHub/ksi_watch/KSI-AFR-02_25-12A_KeySecurityIndicators/references/cluster-c_evidence-integrity`
**Cluster D Directory:** `/Users/tamnguyen/Documents/GitHub/ksi_watch/KSI-AFR-02_25-12A_KeySecurityIndicators/references/cluster-d_multiagent-trust`

### Generated Files
- `*_metadata.json`: Paper metadata with ArXiv links and relevance scores
- `PAPERS_INDEX.md`: Comprehensive index of all papers (this file)

---

## RECOMMENDATIONS

### For Cluster C (Evidence Integrity)
1. **Hallucination Detection:** Focus on papers with automated detection methods
2. **Truthfulness Evaluation:** Look for benchmark datasets and evaluation frameworks
3. **Mitigation Strategies:** Review fine-tuning approaches and retrieval-augmented generation

### For Cluster D (Multi-Agent Trust)
1. **Delegation Models:** Study identity and capability-based delegation protocols
2. **Trust Frameworks:** Review Byzantine resilience and attestation mechanisms
3. **Orchestration Security:** Examine supply chain and agent communication protocols

---
**Report Generated:** 2026-01-10 18:09:06 
