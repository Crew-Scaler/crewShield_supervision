<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .paper-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
        .paper-meta { color: #666; margin-bottom: 15px; }
        .paper-link { margin: 10px 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .summary { margin-top: 15px; padding: 10px; background: #f5f5f5; }
    </style>
</head>
<body>
    <div class="paper-title">Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment</div>
    <div class="paper-meta">
        <p><strong>ArXiv ID:</strong> 2512.24263v1</p>
        <p><strong>Published:</strong> 2025-12-30</p>
        <p><strong>Relevance Score:</strong> 70/100</p>
        <p><strong>Authors:</strong> Lijun Zhang, Lin Li, Wei Wei, Yajie Qi, Huizhong Song, Jun Wang, Yaodong Yang, Jiye Liang</p>
    </div>
    <div class="paper-link">
        <a href="https://arxiv.org/abs/2512.24263v1" target="_blank">View on ArXiv (Abstract)</a><br>
        <a href="https://arxiv.org/pdf/2512.24263v1.pdf" target="_blank">Download PDF</a>
    </div>
    <div class="summary">
        <strong>Summary:</strong><br>
        When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insuff...
    </div>
</body>
</html>
