<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .paper-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
        .paper-meta { color: #666; margin-bottom: 15px; }
        .paper-link { margin: 10px 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .summary { margin-top: 15px; padding: 10px; background: #f5f5f5; }
    </style>
</head>
<body>
    <div class="paper-title">Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time</div>
    <div class="paper-meta">
        <p><strong>ArXiv ID:</strong> 2512.24574v1</p>
        <p><strong>Published:</strong> 2025-12-31</p>
        <p><strong>Relevance Score:</strong> 70/100</p>
        <p><strong>Authors:</strong> Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun</p>
    </div>
    <div class="paper-link">
        <a href="https://arxiv.org/abs/2512.24574v1" target="_blank">View on ArXiv (Abstract)</a><br>
        <a href="https://arxiv.org/pdf/2512.24574v1.pdf" target="_blank">Download PDF</a>
    </div>
    <div class="summary">
        <strong>Summary:</strong><br>
        Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inco...
    </div>
</body>
</html>
