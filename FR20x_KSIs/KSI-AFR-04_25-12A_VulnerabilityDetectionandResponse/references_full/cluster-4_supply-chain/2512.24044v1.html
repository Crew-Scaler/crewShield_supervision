<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .paper-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
        .paper-meta { color: #666; margin-bottom: 15px; }
        .paper-link { margin: 10px 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .summary { margin-top: 15px; padding: 10px; background: #f5f5f5; }
    </style>
</head>
<body>
    <div class="paper-title">Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?</div>
    <div class="paper-meta">
        <p><strong>ArXiv ID:</strong> 2512.24044v1</p>
        <p><strong>Published:</strong> 2025-12-30</p>
        <p><strong>Relevance Score:</strong> 74/100</p>
        <p><strong>Authors:</strong> Yuan Xin, Dingfan Chen, Linyi Yang, Michael Backes, Xiao Zhang</p>
    </div>
    <div class="paper-link">
        <a href="https://arxiv.org/abs/2512.24044v1" target="_blank">View on ArXiv (Abstract)</a><br>
        <a href="https://arxiv.org/pdf/2512.24044v1.pdf" target="_blank">Download PDF</a>
    </div>
    <div class="summary">
        <strong>Summary:</strong><br>
        As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, pr...
    </div>
</body>
</html>
