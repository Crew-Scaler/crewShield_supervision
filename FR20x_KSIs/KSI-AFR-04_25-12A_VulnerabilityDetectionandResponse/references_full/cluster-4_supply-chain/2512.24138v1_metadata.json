{
  "arxiv_id": "2512.24138v1",
  "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
  "authors": [
    "Haoran He",
    "Yuxiao Ye",
    "Jie Liu",
    "Jiajun Liang",
    "Zhiyong Wang",
    "Ziyang Yuan",
    "Xintao Wang",
    "Hangyu Mao",
    "Pengfei Wan",
    "Ling Pan"
  ],
  "published": "2025-12-30",
  "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions ad...",
  "categories": "cs.LG",
  "relevance_score": 70,
  "color": "GREEN",
  "download_date": "2026-01-10T19:02:32.801692",
  "pdf_url": "https://arxiv.org/pdf/2512.24138v1.pdf",
  "abs_url": "https://arxiv.org/abs/2512.24138v1"
}