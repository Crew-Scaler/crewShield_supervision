<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .paper-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
        .paper-meta { color: #666; margin-bottom: 15px; }
        .paper-link { margin: 10px 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .summary { margin-top: 15px; padding: 10px; background: #f5f5f5; }
    </style>
</head>
<body>
    <div class="paper-title">SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models</div>
    <div class="paper-meta">
        <p><strong>ArXiv ID:</strong> 2512.19317v1</p>
        <p><strong>Published:</strong> 2025-12-22</p>
        <p><strong>Relevance Score:</strong> 70/100</p>
        <p><strong>Authors:</strong> A. A. Gde Yogi Pramana, Jason Ray, Anthony Jaya, Michael Wijaya</p>
    </div>
    <div class="paper-link">
        <a href="https://arxiv.org/abs/2512.19317v1" target="_blank">View on ArXiv (Abstract)</a><br>
        <a href="https://arxiv.org/pdf/2512.19317v1.pdf" target="_blank">Download PDF</a>
    </div>
    <div class="summary">
        <strong>Summary:</strong><br>
        Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalizat...
    </div>
</body>
</html>
