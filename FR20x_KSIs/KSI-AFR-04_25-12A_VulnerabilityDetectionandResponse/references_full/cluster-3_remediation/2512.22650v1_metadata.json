{
  "arxiv_id": "2512.22650v1",
  "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights",
  "authors": [
    "Shuyu Gan",
    "James Mooney",
    "Pan Hao",
    "Renxiang Wang",
    "Mingyi Hong",
    "Qianwen Wang",
    "Dongyeop Kang"
  ],
  "published": "2025-12-27",
  "summary": "Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-...",
  "categories": "cs.LG",
  "relevance_score": 70,
  "color": "GREEN",
  "download_date": "2026-01-10T19:01:50.769528",
  "pdf_url": "https://arxiv.org/pdf/2512.22650v1.pdf",
  "abs_url": "https://arxiv.org/abs/2512.22650v1"
}