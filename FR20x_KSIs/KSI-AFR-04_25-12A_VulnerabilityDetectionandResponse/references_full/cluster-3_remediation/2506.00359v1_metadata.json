{
  "arxiv_id": "2506.00359v1",
  "title": "Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy",
  "authors": [
    "Jie Ren",
    "Zhenwei Dai",
    "Xianfeng Tang",
    "Yue Xing",
    "Shenglai Zeng",
    "Hui Liu",
    "Jingying Zeng",
    "Qiankun Peng",
    "Samarth Varshney",
    "Suhang Wang",
    "Qi He",
    "Charu C. Aggarwal",
    "Hui Liu"
  ],
  "published": "2025-05-31",
  "summary": "Although Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks, growing concerns have emerged over the misuse of sensitive, copyrighted, or harmful data during training. To address these concerns, unlearning techniques have been developed to remove the influence of specific data without retraining from scratch. However, this paper reveals a critical vulnerability in fine-tuning-based unlearning: a malicious user can craft a manipulated forgetting req...",
  "categories": "cs.CR",
  "relevance_score": 70,
  "color": "GREEN",
  "download_date": "2026-01-10T19:02:01.321471",
  "pdf_url": "https://arxiv.org/pdf/2506.00359v1.pdf",
  "abs_url": "https://arxiv.org/abs/2506.00359v1"
}