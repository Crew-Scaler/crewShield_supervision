<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Adversarial Robustness of Vision in Open Foundation Models</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .paper-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
        .paper-meta { color: #666; margin-bottom: 15px; }
        .paper-link { margin: 10px 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .summary { margin-top: 15px; padding: 10px; background: #f5f5f5; }
    </style>
</head>
<body>
    <div class="paper-title">Adversarial Robustness of Vision in Open Foundation Models</div>
    <div class="paper-meta">
        <p><strong>ArXiv ID:</strong> 2512.17902v1</p>
        <p><strong>Published:</strong> 2025-12-19</p>
        <p><strong>Relevance Score:</strong> 74/100</p>
        <p><strong>Authors:</strong> Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</p>
    </div>
    <div class="paper-link">
        <a href="https://arxiv.org/abs/2512.17902v1" target="_blank">View on ArXiv (Abstract)</a><br>
        <a href="https://arxiv.org/pdf/2512.17902v1.pdf" target="_blank">Download PDF</a>
    </div>
    <div class="summary">
        <strong>Summary:</strong><br>
        With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates t...
    </div>
</body>
</html>
