<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .paper-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
        .paper-meta { color: #666; margin-bottom: 15px; }
        .paper-link { margin: 10px 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .summary { margin-top: 15px; padding: 10px; background: #f5f5f5; }
    </style>
</head>
<body>
    <div class="paper-title">Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</div>
    <div class="paper-meta">
        <p><strong>ArXiv ID:</strong> 2512.12069v2</p>
        <p><strong>Published:</strong> 2025-12-12</p>
        <p><strong>Relevance Score:</strong> 79/100</p>
        <p><strong>Authors:</strong> Peichun Hua, Hao Li, Shanghao Shi, Zhiyuan Yu, Ning Zhang</p>
    </div>
    <div class="paper-link">
        <a href="https://arxiv.org/abs/2512.12069v2" target="_blank">View on ArXiv (Abstract)</a><br>
        <a href="https://arxiv.org/pdf/2512.12069v2.pdf" target="_blank">Download PDF</a>
    </div>
    <div class="summary">
        <strong>Summary:</strong><br>
        Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which lim...
    </div>
</body>
</html>
