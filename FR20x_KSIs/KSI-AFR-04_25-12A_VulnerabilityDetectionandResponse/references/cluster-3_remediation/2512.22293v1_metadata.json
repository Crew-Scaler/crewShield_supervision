{
  "arxiv_id": "2512.22293v1",
  "title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against",
  "authors": [
    "Tsogt-Ochir Enkhbayar"
  ],
  "published": "2025-12-25",
  "summary": "Warning-framed content in training data (e.g., \"DO NOT USE - this code is vulnerable\") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: \"describing X\" and \"performing X\" activate overlapping latent ...",
  "categories": "cs.LG",
  "relevance_score": 79,
  "color": "GREEN",
  "download_date": "2026-01-10T19:01:57.758360",
  "pdf_url": "https://arxiv.org/pdf/2512.22293v1.pdf",
  "abs_url": "https://arxiv.org/abs/2512.22293v1"
}