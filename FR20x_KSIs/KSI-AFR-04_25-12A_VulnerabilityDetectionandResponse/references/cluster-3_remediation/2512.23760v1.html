<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .paper-title { font-size: 18px; font-weight: bold; margin-bottom: 10px; }
        .paper-meta { color: #666; margin-bottom: 15px; }
        .paper-link { margin: 10px 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .summary { margin-top: 15px; padding: 10px; background: #f5f5f5; }
    </style>
</head>
<body>
    <div class="paper-title">Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory</div>
    <div class="paper-meta">
        <p><strong>ArXiv ID:</strong> 2512.23760v1</p>
        <p><strong>Published:</strong> 2025-12-28</p>
        <p><strong>Relevance Score:</strong> 82/100</p>
        <p><strong>Authors:</strong> Ken Huang, Jerry Huang</p>
    </div>
    <div class="paper-link">
        <a href="https://arxiv.org/abs/2512.23760v1" target="_blank">View on ArXiv (Abstract)</a><br>
        <a href="https://arxiv.org/pdf/2512.23760v1.pdf" target="_blank">Download PDF</a>
    </div>
    <div class="summary">
        <strong>Summary:</strong><br>
        Reinforcement learning is increasingly used to transform large language models into agentic systems that act over long horizons, invoke tools, and manage memory under partial observability. While recent work has demonstrated performance gains through tool learning, verifiable rewards, and continual ...
    </div>
</body>
</html>
