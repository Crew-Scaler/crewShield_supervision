{
  "arxiv_id": "2601.05214",
  "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
  "authors": [
    "Kait Healy",
    "Bharathi Srinivasan",
    "Visakh Madathil",
    "Jing Wu"
  ],
  "published": "2026-01-08",
  "relevance_score": 53,
  "url": "https://arxiv.org/abs/2601.05214",
  "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhib"
}