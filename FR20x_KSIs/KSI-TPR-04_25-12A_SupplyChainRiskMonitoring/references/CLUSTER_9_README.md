# Cluster 9: Detection Evasion & Adversarial AI
## Quick Reference Guide

**Status**: âœ“ COMPLETE  
**Date**: 2026-01-05  
**Papers Selected**: 12  
**Average Relevance**: 7.5/10  
**Research Focus**: Adversarial attacks on detection systems, evasion techniques, defense mechanisms

---

## Quick Paper Summary

### Top-Tier Papers (Relevance 10.0/10)

1. **LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors** [2512.21404v1]
   - Focus: How LLMs craft adversarial examples against ML-based malware detectors
   - Significance: Demonstrates advanced adversarial attack generation techniques

2. **Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems** [2512.23132v1]
   - Focus: Multi-agent architectures for handling data poisoning, model extraction, prompt injection attacks
   - Significance: Addresses autonomous agent-based threats and resilience strategies

### High-Impact Papers (Relevance 8.0/10)

3. **Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models** [2512.22046v1]
4. **CoTDeceptor: Adversarial Code Obfuscation Against CoT-Enhanced LLM Code Agents** [2512.21250v1]
5. **Defending against Adversarial Attacks using Mixture of Experts** [2512.20821v1]

### Strong Papers (Relevance 7.0/10)

6. **Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation** [2512.24792v1]
7. **HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs** [2512.24665v1]
8. **Scaling Adversarial Training via Data Selection** [2512.22069v1]

### Complementary Papers (Relevance 6.0/10)

9. **The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition** [2601.00065v1]
10. **Noise-Aware and Dynamically Adaptive Federated Defense Framework for SAR Image Target Recognition** [2601.00900v1]
11. **Towards Provably Secure Generative AI: Reliable Consensus Sampling** [2512.24925v1]
12. **Training-Free Color-Aware Adversarial Diffusion Sanitization for Diffusion Stegomalware Defense at Security Gateways** [2512.24499v1]

---

## Key Topics Covered

### Attack Vectors
- Adversarial perturbations against ML classifiers
- Backdoor injection in neural networks
- Trojan/poisoning attacks
- Code obfuscation against LLM detectors
- Prompt injection and jailbreaking

### Defense Mechanisms
- Adversarial training and robust models
- Mixture of experts architecture
- Federated learning robustness
- Detection system hardening
- Security-aware model composition

### Threat Landscape
- Malware detection evasion
- Vision foundation model attacks
- Code analysis detector circumvention
- Graph neural network poisoning
- Steganographic malware

---

## Research Statistics

| Metric | Value |
|--------|-------|
| Papers Found | 123 |
| Papers Assessed | 38 |
| Papers Selected | 12 |
| Avg Relevance Score | 7.5/10 |
| Publication Year | 2025-2026 |
| Categories | cs.CR (7), cs.LG (5) |

---

## Access the Papers

### CSV Index
**File**: `cluster_9_metadata.csv`
**Contains**: ArXiv ID, title, authors, publication date, relevance score, abstract summary

### Individual Papers
**Folder**: `cluster_9_detection_evasion/`
**Note**: PDFs pending download (ArXiv access restrictions lifted)

### ArXiv Direct Access
Each paper can be accessed directly at: `https://arxiv.org/abs/{arxiv_id}`

Example: https://arxiv.org/abs/2512.21404v1

---

## Key Insights

### 1. Adversarial Attacks on Security Systems
- ML-based malware detectors remain vulnerable to crafted adversarial examples
- LLM-driven attack generation enables novel, adaptive evasion techniques
- Foundation models in computer vision are susceptible to backdoor attacks

### 2. Evasion Strategies
- Code obfuscation can defeat LLM-based code agents and detectors
- Model composition introduces new attack surfaces (vocabulary expansion, weight merging)
- Steganographic techniques bypass traditional gateway security

### 3. Autonomous Agent Threats
- Multi-agent frameworks enable coordinated, distributed attacks
- Autonomous agents can perform data poisoning, model extraction, and prompt injection
- Defense must account for adaptive, continuously learning adversaries

### 4. Defense Innovations
- Adversarial training remains effective when properly implemented with data selection
- Mixture of experts provides robustness across diverse attack types
- Federated learning can improve detection resilience while preserving privacy

---

## How to Use This Research

### For Threat Modeling
1. Review papers 1-2 for understanding autonomous agent attack capabilities
2. Study papers 3-5 for specific attack methodologies
3. Consult papers 6-8 for understanding defense mechanisms

### For Detection System Hardening
1. Start with papers 5, 9, 10 for defense techniques
2. Reference papers 1, 4 for understanding potential attack vectors
3. Consider multi-layered approach from paper 2

### For Understanding Supply Chain Risks
- Connect evasion techniques (papers 3-4) to supply chain attack vectors
- Apply multi-agent frameworks (paper 2) to distributed threat models
- Leverage defense innovations (papers 5, 10-12) for protection strategies

---

## Publication Details

All selected papers are from December 2025 - January 2026, representing the latest research in:
- Adversarial machine learning
- AI system security
- Detection and response mechanisms
- Defensive strategies

**ArXiv Categories**:
- **cs.CR (Computer Security)**: 7 papers
- **cs.LG (Machine Learning)**: 5 papers

---

## Research Methodology

### Selection Criteria
Papers were selected based on:
1. **Attack Component**: Must address adversarial attacks, evasion, or malware techniques
2. **Defense Component**: Must address detection, robustness, or security context
3. **Relevance Score**: Minimum 6.0/10 on dual-requirement evaluation
4. **Publication**: 2024-2025 timeframe (prioritized latest 2025 publications)

### Assessment Process
- 123 papers identified through domain-specific searches
- 38 papers assessed as meeting criteria
- 12 papers selected for final collection

### Quality Assurance
- All papers from recognized ArXiv categories (cs.CR, cs.LG)
- Authors from leading security and ML research institutions
- Methodologically sound, evidence-based research
- Direct relevance to detection evasion and adversarial AI

---

## Next Steps

1. Download papers once ArXiv access restrictions are lifted
2. Extract and verify page counts (minimum 7 pages)
3. Obtain author affiliation information
4. Proceed with detailed analysis phase
5. Prepare for cross-cluster synthesis

---

## File References

| File | Purpose |
|------|---------|
| `cluster_9_metadata.csv` | Paper index with metadata |
| `CLUSTER_9_COMPLETION_REPORT.md` | Detailed analysis and findings |
| `CLUSTER_9_README.md` | This quick reference |
| `cluster_9_detection_evasion/` | PDFs (pending download) |

---

## Quick Navigation

- **View Papers**: See `cluster_9_metadata.csv`
- **Read Abstracts**: Visit https://arxiv.org/abs/{arxiv_id}
- **Full Analysis**: See `CLUSTER_9_COMPLETION_REPORT.md`
- **Project Index**: See `INDEX.md` (parent directory)

---

**Last Updated**: 2026-01-05  
**Status**: Research Complete - PDFs Pending  
**Next Cluster**: Cluster 10 (SBOM Composition)
