{
  "arxiv_id": "2512.23518v1",
  "title": "Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias",
  "authors": [
    "Hazel Kim",
    "Philip Torr"
  ],
  "affiliation": "",
  "published_date": "2025-12-29",
  "url": "https://arxiv.org/abs/2512.23518v1",
  "relevance_score": 91,
  "key_topics": [],
  "summary": "Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenome"
}