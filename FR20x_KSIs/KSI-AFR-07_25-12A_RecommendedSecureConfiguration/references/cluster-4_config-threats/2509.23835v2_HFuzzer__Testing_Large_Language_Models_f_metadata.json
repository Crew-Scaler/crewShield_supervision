{
  "arxiv_id": "2509.23835v2",
  "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing",
  "authors": [
    "Yukai Zhao",
    "Menghan Wu",
    "Xing Hu"
  ],
  "published_date": "2025-09",
  "url": "https://arxiv.org/abs/2509.23835v2",
  "relevance_score": 86,
  "query_id": "4.1",
  "cluster": "cluster-4_config-threats",
  "key_topics": [
    "attack",
    "threat",
    "security"
  ],
  "summary": "Large Language Models (LLMs) are widely used for code generation, but they face critical security risks when applied to practical production due to package hallucinations, in which LLMs recommend non-..."
}