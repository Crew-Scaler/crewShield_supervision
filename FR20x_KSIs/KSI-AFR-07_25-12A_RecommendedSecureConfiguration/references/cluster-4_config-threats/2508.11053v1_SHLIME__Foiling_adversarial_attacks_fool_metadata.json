{
  "arxiv_id": "2508.11053v1",
  "title": "SHLIME: Foiling adversarial attacks fooling SHAP and LIME",
  "authors": [
    "Sam Chauhan",
    "Estelle Duguet",
    "Karthik Ramakrishnan"
  ],
  "published_date": "2025-08",
  "url": "https://arxiv.org/abs/2508.11053v1",
  "relevance_score": 87,
  "query_id": "4.5",
  "cluster": "cluster-4_config-threats",
  "key_topics": [
    "attack",
    "threat",
    "security"
  ],
  "summary": "Post hoc explanation methods, such as LIME and SHAP, provide interpretable insights into black-box classifiers and are increasingly used to assess model biases and generalizability. However, these met..."
}