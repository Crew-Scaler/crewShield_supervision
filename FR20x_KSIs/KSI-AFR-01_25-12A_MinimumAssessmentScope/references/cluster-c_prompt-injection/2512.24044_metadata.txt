Title: Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?

Authors: Yuan Xin, Dingfan Chen, Linyi Yang, Michael Backes, Xiao Zhang


Published: 2025-12-30

Query ID: C-1.2

Relevance Color: Green
Relevance Score: 0.89

Pages: 32

URL: https://arxiv.org/pdf/2512.24044.pdf

Abstract:
As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address thi...

Relevance Justification:
Discusses LLM jailbreaking techniques; Analyzes adversarial attacks on language models
