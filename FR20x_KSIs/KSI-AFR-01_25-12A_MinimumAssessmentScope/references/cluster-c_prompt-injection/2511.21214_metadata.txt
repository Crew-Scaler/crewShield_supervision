Title: Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines

Authors: Yuhang Wang, Yanxu Zhu, Dongyuan Lu, Jitao Sang


Published: 2025-11-26

Query ID: C-1.8

Relevance Color: Green
Relevance Score: 0.89

Pages: 18

URL: https://arxiv.org/pdf/2511.21214.pdf

Abstract:
Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs....

Relevance Justification:
Discusses LLM jailbreaking techniques; Analyzes adversarial attacks on language models; Proposes defense mechanisms against attacks
