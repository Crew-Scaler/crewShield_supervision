Title: What Matters For Safety Alignment?

Authors: Xing Li, Hui-Ling Zhen, Lihao Yin, Xianzhi Yu, Zhenhua Dong


Published: 2026-01-07

Query ID: C-1.3

Relevance Color: Green
Relevance Score: 0.80

Pages: 28

URL: https://arxiv.org/pdf/2601.03868.pdf

Abstract:
This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spa...

Relevance Justification:
Directly addresses prompt injection attacks; Discusses LLM jailbreaking techniques; Analyzes adversarial attacks on language models
