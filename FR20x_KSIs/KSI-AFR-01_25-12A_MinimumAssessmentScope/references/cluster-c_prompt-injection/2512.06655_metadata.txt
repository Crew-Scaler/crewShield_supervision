Title: GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering

Authors: Jehyeok Yeon, Federico Cinus, Yifan Wu, Luca Luceri


Published: 2025-12-07

Query ID: C-1.8

Relevance Color: Green
Relevance Score: 0.89

Pages: 32

URL: https://arxiv.org/pdf/2512.06655.pdf

Abstract:
Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal ...

Relevance Justification:
Discusses LLM jailbreaking techniques; Analyzes adversarial attacks on language models; Proposes defense mechanisms against attacks
