Title: Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions

Authors: Shih-Han Chan


Published: 2025-03-29

Query ID: C-3.1

Relevance Color: Green
Relevance Score: 0.81

Pages: 12

URL: https://arxiv.org/pdf/2503.23250.pdf

Abstract:
Security threats like prompt injection attacks pose significant risks to applications that integrate Large Language Models (LLMs), potentially leading to unauthorized actions such as API misuse. Unlike previous approaches that aim to detect these attacks on a best-effort basis, this paper introduces a novel method that appends an Encrypted Prompt to each user prompt, embedding current permissions. These permissions are verified before executing any actions (such as API calls) generated by the LL...

Relevance Justification:
Directly addresses prompt injection attacks; Analyzes adversarial attacks on language models
