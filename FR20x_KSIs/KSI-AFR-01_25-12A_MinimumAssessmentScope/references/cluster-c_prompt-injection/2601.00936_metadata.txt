Title: Emoji-Based Jailbreaking of Large Language Models

Authors: M P V S Gopinadh, S Mahaboob Hussain


Published: 2026-01-02

Query ID: C-1.3

Relevance Color: Green
Relevance Score: 0.74

Pages: 10

URL: https://arxiv.org/pdf/2601.00936.pdf

Abstract:
Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence,...

Relevance Justification:
Discusses LLM jailbreaking techniques; Analyzes adversarial attacks on language models
