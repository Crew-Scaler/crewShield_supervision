Title: Are LLMs Good Safety Agents or a Propaganda Engine?

Authors: Neemesh Yadav, Francesco Ortu, Jiarui Liu, Joeun Yook, Bernhard Sch√∂lkopf


Published: 2025-11-28

Query ID: C-2.5

Relevance Color: Green
Relevance Score: 0.73

Pages: 19

URL: https://arxiv.org/pdf/2511.23174.pdf

Abstract:
Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an expli...

Relevance Justification:
Directly addresses prompt injection attacks; Focuses on AI agent security
