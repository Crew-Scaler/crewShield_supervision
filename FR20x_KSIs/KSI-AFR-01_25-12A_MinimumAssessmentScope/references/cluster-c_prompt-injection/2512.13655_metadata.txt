Title: Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation

Authors: Richard J. Young


Published: 2025-12-15

Query ID: C-2.6

Relevance Color: Green
Relevance Score: 0.73

Pages: 31

URL: https://arxiv.org/pdf/2512.13655.pdf

Abstract:
Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliterat...

Relevance Justification:
Analyzes adversarial attacks on language models
