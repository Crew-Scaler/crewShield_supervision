Title: Agent Safety Alignment via Reinforcement Learning

Authors: Zeyang Sha, Hanling Tian, Zhuoer Xu, Shiwen Cui, Changhua Meng


Published: 2025-07-11

Query ID: C-3.4

Relevance Color: Green
Relevance Score: 0.81

Pages: 15

URL: https://arxiv.org/pdf/2507.08270.pdf

Abstract:
The emergence of autonomous Large Language Model (LLM) agents capable of tool usage has introduced new safety risks that go beyond traditional conversational misuse. These agents, empowered to execute external functions, are vulnerable to both user-initiated threats (e.g., adversarial prompts) and tool-initiated threats (e.g., malicious outputs from compromised tools). In this paper, we propose the first unified safety-alignment framework for tool-using agents, enabling models to handle both cha...

Relevance Justification:
Analyzes adversarial attacks on language models; Focuses on AI agent security
