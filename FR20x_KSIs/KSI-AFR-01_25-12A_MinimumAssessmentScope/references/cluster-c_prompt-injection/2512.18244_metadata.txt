Title: Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation

Authors: Zehao Liu, Xi Lin


Published: 2025-12-20

Query ID: C-1.4

Relevance Color: Green
Relevance Score: 0.81

Pages: 26

URL: https://arxiv.org/pdf/2512.18244.pdf

Abstract:
Large Language Models (LLMs) have gained considerable popularity and protected by increasingly sophisticated safety mechanisms. However, jailbreak attacks continue to pose a critical security threat by inducing models to generate policy-violating behaviors. Current paradigms focus on input-level anomalies, overlooking that the model's internal psychometric state can be systematically manipulated. To address this, we introduce Psychological Jailbreak, a new jailbreak attack paradigm that exposes ...

Relevance Justification:
Discusses LLM jailbreaking techniques; Analyzes adversarial attacks on language models; Proposes defense mechanisms against attacks
