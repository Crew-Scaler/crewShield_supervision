Title: Red Teaming Large Reasoning Models

Authors: Jiawei Chen, Yang Yang, Chao Yu, Yu Tian, Zhi Cao


Published: 2025-11-29

Query ID: D-2.2

Relevance Color: Green
Relevance Score: 0.73

Pages: 36

URL: https://arxiv.org/pdf/2512.00412.pdf

Abstract:
Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM ev...

Relevance Justification:
Addresses RAG pipeline attacks
