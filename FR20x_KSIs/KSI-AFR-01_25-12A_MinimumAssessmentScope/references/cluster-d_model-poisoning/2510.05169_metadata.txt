Title: From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs

Authors: Guangyu Shen, Siyuan Cheng, Xiangzhe Xu, Yuan Zhou, Hanxi Guo


Published: 2025-10-05

Query ID: D-2.8

Relevance Color: Green
Relevance Score: 0.89

Pages: 27

URL: https://arxiv.org/pdf/2510.05169.pdf

Abstract:
Large Language Models (LLMs) can acquire deceptive behaviors through backdoor attacks, where the model executes prohibited actions whenever secret triggers appear in the input. Existing safety training methods largely fail to address this vulnerability, due to the inherent difficulty of uncovering hidden triggers implanted in the model. Motivated by recent findings on LLMs' situational awareness, we propose a novel post-training framework that cultivates self-awareness of backdoor risks and enab...

Relevance Justification:
Discusses backdoor attacks in ML models; Addresses RAG pipeline attacks; Analyzes trigger-based attacks
