arxiv_id,title,authors,publication_date,category,relevance_score,key_findings,affiliation,download_url
2501.17433,Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation,Li et al.,2025-01,Fine-tuning Attacks,9,"Demonstrates harmful fine-tuning attacks bypass LLM safety guardrails; attacks on fine-tuning API endpoints",Multi-institutional,https://arxiv.org/abs/2501.17433
2409.18169,Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey,Halawi et al.,2024-09,Fine-tuning Attacks,9,"Comprehensive survey of fine-tuning attacks and defenses; documents multiple attack vectors",University,https://arxiv.org/abs/2409.18169
2510.05159,Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain,Shen et al.,2025-10,Supply Chain Security,9,"Backdoors in AI agent supply chain; prompt injection jailbreaks; multi-agent vulnerabilities",University,https://arxiv.org/abs/2510.05159
2512.23385,Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?,Mostefaoui et al.,2024-12,Supply Chain Security,8,"Analysis of AI supply chain vulnerabilities from developer reports; practical security solutions",University,https://arxiv.org/abs/2512.23385
2502.11687,Concealed Backdoor Attack on Deep Neural Networks using Machine Unlearning,Unknown,2025-02,Backdoor Attacks,8,"Concealed backdoors evading detection; low pre-deployment ASR; machine unlearning techniques",DAC 2025,https://arxiv.org/abs/2502.11687
2510.20792,BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation,Wang et al.,2025-10,Backdoor Attacks,8,"Backdoor attacks on diffusion models; textual triggers; text-guided graph generation",University,https://arxiv.org/abs/2510.20792
2510.08094,DarkHash: A Data-Free Backdoor Attack Against Deep Hashing,Zhou et al.,2025-10,Backdoor Attacks,8,"Data-free backdoor attacks on hashing models; poison effectiveness metrics",University,https://arxiv.org/abs/2510.08094
2406.06852,A Survey of Recent Backdoor Attacks and Defenses in Large Language Models,Xing et al.,2024-06,Backdoor Attacks,8,"Survey of LLM backdoor attacks and defenses; detection mechanisms",University,https://arxiv.org/abs/2406.06852
2505.01811,BadPatches: Backdoor Attacks Against Patch-based Mixture of Experts Architectures,Ren et al.,2025-05,Backdoor Attacks,8,"Backdoor attacks on MoE architectures; 100% attack success with 2% poisoning",University,https://arxiv.org/abs/2505.01811
2508.01605,Practical, Generalizable and Robust Backdoor Attacks on Text-to-Image Diffusion Models,Zhang et al.,2025-08,Backdoor Attacks,9,">90% attack success rate with minimal backdoored samples; diffusion model poisoning",University,https://arxiv.org/abs/2508.01605
2412.16905,Backdoor Attack with Invisible Triggers Based on Model Architecture Modification,Liu et al.,2024-12,Backdoor Attacks,8,"Invisible trigger backdoors; architecture-based backdoors; detection evasion",University,https://arxiv.org/abs/2412.16905
2506.07214,Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation,Chen et al.,2025-06,Backdoor Attacks,8,"BadSem: 98% attack success rate on VLMs; semantic manipulation attacks",University,https://arxiv.org/abs/2506.07214
2508.09456,IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding,Li et al.,2025-08,Backdoor Attacks,7,"Input-aware backdoor attacks on vision-language models; visual grounding",University,https://arxiv.org/abs/2508.09456
2503.09712,Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain,Han et al.,2025-03,Backdoor Attacks,7,"Frequency domain backdoor attacks on time series; trigger generation mechanisms",University,https://arxiv.org/abs/2503.09712
2404.15611,Model Poisoning Attacks to Federated Learning via Multi-Round Consistency,Xiao et al.,2024-04,Model Poisoning,9,"PoisonedFL attack; breaks 8 state-of-the-art fedenses; multi-round poisoning",University,https://arxiv.org/abs/2404.15611
2401.05566,Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training,Hubinger et al.,2024-01,Model Poisoning,10,"Landmark paper: deceptive backdoors persist through safety training; demonstrates safety failures",Anthropic,https://arxiv.org/abs/2401.05566
2511.15992,Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis,Kumar et al.,2025-11,Detection,8,"92.5% detection accuracy for backdoored LLMs; semantic drift analysis method",University,https://arxiv.org/abs/2511.15992
2405.20539,SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents,Zou et al.,2024-05,Model Poisoning,8,"Universal backdoor attacks on RL agents; trigger-based poisoning",University,https://arxiv.org/abs/2405.20539
2306.05499,Prompt Injection attack against LLM-integrated Applications,Liu et al.,2023-06,Model Poisoning,7,"Prompt injection attacks; 31/36 applications vulnerable; LLM-integrated systems",University,https://arxiv.org/abs/2306.05499
2004.06660,Weight Poisoning Attacks on Pre-trained Models,Kurita et al.,2020-04,Model Poisoning,7,"Weight poisoning in transfer learning; backdoors in pre-trained models",CMU,https://arxiv.org/abs/2004.06660
