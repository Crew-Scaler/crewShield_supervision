{
  "search_date": "2025-12-25T10:03:43.908206",
  "total_archived": 110,
  "papers": [
    {
      "arxiv_id": "2512.15053v1",
      "title": "The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops",
      "authors": [
        "Fanzhe Fu"
      ],
      "summary": "The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based \"prompt engineering,\" fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for \"Observable Software Engineering\" in the era of probabilistic computing.",
      "published": "2025-12-17T03:32:21Z",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.15053v1.pdf",
      "relevance_score": 89.0
    },
    {
      "arxiv_id": "2511.12385v1",
      "title": "GenSIaC: Toward Security-Aware Infrastructure-as-Code Generation with Large Language Models",
      "authors": [
        "Yikun Li",
        "Matteo Grella",
        "Daniel Nahmias",
        "Gal Engelberg",
        "Dan Klein",
        "Giancarlo Guizzardi",
        "Thijs van Ede",
        "Andrea Continella"
      ],
      "summary": "In recent years, Infrastructure as Code (IaC) has emerged as a critical approach for managing and provisioning IT infrastructure through code and automation. IaC enables organizations to create scalable and consistent environments, effectively managing servers and development settings. However, the growing complexity of cloud infrastructures has led to an increased risk of misconfigurations and security vulnerabilities in IaC scripts. To address this problem, this paper investigates the potential of Large Language Models (LLMs) in generating security-aware IaC code, avoiding misconfigurations introduced by developers and administrators. While LLMs have made significant progress in natural language processing and code generation, their ability to generate secure IaC scripts remains unclear. This paper addresses two major problems: 1) the lack of understanding of security weaknesses in IaC scripts generated by LLMs, and 2) the absence of techniques for enhancing security in generating IaC code with LLMs. To assess the extent to which LLMs contain security knowledge, we first conduct a comprehensive evaluation of base LLMs in recognizing major IaC security weaknesses during the generation and inspection of IaC code. Then, we propose GenSIaC, an instruction fine-tuning dataset designed to improve LLMs' ability to recognize potential security weaknesses. Leveraging GenSIaC, we fine-tune LLMs and instruct models to generate security-aware IaC code. Our evaluation demonstrates that our models achieve substantially improved performance in recognizing and preventing IaC security misconfigurations, e.g., boosting the F1-score from 0.303 to 0.858. Additionally, we perform ablation studies and explore GenSIaC's generalizability to other LLMs and its cross-language capabilities.",
      "published": "2025-11-15T23:23:52Z",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2511.12385v1.pdf",
      "relevance_score": 88.0
    },
    {
      "arxiv_id": "2512.20973v1",
      "title": "DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination",
      "authors": [
        "Yihan Xia",
        "Taotao Wang",
        "Wenxin Xu",
        "Shengli Zhang"
      ],
      "summary": "Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.",
      "published": "2025-12-24T06:00:39Z",
      "year": "2025",
      "categories": [
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20973v1.pdf",
      "relevance_score": 88.0
    },
    {
      "arxiv_id": "2512.01549v1",
      "title": "Delta Sum Learning: an approach for fast and global convergence in Gossip Learning",
      "authors": [
        "Tom Goethals",
        "Merlijn Sebrechts",
        "Stijn De Schrijver",
        "Filip De Turck",
        "Bruno Volckaert"
      ],
      "summary": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.",
      "published": "2025-12-01T11:23:51Z",
      "year": "2025",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.01549v1.pdf",
      "relevance_score": 86.0
    },
    {
      "arxiv_id": "2510.17311v1",
      "title": "The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment",
      "authors": [
        "Eduard Marin",
        "Jinwoo Kim",
        "Alessio Pavoni",
        "Mauro Conti",
        "Roberto Di Pietro"
      ],
      "summary": "Serverless computing has rapidly emerged as a prominent cloud paradigm, enabling developers to focus solely on application logic without the burden of managing servers or underlying infrastructure. Public serverless repositories have become key to accelerating the development of serverless applications. However, their growing popularity makes them attractive targets for adversaries. Despite this, the security posture of these repositories remains largely unexplored, exposing developers and organizations to potential risks. In this paper, we present the first comprehensive analysis of the security landscape of serverless components hosted in public repositories. We analyse 2,758 serverless components from five widely used public repositories popular among developers and enterprises, and 125,936 Infrastructure as Code (IaC) templates across three widely used IaC frameworks. Our analysis reveals systemic vulnerabilities including outdated software packages, misuse of sensitive parameters, exploitable deployment configurations, susceptibility to typo-squatting attacks and opportunities to embed malicious behaviour within compressed serverless components. Finally, we provide practical recommendations to mitigate these threats.",
      "published": "2025-10-20T08:54:31Z",
      "year": "2025",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2510.17311v1.pdf",
      "relevance_score": 85.0
    },
    {
      "arxiv_id": "2512.20795v1",
      "title": "RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale",
      "authors": [
        "Aymen Alsaadi",
        "Mason Hooten",
        "Mariya Goliyad",
        "Andre Merzky",
        "Andrew Shao",
        "Mikhail Titov",
        "Tianle Wang",
        "Yian Chen",
        "Maria Kalantzi",
        "Kent Lee",
        "Andrew Park",
        "Indira Pimpalkhare",
        "Nick Radcliffe",
        "Colin Wahl",
        "Pete Mendygral",
        "Matteo Turilli",
        "Shantenu Jha"
      ],
      "summary": "Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.",
      "published": "2025-12-23T21:42:12Z",
      "year": "2025",
      "categories": [
        "cs.DC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20795v1.pdf",
      "relevance_score": 85.0
    },
    {
      "arxiv_id": "2510.03902v1",
      "title": "Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code",
      "authors": [
        "Rana Nameer Hussain Khan",
        "Dawood Wasif",
        "Jin-Hee Cho",
        "Ali Butt"
      ],
      "summary": "The increasing complexity of cloud-native infrastructure has made Infrastructure-as-Code (IaC) essential for reproducible and scalable deployments. While large language models (LLMs) have shown promise in generating IaC snippets from natural language prompts, their monolithic, single-pass generation approach often results in syntactic errors, policy violations, and unscalable designs. In this paper, we propose MACOG (Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based architecture for IaC generation that decomposes the task into modular subtasks handled by specialized agents: Architect, Provider Harmonizer, Engineer, Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory Curator. The agents interact via a shared-blackboard, finite-state orchestrator layer, and collectively produce Terraform configurations that are not only syntactically valid but also policy-compliant and semantically coherent. To ensure infrastructure correctness and governance, we incorporate Terraform Plan for execution validation and Open Policy Agent (OPA) for customizable policy enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02 and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU, CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93, respectively.",
      "published": "2025-10-04T18:55:45Z",
      "year": "2025",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2510.03902v1.pdf",
      "relevance_score": 83.0
    },
    {
      "arxiv_id": "2512.21066v1",
      "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
      "authors": [
        "Tomoaki Yamaguchi",
        "Yutong Zhou",
        "Masahiro Ryo",
        "Keisuke Katsura"
      ],
      "summary": "Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.",
      "published": "2025-12-24T09:19:15Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.21066v1.pdf",
      "relevance_score": 83.0
    },
    {
      "arxiv_id": "2512.03024v1",
      "title": "TokenPowerBench: Benchmarking the Power Consumption of LLM Inference",
      "authors": [
        "Chenxu Niu",
        "Wei Zhang",
        "Jie Li",
        "Yongjian Zhao",
        "Tongyang Wang",
        "Xi Wang",
        "Yong Chen"
      ],
      "summary": "Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.",
      "published": "2025-12-02T18:50:17Z",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.DC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.03024v1.pdf",
      "relevance_score": 82.0
    },
    {
      "arxiv_id": "2504.16690v2",
      "title": "Logic and Concepts in the 2-category of Topoi",
      "authors": [
        "Ivan Di Liberti",
        "Lingyuan Ye"
      ],
      "summary": "We use Kan injectivity to axiomatise concepts in the 2-category of topoi. We showcase the expressivity of this language through many examples, and we establish some aspects of the formal theory of Kan extension in this 2-category (pointwise Kan extensions, fully faithful morphisms, etc.). We use this technology to introduce fragments of geometric logic, and we accommodate essentially algebraic, disjunctive, regular, and coherent logic in our framework, together with some more exotic examples. We show that each fragment $\\mathcal{H}$ in our sense identifies a lax-idempotent (relative) pseudomonad $\\mathsf{T}^{\\mathcal{H}}$ on $\\mathsf{lex}$, the $2$-category of finitely complete categories. We show that the algebras for $\\mathsf{T}^{\\mathcal{H}}$ admit a notion of classifying topos, for which we deliver several Diaconescu-type results. The construction of classifying topoi allows us to define conceptually complete fragments of geometric logic.",
      "published": "2025-04-23T13:21:57Z",
      "year": "2025",
      "categories": [
        "math.LO",
        "cs.LO",
        "math.CT"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16690v2.pdf",
      "relevance_score": 82.0
    },
    {
      "arxiv_id": "2510.20211v1",
      "title": "Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents",
      "authors": [
        "Zhenning Yang",
        "Hui Guan",
        "Victor Nicolet",
        "Brandon Paulsen",
        "Joey Dodds",
        "Daniel Kroening",
        "Ang Chen"
      ],
      "summary": "Cloud infrastructure is managed through a mix of interfaces -- traditionally, cloud consoles, command-line interfaces (CLI), and SDKs are the tools of choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the infrastructure in a \"source-of-truth\" configuration. They are capable of automatically carrying out modifications to the cloud -- deploying, updating, or destroying resources -- to bring the actual infrastructure into alignment with the IaC configuration. However, when IaC is used alongside consoles, CLIs, or SDKs, it loses visibility into external changes, causing infrastructure drift, where the configuration becomes outdated, and later IaC operations may undo valid updates or trigger errors. We present NSync, an automated system for IaC reconciliation that propagates out-of-band changes back into the IaC program. Our key insight is that infrastructure changes eventually all occur via cloud API invocations -- the lowest layer for cloud management operations. NSync gleans insights from API traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update the IaC configuration to capture the changes). It employs an agentic architecture that leverages LLMs to infer high-level intents from noisy API sequences, synthesize targeted IaC updates using specialized tools, and continually improve through a self-evolving knowledge base of past reconciliations. We further introduce a novel evaluation pipeline for injecting realistic drifts into cloud infrastructure and assessing reconciliation performance. Experiments across five real-world Terraform projects and 372 drift scenarios show that NSync outperforms the baseline both in terms of accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\\times$ improvement).",
      "published": "2025-10-23T04:57:00Z",
      "year": "2025",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2510.20211v1.pdf",
      "relevance_score": 81.0
    },
    {
      "arxiv_id": "2509.11754v1",
      "title": "A Uniqueness Theorem for Distributed Computation under Physical Constraint",
      "authors": [
        "Zhiyuan Ren",
        "Mingxuan Lu",
        "Wenchi Cheng"
      ],
      "summary": "Foundational models of computation often abstract away physical hardware limitations. However, in extreme environments like In-Network Computing (INC), these limitations become inviolable laws, creating an acute trilemma among communication efficiency, bounded memory, and robust scalability. Prevailing distributed paradigms, while powerful in their intended domains, were not designed for this stringent regime and thus face fundamental challenges. This paper demonstrates that resolving this trilemma requires a shift in perspective - from seeking engineering trade-offs to deriving solutions from logical necessity. We establish a rigorous axiomatic system that formalizes these physical constraints and prove that for the broad class of computations admitting an idempotent merge operator, there exists a unique, optimal paradigm. Any system satisfying these axioms must converge to a single normal form: Self-Describing Parallel Flows (SDPF), a purely data-centric model where stateless executors process flows that carry their own control logic. We further prove this unique paradigm is convergent, Turing-complete, and minimal. In the same way that the CAP theorem established a boundary for what is impossible in distributed state management, our work provides a constructive dual: a uniqueness theorem that reveals what is \\textit{inevitable} for distributed computation flows under physical law.",
      "published": "2025-09-15T10:15:34Z",
      "year": "2025",
      "categories": [
        "cs.DC",
        "cs.NI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2509.11754v1.pdf",
      "relevance_score": 81.0
    },
    {
      "arxiv_id": "2512.20985v1",
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": [
        "Salman Jan",
        "Hassan Ali Razzaqi",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum"
      ],
      "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.",
      "published": "2025-12-24T06:20:28Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20985v1.pdf",
      "relevance_score": 81.0
    },
    {
      "arxiv_id": "2512.20688v1",
      "title": "Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems",
      "authors": [
        "Stefano Grassi"
      ],
      "summary": "Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple \"brains\", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \\mathbf{G}_i = - \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.",
      "published": "2025-12-22T22:22:13Z",
      "year": "2025",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20688v1.pdf",
      "relevance_score": 81.0
    },
    {
      "arxiv_id": "2512.20884v1",
      "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents",
      "authors": [
        "Zan-Kai Chong",
        "Hiroyuki Ohsaki",
        "Bryan Ng"
      ],
      "summary": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($\u03b3$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $\u03b3$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[\u03b8]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.",
      "published": "2025-12-24T02:02:25Z",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20884v1.pdf",
      "relevance_score": 80.0
    },
    {
      "arxiv_id": "2512.20491v2",
      "title": "Step-DeepResearch Technical Report",
      "authors": [
        "Chen Hu",
        "Haikuo Du",
        "Heng Wang",
        "Lin Lin",
        "Mingrui Chen",
        "Peng Liu",
        "Ruihang Miao",
        "Tianchi Yue",
        "Wang You",
        "Wei Ji",
        "Wei Yuan",
        "Wenjin Deng",
        "Xiaojian Yuan",
        "Xiaoyun Zhang",
        "Xiangyu Liu",
        "Xikai Liu",
        "Yanming Xu",
        "Yicheng Cao",
        "Yifei Zhang",
        "Yongyao Wang",
        "Yubo Shu",
        "Yurong Zhang",
        "Yuxiang Zhang",
        "Zheng Gong",
        "Zhichao Chang",
        "Binyan Li",
        "Dan Ma",
        "Furong Jia",
        "Hongyuan Wang",
        "Jiayu Liu",
        "Jing Bai",
        "Junlan Liu",
        "Manjiao Liu",
        "Na Wang",
        "Qiuping Wu",
        "Qinxin Du",
        "Shiwei Li",
        "Wen Sun",
        "Yifeng Gong",
        "Yonglin Chen",
        "Yuling Zhao",
        "Yuxuan Lin",
        "Ziqi Ren",
        "Zixuan Wang",
        "Aihu Zhang",
        "Brian Li",
        "Buyun Ma",
        "Kang An",
        "Li Xie",
        "Mingliang Li",
        "Pan Li",
        "Shidong Yang",
        "Xi Chen",
        "Xiaojia Liu",
        "Yuchu Luo",
        "Yuan Song",
        "YuanHao Ding",
        "Yuanwei Liang",
        "Zexi Li",
        "Zhaoning Zhang",
        "Zixin Zhang",
        "Binxing Jiao",
        "Daxin Jiang",
        "Jiansheng Chen",
        "Jing Li",
        "Xiangyu Zhang",
        "Yibo Zhu"
      ],
      "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
      "published": "2025-12-23T16:32:27Z",
      "year": "2025",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20491v2.pdf",
      "relevance_score": 79.0
    },
    {
      "arxiv_id": "2512.19769v1",
      "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "authors": [
        "Ivan Daunis"
      ],
      "summary": "Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises). Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison. We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.",
      "published": "2025-12-22T05:03:37Z",
      "year": "2025",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19769v1.pdf",
      "relevance_score": 77.0
    },
    {
      "arxiv_id": "2512.20986v1",
      "title": "AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs",
      "authors": [
        "Yihan Wang",
        "Huanqi Yang",
        "Shantanu Pal",
        "Weitao Xu"
      ],
      "summary": "The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.",
      "published": "2025-12-24T06:29:24Z",
      "year": "2025",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20986v1.pdf",
      "relevance_score": 77.0
    },
    {
      "arxiv_id": "2505.11927v1",
      "title": "XiSort: Deterministic Sorting via IEEE-754 Total Ordering and Entropy Minimization",
      "authors": [
        "Faruk Alpay"
      ],
      "summary": "We introduce XiSort, a deterministic and reproducible sorting algorithm for floating-point sequences based on IEEE-754 total ordering and entropy minimization. XiSort guarantees bit-for-bit stability across runs and platforms by resolving tie-breaking via information-theoretic and symbolic methods. The algorithm supports both in-memory and external (out-of-core) operation, offering consistent performance on large datasets. We formalize a curved variant of the sorting metric that integrates into the Alpay Algebra framework, treating XiSort as a recursive operator with provable convergence and symbolic idempotence. This model preserves state-space closure while minimizing local disorder, interpretable as symbolic entropy. Empirical benchmarks demonstrate that XiSort achieves competitive throughput (e.g., sorting 10^8 doubles in approximately 12 seconds in-memory, and 100 GB at around 100 MB/s on SSDs), with applications in scientific computing, high-frequency finance, and reproducible numerical workflows. The results position XiSort as a principled tool for stable data alignment, symbolic preprocessing, and cross-platform float ordering. Keywords: deterministic sorting, IEEE-754, entropy minimization, symbolic algebra, reproducibility, external memory, Alpay Algebra, data pipelines",
      "published": "2025-05-17T09:25:10Z",
      "year": "2025",
      "categories": [
        "cs.DS",
        "cs.IT",
        "math.NA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2505.11927v1.pdf",
      "relevance_score": 76.0
    },
    {
      "arxiv_id": "2511.14215v1",
      "title": "A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints",
      "authors": [
        "Malik Muhammad Umer"
      ],
      "summary": "The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.",
      "published": "2025-11-18T07:45:34Z",
      "year": "2025",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2511.14215v1.pdf",
      "relevance_score": 75.0
    },
    {
      "arxiv_id": "2512.09458v1",
      "title": "Architectures for Building Agentic AI",
      "authors": [
        "S\u0142awomir Nowaczyk"
      ],
      "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",
      "published": "2025-12-10T09:28:40Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.09458v1.pdf",
      "relevance_score": 75.0
    },
    {
      "arxiv_id": "2512.19084v1",
      "title": "$\u03b3(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
      "authors": [
        "Mark Burgess"
      ],
      "summary": "The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $\u03b3(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.",
      "published": "2025-12-22T06:48:53Z",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19084v1.pdf",
      "relevance_score": 75.0
    },
    {
      "arxiv_id": "2510.27194v1",
      "title": "From product to system network challenges in system of systems lifecycle management",
      "authors": [
        "Vahid Salehi",
        "Josef Vilsmeier",
        "Shirui Wang"
      ],
      "summary": "Today, products are no longer isolated artifacts, but nodes in networked systems. This means that traditional, linearly conceived life cycle models are reaching their limits: Interoperability across disciplines, variant and configuration management, traceability, and governance across organizational boundaries are becoming key factors. This collective contribution classifies the state of the art and proposes a practical frame of reference for SoS lifecycle management, model-based systems engineering (MBSE) as the semantic backbone, product lifecycle management (PLM) as the governance and configuration level, CAD-CAE as model-derived domains, and digital thread and digital twin as continuous feedback. Based on current literature and industry experience, mobility, healthcare, and the public sector, we identify four principles: (1) referenced architecture and data models, (2) end-to-end configuration sovereignty instead of tool silos, (3) curated models with clear review gates, and (4) measurable value contributions along time, quality, cost, and sustainability. A three-step roadmap shows the transition from product- to network- centric development: piloting with reference architecture, scaling across variant and supply chain spaces, organizational anchoring (roles, training, compliance). The results are increased change robustness, shorter throughput times, improved reuse, and informed sustainability decisions. This article is aimed at decision-makers and practitioners who want to make complexity manageable and design SoS value streams to be scalable.",
      "published": "2025-10-31T05:36:35Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2510.27194v1.pdf",
      "relevance_score": 74.0
    },
    {
      "arxiv_id": "2505.11565v2",
      "title": "ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?",
      "authors": [
        "Sarthak Munshi",
        "Swapnil Pathak",
        "Sonam Ghatode",
        "Thenuga Priyadarshini",
        "Dhivya Chandramouleeswaran",
        "Ashutosh Rana"
      ],
      "summary": "While Large Language Models have shown promise in cybersecurity applications, their effectiveness in identifying security threats within cloud deployments remains unexplored. This paper introduces AWS Cloud Security Engineering Eval, a novel dataset for evaluating LLMs cloud security threat modeling capabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios, each featuring detailed architectural specifications, Infrastructure as Code implementations, documented security vulnerabilities, and associated threat modeling parameters. Our dataset enables systemic assessment of LLMs abilities to identify security risks, analyze attack vectors, and propose mitigation strategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that GPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro performing optimally in 0-shot scenarios and GPT 4.1 showing superior results in few-shot settings. While GPT 4.1 maintains a slight overall performance advantage, Claude 3.7 Sonnet generates the most semantically sophisticated threat models but struggles with threat categorization and generalization. To promote reproducibility and advance research in automated cybersecurity threat analysis, we open-source our dataset, evaluation metrics, and methodologies.",
      "published": "2025-05-16T08:40:09Z",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2505.11565v2.pdf",
      "relevance_score": 74.0
    },
    {
      "arxiv_id": "2512.04910v1",
      "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming",
      "authors": [
        "Fang Li"
      ],
      "summary": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.",
      "published": "2025-12-04T15:37:59Z",
      "year": "2025",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.04910v1.pdf",
      "relevance_score": 74.0
    },
    {
      "arxiv_id": "2512.20991v1",
      "title": "FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning",
      "authors": [
        "Toqeer Ali Syed",
        "Abdulaziz Alshahrani",
        "Ali Ullah",
        "Ali Akarma",
        "Sohail Khan",
        "Muhammad Nauman",
        "Salman Jan"
      ],
      "summary": "The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.",
      "published": "2025-12-24T06:33:17Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20991v1.pdf",
      "relevance_score": 74.0
    },
    {
      "arxiv_id": "2512.07397v1",
      "title": "From sparse recovery to plug-and-play priors, understanding trade-offs for stable recovery with generalized projected gradient descent",
      "authors": [
        "Ali Joundi",
        "Yann Traonmilin",
        "Jean-Fran\u00e7ois Aujol"
      ],
      "summary": "We consider the problem of recovering an unknown low-dimensional vector from noisy, underdetermined observations. We focus on the Generalized Projected Gradient Descent (GPGD) framework, which unifies traditional sparse recovery methods and modern approaches using learned deep projective priors. We extend previous convergence results to robustness to model and projection errors. We use these theoretical results to explore ways to better control stability and robustness constants. To reduce recovery errors due to measurement noise, we consider generalized back-projection strategies to adapt GPGD to structured noise, such as sparse outliers. To improve the stability of GPGD, we propose a normalized idempotent regularization for the learning of deep projective priors. We provide numerical experiments in the context of sparse recovery and image inverse problems, highlighting the trade-offs between identifiability and stability that can be achieved with such methods.",
      "published": "2025-12-08T10:31:48Z",
      "year": "2025",
      "categories": [
        "eess.IV",
        "cs.NE",
        "math.OC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.07397v1.pdf",
      "relevance_score": 73.0
    },
    {
      "arxiv_id": "2512.03916v1",
      "title": "New Perspectives on Semiring Applications to Dynamic Programming",
      "authors": [
        "Ambroise Baril",
        "Miguel Couceiro",
        "Victor Lagerkvist"
      ],
      "summary": "Semiring algebras have been shown to provide a suitable language to formalize many noteworthy combinatorial problems. For instance, the Shortest-Path problem can be seen as a special case of the Algebraic-Path problem when applied to the tropical semiring. The application of semirings typically makes it possible to solve extended problems without increasing the computational complexity. In this article we further exploit the idea of using semiring algebras to address and tackle several extensions of classical computational problems by dynamic programming. We consider a general approach which allows us to define a semiring extension of any problem with a reasonable notion of a certificate (e.g., an NP problem). This allows us to consider cost variants of these combinatorial problems, as well as their counting extensions where the goal is to determine how many solutions a given problem admits. The approach makes no particular assumptions (such as idempotence) on the semiring structure. We also propose a new associative algebraic operation on semirings, called $\u0394$-product, which enables our dynamic programming algorithms to count the number of solutions of minimal costs. We illustrate the advantages of our framework on two well-known but computationally very different NP-hard problems, namely, Connected-Dominating-Set problems and finite-domain Constraint Satisfaction Problems (CSPs). In particular, we prove fixed parameter tractability (FPT) with respect to clique-width and tree-width of the input. This also allows us to count solutions of minimal cost, which is an overlooked problem in the literature.",
      "published": "2025-12-03T16:02:24Z",
      "year": "2025",
      "categories": [
        "cs.CC",
        "math.RA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.03916v1.pdf",
      "relevance_score": 73.0
    },
    {
      "arxiv_id": "2510.22536v1",
      "title": "ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole",
      "authors": [
        "Jotaro Yano"
      ],
      "summary": "We formalize a cross-domain \"ZK coprocessor bridge\" that lets Solana programs request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a Solana program that posts messages to Wormhole Core with explicit finality; (ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound payload secretHash||m from the attested VAA, derives a domain-separated field commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we provide migration guidance to the payload-bound interface); (iii) a minimal Aztec contract that consumes the message privately; and (iv) an off-chain relayer that ferries VAAs and can record receipts on Solana. We present state machines, message formats, and proof sketches for replay-safety, origin authenticity, finality alignment, parameter binding (no relayer front-running of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a concise Reproducibility note with pinned versions and artifacts to replicate a public testnet run.",
      "published": "2025-10-26T05:06:56Z",
      "year": "2025",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2510.22536v1.pdf",
      "relevance_score": 73.0
    },
    {
      "arxiv_id": "2509.03365v1",
      "title": "The distribution of calibrated likelihood functions on the probability-likelihood Aitchison simplex",
      "authors": [
        "Paul-Gauthier No\u00e9",
        "Andreas Nautsch",
        "Driss Matrouf",
        "Pierre-Michel Bousquet",
        "Jean-Fran\u00e7ois Bonastre"
      ],
      "summary": "While calibration of probabilistic predictions has been widely studied, this paper rather addresses calibration of likelihood functions. This has been discussed, especially in biometrics, in cases with only two exhaustive and mutually exclusive hypotheses (classes) where likelihood functions can be written as log-likelihood-ratios (LLRs). After defining calibration for LLRs and its connection with the concept of weight-of-evidence, we present the idempotence property and its associated constraint on the distribution of the LLRs. Although these results have been known for decades, they have been limited to the binary case. Here, we extend them to cases with more than two hypotheses by using the Aitchison geometry of the simplex, which allows us to recover, in a vector form, the additive form of the Bayes' rule; extending therefore the LLR and the weight-of-evidence to any number of hypotheses. Especially, we extend the definition of calibration, the idempotence, and the constraint on the distribution of likelihood functions to this multiple hypotheses and multiclass counterpart of the LLR: the isometric-log-ratio transformed likelihood function. This work is mainly conceptual, but we still provide one application to machine learning by presenting a non-linear discriminant analysis where the discriminant components form a calibrated likelihood function over the classes, improving therefore the interpretability and the reliability of the method.",
      "published": "2025-09-03T14:48:11Z",
      "year": "2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2509.03365v1.pdf",
      "relevance_score": 73.0
    },
    {
      "arxiv_id": "2512.11942v1",
      "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play",
      "authors": [
        "Vince Trencsenyi"
      ],
      "summary": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.",
      "published": "2025-12-12T11:08:15Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.GT"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.11942v1.pdf",
      "relevance_score": 72.0
    },
    {
      "arxiv_id": "2512.20778v1",
      "title": "Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication",
      "authors": [
        "Moshe Rafaeli Shimron",
        "Vadim Indelman"
      ],
      "summary": "Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.",
      "published": "2025-12-23T21:25:53Z",
      "year": "2025",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20778v1.pdf",
      "relevance_score": 72.0
    },
    {
      "arxiv_id": "2512.20618v1",
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "authors": [
        "Runtao Liu",
        "Ziyi Liu",
        "Jiaqi Tang",
        "Yue Ma",
        "Renjie Pi",
        "Jipeng Zhang",
        "Qifeng Chen"
      ],
      "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
      "published": "2025-12-23T18:59:49Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20618v1.pdf",
      "relevance_score": 72.0
    },
    {
      "arxiv_id": "2512.12594v1",
      "title": "ceLLMate: Sandboxing Browser AI Agents",
      "authors": [
        "Luoxi Meng",
        "Henry Feng",
        "Ilia Shumailov",
        "Earlence Fernandes"
      ],
      "summary": "Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.",
      "published": "2025-12-14T08:25:31Z",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.12594v1.pdf",
      "relevance_score": 71.0
    },
    {
      "arxiv_id": "2506.05623v1",
      "title": "Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework",
      "authors": [
        "Tianyi Zhang",
        "Shidong Pan",
        "Zejun Zhang",
        "Zhenchang Xing",
        "Xiaoyu Sun"
      ],
      "summary": "Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions, but current evaluation focuses on syntactic correctness while ignoring deployability, the fatal measure of IaC template utility. We address this gap through two contributions: (1) IaCGen, an LLM-based deployability-centric framework that uses iterative feedback mechanism to generate IaC templates, and (2) DPIaC-Eval, a deployability-centric IaC template benchmark consists of 153 real-world scenarios that can evaluate syntax, deployment, user intent, and security. Our evaluation reveals that state-of-the-art LLMs initially performed poorly, with Claude-3.5 and Claude-3.7 achieving only 30.2% and 26.8% deployment success on the first attempt respectively. However, IaCGen transforms this performance dramatically: all evaluated models reach over 90% passItr@25, with Claude-3.5 and Claude-3.7 achieving 98% success rate. Despite these improvements, critical challenges remain in user intent alignment (25.2% accuracy) and security compliance (8.4% pass rate), highlighting areas requiring continued research. Our work provides the first comprehensive assessment of deployability-centric IaC template generation and establishes a foundation for future research.",
      "published": "2025-06-05T22:53:12Z",
      "year": "2025",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2506.05623v1.pdf",
      "relevance_score": 70.0
    },
    {
      "arxiv_id": "2512.09412v1",
      "title": "Simple Modal Types for Functional Reactive Programming",
      "authors": [
        "Patrick Bahr"
      ],
      "summary": "Functional reactive programming (FRP) is a declarative programming paradigm for implementing reactive programs at a high level of abstraction. It applies functional programming principles to construct and manipulate time-varying values, also known as signals. However, for this programming paradigm to work in practice, an FRP language must ensure that programs are causal, productive, and free from space leaks. Over the past fifteen years, several modal type systems to enforce these operational properties have been developed. We present a new FRP language with a significantly simplified modal type system that imposes fewer restrictions than previous modal FRP languages while still guaranteeing the central operational properties of causality, productivity, and absence of space leaks. The key enabling idea is to alter the semantics of signals so that the type system can safely allow more programs to type-check, which also makes the language more expressive. With this new semantics, signals are modelled as mutable references whose mutability is tightly controlled by the 'later' type modality. This disciplined form of mutability also enables more efficient in-place updates of signals, all while preserving a functional programming style.",
      "published": "2025-12-10T08:14:19Z",
      "year": "2025",
      "categories": [
        "cs.PL"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.09412v1.pdf",
      "relevance_score": 70.0
    },
    {
      "arxiv_id": "2510.13882v1",
      "title": "Structure-Preserving Error-Correcting Codes for Polynomial Frames",
      "authors": [
        "Baigang Chen",
        "Dongfang Zhao"
      ],
      "summary": "Modern FFT/NTT analytics, coded computation, and privacy-preserving ML interface routinely move polynomial frames across NICs, storage, and accelerators. However, even rare silent data corruption (SDC) can flip a few ring coefficients and cascade through downstream arithmetic. Conventional defenses are ill-matched to current low-latency pipelines: detect-and-retransmit adds RTTs, while byte-stream ECC ignores the algebraic structure and forces format conversions. To that end, we propose a structure-preserving reliability layer that operates in the encoded data's original polynomial ring, adds a small amount of systematic redundancy, and corrects symbol errors/flagged erasures without round-trip or format changes. We construct two complementary schemes: one for odd length $N_{odd}$ via a Hensel-lifted BCH ideal with an idempotent encoder, and one for power-of-two length $N_{2^m}$ via a repeated-root negacyclic code with derivative-style decoding. In particular, to stay robust against clustered errors, a ring automorphism provides in-place interleaving to disperse bursts. Implementation wise, on four frame sizes $N\\!=\\!1024, 2048, 4096, 8192$, we meet a per-frame failure target of $10^{-9}$ at symbol error rates $10^{-6}\\text{--}10^{-5}$ with $t\\!=\\!8\\text{--}9$, incurring only $0.20\\%\\text{--}1.56\\%$ overhead and tolerating $\\sim\\!32\\text{--}72$\\,B unknown-error bursts (roughly doubled when flagged as erasures) after interleaving. By aligning error correction with ring semantics, we take a practical step toward deployable robustness for polynomial-frame computations from an algebraic coding perspective.",
      "published": "2025-10-14T00:59:45Z",
      "year": "2025",
      "categories": [
        "cs.IT"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2510.13882v1.pdf",
      "relevance_score": 70.0
    },
    {
      "arxiv_id": "2508.11136v2",
      "title": "Automating the Derivation of Unification Algorithms: A Case Study in Deductive Program Synthesis",
      "authors": [
        "Richard Waldinger"
      ],
      "summary": "The unification algorithm has long been a target for program synthesis research, but a fully automatic derivation remains a research goal. In deductive program synthesis, computer programming is phrased as a task in theorem proving; a declarative specification is expressed in logical form and presented to an automatic theorem prover, and a program meeting the specification is extracted from the proof. The correctness of the program is supported by the proof, which also provides an explanation of how the program works. The proof is conducted in an appropriate axiomatic subject-domain theory, which defines the concepts in the specification and the constructs in the target programming language and provides the background knowledge necessary to connect them. For the unification proof, we generalize and automate the manual proof presented in Manna and Waldinger [1981]. The new program unifies two given symbolic expressions (s-expressions) relative to a given \"environment\" substitution. The proof establishes the existence of an output substitution that is a most-general idempotent unifier of the given expressions and is an \"extension\" of the environment substitution. If no such substitution exists and the expressions are not unifiable, the program is to produce a failure indicator. Initially the environment substitution is the empty substitution, which makes no replacements at all; during execution of recursive calls, the environment substitution records the replacements that have been found so far. Our own unification algorithm employs an environment, and such algorithms appear in the literature [e.g., Luger and Stubblefield, 1997]. We suspect, in addition to being more efficient, the three-argument algorithm with an environment is easier to synthesize automatically than the two-argument version from the Manna-Waldinger paper.",
      "published": "2025-08-15T01:06:27Z",
      "year": "2025",
      "categories": [
        "cs.LO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2508.11136v2.pdf",
      "relevance_score": 70.0
    },
    {
      "arxiv_id": "2507.23179v2",
      "title": "Cyclotomy, cyclotomic cosets and arithmetic properties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$",
      "authors": [
        "Juncheng Zhou",
        "Hongfeng Wu"
      ],
      "summary": "Arithmetic properties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$ are obtained by using the cyclotomic classes of order 2 with respect to $n=p^sq^t$, where $p\\equiv3 \\mathrm{mod} 4$, $\\gcd(\u03c6(p^s),\u03c6(q^t))=2$, $l$ is a primitive root modulo $q^t$ and $\\mathrm{ord}_{p^s}(l)=\u03c6(p^s)/2$. The form of these cyclotomic classes enables us to further generalize the results obtained in \\cite{ref1}. The explicit expressions of primitive idempotents of minimal ideals in $\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$ are also obtained.",
      "published": "2025-07-31T01:19:47Z",
      "year": "2025",
      "categories": [
        "math.NT",
        "cs.IT"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.23179v2.pdf",
      "relevance_score": 70.0
    },
    {
      "arxiv_id": "2505.00140v1",
      "title": "Counting Specific Classes of Relations Regarding Fixed Points and Reflexive Points",
      "authors": [
        "Rudolf Berghammer",
        "Jules Desharnais",
        "Michael Winter"
      ],
      "summary": "Given a finite and non-empty set $X$ and randomly selected specific functions and relations on $X$, we investigate the existence and non-existence of fixed points and reflexive points, respectively. First, we consider the class of functions, weaken it to the classes of partial functions, total relations and general relations and also strengthen it to the class of permutations. Then we investigate the class of involutions and the subclass of proper involutions. Finally, we treat idempotent functions, partial idempotent functions and related concepts. We count relations, calculate corresponding probabilities and also calculate the limiting values of the latter in case that the cardinality of $X$ tends to infinity. All these results have been motivated and also supported by numerous experiments performed with the RelView tool.",
      "published": "2025-04-30T19:23:51Z",
      "year": "2025",
      "categories": [
        "cs.DM"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2505.00140v1.pdf",
      "relevance_score": 70.0
    },
    {
      "arxiv_id": "2511.05496v1",
      "title": "DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows",
      "authors": [
        "Hao Zhang",
        "Qinghua Lu",
        "Liming Zhu"
      ],
      "summary": "Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are \"good enough\" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation.",
      "published": "2025-09-12T08:09:09Z",
      "year": "2025",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2511.05496v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2508.01492v1",
      "title": "OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications",
      "authors": [
        "Angel C. Chavez-Moreno",
        "Cristina L. Abad"
      ],
      "summary": "Function-as-a-Service (FaaS) is at the core of serverless computing, enabling developers to easily deploy applications without managing computing resources. With an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless Framework use YAML configurations to define and deploy APIs, tasks, workflows, and event-driven applications on cloud providers, promoting zero-friction development. As with any rapidly evolving ecosystem, there is a need for updated insights into how these tools are used in real-world projects. Building on the methodology established by the Wonderless dataset for serverless computing (and applying multiple new filtering steps), OpenLambdaVerse addresses this gap by creating a dataset of current GitHub repositories that use the Serverless Framework in applications that contain one or more AWS Lambda functions. We then analyze and characterize this dataset to get an understanding of the state-of-the-art in serverless architectures based on this stack. Through this analysis we gain important insights on the size and complexity of current applications, which languages and runtimes they employ, how are the functions triggered, the maturity of the projects, and their security practices (or lack of). OpenLambdaVerse thus offers a valuable, up-to-date resource for both practitioners and researchers that seek to better understand evolving serverless workloads.",
      "published": "2025-08-02T21:30:01Z",
      "year": "2025",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2508.01492v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2507.09037v1",
      "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making",
      "authors": [
        "Bharadwaj Ravichandran",
        "David Joy",
        "Paul Elliott",
        "Brian Hu",
        "Jadie Adams",
        "Christopher Funk",
        "Emily Veenhuis",
        "Anthony Hoogs",
        "Arslan Basharat"
      ],
      "summary": "Large language models (LLMs) are increasingly being used as decision aids. However, users have diverse values and preferences that can affect their decision-making, which requires novel methods for LLM alignment and personalization. Existing LLM comparison tools largely focus on benchmarking tasks, such as knowledge-based question answering. In contrast, our proposed ALIGN system focuses on dynamic personalization of LLM-based decision-makers through prompt-based alignment to a set of fine-grained attributes. Key features of our system include robust configuration management, structured output generation with reasoning, and several algorithm implementations with swappable LLM backbones, enabling different types of analyses. Our user interface enables a qualitative, side-by-side comparison of LLMs and their alignment to various attributes, with a modular backend for easy algorithm integration. Additionally, we perform a quantitative analysis comparing alignment approaches in two different domains: demographic alignment for public opinion surveys and value alignment for medical triage decision-making. The entire ALIGN framework is open source and will enable new research on reliable, responsible, and personalized LLM-based decision-makers.",
      "published": "2025-07-11T21:33:38Z",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.09037v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2506.12270v1",
      "title": "Cloud Infrastructure Management in the Age of AI Agents",
      "authors": [
        "Zhenning Yang",
        "Archit Bhatnagar",
        "Yiming Qiu",
        "Tongyuan Miao",
        "Patrick Tser Jern Kon",
        "Yunming Xiao",
        "Yibo Huang",
        "Martin Casado",
        "Ang Chen"
      ],
      "summary": "Cloud infrastructure is the cornerstone of the modern IT industry. However, managing this infrastructure effectively requires considerable manual effort from the DevOps engineering team. We make a case for developing AI agents powered by large language models (LLMs) to automate cloud infrastructure management tasks. In a preliminary study, we investigate the potential for AI agents to use different cloud/user interfaces such as software development kits (SDK), command line interfaces (CLI), Infrastructure-as-Code (IaC) platforms, and web portals. We report takeaways on their effectiveness on different management tasks, and identify research challenges and potential solutions.",
      "published": "2025-06-13T22:50:12Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2506.12270v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2512.21137v1",
      "title": "Declarative distributed broadcast using three-valued modal logic and semitopologies",
      "authors": [
        "Murdoch J. Gabbay"
      ],
      "summary": "We demonstrate how to formally specify distributed algorithms as declarative axiomatic theories in a modal logic. We exhibit the method on a simple voting protocol, a simple broadcast protocol, and a simple agreement protocol. The methods scale well and have been used to find errors in a proposed industrial protocol. The key novelty is to use modal logic to capture a declarative, high-level representation of essential system properties -- the logical essence of the algorithm -- while abstracting away from transitions of an abstract machine that implements it. It is like the difference between specifying code in a functional or logic programming language, versus specifying code in an imperative one. A logical axiomatisation in the style we propose provides a precise, compact, human-readable specification that abstractly captures essential system properties, while eliding low-level implementation details; it is more precise than a natural language description, yet more abstract than source code or a logical specification thereof. This creates new opportunities for reasoning about correctness, resilience, and failure, and could serve as a foundation for human- and machine verification efforts, design improvements, and even alternative protocol implementations.",
      "published": "2025-12-24T12:07:25Z",
      "year": "2025",
      "categories": [
        "cs.LO",
        "cs.DC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.21137v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2509.21470v1",
      "title": "Score-based Idempotent Distillation of Diffusion Models",
      "authors": [
        "Shehtab Zaman",
        "Chengyan Liu",
        "Kenneth Chiu"
      ],
      "summary": "Idempotent generative networks (IGNs) are a new line of generative models based on idempotent mapping to a target manifold. IGNs support both single-and multi-step generation, allowing for a flexible trade-off between computational cost and sample quality. But similar to Generative Adversarial Networks (GANs), conventional IGNs require adversarial training and are prone to training instabilities and mode collapse. Diffusion and score-based models are popular approaches to generative modeling that iteratively transport samples from one distribution, usually a Gaussian, to a target data distribution. These models have gained popularity due to their stable training dynamics and high-fidelity generation quality. However, this stability and quality come at the cost of high computational cost, as the data must be transported incrementally along the entire trajectory. New sampling methods, model distillation, and consistency models have been developed to reduce the sampling cost and even perform one-shot sampling from diffusion models. In this work, we unite diffusion and IGNs by distilling idempotent models from diffusion model scores, called SIGN. Our proposed method is highly stable and does not require adversarial losses. We provide a theoretical analysis of our proposed score-based training methods and empirically show that IGNs can be effectively distilled from a pre-trained diffusion model, enabling faster inference than iterative score-based models. SIGNs can perform multi-step sampling, allowing users to trade off quality for efficiency. These models operate directly on the source domain; they can project corrupted or alternate distributions back onto the target manifold, enabling zero-shot editing of inputs. We validate our models on multiple image datasets, achieving state-of-the-art results for idempotent models on the CIFAR and CelebA datasets.",
      "published": "2025-09-25T19:36:10Z",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2509.21470v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2508.01323v1",
      "title": "Idempotent Equilibrium Analysis of Hybrid Workflow Allocation: A Mathematical Schema for Future Work",
      "authors": [
        "Faruk Alpay",
        "Bugra Kilictas",
        "Taylan Alpay",
        "Hamdi Alakkad"
      ],
      "summary": "The rapid advance of large-scale AI systems is reshaping how work is divided between people and machines. We formalise this reallocation as an iterated task-delegation map and show that--under broad, empirically grounded assumptions--the process converges to a stable idempotent equilibrium in which every task is performed by the agent (human or machine) with enduring comparative advantage. Leveraging lattice-theoretic fixed-point tools (Tarski and Banach), we (i) prove existence of at least one such equilibrium and (ii) derive mild monotonicity conditions that guarantee uniqueness. In a stylised continuous model the long-run automated share takes the closed form $x^* = \u03b1/ (\u03b1+ \u03b2)$, where $\u03b1$ captures the pace of automation and $\u03b2$ the rate at which new, human-centric tasks appear; hence full automation is precluded whenever $\u03b2> 0$. We embed this analytic result in three complementary dynamical benchmarks--a discrete linear update, an evolutionary replicator dynamic, and a continuous Beta-distributed task spectrum--each of which converges to the same mixed equilibrium and is reproducible from the provided code-free formulas. A 2025-to-2045 simulation calibrated to current adoption rates projects automation rising from approximately 10% of work to approximately 65%, leaving a persistent one-third of tasks to humans. We interpret that residual as a new profession of workflow conductor: humans specialise in assigning, supervising and integrating AI modules rather than competing with them. Finally, we discuss implications for skill development, benchmark design and AI governance, arguing that policies which promote \"centaur\" human-AI teaming can steer the economy toward the welfare-maximising fixed point.",
      "published": "2025-08-02T11:28:34Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.CY",
        "econ.GN"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2508.01323v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2507.12017v1",
      "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection",
      "authors": [
        "Xiwei Zhang",
        "Chunjin Yang",
        "Yiming Xiao",
        "Runtong Zhang",
        "Fanman Meng"
      ],
      "summary": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB domain as a unified domain and neglect the multiple subdomains within it, such as daytime, nighttime, and foggy scenes. We argue that decoupling the domain-invariant (DI) and domain-specific (DS) features across these multiple subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper proposes a new SS-DC framework based on a decoupling-coupling strategy. In terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID) module in the aspect of spectral decomposition. Due to the style and content information being highly embedded in different frequency bands, this module can decouple DI and DS components more accurately and interpretably. A novel filter bank-based spectral processing paradigm and a self-distillation-driven decoupling loss are proposed to improve the spectral domain decoupling. In terms of coupling, a new spatial-spectral coupling method is proposed, which realizes joint coupling through spatial and spectral DI feature pyramids. Meanwhile, this paper introduces DS from decoupling to reduce the domain bias. Extensive experiments demonstrate that our method can significantly improve the baseline performance and outperform existing UDAOD methods on multiple RGB-IR datasets, including a new experimental protocol proposed in this paper based on the FLIR-ADAS dataset.",
      "published": "2025-07-16T08:21:41Z",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.12017v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2503.16364v1",
      "title": "Neural Networks: According to the Principles of Grassmann Algebra",
      "authors": [
        "Z. Zarezadeh",
        "N. Zarezadeh"
      ],
      "summary": "In this paper, we explore the algebra of quantum idempotents and the quantization of fermions which gives rise to a Hilbert space equal to the Grassmann algebra associated with the Lie algebra. Since idempotents carry representations of the algebra under consideration, they form algebraic varieties and smooth manifolds in the natural topology. In addition to the motivation of linking up mathematical physics with machine learning, it is also shown that by using idempotents and invariant subspace of the corresponding algebras, these representations encode and perhaps provide a probabilistic interpretation of reasoning and relational paths in geometrical terms.",
      "published": "2025-03-20T17:21:23Z",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16364v1.pdf",
      "relevance_score": 69.0
    },
    {
      "arxiv_id": "2502.19967v1",
      "title": "Automatically Verifying Replication-aware Linearizability",
      "authors": [
        "Vimala Soundarapandian",
        "Kartik Nagar",
        "Aseem Rastogi",
        "KC Sivaramakrishnan"
      ],
      "summary": "Data replication is crucial for enabling fault tolerance and uniform low latency in modern decentralized applications. Replicated Data Types (RDTs) have emerged as a principled approach for developing replicated implementations of basic data structures such as counter, flag, set, map, etc. While the correctness of RDTs is generally specified using the notion of strong eventual consistency--which guarantees that replicas that have received the same set of updates would converge to the same state--a more expressive specification which relates the converged state to updates received at a replica would be more beneficial to RDT users. Replication-aware linearizability is one such specification, which requires all replicas to always be in a state which can be obtained by linearizing the updates received at the replica. In this work, we develop a novel fully automated technique for verifying replication-aware linearizability for Mergeable Replicated Data Types (MRDTs). We identify novel algebraic properties for MRDT operations and the merge function which are sufficient for proving an implementation to be linearizable and which go beyond the standard notions of commutativity, associativity, and idempotence. We also develop a novel inductive technique called bottom-up linearization to automatically verify the required algebraic properties. Our technique can be used to verify both MRDTs and state-based CRDTs. We have successfully applied our approach to a number of complex MRDT and CRDT implementations including a novel JSON MRDT.",
      "published": "2025-02-27T10:47:17Z",
      "year": "2025",
      "categories": [
        "cs.PL"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2502.19967v1.pdf",
      "relevance_score": 68.0
    },
    {
      "arxiv_id": "2512.21133v1",
      "title": "SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation",
      "authors": [
        "Xiaoyu Mo",
        "Jintian Ge",
        "Zifan Wang",
        "Chen Lv",
        "Karl Henrik Johansson"
      ],
      "summary": "Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.",
      "published": "2025-12-24T12:02:35Z",
      "year": "2025",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.21133v1.pdf",
      "relevance_score": 68.0
    },
    {
      "arxiv_id": "2512.20722v1",
      "title": "Learning-Enabled Elastic Network Topology for Distributed ISAC Service Provisioning",
      "authors": [
        "Jie Chen",
        "Xianbin Wang"
      ],
      "summary": "Conventional mobile networks, including both localized cell-centric and cooperative cell-free networks (CCN/CFN), are built upon rigid network topologies. However, neither architecture is adequate to flexibly support distributed integrated sensing and communication (ISAC) services, due to the increasing difficulty of aligning spatiotemporally distributed heterogeneous service demands with available radio resources. In this paper, we propose an elastic network topology (ENT) for distributed ISAC service provisioning, where multiple co-existing localized CCNs can be dynamically aggregated into CFNs with expanded boundaries for federated network operation. This topology elastically orchestrates localized CCN and federated CFN boundaries to balance signaling overhead and distributed resource utilization, thereby enabling efficient ISAC service provisioning. A two-phase operation protocol is then developed. In Phase I, each CCN autonomously classifies ISAC services as either local or federated and partitions its resources into dedicated and shared segments. In Phase II, each CCN employs its dedicated resources for local ISAC services, while the aggregated CFN consolidates shared resources from its constituent CCNs to cooperatively deliver federated services. Furthermore, we design a utility-to-signaling ratio (USR) to quantify the tradeoff between sensing/communication utility and signaling overhead. Consequently, a USR maximization problem is formulated by jointly optimizing the network topology (i.e., service classification and CCN aggregation) and the allocation of dedicated and shared resources. However, this problem is challenging due to its distributed optimization nature and the absence of complete channel state information. To address this problem efficiently, we propose a multi-agent deep reinforcement learning (MADRL) framework with centralized training and decentralized execution.",
      "published": "2025-12-23T19:34:29Z",
      "year": "2025",
      "categories": [
        "eess.SP",
        "cs.IT"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20722v1.pdf",
      "relevance_score": 67.0
    },
    {
      "arxiv_id": "2512.20457v1",
      "title": "When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)",
      "authors": [
        "Marco Aruta",
        "Francesco Improta",
        "Vadim Malvone",
        "Aniello Murano"
      ],
      "summary": "In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.",
      "published": "2025-12-23T15:51:02Z",
      "year": "2025",
      "categories": [
        "cs.MA",
        "cs.LO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20457v1.pdf",
      "relevance_score": 67.0
    },
    {
      "arxiv_id": "2512.20417v1",
      "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
      "authors": [
        "Pedro Domingos",
        "Jo\u00e3o Pereira",
        "Vasco Lopes",
        "Jo\u00e3o Neves",
        "David Semedo"
      ],
      "summary": "Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.",
      "published": "2025-12-23T15:01:05Z",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20417v1.pdf",
      "relevance_score": 66.0
    },
    {
      "arxiv_id": "2512.20260v1",
      "title": "${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations",
      "authors": [
        "Jiawei Ge",
        "Jiuxin Cao",
        "Xinyi Li",
        "Xuelin Zhu",
        "Chang Liu",
        "Bo Liu",
        "Chen Feng",
        "Ioannis Patras"
      ],
      "summary": "Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.",
      "published": "2025-12-23T11:16:16Z",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20260v1.pdf",
      "relevance_score": 66.0
    },
    {
      "arxiv_id": "2512.20136v2",
      "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation",
      "authors": [
        "Hyeongcheol Park",
        "Jiyoung Seo",
        "Jaewon Mun",
        "Hogun Park",
        "Wonmin Byeon",
        "Sung June Kim",
        "Hyeonsoo Im",
        "JeungSub Lee",
        "Sangpil Kim"
      ],
      "summary": "Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.",
      "published": "2025-12-23T07:54:03Z",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20136v2.pdf",
      "relevance_score": 66.0
    },
    {
      "arxiv_id": "2509.20617v1",
      "title": "DELM: a Python toolkit for Data Extraction with Language Models",
      "authors": [
        "Eric Fithian",
        "Kirill Skobelev"
      ],
      "summary": "Large Language Models (LLMs) have become powerful tools for annotating unstructured data. However, most existing workflows rely on ad hoc scripts, making reproducibility, robustness, and systematic evaluation difficult. To address these challenges, we introduce DELM (Data Extraction with Language Models), an open-source Python toolkit designed for rapid experimental iteration of LLM-based data extraction pipelines and for quantifying the trade-offs between them. DELM minimizes boilerplate code and offers a modular framework with structured outputs, built-in validation, flexible data-loading and scoring strategies, and efficient batch processing. It also includes robust support for working with LLM APIs, featuring retry logic, result caching, detailed cost tracking, and comprehensive configuration management. We showcase DELM's capabilities through two case studies: one featuring a novel prompt optimization algorithm, and another illustrating how DELM quantifies trade-offs between cost and coverage when selecting keywords to decide which paragraphs to pass to an LLM. DELM is available at \\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
      "published": "2025-09-24T23:47:55Z",
      "year": "2025",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2509.20617v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2507.05659v1",
      "title": "MCNP-GO: A python package for assembling MCNP input files with a systems engineering approach",
      "authors": [
        "Alexandre Friou"
      ],
      "summary": "This article introduces MCNP-GO (https://github.com/afriou/mcnpgo), a Python package designed to manipulate and assemble MCNP input files, allowing users to assemble a set of independent objects, each described by a valid MCNP file, into a single cohesive file. This tool is particularly useful for applications where precise modeling and positioning of equipment are crucial. The package addresses the challenges of managing large databases of MCNP input files, ensuring reliability and traceability through configuration management systems. MCNP-GO provides functionalities such as renumbering, extracting subsets of files, transforming files, and assembling files while managing collisions and materials. It also keeps track of the operations performed on files, enhancing traceability and ease of modification. The article demonstrates the package's capabilities through a practical example of assembling an MCNP input file for a tomographic experiment, highlighting its efficiency and user-friendliness. MCNP-GO is designed for users with minimal Python knowledge.",
      "published": "2025-07-07T07:35:07Z",
      "year": "2025",
      "categories": [
        "cs.CE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.05659v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2512.06589v1",
      "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
      "authors": [
        "Xiaojun Jia",
        "Jie Liao",
        "Qi Guo",
        "Teng Ma",
        "Simeng Qin",
        "Ranjie Duan",
        "Tianlin Li",
        "Yihao Huang",
        "Zhitao Zeng",
        "Dongxian Wu",
        "Yiming Li",
        "Wenqi Ren",
        "Xiaochun Cao",
        "Yang Liu"
      ],
      "summary": "Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.",
      "published": "2025-12-06T22:56:29Z",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.06589v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2511.02614v1",
      "title": "A Non-Adversarial Approach to Idempotent Generative Modelling",
      "authors": [
        "Mohammed Al-Jaff",
        "Giovanni Luca Marchetti",
        "Michael C Welle",
        "Jens Lundell",
        "Mats G. Gustafsson",
        "Gustav Eje Henter",
        "Hossein Azizpour",
        "Danica Kragic"
      ],
      "summary": "Idempotent Generative Networks (IGNs) are deep generative models that also function as local data manifold projectors, mapping arbitrary inputs back onto the manifold. They are trained to act as identity operators on the data and as idempotent operators off the data manifold. However, IGNs suffer from mode collapse, mode dropping, and training instability due to their objectives, which contain adversarial components and can cause the model to cover the data manifold only partially -- an issue shared with generative adversarial networks. We introduce Non-Adversarial Idempotent Generative Networks (NAIGNs) to address these issues. Our loss function combines reconstruction with the non-adversarial generative objective of Implicit Maximum Likelihood Estimation (IMLE). This improves on IGN's ability to restore corrupted data and generate new samples that closely match the data distribution. We moreover demonstrate that NAIGNs implicitly learn the distance field to the data manifold, as well as an energy-based model.",
      "published": "2025-11-04T14:37:23Z",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2511.02614v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2511.00992v1",
      "title": "Free polynomial strong bimonoids",
      "authors": [
        "Manfred Droste",
        "Zolt\u00e1n F\u00fcl\u00f6p"
      ],
      "summary": "Recently, in weighted automata theory the weight structure of strong bimonoids has found much interest; they form a generalization of semirings and are closely related to near-semirings studied in algebra. Here, we define polynomials over a set $X$ of indeterminates as well as an addition and a multiplication. We show that with these operations, they form a right-distributive strong bimonoid, that this polynomial strong bimonoid is free over $X$ in the class of all right-distributive strong bimonoids and that it is both left- and right-cancellative. We show by purely algebraic reasoning that two arbitrary terms are equivalent modulo the laws of right-distributive strong bimonoids if and only if their representing polynomials are equivalent by the laws of only associativity and commutativity of addition and associativity of multiplication. We give effective procedures for constructing the representing polynomials. As a consequence, we obtain that the equivalence of arbitrary terms modulo the laws of right-distributive strong bimonoids can be decided in exponential time. Using term-rewriting methods, we show that each term can be reduced to a unique polynomial as normal form. We also derive corresponding results for the free idempotent right-distributive polynomial strong bimonoid over $X$. We construct an idempotent strong bimonoid which is weakly locally finite but not locally finite and show an application of it in weighted automata theory.",
      "published": "2025-11-02T16:05:04Z",
      "year": "2025",
      "categories": [
        "math.RA",
        "cs.DM"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2511.00992v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2509.05094v1",
      "title": "Partializations of Markov categories",
      "authors": [
        "Areeb Shah Mohammed"
      ],
      "summary": "The present work develops a construction of a CD category of partial kernels from a particular type of Markov category called a partializable Markov category. These are a generalization of earlier models of categories of partial morphisms such as p-categories, dominical categories, restriction categories, etc. to a non-deterministic/non-cartesian setting. Here all morphisms are quasi-total, with a natural poset enrichment corresponding to one morphism being a restriction of the other. Furthermore, various properties important to categorical probability are preserved, such as positivity, representability, conditionals, Kolmogorov products, and splittings of idempotents. We additionally discuss an alternative notion of Kolmogorov product suitable for partial maps, as well as partial algebras for probability monads. The primary example is that of the partialization of the category of standard Borel spaces and Markov kernels. Other examples include variants where the distributions are finitely supported, or where one considers multivalued maps instead.",
      "published": "2025-09-05T13:31:09Z",
      "year": "2025",
      "categories": [
        "math.CT",
        "cs.LO",
        "math.PR"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2509.05094v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2509.00168v1",
      "title": "Generalised M\u00f6bius Categories and Convolution Kleene Algebras",
      "authors": [
        "James Cranch",
        "Georg Struth",
        "Jana Wagemaker"
      ],
      "summary": "Convolution algebras on maps from structures such as monoids, groups or categories into semirings, rings or fields abound in mathematics and the sciences. Of special interest in computing are convolution algebras based on variants of Kleene algebras, which are additively idempotent semirings equipped with a Kleene star. Yet an obstacle to the construction of convolution Kleene algebras on a wide class of structures has so far been the definition of a suitable star. We show that a generalisation of M\u00f6bius categories combined with a generalisation of a classical definition of a star for formal power series allow such a construction. We discuss several instances of this construction on generalised M\u00f6bius categories: convolution Kleene algebras with tests, modal convolution Kleene algebras, concurrent convolution Kleene algebras and higher convolution Kleene algebras (e.g. on strict higher categories and higher relational monoids). These are relevant to the verification of weighted and probabilistic sequential and concurrent programs, using quantitative Hoare logics or predicate transformer algebras, as well as for algebraic reasoning in higher-dimensional rewriting. We also adapt the convolution Kleene algebra construction to Conway semirings, which is widely studied in the context of weighted automata. Finally, we compare the convolution Kleene algebra construction with a previous construction of convolution quantales and present concrete example structures in preparation for future applications.",
      "published": "2025-08-29T18:08:33Z",
      "year": "2025",
      "categories": [
        "cs.FL",
        "cs.LO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2509.00168v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2507.18418v2",
      "title": "Distributing Retractions, Weak Distributive Laws and Applications to Monads of Hyperspaces, Continuous Valuations and Measures",
      "authors": [
        "Jean Goubault-Larrecq"
      ],
      "summary": "Given two monads $S$, $T$ on a category where idempotents split, and a weak distributive law between them, one can build a combined monad $U$. Making explicit what this monad $U$ is requires some effort. When we already have an idea what $U$ should be, we show how to recognize that $U$ is indeed the combined monad obtained from $S$ and $T$: it suffices to exhibit what we call a distributing retraction of $ST$ onto $U$. We show that distributing retractions and weak distributive laws are in one-to-one correspondence, in a 2-categorical setting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin hyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad of previsions or of forks, depending on the case. As a byproduct, this allows us to describe the algebras of monads of superlinear, resp. sublinear previsions. In the category of compact Hausdorff spaces, the Plotkin hyperspace monad is sometimes known as the Vietoris monad, the monad of probability valuations coincides with the Radon monad, and we infer that the associated combined monad is the monad of normalized forks.",
      "published": "2025-07-24T13:57:24Z",
      "year": "2025",
      "categories": [
        "cs.LO",
        "math.CT"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.18418v2.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2507.04013v1",
      "title": "LCD and self-orthogonal twisted group codes over finite commutative chain rings",
      "authors": [
        "Samir Assuena",
        "Andr\u00e9 Luiz Martins Pereira"
      ],
      "summary": "In this paper, we shall study k-Galois LCD constacyclic group codes over finite commutative chain rings with identity. In particular, we shall characterize Galois LCD constacyclic codes over finite commutative chain ring with identity in terms of its idempotent generators and the classical involution using the twisted group ring structures and find some good LCD codes.",
      "published": "2025-07-05T11:49:07Z",
      "year": "2025",
      "categories": [
        "math.RA",
        "cs.IT"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.04013v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2505.09960v1",
      "title": "Quantitative Types for the Functional Machine Calculus",
      "authors": [
        "Willem Heijltjes"
      ],
      "summary": "The Functional Machine Calculus (FMC, Heijltjes 2022) extends the lambda-calculus with the computational effects of global mutable store, input/output, and probabilistic choice while maintaining confluent reduction and simply-typed strong normalization. Based in a simple call-by-name stack machine in the style of Krivine, the FMC models effects through additional argument stacks, and introduces sequential composition through a continuation stack to encode call-by-value behaviour, where simple types guarantee termination of the machine. The present paper provides a discipline of quantitative types, also known as non-idempotent intersection types, for the FMC, in two variants. In the weak variant, typeability coincides with termination of the stack machine and with spine normalization, while exactly measuring the transitions in machine evaluation. The strong variant characterizes strong normalization through a notion of perpetual evaluation, while giving an upper bound to the length of reductions. Through the encoding of effects, quantitative typeability coincides with termination for higher-order mutable store, input/output, and probabilistic choice.",
      "published": "2025-05-15T04:41:22Z",
      "year": "2025",
      "categories": [
        "cs.LO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2505.09960v1.pdf",
      "relevance_score": 65.0
    },
    {
      "arxiv_id": "2512.19842v1",
      "title": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform",
      "authors": [
        "Andrea Sordello",
        "Marco Mellia",
        "Idilio Drago",
        "Rodolfo Valentim",
        "Francesco Musumeci",
        "Massimo Tornatore",
        "Federico Cerutti",
        "Martino Trevisan",
        "Alessio Botta",
        "Willen Borges Coelho"
      ],
      "summary": "The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.",
      "published": "2025-12-22T19:54:14Z",
      "year": "2025",
      "categories": [
        "cs.DC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19842v1.pdf",
      "relevance_score": 64.0
    },
    {
      "arxiv_id": "2512.17093v1",
      "title": "A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving",
      "authors": [
        "Timo Pierre Schrader",
        "Lukas Lange",
        "Tobias Kaminski",
        "Simon Razniewski",
        "Annemarie Friedrich"
      ],
      "summary": "The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase. In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.",
      "published": "2025-12-18T21:45:45Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.17093v1.pdf",
      "relevance_score": 64.0
    },
    {
      "arxiv_id": "2512.12918v1",
      "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming",
      "authors": [
        "Nijesh Upreti",
        "Vaishak Belle"
      ],
      "summary": "Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.",
      "published": "2025-12-15T02:08:32Z",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.12918v1.pdf",
      "relevance_score": 64.0
    },
    {
      "arxiv_id": "2512.05653v1",
      "title": "Executing Discrete/Continuous Declarative Process Specifications via Complex Event Processing",
      "authors": [
        "Stefan Sch\u00f6nig",
        "Leo Poss",
        "Fabrizio Maria Maggi"
      ],
      "summary": "Traditional Business Process Management (BPM) focuses on discrete events and fails to incorporate critical continuous sensor data in cyber-physical environments. Hybrid declarative specifications, utilizing Signal Temporal Logic (STL), address this limitation by allowing constraints over both discrete events and real-valued signals. However, existing work has been limited to monitoring and post-hoc conformance checking. This paper introduces a novel Complex Event Processing (CEP)-based execution architecture that enables the real-time execution and enforcement of hybrid declarative models. Our three-layer approach integrates STL-inspired predicates into the execution flow, allowing the system to actively trigger activities and enforce process boundaries based on continuous sensor behavior. This approach bridges the gap between hybrid specification and operational control.",
      "published": "2025-12-05T11:57:52Z",
      "year": "2025",
      "categories": [
        "cs.SE",
        "cs.LO",
        "eess.SY"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.05653v1.pdf",
      "relevance_score": 64.0
    },
    {
      "arxiv_id": "2512.03272v1",
      "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?",
      "authors": [
        "Zhiyuan He",
        "Dingmin Wang"
      ],
      "summary": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.",
      "published": "2025-12-02T22:23:26Z",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.03272v1.pdf",
      "relevance_score": 64.0
    },
    {
      "arxiv_id": "2512.13228v1",
      "title": "ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data",
      "authors": [
        "Melvin Barbaux"
      ],
      "summary": "Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.",
      "published": "2025-12-15T11:43:14Z",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.13228v1.pdf",
      "relevance_score": 63.0
    },
    {
      "arxiv_id": "2512.02289v1",
      "title": "Multi-Objective Agentic Rewrites for Unstructured Data Processing",
      "authors": [
        "Lindsey Linxi Wei",
        "Shreya Shankar",
        "Sepanta Zeighami",
        "Yeounoh Chung",
        "Fatma Ozcan",
        "Aditya G. Parameswaran"
      ],
      "summary": "One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter(\"is this email sent from an executive and discussing fraud?\") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both? We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost.",
      "published": "2025-12-02T00:08:24Z",
      "year": "2025",
      "categories": [
        "cs.DB"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.02289v1.pdf",
      "relevance_score": 63.0
    },
    {
      "arxiv_id": "2512.19479v1",
      "title": "Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation",
      "authors": [
        "Guoli Jia",
        "Junyao Hu",
        "Xinwei Long",
        "Kai Tian",
        "Kaiyan Zhang",
        "KaiKai Zhao",
        "Ning Ding",
        "Bowen Zhou"
      ],
      "summary": "Image generation based on diffusion models has demonstrated impressive capability, motivating exploration into diverse and specialized applications. Owing to the importance of emotion in advertising, emotion-oriented image generation has attracted increasing attention. However, current emotion-oriented methods suffer from an affective shortcut, where emotions are approximated to semantics. As evidenced by two decades of research, emotion is not equivalent to semantics. To this end, we propose Emotion-Director, a cross-modal collaboration framework consisting of two modules. First, we propose a cross-Modal Collaborative diffusion model, abbreviated as MC-Diffusion. MC-Diffusion integrates visual prompts with textual prompts for guidance, enabling the generation of emotion-oriented images beyond semantics. Further, we improve the DPO optimization by a negative visual prompt, enhancing the model's sensitivity to different emotions under the same semantics. Second, we propose MC-Agent, a cross-Modal Collaborative Agent system that rewrites textual prompts to express the intended emotions. To avoid template-like rewrites, MC-Agent employs multi-agents to simulate human subjectivity toward emotions, and adopts a chain-of-concept workflow that improves the visual expressiveness of the rewritten prompts. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director in emotion-oriented image generation.",
      "published": "2025-12-22T15:32:18Z",
      "year": "2025",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19479v1.pdf",
      "relevance_score": 63.0
    },
    {
      "arxiv_id": "2512.21257v1",
      "title": "ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling",
      "authors": [
        "Chuan Wang",
        "Gaoming Yang",
        "Han Wu",
        "Jiakai Tang",
        "Jiahao Yu",
        "Jian Wu",
        "Jianwu Hu",
        "Junjun Zheng",
        "Shuwen Xiao",
        "Yeqiu Yang",
        "Yuning Jiang",
        "Ahjol Nurlanbek",
        "Binbin Cao",
        "Bo Zheng",
        "Fangmei Zhu",
        "Gaoming Zhou",
        "Huimin Yi",
        "Huiping Chu",
        "Jin Huang",
        "Jinzhe Shan",
        "Kenan Cui",
        "Longbin Li",
        "Silu Zhou",
        "Wen Chen",
        "Xia Ming",
        "Xiang Gao",
        "Xin Yao",
        "Xingyu Wen",
        "Yan Zhang",
        "Yiwen Hu",
        "Yulin Wang",
        "Ziheng Bao",
        "Zongyuan Wu"
      ],
      "summary": "Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics and cross-domain behavioral patterns that Large Language Models have learned from vast corpora. To address these challenges, we introduce ReaSeq, a reasoning-enhanced framework that leverages world knowledge in Large Language Models to address both limitations through explicit and implicit reasoning. Specifically, ReaSeq employs explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into semantically enriched item representations, and latent reasoning via Diffusion Large Language Models to infer plausible beyond-log behaviors. Deployed on Taobao's ranking system serving hundreds of millions of users, ReaSeq achieves substantial gains: >6.0% in IPV and CTR, >2.9% in Orders, and >2.5% in GMV, validating the effectiveness of world-knowledge-enhanced reasoning over purely log-driven approaches.",
      "published": "2025-12-24T16:06:20Z",
      "year": "2025",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.21257v1.pdf",
      "relevance_score": 62.0
    },
    {
      "arxiv_id": "2512.20936v1",
      "title": "Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation",
      "authors": [
        "Hongxing Fan",
        "Shuyu Zhao",
        "Jiayang Ao",
        "Lu Sheng"
      ],
      "summary": "Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.",
      "published": "2025-12-24T04:39:45Z",
      "year": "2025",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20936v1.pdf",
      "relevance_score": 62.0
    },
    {
      "arxiv_id": "2511.03094v1",
      "title": "ALAS: Transactional and Dynamic Multi-Agent LLM Planning",
      "authors": [
        "Longling Geng",
        "Edward Y. Chang"
      ],
      "summary": "Large language models enable flexible multi-agent planning but remain fragile in practice: verification is often circular, state changes are not tracked for repair, and small faults trigger costly global recomputation. We present ALAS, a stateful, disruption-aware framework that separates planning from non-circular validation, records a versioned execution log for grounded checks and restore points, and performs localized repair that preserves work in progress. The validator operates independently of the planning LLM with fresh, bounded context, avoiding self-check loops and mid-context attrition. The repair protocol edits only the minimal affected region under explicit policies (retry, catch, timeout, backoff, idempotency keys, compensation, loop guards) defined in a canonical workflow IR that maps to Amazon States Language and Argo Workflows. On job-shop scheduling suites (DMU, TA) across five classical benchmarks, ALAS matches or exceeds strong single-LLM and multi-agent baselines, achieving 83.7% success, reducing token usage by 60%, and running 1.82times faster under comparable settings. A minimal reliability study shows that the validator detects injected structural faults with low overhead, and that localized repair contains runtime perturbations with a bounded edit radius and less makespan degradation than global recompute. Results indicate that the combination of validator isolation, versioned execution logs, and localized repair provides measurable efficiency, feasibility, and scalability for multi-agent LLM planning. Code and seeds will be released.",
      "published": "2025-11-05T00:55:51Z",
      "year": "2025",
      "categories": [
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2511.03094v1.pdf",
      "relevance_score": 61.0
    },
    {
      "arxiv_id": "2512.21024v1",
      "title": "Policy-Conditioned Policies for Multi-Agent Task Solving",
      "authors": [
        "Yue Lin",
        "Shuhui Zhu",
        "Wenhao Li",
        "Ang Li",
        "Dan Qiao",
        "Pascal Poupart",
        "Hongyuan Zha",
        "Baoxiang Wang"
      ],
      "summary": "In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.",
      "published": "2025-12-24T07:42:10Z",
      "year": "2025",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.21024v1.pdf",
      "relevance_score": 61.0
    },
    {
      "arxiv_id": "2512.20845v1",
      "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs",
      "authors": [
        "Onat Ozer",
        "Grace Wu",
        "Yuchen Wang",
        "Daniel Dosti",
        "Honghao Zhang",
        "Vivi De La Rue"
      ],
      "summary": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.",
      "published": "2025-12-23T23:47:31Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20845v1.pdf",
      "relevance_score": 61.0
    },
    {
      "arxiv_id": "2512.20206v1",
      "title": "TongSIM: A General Platform for Simulating Intelligent Machines",
      "authors": [
        "Zhe Sun",
        "Kunlun Wu",
        "Chuanjian Fu",
        "Zeming Song",
        "Langyong Shi",
        "Zihe Xue",
        "Bohan Jing",
        "Ying Yang",
        "Xiaomeng Gao",
        "Aijia Li",
        "Tianyu Guo",
        "Huiying Li",
        "Xueyuan Yang",
        "Rongkai Liu",
        "Xinyi He",
        "Yuxi Wang",
        "Yue Li",
        "Mingyuan Liu",
        "Yujie Lu",
        "Hongzhao Xie",
        "Shiyun Zhao",
        "Bo Dai",
        "Wei Wang",
        "Tao Yuan",
        "Song-Chun Zhu",
        "Yujia Peng",
        "Zhenliang Zhang"
      ],
      "summary": "As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.",
      "published": "2025-12-23T10:00:43Z",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.20206v1.pdf",
      "relevance_score": 61.0
    },
    {
      "arxiv_id": "2512.19972v1",
      "title": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions",
      "authors": [
        "Pengchao Han",
        "Xi Huang",
        "Yi Fang",
        "Guojun Han"
      ],
      "summary": "Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.",
      "published": "2025-12-23T01:34:23Z",
      "year": "2025",
      "categories": [
        "cs.DC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19972v1.pdf",
      "relevance_score": 61.0
    },
    {
      "arxiv_id": "2512.19841v1",
      "title": "A Multi-Agent Retrieval-Augmented Framework for Work-in-Progress Predictio",
      "authors": [
        "Yousef Mehrdad Bibalan",
        "Behrouz Far",
        "Mohammad Moshirpour",
        "Bahareh Ghiyasian"
      ],
      "summary": "Work-in-Progress (WiP) prediction is critical for predictive process monitoring, enabling accurate anticipation of workload fluctuations and optimized operational planning. This paper proposes a retrieval-augmented, multi-agent framework that combines retrieval-augmented generation (RAG) and collaborative multi-agent reasoning for WiP prediction. The narrative generation component transforms structured event logs into semantically rich natural language stories, which are embedded into a semantic vector-based process memory to facilitate dynamic retrieval of historical context during inference. The framework includes predictor agents that independently leverage retrieved historical contexts and a decision-making assistant agent that extracts high-level descriptive signals from recent events. A fusion agent then synthesizes predictions using ReAct-style reasoning over agent outputs and retrieved narratives. We evaluate our framework on two real-world benchmark datasets. Results show that the proposed retrieval-augmented multi-agent approach achieves competitive prediction accuracy, obtaining a Mean Absolute Percentage Error (MAPE) of 1.50\\% on one dataset, and surpassing Temporal Convolutional Networks (TCN), Long Short-Term Memory (LSTM), and persistence baselines. The results highlight improved robustness, demonstrating the effectiveness of integrating retrieval mechanisms and multi-agent reasoning in WiP prediction.",
      "published": "2025-12-22T19:51:59Z",
      "year": "2025",
      "categories": [
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19841v1.pdf",
      "relevance_score": 61.0
    },
    {
      "arxiv_id": "2512.19299v1",
      "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
      "authors": [
        "Haoyu Jiang",
        "Fanjie Zeng",
        "Boan Qu",
        "Xiaojie Lin",
        "Wei Zhong"
      ],
      "summary": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
      "published": "2025-12-22T11:43:35Z",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19299v1.pdf",
      "relevance_score": 61.0
    },
    {
      "arxiv_id": "2512.00387v2",
      "title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing",
      "authors": [
        "Kaihang Pan",
        "Weile Chen",
        "Haiyi Qiu",
        "Qifan Yu",
        "Wendong Bu",
        "Zehan Wang",
        "Yun Zhu",
        "Juncheng Li",
        "Siliang Tang"
      ],
      "summary": "Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.",
      "published": "2025-11-29T08:32:35Z",
      "year": "2025",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.00387v2.pdf",
      "relevance_score": 60.0
    },
    {
      "arxiv_id": "2511.09788v1",
      "title": "Why Open Small AI Models Matter for Interactive Art",
      "authors": [
        "Mar Canet Sola",
        "Varvara Guljajeva"
      ],
      "summary": "This position paper argues for the importance of open small AI models in creative independence for interactive art practices. Deployable locally, these models offer artists vital control over infrastructure and code, unlike dominant large, closed-source corporate systems. Such centralized platforms function as opaque black boxes, imposing severe limitations on interactive artworks, including restrictive content filters, preservation issues, and technical challenges such as increased latency and limited interfaces. In contrast, small AI models empower creators with more autonomy, control, and sustainability for these artistic processes. They enable the ability to use a model as long as they want, create their own custom model, either by making code changes to integrate new interfaces, or via new datasets by re-training or fine-tuning the model. This fosters technological self-determination, offering greater ownership and reducing reliance on corporate AI ill-suited for interactive art's demands. Critically, this approach empowers the artist and supports long-term preservation and exhibition of artworks with AI components. This paper explores the practical applications and implications of using open small AI models in interactive art, contrasting them with closed-source alternatives.",
      "published": "2025-11-12T22:39:14Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2511.09788v1.pdf",
      "relevance_score": 59.0
    },
    {
      "arxiv_id": "2508.12385v1",
      "title": "System-driven Interactive Design Support for Cloud Architecture: A Qualitative User Experience Study with Novice Engineers",
      "authors": [
        "Ryosuke Kohita",
        "Akira Kasuga"
      ],
      "summary": "Cloud architecture design presents significant challenges due to the necessity of clarifying ambiguous requirements and systematically addressing complex trade-offs, especially for novice engineers with limited cloud experience. While recent advances in the use of AI tools have broadened available options, system-driven approaches that offer explicit guidance and step-by-step information management may be especially effective in supporting novices during the design process. This study qualitatively examines the experiences of 60 novice engineers using such a system-driven cloud design support tool. The findings indicate that structured and proactive system guidance helps novices engage more effectively in architectural design, especially when addressing tasks where knowledge and experience gaps are most critical. For example, participants found it easier to create initial architectures and did not need to craft prompts themselves. In addition, participants reported that the ability to simulate and compare multiple architecture options enabled them to deepen their understanding of cloud design principles and trade-offs, demonstrating the educational value of system-driven support. The study also identifies areas for improvement, including more adaptive information delivery tailored to user expertise, mechanisms for validating system outputs, and better integration with implementation workflows such as infrastructure-as-code generation and deployment guidance. Addressing these aspects can further enhance the educational and practical value of system-driven support tools for cloud architecture design.",
      "published": "2025-08-17T14:48:09Z",
      "year": "2025",
      "categories": [
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2508.12385v1.pdf",
      "relevance_score": 59.0
    },
    {
      "arxiv_id": "2512.19509v1",
      "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models",
      "authors": [
        "Shangbo Yun",
        "Xiaodong Gu",
        "Jianghong Huang",
        "Beijun Shen"
      ],
      "summary": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.",
      "published": "2025-12-22T16:04:56Z",
      "year": "2025",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19509v1.pdf",
      "relevance_score": 59.0
    },
    {
      "arxiv_id": "2512.12128v1",
      "title": "A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery",
      "authors": [
        "Thomas Manzini",
        "Priyankari Perali",
        "Raisa Karnik",
        "Robin R. Murphy"
      ],
      "summary": "This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\\% Macro IoU. If spatial alignment is not considered, approximately 8\\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.",
      "published": "2025-12-13T01:42:49Z",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.12128v1.pdf",
      "relevance_score": 59.0
    },
    {
      "arxiv_id": "2512.09886v1",
      "title": "HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression",
      "authors": [
        "Gustavo Coelho Haase",
        "Paulo Henrique Dourado da Silva"
      ],
      "summary": "Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \\textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.",
      "published": "2025-12-10T18:15:15Z",
      "year": "2025",
      "categories": [
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.09886v1.pdf",
      "relevance_score": 58.0
    },
    {
      "arxiv_id": "2506.00352v1",
      "title": "Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments",
      "authors": [
        "Chinkit Patel",
        "Kee Siong Ng"
      ],
      "summary": "Many large enterprises that operate highly governed and complex ICT environments have no efficient and effective way to support their Data and AI teams in rapidly spinning up and tearing down self-service data and compute infrastructure, to experiment with new data analytic tools, and deploy data products into operational use. This paper proposes a key piece of the solution to the overall problem, in the form of an on-demand self-service data-platform infrastructure to empower de-centralised data teams to build data products on top of centralised templates, policies and governance. The core innovation is an efficient method to leverage immutable container operating systems and infrastructure-as-code methodologies for creating, from scratch, vendor-neutral and short-lived Kubernetes clusters on-premises and in any cloud environment. Our proposed approach can serve as a repeatable, portable and cost-efficient alternative or complement to commercial Platform-as-a-Service (PaaS) offerings, and this is particularly important in supporting interoperability in complex data mesh environments with a mix of modern and legacy compute infrastructure.",
      "published": "2025-05-31T02:30:22Z",
      "year": "2025",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2506.00352v1.pdf",
      "relevance_score": 58.0
    },
    {
      "arxiv_id": "2512.06636v1",
      "title": "Distribution-Aware Exploration for Adaptive HNSW Search",
      "authors": [
        "Chao Zhang",
        "Ren\u00e9e J. Miller"
      ],
      "summary": "Hierarchical Navigable Small World (HNSW) is widely adopted for approximate nearest neighbor search (ANNS) for its ability to deliver high recall with low latency on large-scale, high-dimensional embeddings. The exploration factor, commonly referred to as ef, is a key parameter in HNSW-based vector search that balances accuracy and efficiency. However, existing systems typically rely on manually and statically configured ef values that are uniformly applied across all queries. This results in a distribution-agnostic configuration that fails to account for the non-uniform and skewed nature of real-world embedding data and query workloads. As a consequence, HNSW-based systems suffer from two key practical issues: (i) the absence of recall guarantees, and (ii) inefficient ANNS performance due to over- or under-searching. In this paper, we propose Adaptive-ef (Ada-ef), a data-driven, update-friendly, query-adaptive approach that dynamically configures ef for each query at runtime to approximately meet a declarative target recall with minimal computation. The core of our approach is a theoretically grounded statistical model that captures the similarity distribution between each query and the database vectors. Based on this foundation, we design a query scoring mechanism that distinguishes between queries requiring only small ef and those that need larger ef to meet a target recall, and accordingly assigns an appropriate ef to each query. Experimental results on real-world embeddings produced by state-of-the-art Transformer models from OpenAI and Cohere show that, compared with state-of-the-art learning-based adaptive approaches, our method achieves the target recall while avoiding both over- and under-searching, reducing online query latency by up to 4x, offline computation time by 50x, and offline memory usage by 100x.",
      "published": "2025-12-07T02:55:56Z",
      "year": "2025",
      "categories": [
        "cs.DB"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.06636v1.pdf",
      "relevance_score": 58.0
    },
    {
      "arxiv_id": "2512.18542v1",
      "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "authors": [
        "Scott Thornton"
      ],
      "summary": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code). Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance. Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
      "published": "2025-12-20T23:52:12Z",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.18542v1.pdf",
      "relevance_score": 57.0
    },
    {
      "arxiv_id": "2507.08892v1",
      "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine",
      "authors": [
        "Alexander Sasha Vezhnevets",
        "Jayd Matyas",
        "Logan Cross",
        "Davide Paglieri",
        "Minsuk Chang",
        "William A. Cunningham",
        "Simon Osindero",
        "William S. Isaac",
        "Joel Z. Leibo"
      ],
      "summary": "Generative AI can be used in multi-actor environments with purposes ranging from social science modeling to interactive narrative and AI evaluation. Supporting this diversity of use cases -- which we classify as Simulationist, Dramatist, and Evaluationist -- demands a flexible scenario definition framework. We argue here that a good approach is to take inspiration from tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible for the environment and generates all parts of the story not directly determined by the voluntary actions of player characters. We argue that the Entity-Component architectural pattern is useful here. In such a system, the GM is not a hardcoded computer game but is itself a configurable entity, composed of components just like any other actor. By design, the approach allows for a separation between the underlying implementation details handled by an engineer, the creation of reusable components, and their composition and configuration managed by a designer who constructs entities from the components. This separation of concerns is instrumental for achieving rapid iteration, maintaining modularity, and ultimately to ensure scalability. We describe the ongoing evolution of the Concordia library in terms of this philosophy, demonstrating how it allows users to effectively configure scenarios that align with their specific goals.",
      "published": "2025-07-10T22:31:09Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.08892v1.pdf",
      "relevance_score": 57.0
    },
    {
      "arxiv_id": "2512.19321v1",
      "title": "Learning-Assisted Multi-Operator Variable Neighborhood Search for Urban Cable Routing",
      "authors": [
        "Wei Liu",
        "Tao Zhang",
        "Chenhui Lin",
        "Kaiwen Li",
        "Rui Wang"
      ],
      "summary": "Urban underground cable construction is essential for enhancing the reliability of city power grids, yet its high construction costs make planning a worthwhile optimization task. In urban environments, road layouts tightly constrain cable routing. This, on the one hand, renders relation-only models (i.e., those without explicit routes) used in prior work overly simplistic, and on the other hand, dramatically enlarges the combinatorial search space, thereby imposing much higher demands on algorithm design. In this study, we formulate urban cable routing as a connectivity-path co-optimization problem and propose a learning-assisted multi-operator variable neighborhood search (L-MVNS) algorithm. The framework first introduces an auxiliary task to generate high-quality feasible initial solutions. A hybrid genetic search (HGS) and A* serve as the connectivity optimizer and the route-planning optimizer, respectively. Building on these, a multi-operator variable neighborhood search (MVNS) iteratively co-optimizes inter-substation connectivity and detailed routes via three complementary destruction operators, a modified A* repair operator, and an adaptive neighborhood-sizing mechanism. A multi-agent deep reinforcement learning module is further embedded to prioritize promising neighborhoods. We also construct a standardized and scalable benchmark suite for evaluation. Across these cases, comprehensive experiments demonstrate effectiveness and stability: relative to representative approaches, MVNS and L-MVNS reduce total construction cost by approximately 30-50%, with L-MVNS delivering additional gains on larger instances and consistently higher stability.",
      "published": "2025-12-22T12:13:59Z",
      "year": "2025",
      "categories": [
        "cs.NE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19321v1.pdf",
      "relevance_score": 57.0
    },
    {
      "arxiv_id": "2512.18745v1",
      "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
      "authors": [
        "Kaican Li",
        "Lewei Yao",
        "Jiannan Wu",
        "Tiezheng Yu",
        "Jierun Chen",
        "Haoli Bai",
        "Lu Hou",
        "Lanqing Hong",
        "Wei Zhang",
        "Nevin L. Zhang"
      ],
      "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
      "published": "2025-12-21T14:23:07Z",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.18745v1.pdf",
      "relevance_score": 57.0
    },
    {
      "arxiv_id": "2512.19332v1",
      "title": "A Logical View of GNN-Style Computation and the Role of Activation Functions",
      "authors": [
        "Pablo Barcel\u00f3",
        "Floris Geerts",
        "Matthias Lanzinger",
        "Klara Pakhomenko",
        "Jan Van den Bussche"
      ],
      "summary": "We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.",
      "published": "2025-12-22T12:27:36Z",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.19332v1.pdf",
      "relevance_score": 55.0
    },
    {
      "arxiv_id": "2512.04908v1",
      "title": "Logic-Driven Cybersecurity: A Novel Framework for System Log Anomaly Detection using Answer Set Programming",
      "authors": [
        "Fang Li",
        "Fei Zuo",
        "Gopal Gupta"
      ],
      "summary": "This study explores the application of Answer Set Programming (ASP) for detecting anomalies in system logs, addressing the challenges posed by evolving cyber threats. We propose a novel framework that leverages ASP's declarative nature and logical reasoning capabilities to encode complex security rules as logical predicates. Our ASP-based system was applied to a real-world Linux system log dataset, demonstrating its effectiveness in identifying various anomalies such as potential brute-force attacks, privilege escalations, frequent network connections from specific IPs, and various system-level issues. Key findings highlight ASP's strengths in handling structured log data, rule flexibility, and event correlation. The approach shows promise in providing explainable alerts from real-world data. This research contributes to computer forensics by demonstrating a logic-based paradigm for log analysis on a practical dataset, opening avenues for more nuanced and adaptive cyber intelligence systems.",
      "published": "2025-12-04T15:37:32Z",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.LO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.04908v1.pdf",
      "relevance_score": 55.0
    },
    {
      "arxiv_id": "2512.03775v1",
      "title": "\"MCP Does Not Stand for Misuse Cryptography Protocol\": Uncovering Cryptographic Misuse in Model Context Protocol at Scale",
      "authors": [
        "Biwei Yan",
        "Yue Zhang",
        "Minghui Xu",
        "Hao Wu",
        "Yechao Zhang",
        "Kun Li",
        "Guoming Zhang",
        "Xiuzhen Cheng"
      ],
      "summary": "The Model Context Protocol (MCP) is rapidly emerging as the middleware for LLM-based applications, offering a standardized interface for tool integration. However, its built-in security mechanisms are minimal: while schemas and declarations prevent malformed requests, MCP provides no guarantees of authenticity or confidentiality, forcing developers to implement cryptography themselves. Such ad hoc practices are historically prone to misuse, and within MCP they threaten sensitive data and services. We present MICRYSCOPE, the first domain-specific framework for detecting cryptographic misuses in MCP implementations. MICRYSCOPE combines three key innovations: a cross-language intermediate representation that normalizes cryptographic APIs across diverse ecosystems, a hybrid dependency analysis that uncovers explicit and implicit function relationships (including insecure runtime compositions orchestrated by LLMs) and a taint-based misuse detector that tracks sensitive data flows and flags violations of established cryptographic rules. Applying MICRYSCOPE to 9,403 MCP servers, we identified 720 with cryptographic logic, of which 19.7% exhibited misuses. These flaws are concentrated in certain markets (e.g., Smithery Registry with 42% insecure servers), languages (Python at 34% misuse rate), and categories (Developer Tools and Data Science & ML accounting for over 50% of all misuses). Case studies reveal real-world consequences, including leaked API keys, insecure DES/ECB tools, and MD5-based authentication bypasses. Our study establishes the first ecosystem-wide view of cryptographic misuse in MCP and provides both tools and insights to strengthen the security foundations of this rapidly growing protocol.",
      "published": "2025-12-03T13:25:59Z",
      "year": "2025",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.03775v1.pdf",
      "relevance_score": 55.0
    },
    {
      "arxiv_id": "2512.01733v1",
      "title": "Answering Constraint Path Queries over Graphs",
      "authors": [
        "Heyang Li",
        "Anthony Widjaja Lin",
        "Domagoj Vrgo\u010d"
      ],
      "summary": "Constraints are powerful declarative constructs that allow users to conveniently restrict variable values that potentially range over an infinite domain. In this paper, we propose a constraint path query language over property graphs, which extends Regular Path Queries (RPQs) with SMT constraints on data attributes in the form of equality constraints and Linear Real Arithmetic (LRA) constraints. We provide efficient algorithms for evaluating such path queries over property graphs, which exploits optimization of macro-states (among others, using theory-specific techniques). In particular, we demonstrate how such an algorithm may effectively utilize highly optimized SMT solvers for resolving such constraints over paths. We implement our algorithm in MillenniumDB, an open-source graph engine supporting property graph queries and GQL. Our extensive empirical evaluation in a real-world setting demonstrates the viability of our approach.",
      "published": "2025-12-01T14:40:35Z",
      "year": "2025",
      "categories": [
        "cs.DB"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.01733v1.pdf",
      "relevance_score": 55.0
    },
    {
      "arxiv_id": "2512.14739v1",
      "title": "How Deep Does Your Dependency Tree Go? An Empirical Study of Dependency Amplification Across 10 Package Ecosystems",
      "authors": [
        "Jahidul Arafat"
      ],
      "summary": "Modern software development relies on package ecosystems where a single declared dependency can pull in many additional transitive packages. This dependency amplification, defined as the ratio of transitive to direct dependencies, has major implications for software supply chain security, yet amplification patterns across ecosystems have not been compared at scale. We present an empirical study of 500 projects across ten major ecosystems, including Maven Central for Java, npm Registry for JavaScript, crates io for Rust, PyPI for Python, NuGet Gallery for dot NET, RubyGems for Ruby, Go Modules for Go, Packagist for PHP, CocoaPods for Swift and Objective C, and Pub for Dart. Our analysis shows that Maven exhibits mean amplification of 24.70 times, compared to 4.48 times for Go Modules, 4.32 times for npm, and 0.32 times for CocoaPods. We find significant differences with large effect sizes in 22 of 45 pairwise comparisons, challenging the assumption that npm has the highest amplification due to its many small purpose packages. We observe that 28 percent of Maven projects exceed 10 times amplification, indicating a systematic pattern rather than isolated outliers, compared to 14 percent for RubyGems, 12 percent for npm, and zero percent for Cargo, PyPI, Packagist, CocoaPods, and Pub. We attribute these differences to ecosystem design choices such as dependency resolution behavior, standard library completeness, and platform constraints. Our findings suggest adopting ecosystem specific security strategies, including systematic auditing for Maven environments, targeted outlier detection for npm and RubyGems, and continuation of current practices for ecosystems with controlled amplification. We provide a full replication package with data and analysis scripts.",
      "published": "2025-12-12T05:53:32Z",
      "year": "2025",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.14739v1.pdf",
      "relevance_score": 54.0
    },
    {
      "arxiv_id": "2512.06732v1",
      "title": "\"The Dentist is an involved parent, the bartender is not\": Revealing Implicit Biases in QA with Implicit BBQ",
      "authors": [
        "Aarushi Wagh",
        "Saniya Srivastava"
      ],
      "summary": "Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the \"sexual orientation\" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.",
      "published": "2025-12-07T08:57:27Z",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.06732v1.pdf",
      "relevance_score": 54.0
    },
    {
      "arxiv_id": "2512.06205v1",
      "title": "On measuring grounding and generalizing grounding problems",
      "authors": [
        "Daniel Quigley",
        "Eric Maynard"
      ],
      "summary": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.",
      "published": "2025-12-05T22:58:47Z",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.06205v1.pdf",
      "relevance_score": 54.0
    },
    {
      "arxiv_id": "2512.05224v1",
      "title": "NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM",
      "authors": [
        "Miguel de Oliveira Guerreiro"
      ],
      "summary": "Actor-based systems like Erlang/OTP power critical infrastructure -- from telecommunications to messaging platforms -- handling millions of concurrent connections with legendary reliability. Yet these systems lack static guarantees about message protocols: processes communicate by sending arbitrary messages that pattern-matched at runtime, deferring protocol violations to production failures. We present NVLang, a statically typed functional language that brings comprehensive type safety to the BEAM virtual machine while preserving actor model's simplicity and power. NVLang's central contribution that algebraic data types (ADTs) naturally encode actor message protocols: each actor declares the sum type representing its message vocabulary, and the type system enforces protocol conformance at compile time. We introduce typed process identifiers (Pid[T]) that encode the protocol an actor expects, and typed futures (Future[T]) that provide type-safe request-reply patterns. By extending Hindley-Milner type inference to track message protocols, NVLang eliminates an entire class of message-passing errors while maintaining clean syntax that rivals dynamically typed alternatives. Our implementation compiles to Core Erlang, enabling seamless interoperability with the existing Erlang ecosystem. We formalize the type system and provide proof sketches for type soundness, demonstrating that well-typed NVLang programs cannot send messages that violate actor protocols.",
      "published": "2025-12-04T19:52:42Z",
      "year": "2025",
      "categories": [
        "cs.PL",
        "cs.DC"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.05224v1.pdf",
      "relevance_score": 54.0
    },
    {
      "arxiv_id": "2512.16959v1",
      "title": "Resilient Microservices: A Systematic Review of Recovery Patterns, Strategies, and Evaluation Frameworks",
      "authors": [
        "Muzeeb Mohammad"
      ],
      "summary": "Microservice based systems underpin modern distributed computing environments but remain vulnerable to partial failures, cascading timeouts, and inconsistent recovery behavior. Although numerous resilience and recovery patterns have been proposed, existing surveys are largely descriptive and lack systematic evidence synthesis or quantitative rigor. This paper presents a PRISMA aligned systematic literature review of empirical studies on microservice recovery strategies published between 2014 and 2025 across IEEE Xplore, ACM Digital Library, and Scopus. From an initial corpus of 412 records, 26 high quality studies were selected using transparent inclusion, exclusion, and quality assessment criteria. The review identifies nine recurring resilience themes encompassing circuit breakers, retries with jitter and budgets, sagas with compensation, idempotency, bulkheads, adaptive backpressure, observability, and chaos validation. As a data oriented contribution, the paper introduces a Recovery Pattern Taxonomy, a Resilience Evaluation Score checklist for standardized benchmarking, and a constraint aware decision matrix mapping latency, consistency, and cost trade offs to appropriate recovery mechanisms. The results consolidate fragmented resilience research into a structured and analyzable evidence base that supports reproducible evaluation and informed design of fault tolerant and performance aware microservice systems.",
      "published": "2025-12-18T04:13:54Z",
      "year": "2025",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.16959v1.pdf",
      "relevance_score": 54.0
    },
    {
      "arxiv_id": "2508.18949v1",
      "title": "Energy-Based Flow Matching for Generating 3D Molecular Structure",
      "authors": [
        "Wenyin Zhou",
        "Christopher Iliffe Sprague",
        "Vsevolod Viliuga",
        "Matteo Tadiello",
        "Arne Elofsson",
        "Hossein Azizpour"
      ],
      "summary": "Molecular structure generation is a fundamental problem that involves determining the 3D positions of molecules' constituents. It has crucial biological applications, such as molecular docking, protein folding, and molecular design. Recent advances in generative modeling, such as diffusion models and flow matching, have made great progress on these tasks by modeling molecular conformations as a distribution. In this work, we focus on flow matching and adopt an energy-based perspective to improve training and inference of structure generation models. Our view results in a mapping function, represented by a deep network, that is directly learned to \\textit{iteratively} map random configurations, i.e. samples from the source distribution, to target structures, i.e. points in the data manifold. This yields a conceptually simple and empirically effective flow matching setup that is theoretically justified and has interesting connections to fundamental properties such as idempotency and stability, as well as the empirically useful techniques such as structure refinement in AlphaFold. Experiments on protein docking as well as protein backbone generation consistently demonstrate the method's effectiveness, where it outperforms recent baselines of task-associated flow matching and diffusion models, using a similar computational budget.",
      "published": "2025-08-26T11:42:57Z",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2508.18949v1.pdf",
      "relevance_score": 53.0
    },
    {
      "arxiv_id": "2512.15252v1",
      "title": "Gaming the Arena: AI Model Evaluation and the Viral Capture of Attention",
      "authors": [
        "Sam Hind"
      ],
      "summary": "Innovation in artificial intelligence (AI) has always been dependent on technological infrastructures, from code repositories to computing hardware. Yet industry -- rather than universities -- has become increasingly influential in shaping AI innovation. As generative forms of AI powered by large language models (LLMs) have driven the breakout of AI into the wider world, the AI community has sought to develop new methods for independently evaluating the performance of AI models. How best, in other words, to compare the performance of AI models against other AI models -- and how best to account for new models launched on nearly a daily basis? Building on recent work in media studies, STS, and computer science on benchmarking and the practices of AI evaluation, I examine the rise of so-called 'arenas' in which AI models are evaluated with reference to gladiatorial-style 'battles'. Through a technography of a leading user-driven AI model evaluation platform, LMArena, I consider five themes central to the emerging 'arena-ization' of AI innovation. Accordingly, I argue that the arena-ization is being powered by a 'viral' desire to capture attention both in, and outside of, the AI community, critical to the scaling and commercialization of AI products. In the discussion, I reflect on the implications of 'arena gaming', a phenomenon through which model developers hope to capture attention.",
      "published": "2025-12-17T09:50:13Z",
      "year": "2025",
      "categories": [
        "cs.CY"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.15252v1.pdf",
      "relevance_score": 50.0
    },
    {
      "arxiv_id": "2512.08782v1",
      "title": "An Explainable AI Model for the Detecting Malicious Smart Contracts Based on EVM Opcode Based Features",
      "authors": [
        "Roopak Surendran"
      ],
      "summary": "Hackers may create malicious solidity programs and deploy it in the Ethereum block chain. These malicious smart contracts try to attack legitimate programs by exploiting its vulnerabilities such as reentrancy, tx.origin attack, bad randomness, deligatecall and so on. This may lead to drain of the funds, denial of service and so on . Hence, it is necessary to identify and prevent the malicious smart contract before deploying it into the blockchain. In this paper, we propose an ML based malicious smart contract detection mechanism by analyzing the EVM opcodes. After balancing the opcode frequency dataset with SMOTE algorithm, we transformed opcode frequencies to the binary values (0,1) using an entropy based supervised binning method. Then, an explainable AI model is trained with the proposed binary opcode based features. From the implementations, we found that the proposed mechanism can detect 99% of malicious smart contracts with a false positive rate of only 0.01. Finally, we incorporated LIME algorithm in our classifier to justify its predictions. We found that, LIME algorithm can explain why a particular smart contract app is declared as malicious by our ML classifier based on the binary value of EVM opcodes.",
      "published": "2025-12-09T16:34:23Z",
      "year": "2025",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.08782v1.pdf",
      "relevance_score": 50.0
    },
    {
      "arxiv_id": "2512.03175v3",
      "title": "The Seifert-van Kampen Theorem via Computational Paths: A Formalized Approach to Computing Fundamental Groups",
      "authors": [
        "Arthur F. Ramos",
        "Tiago M. L. de Veras",
        "Ruy J. G. B. de Queiroz",
        "Anjolina G. de Oliveira"
      ],
      "summary": "The Seifert-van Kampen theorem computes the fundamental group of a space from the fundamental groups of its constituents. We develop a modular SVK framework within the setting of computational paths - an approach to equality where witnesses are explicit sequences of rewrites governed by the LNDEQ-TRS. Our contributions are: (i) pushouts as higher-inductive types with modular typeclass assumptions for computation rules; (ii) free products and amalgamated free products as quotients of word representations; (iii) an SVK equivalence schema parametric in user-supplied encode/decode structure; and (iv) instantiations for classical spaces - figure-eight (pi_1(S^1 v S^1) = Z * Z), 2-sphere (pi_1(S^2) = 1), and 3-sphere (pi_1(S^3) = 1) with Hopf fibration context. Recent extensions include higher homotopy groups pi_n via weak infinity-groupoid structure (with pi_2 abelian via Eckmann-Hilton), and pi_1 >= 1 in the 1-groupoid truncated setting; truncation levels connecting the framework to HoTT; automated path simplification tactics; basic covering space theory with pi_1-actions on fibers; fibration theory with long exact sequences; and Eilenberg-MacLane space characterization (S^1 = K(Z,1)). The development is formalized in Lean 4 with 41,130 lines across 107 modules, using 36 kernel axioms for HIT type-constructor declarations.",
      "published": "2025-12-02T19:21:55Z",
      "year": "2025",
      "categories": [
        "cs.LO"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2512.03175v3.pdf",
      "relevance_score": 50.0
    },
    {
      "arxiv_id": "2510.08570v1",
      "title": "Who Said Neural Networks Aren't Linear?",
      "authors": [
        "Nimrod Berman",
        "Assaf Hallak",
        "Assaf Shocher"
      ],
      "summary": "Neural networks are famously nonlinear. However, linearity is defined relative to a pair of vector spaces, $f$$:$$X$$\\to$$Y$. Is it possible to identify a pair of non-standard vector spaces for which a conventionally nonlinear function is, in fact, linear? This paper introduces a method that makes such vector spaces explicit by construction. We find that if we sandwich a linear operator $A$ between two invertible neural networks, $f(x)=g_y^{-1}(A g_x(x))$, then the corresponding vector spaces $X$ and $Y$ are induced by newly defined addition and scaling actions derived from $g_x$ and $g_y$. We term this kind of architecture a Linearizer. This framework makes the entire arsenal of linear algebra, including SVD, pseudo-inverse, orthogonal projection and more, applicable to nonlinear mappings. Furthermore, we show that the composition of two Linearizers that share a neural network is also a Linearizer. We leverage this property and demonstrate that training diffusion models using our architecture makes the hundreds of sampling steps collapse into a single step. We further utilize our framework to enforce idempotency (i.e. $f(f(x))=f(x)$) on networks leading to a globally projective generative model and to demonstrate modular style transfer.",
      "published": "2025-10-09T17:59:57Z",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2510.08570v1.pdf",
      "relevance_score": 50.0
    },
    {
      "arxiv_id": "2507.01320v1",
      "title": "Robust Multi-generation Learned Compression of Point Cloud Attribute",
      "authors": [
        "Xiangzuo Liu",
        "Zhikai Liu",
        "PengPeng Yu",
        "Ruishan Huang",
        "Fan Liang"
      ],
      "summary": "Existing learned point cloud attribute compression methods primarily focus on single-pass rate-distortion optimization, while overlooking the issue of cumulative distortion in multi-generation compression scenarios. This paper, for the first time, investigates the multi-generation issue in learned point cloud attribute compression. We identify two primary factors contributing to quality degradation in multi-generation compression: quantization-induced non-idempotency and transformation irreversibility. To address the former, we propose a Mapping Idempotency Constraint, that enables the network to learn the complete compression-decompression mapping, enhancing its robustness to repeated processes. To address the latter, we introduce a Transformation Reversibility Constraint, which preserves reversible information flow via a quantization-free training path. Further, we propose a Latent Variable Consistency Constraint which enhances the multi-generation compression robustness by incorporating a decompression-compression cross-generation path and a latent variable consistency loss term. Extensive experiments conducted on the Owlii and 8iVFB datasets verify that the proposed methods can effectively suppress multi-generation loss while maintaining single-pass rate-distortion performance comparable to baseline models.",
      "published": "2025-07-02T03:08:37Z",
      "year": "2025",
      "categories": [
        "cs.MM"
      ],
      "primary_category": "",
      "pdf_url": "http://arxiv.org/pdf/2507.01320v1.pdf",
      "relevance_score": 50.0
    }
  ]
}