{
  "total_archived": 376,
  "archived_date": "2025-12-25T09:31:19.614205",
  "papers": [
    {
      "arxiv_id": "2512.15081v1",
      "title": "Quantifying Return on Security Controls in LLM Systems",
      "authors": [
        "Richard Helder Moulton",
        "Austin O'Brien",
        "John D. Hastings"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T04:58:09Z",
      "summary": "Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).",
      "pdf_url": "https://arxiv.org/pdf/2512.15081v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "relevance_score": 53,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.14744v1",
      "title": "VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation",
      "authors": [
        "Adewale Akinfaderin",
        "Shreyas Subramanian"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T17:17:43Z",
      "summary": "Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\\% factual correctness, VERAFI's integrated approach reaches 94.7\\%, an 81\\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.",
      "pdf_url": "https://arxiv.org/pdf/2512.14744v1",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "relevance_score": 52,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'policy generation' (+15)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19025v2",
      "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
      "authors": [
        "Hengrui Jia",
        "Taoran Li",
        "Jonas Guan",
        "Varun Chandrasekaran"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T04:42:41Z",
      "summary": "Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have \"forgotten\" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.",
      "pdf_url": "https://arxiv.org/pdf/2512.19025v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 52,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.16965v1",
      "title": "AutoDFBench 1.0: A Benchmarking Framework for Digital Forensic Tool Testing and Generated Code Evaluation",
      "authors": [
        "Akila Wickramasekara",
        "Tharusha Mihiranga",
        "Aruna Withanage",
        "Buddhima Weerasinghe",
        "Frank Breitinger",
        "John Sheppard",
        "Mark Scanlon"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T11:16:33Z",
      "summary": "The National Institute of Standards and Technology (NIST) Computer Forensic Tool Testing (CFTT) programme has become the de facto standard for providing digital forensic tool testing and validation. However to date, no comprehensive framework exists to automate benchmarking across the diverse forensic tasks included in the programme. This gap results in inconsistent validation, challenges in comparing tools, and limited validation reproducibility. This paper introduces AutoDFBench 1.0, a modular benchmarking framework that supports the evaluation of both conventional DF tools and scripts, as well as AI-generated code and agentic approaches. The framework integrates five areas defined by the CFTT programme: string search, deleted file recovery, file carving, Windows registry recovery, and SQLite data recovery. AutoDFBench 1.0 includes ground truth data comprising of 63 test cases and 10,968 unique test scenarios, and execute evaluations through a RESTful API that produces structured JSON outputs with standardised metrics, including precision, recall, and F1~score for each test case, and the average of these F1~scores becomes the AutoDFBench Score. The benchmarking framework is validated against CFTT's datasets. The framework enables fair and reproducible comparison across tools and forensic scripts, establishing the first unified, automated, and extensible benchmarking framework for digital forensic tool testing and validation. AutoDFBench 1.0 supports tool vendors, researchers, practitioners, and standardisation bodies by facilitating transparent, reproducible, and comparable assessments of DF technologies.",
      "pdf_url": "https://arxiv.org/pdf/2512.16965v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "relevance_score": 51,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.05439v1",
      "title": "BEAVER: An Efficient Deterministic LLM Verifier",
      "authors": [
        "Tarun Suresh",
        "Nalin Wadhwa",
        "Debangshu Banerjee",
        "Gagandeep Singh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T05:34:06Z",
      "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
      "pdf_url": "https://arxiv.org/pdf/2512.05439v1",
      "categories": [
        "cs.AI",
        "cs.FL"
      ],
      "relevance_score": 51,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'nist' (+10)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18542v1",
      "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "authors": [
        "Scott Thornton"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T23:52:12Z",
      "summary": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).   Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.   Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
      "pdf_url": "https://arxiv.org/pdf/2512.18542v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "relevance_score": 50,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.18462v1",
      "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
      "authors": [
        "Christopher Román Jaimes"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T18:30:54Z",
      "summary": "Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.",
      "pdf_url": "https://arxiv.org/pdf/2512.18462v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 50,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.15468v1",
      "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
      "authors": [
        "Hua Yang",
        "Alejandro Velasco",
        "Thanh Le-Cong",
        "Md Nazmul Haque",
        "Bowen Xu",
        "Denys Poshyvanyk"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T14:12:54Z",
      "summary": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.   In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
      "pdf_url": "https://arxiv.org/pdf/2512.15468v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "relevance_score": 50,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15688v1",
      "title": "BashArena: A Control Setting for Highly Privileged AI Agents",
      "authors": [
        "Adam Kaufman",
        "James Lucassen",
        "Tyler Tracy",
        "Cody Rushing",
        "Aryan Bhatt"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T18:45:25Z",
      "summary": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.",
      "pdf_url": "https://arxiv.org/pdf/2512.15688v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 49,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11482v1",
      "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models",
      "authors": [
        "Melih Catal",
        "Pooja Rani",
        "Harald C. Gall"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T11:31:13Z",
      "summary": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.",
      "pdf_url": "https://arxiv.org/pdf/2512.11482v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "relevance_score": 49,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.10485v1",
      "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
      "authors": [
        "Chaomeng Lu",
        "Bert Lagaisse"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T10:04:54Z",
      "summary": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
      "pdf_url": "https://arxiv.org/pdf/2512.10485v1",
      "categories": [
        "cs.CR",
        "cs.LG",
        "cs.SE"
      ],
      "relevance_score": 49,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.20159v1",
      "title": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration",
      "authors": [
        "Ruiqi Wang",
        "Xinchen Wang",
        "Cuiyun Gao",
        "Chun Yong Chong",
        "Xin Xia",
        "Qing Liao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T08:39:22Z",
      "summary": "Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.   To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...",
      "pdf_url": "https://arxiv.org/pdf/2512.20159v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 49,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2511.21758v1",
      "title": "A Longitudinal Measurement of Privacy Policy Evolution for Large Language Models",
      "authors": [
        "Zhen Tao",
        "Shidong Pan",
        "Zhenchang Xing",
        "Emily Black",
        "Talia Gillis",
        "Chunyang Chen"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-24T12:40:15Z",
      "summary": "Large language model (LLM) services have been rapidly integrated into people's daily lives as chatbots and agentic systems. They are nourished by collecting rich streams of data, raising privacy concerns around excessive collection of sensitive personal information. Privacy policies are the fundamental mechanism for informing users about data practices in modern information privacy paradigm. Although traditional web and mobile policies are well studied, the privacy policies of LLM providers, their LLM-specific content, and their evolution over time remain largely underexplored. In this paper, we present the first longitudinal empirical study of privacy policies for mainstream LLM providers worldwide. We curate a chronological dataset of 74 historical privacy policies and 115 supplemental privacy documents from 11 LLM providers across 5 countries up to August 2025, and extract over 3,000 sentence-level edits between consecutive policy versions. We compare LLM privacy policies to those of other software formats, propose a taxonomy tailored to LLM privacy policies, annotate policy edits and align them with a timeline of key LLM ecosystem events. Results show they are substantially longer, demand college-level reading ability, and remain highly vague. Our taxonomy analysis reveals patterns in how providers disclose LLM-specific practices and highlights regional disparities in coverage. Policy edits are concentrated in first-party data collection and international/specific-audience sections, and that product releases and regulatory actions are the primary drivers, shedding light on the status quo and the evolution of LLM privacy policies.",
      "pdf_url": "https://arxiv.org/pdf/2511.21758v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "relevance_score": 48,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.10449v2",
      "title": "When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection",
      "authors": [
        "Devanshu Sahoo",
        "Manish Prasad",
        "Vasudev Majhi",
        "Jahnvi Singh",
        "Vinay Chamola",
        "Yash Sinha",
        "Murari Mandal",
        "Dhruv Kumar"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T09:13:36Z",
      "summary": "The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the \"Lazy Reviewer\" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these \"LLM-as-a-Judge\" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping \"Reject\" decisions to \"Accept,\" for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like \"Maximum Mark Magyk\" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.",
      "pdf_url": "https://arxiv.org/pdf/2512.10449v2",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "relevance_score": 48,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.21028v1",
      "title": "Artificial or Just Artful? Do LLMs Bend the Rules in Programming?",
      "authors": [
        "Oussama Ben Sghaier",
        "Kevin Delcourt",
        "Houari Sahraoui"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T07:51:15Z",
      "summary": "Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.",
      "pdf_url": "https://arxiv.org/pdf/2512.21028v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 48,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.06787v1",
      "title": "LLM4SFC: Sequential Function Chart Generation via Large Language Models",
      "authors": [
        "Ofek Glick",
        "Vladimir Tchuiev",
        "Marah Ghoummaid",
        "Michal Moshkovitz",
        "Dotan Di-Castro"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T11:02:45Z",
      "summary": "While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.",
      "pdf_url": "https://arxiv.org/pdf/2512.06787v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.04416v2",
      "title": "DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows",
      "authors": [
        "Zhou Liu",
        "Zhaoyang Han",
        "Guochen Yan",
        "Hao Liang",
        "Bohan Zeng",
        "Xing Chen",
        "Yuanfeng Song",
        "Wentao Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T03:25:12Z",
      "summary": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce DataGovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. DataGovBench employs a novel \"reversed-objective\" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on DataGovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.",
      "pdf_url": "https://arxiv.org/pdf/2512.04416v2",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.16962v1",
      "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
      "authors": [
        "Saksham Sahai Srivastava",
        "Haoyu He"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T08:34:40Z",
      "summary": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.",
      "pdf_url": "https://arxiv.org/pdf/2512.16962v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.13325v1",
      "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models",
      "authors": [
        "Malte Hellmeier"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-15T13:40:00Z",
      "summary": "Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",
      "pdf_url": "https://arxiv.org/pdf/2512.13325v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.10415v1",
      "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation",
      "authors": [
        "Devanshu Sahoo",
        "Vasudev Majhi",
        "Arjun Neekhra",
        "Yash Sinha",
        "Murari Mandal",
        "Dhruv Kumar"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T08:28:33Z",
      "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.",
      "pdf_url": "https://arxiv.org/pdf/2512.10415v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06747v1",
      "title": "PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance",
      "authors": [
        "Jifar Wakuma Ayana",
        "Huang Qiming"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T09:20:14Z",
      "summary": "Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.",
      "pdf_url": "https://arxiv.org/pdf/2512.06747v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20168v1",
      "title": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
      "authors": [
        "Songze Li",
        "Jiameng Cheng",
        "Yiming Li",
        "Xiaojun Jia",
        "Dacheng Tao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T08:53:36Z",
      "summary": "By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.20168v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20677v1",
      "title": "Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System",
      "authors": [
        "Zhang Wei",
        "Peilu Hu",
        "Shengning Lang",
        "Hao Yan",
        "Li Mei",
        "Yichao Zhang",
        "Chen Yang",
        "Junfeng Hao",
        "Zhimo Han"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T19:12:44Z",
      "summary": "As large language models (LLMs) are increasingly deployed in high-stakes domains, ensuring their security and alignment has become a critical challenge. Existing red-teaming practices depend heavily on manual testing, which limits scalability and fails to comprehensively cover the vast space of potential adversarial behaviors. This paper introduces an automated red-teaming framework that systematically generates, executes, and evaluates adversarial prompts to uncover security vulnerabilities in LLMs. Our framework integrates meta-prompting-based attack synthesis, multi-modal vulnerability detection, and standardized evaluation protocols spanning six major threat categories -- reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation. Experiments on the GPT-OSS-20B model reveal 47 distinct vulnerabilities, including 21 high-severity and 12 novel attack patterns, achieving a $3.9\\times$ improvement in vulnerability discovery rate over manual expert testing while maintaining 89\\% detection accuracy. These results demonstrate the framework's effectiveness in enabling scalable, systematic, and reproducible AI safety evaluations. By providing actionable insights for improving alignment robustness, this work advances the state of automated LLM red-teaming and contributes to the broader goal of building secure and trustworthy AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.20677v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.09485v1",
      "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks",
      "authors": [
        "Xinye Cao",
        "Yihan Lin",
        "Guoshun Nan",
        "Qinchuan Zhou",
        "Yuhang Luo",
        "Yurui Gao",
        "Zeliang Zhang",
        "Haolang Lu",
        "Qimei Cui",
        "Yanzhao Hou",
        "Xiaofeng Tao",
        "Tony Q. S. Quek"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-10T10:04:11Z",
      "summary": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.",
      "pdf_url": "https://arxiv.org/pdf/2512.09485v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06781v1",
      "title": "From Description to Score: Can LLMs Quantify Vulnerabilities?",
      "authors": [
        "Sima Jafarikhah",
        "Daniel Thompson",
        "Eva Deans",
        "Hossein Siadati",
        "Yi Liu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T10:47:00Z",
      "summary": "Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \\textit{Availability Impact}), while offering more modest gains on others (e.g., \\textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.",
      "pdf_url": "https://arxiv.org/pdf/2512.06781v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PL"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06555v1",
      "title": "BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models",
      "authors": [
        "Arush Sachdeva",
        "Rajendraprasad Saravanan",
        "Gargi Sarkar",
        "Kavita Vemuri",
        "Sandeep Kumar Shukla"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T19:59:24Z",
      "summary": "Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.",
      "pdf_url": "https://arxiv.org/pdf/2512.06555v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18682v1",
      "title": "Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design",
      "authors": [
        "Yuchen Li",
        "Handing Wang",
        "Bing Xue",
        "Mengjie Zhang",
        "Yaochu Jin"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T10:40:36Z",
      "summary": "In the high-cost simulation-driven design domain, translating ambiguous design requirements into a mathematical optimization formulation is a bottleneck for optimizing product performance. This process is time-consuming and heavily reliant on expert knowledge. While large language models (LLMs) offer potential for automating this task, existing approaches either suffer from poor formalization that fails to accurately align with the design intent or rely on solver feedback for data filtering, which is unavailable due to the high simulation costs. To address this challenge, we propose APF, a framework for solver-independent, automated problem formulation via LLMs designed to automatically convert engineers' natural language requirements into executable optimization models. The core of this framework is an innovative pipeline for automatically generating high-quality data, which overcomes the difficulty of constructing suitable fine-tuning datasets in the absence of high-cost solver feedback with the help of data generation and test instance annotation. The generated high-quality dataset is used to perform supervised fine-tuning on LLMs, significantly enhancing their ability to generate accurate and executable optimization problem formulations. Experimental results on antenna design demonstrate that APF significantly outperforms the existing methods in both the accuracy of requirement formalization and the quality of resulting radiation efficiency curves in meeting the design goals.",
      "pdf_url": "https://arxiv.org/pdf/2512.18682v1",
      "categories": [
        "cs.CL",
        "cs.SE"
      ],
      "relevance_score": 47,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.17540v1",
      "title": "SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review",
      "authors": [
        "Kai Wang",
        "Bingcheng Mao",
        "Shuai Jia",
        "Yujie Ding",
        "Dongming Han",
        "Tianyi Ma",
        "Bin Cao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T13:02:22Z",
      "summary": "Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.",
      "pdf_url": "https://arxiv.org/pdf/2512.17540v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 46,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18102v1",
      "title": "From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines",
      "authors": [
        "Kishan Kumar Ganguly",
        "Tim Menzies"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T22:15:53Z",
      "summary": "Context: Exhaustive fuzzing of modern JavaScript engines is infeasible due to the vast number of program states and execution paths. Coverage-guided fuzzers waste effort on low-risk inputs, often ignoring vulnerability-triggering ones that do not increase coverage. Existing heuristics proposed to mitigate this require expert effort, are brittle, and hard to adapt.   Objective: We propose a data-centric, LLM-boosted alternative that learns from historical vulnerabilities to automatically identify minimal static (code) and dynamic (runtime) features for detecting high-risk inputs.   Method: Guided by historical V8 bugs, iterative prompting generated 115 static and 49 dynamic features, with the latter requiring only five trace flags, minimizing instrumentation cost. After feature selection, 41 features remained to train an XGBoost model to predict high-risk inputs during fuzzing.   Results: Combining static and dynamic features yields over 85% precision and under 1% false alarms. Only 25% of these features are needed for comparable performance, showing that most of the search space is irrelevant.   Conclusion: This work introduces feature-guided fuzzing, an automated data-driven approach that replaces coverage with data-directed inference, guiding fuzzers toward high-risk states for faster, targeted, and reproducible vulnerability discovery. To support open science, all scripts and data are available at https://github.com/KKGanguly/DataCentricFuzzJS .",
      "pdf_url": "https://arxiv.org/pdf/2512.18102v1",
      "categories": [
        "cs.SE",
        "cs.CR",
        "cs.LG"
      ],
      "relevance_score": 46,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.10172v1",
      "title": "Offscript: Automated Auditing of Instruction Adherence in LLMs",
      "authors": [
        "Nicholas Clark",
        "Ryan Bai",
        "Tanu Mitra"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T00:11:50Z",
      "summary": "Large Language Models (LLMs) and generative search systems are increasingly used for information seeking by diverse populations with varying preferences for knowledge sourcing and presentation. While users can customize LLM behavior through custom instructions and behavioral prompts, no mechanism exists to evaluate whether these instructions are being followed effectively. We present Offscript, an automated auditing tool that efficiently identifies potential instruction following failures in LLMs. In a pilot study analyzing custom instructions sourced from Reddit, Offscript detected potential deviations from instructed behavior in 86.4% of conversations, 22.2% of which were confirmed as material violations through human review. Our findings suggest that automated auditing serves as a viable approach for evaluating compliance to behavioral instructions related to information seeking.",
      "pdf_url": "https://arxiv.org/pdf/2512.10172v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.05951v2",
      "title": "Trusted AI Agents in the Cloud",
      "authors": [
        "Teofil Bodea",
        "Masanori Misono",
        "Julian Pritzi",
        "Patrick Sabanic",
        "Thore Sommer",
        "Harshavardhan Unnibhavi",
        "David Schall",
        "Nuno Santos",
        "Dimitrios Stavrakakis",
        "Pramod Bhatotia"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T18:48:53Z",
      "summary": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.",
      "pdf_url": "https://arxiv.org/pdf/2512.05951v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.05383v1",
      "title": "Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation",
      "authors": [
        "Mara Downing",
        "Matthew Peng",
        "Jacob Granley",
        "Michael Beyeler",
        "Tevfik Bultan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T02:43:22Z",
      "summary": "Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.",
      "pdf_url": "https://arxiv.org/pdf/2512.05383v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20164v1",
      "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
      "authors": [
        "Honglin Mu",
        "Jinghao Liu",
        "Kaiyang Wan",
        "Rui Xing",
        "Xiuying Chen",
        "Timothy Baldwin",
        "Wanxiang Che"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T08:42:09Z",
      "summary": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",
      "pdf_url": "https://arxiv.org/pdf/2512.20164v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.12069v1",
      "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring",
      "authors": [
        "Peichun Hua",
        "Hao Li",
        "Shanghao Shi",
        "Zhiyuan Yu",
        "Ning Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T22:31:38Z",
      "summary": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.",
      "pdf_url": "https://arxiv.org/pdf/2512.12069v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20586v1",
      "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
      "authors": [
        "Humza Nusrat",
        "Luke Francisco",
        "Bing Luo",
        "Hassan Bagher-Ebadian",
        "Joshua Kim",
        "Karen Chin-Snyder",
        "Salim Siddiqui",
        "Mira Shah",
        "Eric Mellon",
        "Mohammad Ghassemi",
        "Anthony Doemer",
        "Benjamin Movsas",
        "Kundan Thind"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T18:32:17Z",
      "summary": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.",
      "pdf_url": "https://arxiv.org/pdf/2512.20586v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06716v1",
      "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents",
      "authors": [
        "Zhibo Liang",
        "Tianze Hu",
        "Zaiye Chen",
        "Mingjie Tang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T08:11:19Z",
      "summary": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.",
      "pdf_url": "https://arxiv.org/pdf/2512.06716v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18880v1",
      "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "authors": [
        "Ming Li",
        "Han Chen",
        "Yunze Xiao",
        "Jian Chen",
        "Hong Jiao",
        "Tianyi Zhou"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T20:41:36Z",
      "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
      "pdf_url": "https://arxiv.org/pdf/2512.18880v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.15082v1",
      "title": "The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks",
      "authors": [
        "Wanfu Gao",
        "Zebin He",
        "Jun Gao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T04:58:44Z",
      "summary": "Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.",
      "pdf_url": "https://arxiv.org/pdf/2512.15082v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 45,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.05314v1",
      "title": "WhatsCode: Large-Scale GenAI Deployment for Developer Efficiency at WhatsApp",
      "authors": [
        "Ke Mao",
        "Timotej Kapus",
        "Cons T Åhs",
        "Matteo Marescotti",
        "Daniel Ip",
        "Ákos Hajdu",
        "Sopot Cela",
        "Aparup Banerjee"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T23:25:06Z",
      "summary": "The deployment of AI-assisted development tools in compliance-relevant, large-scale industrial environments represents significant gaps in academic literature, despite growing industry adoption. We report on the industrial deployment of WhatsCode, a domain-specific AI development system that supports WhatsApp (serving over 2 billion users) and processes millions of lines of code across multiple platforms. Over 25 months (2023-2025), WhatsCode evolved from targeted privacy automation to autonomous agentic workflows integrated with end-to-end feature development and DevOps processes.   WhatsCode achieved substantial quantifiable impact, improving automated privacy verification coverage 3.5x from 15% to 53%, identifying privacy requirements, and generating over 3,000 accepted code changes with acceptance rates ranging from 9% to 100% across different automation domains. The system committed 692 automated refactor/fix changes, 711 framework adoptions, 141 feature development assists and maintained 86% precision in bug triage. Our study identifies two stable human-AI collaboration patterns that emerged from production deployment: one-click rollout for high-confidence changes (60% of cases) and commandeer-revise for complex decisions (40%). We demonstrate that organizational factors, such as ownership models, adoption dynamics, and risk management, are as decisive as technical capabilities for enterprise-scale AI success. The findings provide evidence-based guidance for large-scale AI tool deployment in compliance-relevant environments, showing that effective human-AI collaboration, not full automation, drives sustainable business impact.",
      "pdf_url": "https://arxiv.org/pdf/2512.05314v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 44,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2511.18632v1",
      "title": "The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion",
      "authors": [
        "Jan Benedikt Ruhland",
        "Doguhan Bahcivan",
        "Jan-Peter Sowa",
        "Ali Canbay",
        "Dominik Heider"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-23T22:12:35Z",
      "summary": "Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.   In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.   Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.",
      "pdf_url": "https://arxiv.org/pdf/2511.18632v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 44,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.04259v1",
      "title": "WildCode: An Empirical Analysis of Code Generated by ChatGPT",
      "authors": [
        "Kobra Khanmohammadi",
        "Pooria Roy",
        "Raphael Khoury",
        "Abdelwahab Hamou-Lhadj",
        "Wilfried Patrick Konan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-03T20:54:24Z",
      "summary": "LLM models are increasingly used to generate code, but the quality and security of this code are often uncertain. Several recent studies have raised alarm bells, indicating that such AI-generated code may be particularly vulnerable to cyberattacks. However, most of these studies rely on code that is generated specifically for the study, which raises questions about the realism of such experiments. In this study, we perform a large-scale empirical analysis of real-life code generated by ChatGPT. We evaluate code generated by ChatGPT both with respect to correctness and security and delve into the intentions of users who request code from the model. Our research confirms previous studies that used synthetic queries and yielded evidence that LLM-generated code is often inadequate with respect to security. We also find that users exhibit little curiosity about the security features of the code they ask LLMs to generate, as evidenced by their lack of queries on this topic.",
      "pdf_url": "https://arxiv.org/pdf/2512.04259v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "relevance_score": 44,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'gpt' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.14990v1",
      "title": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent",
      "authors": [
        "Mehil B Shah",
        "Mohammad Masudur Rahman",
        "Foutse Khomh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T00:50:58Z",
      "summary": "Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.",
      "pdf_url": "https://arxiv.org/pdf/2512.14990v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 44,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.12885v1",
      "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition",
      "authors": [
        "Minghao Zhu",
        "Zhihao Zhang",
        "Anmol Sidhu",
        "Keith Redmill"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T23:56:34Z",
      "summary": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.",
      "pdf_url": "https://arxiv.org/pdf/2512.12885v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.RO"
      ],
      "relevance_score": 43,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.08213v1",
      "title": "Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs",
      "authors": [
        "Md Nazmul Haque",
        "Elizabeth Lin",
        "Lawrence Arkoh",
        "Biruk Tadesse",
        "Bowen Xu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T03:47:31Z",
      "summary": "Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.   In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.",
      "pdf_url": "https://arxiv.org/pdf/2512.08213v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 43,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.07827v1",
      "title": "An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning",
      "authors": [
        "Lukas Johannes Möller"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T18:55:26Z",
      "summary": "The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.",
      "pdf_url": "https://arxiv.org/pdf/2512.07827v1",
      "categories": [
        "cs.CR",
        "cs.DC",
        "cs.LG"
      ],
      "relevance_score": 43,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.06846v1",
      "title": "CKG-LLM: LLM-Assisted Detection of Smart Contract Access Control Vulnerabilities Based on Knowledge Graphs",
      "authors": [
        "Xiaoqi Li",
        "Hailu Kuang",
        "Wenkai Li",
        "Zongwei Li",
        "Shipeng Ye"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T13:58:37Z",
      "summary": "Traditional approaches for smart contract analysis often rely on intermediate representations such as abstract syntax trees, control-flow graphs, or static single assignment form. However, these methods face limitations in capturing both semantic structures and control logic. Knowledge graphs, by contrast, offer a structured representation of entities and relations, enabling richer intermediate abstractions of contract code and supporting the use of graph query languages to identify rule-violating elements. This paper presents CKG-LLM, a framework for detecting access-control vulnerabilities in smart contracts. Leveraging the reasoning and code generation capabilities of large language models, CKG-LLM translates natural-language vulnerability patterns into executable queries over contract knowledge graphs to automatically locate vulnerable code elements. Experimental evaluation demonstrates that CKG-LLM achieves superior performance in detecting access-control vulnerabilities compared to existing tools. Finally, we discuss potential extensions of CKG-LLM as part of future research directions.",
      "pdf_url": "https://arxiv.org/pdf/2512.06846v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 43,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.06248v1",
      "title": "CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models",
      "authors": [
        "Cheng Cheng",
        "Jinqiu Yang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T02:20:31Z",
      "summary": "Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.",
      "pdf_url": "https://arxiv.org/pdf/2512.06248v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 43,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.17023v1",
      "title": "LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation",
      "authors": [
        "Patrick Diehl",
        "Noujoud Nader",
        "Deepti Gupta"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T19:37:33Z",
      "summary": "Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.17023v1",
      "categories": [
        "cs.DC",
        "cs.SE"
      ],
      "relevance_score": 43,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18244v1",
      "title": "Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation",
      "authors": [
        "Zehao Liu",
        "Xi Lin"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T07:02:00Z",
      "summary": "Large Language Models (LLMs) have gained considerable popularity and protected by increasingly sophisticated safety mechanisms. However, jailbreak attacks continue to pose a critical security threat by inducing models to generate policy-violating behaviors. Current paradigms focus on input-level anomalies, overlooking that the model's internal psychometric state can be systematically manipulated. To address this, we introduce Psychological Jailbreak, a new jailbreak attack paradigm that exposes a stateful psychological attack surface in LLMs, where attackers exploit the manipulation of a model's psychological state across interactions. Building on this insight, we propose Human-like Psychological Manipulation (HPM), a black-box jailbreak method that dynamically profiles a target model's latent psychological vulnerabilities and synthesizes tailored multi-turn attack strategies. By leveraging the model's optimization for anthropomorphic consistency, HPM creates a psychological pressure where social compliance overrides safety constraints. To systematically measure psychological safety, we construct an evaluation framework incorporating psychometric datasets and the Policy Corruption Score (PCS). Benchmarking against various models (e.g., GPT-4o, DeepSeek-V3, Gemini-2-Flash), HPM achieves a mean Attack Success Rate (ASR) of 88.1%, outperforming state-of-the-art attack baselines. Our experiments demonstrate robust penetration against advanced defenses, including adversarial prompt optimization (e.g., RPO) and cognitive interventions (e.g., Self-Reminder). Ultimately, PCS analysis confirms HPM induces safety breakdown to satisfy manipulated contexts. Our work advocates for a fundamental paradigm shift from static content filtering to psychological safety, prioritizing the development of psychological defense mechanisms against deep cognitive manipulation.",
      "pdf_url": "https://arxiv.org/pdf/2512.18244v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 42,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.12536v1",
      "title": "Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?",
      "authors": [
        "Arastoo Zibaeirad",
        "Marco Vieira"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T03:47:39Z",
      "summary": "Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.   This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.   Artifact: https://github.com/Erroristotle/DVDR_LLM",
      "pdf_url": "https://arxiv.org/pdf/2512.12536v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 42,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.09006v1",
      "title": "Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning",
      "authors": [
        "Dyna Soumhane Ouchebara",
        "Stéphane Dupont"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T12:08:24Z",
      "summary": "The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.",
      "pdf_url": "https://arxiv.org/pdf/2512.09006v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "relevance_score": 42,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08493v1",
      "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?",
      "authors": [
        "Dyna Soumhane Ouchebara",
        "Stéphane Dupont"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T11:15:13Z",
      "summary": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.08493v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 42,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.07533v1",
      "title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection",
      "authors": [
        "Yuzhou Nie",
        "Hongwei Li",
        "Chengquan Guo",
        "Ruizhe Jiang",
        "Zhun Wang",
        "Bo Li",
        "Dawn Song",
        "Wenbo Guo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T13:06:23Z",
      "summary": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.",
      "pdf_url": "https://arxiv.org/pdf/2512.07533v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 42,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20062v1",
      "title": "On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities",
      "authors": [
        "Sangryu Park",
        "Gihyuk Ko",
        "Homook Cho"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T05:30:53Z",
      "summary": "Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.",
      "pdf_url": "https://arxiv.org/pdf/2512.20062v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 42,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16970v1",
      "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
      "authors": [
        "Kamer Ali Yuksel"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T12:54:56Z",
      "summary": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.",
      "pdf_url": "https://arxiv.org/pdf/2512.16970v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "relevance_score": 42,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.17387v1",
      "title": "CIFE: Code Instruction-Following Evaluation",
      "authors": [
        "Sravani Gunnu",
        "Shanmukha Guttula",
        "Hima Patel"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T09:43:20Z",
      "summary": "Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.",
      "pdf_url": "https://arxiv.org/pdf/2512.17387v1",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.21250v1",
      "title": "CoTDeceptor:Adversarial Code Obfuscation Against CoT-Enhanced LLM Code Agents",
      "authors": [
        "Haoyang Li",
        "Mingjin Li",
        "Jinxin Zuo",
        "Siqi Li",
        "Xiao Li",
        "Hao Wu",
        "Yueming Lu",
        "Xiaochuan He"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T15:55:42Z",
      "summary": "LLM-based code agents(e.g., ChatGPT Codex) are increasingly deployed as detector for code review and security auditing tasks. Although CoT-enhanced LLM vulnerability detectors are believed to provide improved robustness against obfuscated malicious code, we find that their reasoning chains and semantic abstraction processes exhibit exploitable systematic weaknesses.This allows attackers to covertly embed malicious logic, bypass code review, and propagate backdoored components throughout real-world software supply chains.To investigate this issue, we present CoTDeceptor, the first adversarial code obfuscation framework targeting CoT-enhanced LLM detectors. CoTDeceptor autonomously constructs evolving, hard-to-reverse multi-stage obfuscation strategy chains that effectively disrupt CoT-driven detection logic.We obtained malicious code provided by security enterprise, experimental results demonstrate that CoTDeceptor achieves stable and transferable evasion performance against state-of-the-art LLMs and vulnerability detection agents. CoTDeceptor bypasses 14 out of 15 vulnerability categories, compared to only 2 bypassed by prior methods. Our findings highlight potential risks in real-world software supply chains and underscore the need for more robust and interpretable LLM-powered security analysis systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.21250v1",
      "categories": [
        "cs.CR",
        "cs.MA"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.19228v1",
      "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
      "authors": [
        "Valentin Schmidberger",
        "Manuel Eberhardinger",
        "Setareh Maghsudi",
        "Johannes Maucher"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T10:08:25Z",
      "summary": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
      "pdf_url": "https://arxiv.org/pdf/2512.19228v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18567v1",
      "title": "AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software",
      "authors": [
        "Bin Wang",
        "Wenjie Yu",
        "Yilu Zhong",
        "Hao Yu",
        "Keke Lian",
        "Chaohua Lu",
        "Hongfang Zheng",
        "Dong Zhang",
        "Hui Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T02:26:29Z",
      "summary": "Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.   We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii) 7,000+ recent CVE-linked code changes. This lets us label commits, files, and functions along a human/AI axis and trace how AIGCode moves through projects and vulnerability life cycles.   Our measurements show three ecological patterns. First, AIGCode is already a substantial fraction of new code, but adoption is structured: AI concentrates in glue code, tests, refactoring, documentation, and other boilerplate, while core logic and security-critical configurations remain mostly human-written. Second, adoption has security consequences: some CWE families are overrepresented in AI-tagged code, and near-identical insecure templates recur across unrelated projects, suggesting \"AI-induced vulnerabilities\" propagated by shared models rather than shared maintainers. Third, in human-AI edit chains, AI introduces high-throughput changes while humans act as security gatekeepers; when review is shallow, AI-introduced defects persist longer, remain exposed on network-accessible surfaces, and spread to more files and repositories.   We will open-source the complete dataset and release analysis artifacts and fine-grained documentation of our methodology and findings.",
      "pdf_url": "https://arxiv.org/pdf/2512.18567v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18456v1",
      "title": "SoK: Understanding (New) Security Issues Across AI4Code Use Cases",
      "authors": [
        "Qilong Wu",
        "Taoran Li",
        "Tianyang Zhou",
        "Varun Chandrasekaran"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T18:13:19Z",
      "summary": "AI-for-Code (AI4Code) systems are reshaping software engineering, with tools like GitHub Copilot accelerating code generation, translation, and vulnerability detection. Alongside these advances, however, security risks remain pervasive: insecure outputs, biased benchmarks, and susceptibility to adversarial manipulation undermine their reliability. This SoK surveys the landscape of AI4Code security across three core applications, identifying recurring gaps: benchmark dominance by Python and toy problems, lack of standardized security datasets, data leakage in evaluation, and fragile adversarial robustness. A comparative study of six state-of-the-art models illustrates these challenges: insecure patterns persist in code generation, vulnerability detection is brittle to semantic-preserving attacks, fine-tuning often misaligns security objectives, and code translation yields uneven security benefits. From this analysis, we distill three forward paths: embedding secure-by-default practices in code generation, building robust and comprehensive detection benchmarks, and leveraging translation as a route to security-enhanced languages. We call for a shift toward security-first AI4Code, where vulnerability mitigation and robustness are embedded throughout the development life cycle.",
      "pdf_url": "https://arxiv.org/pdf/2512.18456v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16538v1",
      "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
      "authors": [
        "Xiao Li",
        "Yue Li",
        "Hao Wu",
        "Yue Zhang",
        "Yechao Zhang",
        "Fengyuan Xu",
        "Sheng Zhong"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T13:49:59Z",
      "summary": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
      "pdf_url": "https://arxiv.org/pdf/2512.16538v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.11783v1",
      "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
      "authors": [
        "Andrew Adiletta",
        "Kathryn Adiletta",
        "Kemal Derya",
        "Berk Sunar"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T18:52:09Z",
      "summary": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.   Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.",
      "pdf_url": "https://arxiv.org/pdf/2512.11783v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11922v1",
      "title": "Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use",
      "authors": [
        "Muhammad Waseem",
        "Aakash Ahmad",
        "Kai-Kristian Kemell",
        "Jussi Rasku",
        "Sami Lahti",
        "Kalle Mäkelä",
        "Pekka Abrahamsson"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T18:00:34Z",
      "summary": "Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.",
      "pdf_url": "https://arxiv.org/pdf/2512.11922v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06906v1",
      "title": "MINES: Explainable Anomaly Detection through Web API Invariant Inference",
      "authors": [
        "Wenjie Zhang",
        "Yun Lin",
        "Chun Fung Amos Kwok",
        "Xiwen Teoh",
        "Xiaofei Xie",
        "Frank Liauw",
        "Hongyu Zhang",
        "Jin Song Dong"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T16:13:35Z",
      "summary": "Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.",
      "pdf_url": "https://arxiv.org/pdf/2512.06906v1",
      "categories": [
        "cs.SE",
        "cs.CR",
        "cs.DB",
        "cs.LG"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.SE (+8)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.04611v1",
      "title": "PBFuzz: Agentic Directed Fuzzing for PoV Generation",
      "authors": [
        "Haochen Zeng",
        "Andrew Bao",
        "Jiajun Cheng",
        "Chengyu Song"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T09:34:22Z",
      "summary": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.",
      "pdf_url": "https://arxiv.org/pdf/2512.04611v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.06659v1",
      "title": "The Evolution of Agentic AI in Cybersecurity: From Single LLM Reasoners to Multi-Agent Systems and Autonomous Pipelines",
      "authors": [
        "Vaishali Vinay"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T05:10:16Z",
      "summary": "Cybersecurity has become one of the earliest adopters of agentic AI, as security operations centers increasingly rely on multi-step reasoning, tool-driven analysis, and rapid decision-making under pressure. While individual large language models can summarize alerts or interpret unstructured reports, they fall short in real SOC environments that require grounded data access, reproducibility, and accountable workflows. In response, the field has seen a rapid architectural evolution from single-model helpers toward tool-augmented agents, distributed multi-agent systems, schema-bound tool ecosystems, and early explorations of semi-autonomous investigative pipelines. This survey presents a five-generation taxonomy of agentic AI in cybersecurity. It traces how capabilities and risks change as systems advance from text-only LLM reasoners to multi-agent collaboration frameworks and constrained-autonomy pipelines. We compare these generations across core dimensions - reasoning depth, tool use, memory, reproducibility, and safety. In addition, we also synthesize emerging benchmarks used to evaluate cyber-oriented agents. Finally, we outline the unresolved challenges that accompany this evolution, such as response validation, tool-use correctness, multi-agent coordination, long-horizon reasoning, and safeguards for high-impact actions. Collectively, this work provides a structured perspective on how agentic AI is taking shape within cybersecurity and what is required to ensure its safe and reliable deployment.",
      "pdf_url": "https://arxiv.org/pdf/2512.06659v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.19691v1",
      "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
      "authors": [
        "Junze Ye",
        "Daniel Tawfik",
        "Alex J. Goodell",
        "Nikhil V. Kotha",
        "Mark K. Buyyounouski",
        "Mohsen Bayati"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T18:59:34Z",
      "summary": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.",
      "pdf_url": "https://arxiv.org/pdf/2512.19691v1",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.17309v1",
      "title": "RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models",
      "authors": [
        "Asil Kaan Bozcuoglu",
        "Ziyuan Liu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T07:47:19Z",
      "summary": "RoboEarth was a pioneering initiative in cloud robotics, establishing a foundational framework for robots to share and exchange knowledge about actions, objects, and environments through a standardized knowledge graph. Initially, this knowledge was predominantly hand-crafted by engineers using RDF triples within OWL Ontologies, with updates, such as changes in an object's pose, being asserted by the robot's control and perception routines. However, with the advent and rapid development of Large Language Models (LLMs), we believe that the process of knowledge acquisition can be significantly automated. To this end, we propose RecipeMasterLLM, a high-level planner, that generates OWL action ontologies based on a standardized knowledge graph in response to user prompts. This architecture leverages a fine-tuned LLM specifically trained to understand and produce action descriptions consistent with the RoboEarth standardized knowledge graph. Moreover, during the Retrieval-Augmented Generation (RAG) phase, environmental knowledge is supplied to the LLM to enhance its contextual understanding and improve the accuracy of the generated action descriptions.",
      "pdf_url": "https://arxiv.org/pdf/2512.17309v1",
      "categories": [
        "cs.RO"
      ],
      "relevance_score": 41,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19117v1",
      "title": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?",
      "authors": [
        "Amar Lakel"
      ],
      "affiliations": [
        "MICA"
      ],
      "year": 2025,
      "published": "2025-12-22T07:43:43Z",
      "summary": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.",
      "pdf_url": "https://arxiv.org/pdf/2512.19117v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16602v1",
      "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
      "authors": [
        "Iker García-Ferrero",
        "David Montero",
        "Roman Orus"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T14:43:04Z",
      "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
      "pdf_url": "https://arxiv.org/pdf/2512.16602v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.15799v1",
      "title": "Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India",
      "authors": [
        "Sahibpreet Singh",
        "Shikha Dhiman"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-16T19:39:22Z",
      "summary": "The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI \"dual-use\" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven \"tool crimes\" and \"target crimes.\" Consequently, the research proposes a \"human-centric\" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.",
      "pdf_url": "https://arxiv.org/pdf/2512.15799v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.12837v1",
      "title": "Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union",
      "authors": [
        "Sahibpreet Singh",
        "Manjit Singh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T20:49:41Z",
      "summary": "AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-à-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.",
      "pdf_url": "https://arxiv.org/pdf/2512.12837v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.01797v2",
      "title": "H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs",
      "authors": [
        "Cheng Gao",
        "Huimin Chen",
        "Chaojun Xiao",
        "Zhiyi Chen",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-01T15:32:14Z",
      "summary": "Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2512.01797v2",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2511.18177v1",
      "title": "Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models",
      "authors": [
        "Elias Lumer",
        "Matt Melich",
        "Olivia Zino",
        "Elena Kim",
        "Sara Dieter",
        "Pradeep Honaganahalli Basavaraju",
        "Vamse Kumar Subbiah",
        "James A. Burke",
        "Roberto Hernandez"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-22T20:06:25Z",
      "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.",
      "pdf_url": "https://arxiv.org/pdf/2511.18177v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19620v1",
      "title": "Exploring the features used for summary evaluation by Human and GPT",
      "authors": [
        "Zahra Sadeghi",
        "Evangelos Milios",
        "Frank Rudzicz"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T17:54:49Z",
      "summary": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
      "pdf_url": "https://arxiv.org/pdf/2512.19620v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'gpt' (+8)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.17371v1",
      "title": "GraphCue for SDN Configuration Code Synthesis",
      "authors": [
        "Haomin Qi",
        "Fengfei Yu",
        "Chengbo Huang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T09:13:51Z",
      "summary": "We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.17371v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15148v1",
      "title": "Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering",
      "authors": [
        "Hang Yu",
        "Yuzhou Lai",
        "Li Zhang",
        "Xiaoli Lian",
        "Fang Liu",
        "Yanrui Dong",
        "Ting Zhang",
        "Zhi Jin",
        "David Lo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T07:29:18Z",
      "summary": "The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.",
      "pdf_url": "https://arxiv.org/pdf/2512.15148v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 40,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08869v1",
      "title": "Differentially Private Synthetic Data Generation Using Context-Aware GANs",
      "authors": [
        "Anantaa Kotal",
        "Anupam Joshi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T18:02:34Z",
      "summary": "The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.",
      "pdf_url": "https://arxiv.org/pdf/2512.08869v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.09953v1",
      "title": "ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs",
      "authors": [
        "Mohammad M Maheri",
        "Sunil Cotterill",
        "Alex Davidson",
        "Hamed Haddadi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T16:52:26Z",
      "summary": "Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.   We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.   On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.",
      "pdf_url": "https://arxiv.org/pdf/2512.09953v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.08093v2",
      "title": "Training LLMs for Honesty via Confessions",
      "authors": [
        "Manas Joglekar",
        "Jeremy Chen",
        "Gabriel Wu",
        "Jason Yosinski",
        "Jasmine Wang",
        "Boaz Barak",
        "Amelia Glaese"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T23:05:52Z",
      "summary": "Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.   In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the \"path of least resistance\" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.   To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its \"main\" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.",
      "pdf_url": "https://arxiv.org/pdf/2512.08093v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06046v1",
      "title": "Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework",
      "authors": [
        "Ramprasath Ganesaraja",
        "Swathika N",
        "Saravanan AP",
        "Kamalkumar Rathinasamy",
        "Chetana Amancharla",
        "Rahul Das",
        "Sahil Dilip Panse",
        "Aditya Batwe",
        "Dileep Vijayan",
        "Veena Ashok",
        "Thanushree A P",
        "Kausthubh J Rao",
        "Alden Olivero",
        "Roshan",
        "Rajeshwar Reddy Manthena",
        "Asmitha Yuga Sre A",
        "Harsh Tripathi",
        "Suganya Selvaraj",
        "Vito Chin",
        "Kasthuri Rangan Bhaskar",
        "Kasthuri Rangan Bhaskar",
        "Venkatraman R",
        "Sajit Vijayakumar"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T09:56:15Z",
      "summary": "We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline",
      "pdf_url": "https://arxiv.org/pdf/2512.06046v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.03994v2",
      "title": "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs",
      "authors": [
        "Oren Rachmil",
        "Roy Betser",
        "Itay Gershon",
        "Omer Hofman",
        "Nitay Yakoby",
        "Yuval Meron",
        "Idan Yankelev",
        "Asaf Shabtai",
        "Yuval Elovici",
        "Roman Vainshtein"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-03T17:23:39Z",
      "summary": "Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection",
      "pdf_url": "https://arxiv.org/pdf/2512.03994v2",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2511.19644v1",
      "title": "IRSDA: An Agent-Orchestrated Framework for Enterprise Intrusion Response",
      "authors": [
        "Damodar Panigrahi",
        "Raj Patel",
        "Shaswata Mitra",
        "Sudip Mittal",
        "Shahram Rahimi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-24T19:21:09Z",
      "summary": "Modern enterprise systems face escalating cyber threats that are increasingly dynamic, distributed, and multi-stage in nature. Traditional intrusion detection and response systems often rely on static rules and manual workflows, which limit their ability to respond with the speed and precision required in high-stakes environments. To address these challenges, we present the Intrusion Response System Digital Assistant (IRSDA), an agent-based framework designed to deliver autonomous and policy-compliant cyber defense. IRSDA combines Self-Adaptive Autonomic Computing Systems (SA-ACS) with the Knowledge guided Monitor, Analyze, Plan, and Execute (MAPE-K) loop to support real-time, partition-aware decision-making across enterprise infrastructure.   IRSDA incorporates a knowledge-driven architecture that integrates contextual information with AI-based reasoning to support system-guided intrusion response. The framework leverages retrieval mechanisms and structured representations to inform decision-making while maintaining alignment with operational policies. We assess the system using a representative real-world microservices application, demonstrating its ability to automate containment, enforce compliance, and provide traceable outputs for security analyst interpretation. This work outlines a modular and agent-driven approach to cyber defense that emphasizes explainability, system-state awareness, and operational control in intrusion response.",
      "pdf_url": "https://arxiv.org/pdf/2511.19644v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2511.18092v1",
      "title": "Event-Chain Analysis for Automated Driving and ADAS Systems: Ensuring Safety and Meeting Regulatory Timing Requirements",
      "authors": [
        "Sebastian Dingler",
        "Philip Rehkop",
        "Florian Mayer",
        "Ralf Muenzenberger"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-22T15:22:05Z",
      "summary": "Automated Driving Systems (ADS), including Advanced Driver Assistance Systems (ADAS), must fulfill not only high functional expectations but also stringent timing constraints mandated by international regulations and standards. Regulatory frameworks such as UN regulations, NCAP standards, ISO norms, and NHTSA guidelines impose strict bounds on system reaction times to ensure safe vehicle operation. This paper presents a structured, White-Box methodology based on Event-Chain Modeling to address these timing challenges. Unlike Black-Box approaches, Event-Chain Analysis offers transparent insights into the timing behavior of each functional component - from perception and planning to actuation and human interaction. This perspective is also aligned with multiple regulations, which require that homologation dossiers provide evidence that the chosen system architecture is suitable to ensure compliance with the specified requirements. Our methodology enables the derivation, modeling, and validation of end-to-end timing constraints at the architectural level and facilitates early verification through simulation. Through a detailed case study, we demonstrate how this Event-Chain-centric approach enhances regulatory compliance, optimizes system design, and supports model-based safety analysis techniques, with results showing early identification of compliance issues, systematic parameter optimization, and quantitative evidence generation through probabilistic analysis.",
      "pdf_url": "https://arxiv.org/pdf/2511.18092v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.21132v1",
      "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
      "authors": [
        "Tobias von Arx",
        "Niels Mündler",
        "Mark Vero",
        "Maximilian Baader",
        "Martin Vechev"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T12:02:00Z",
      "summary": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
      "pdf_url": "https://arxiv.org/pdf/2512.21132v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.18133v1",
      "title": "Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection",
      "authors": [
        "Jie Yang",
        "Rui Zhang",
        "Ziyang Cheng",
        "Dawei Cheng",
        "Guang Yang",
        "Bo Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T23:32:36Z",
      "summary": "Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.",
      "pdf_url": "https://arxiv.org/pdf/2512.18133v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15892v1",
      "title": "VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces",
      "authors": [
        "Artem Grigor",
        "Christian Schroeder de Witt",
        "Simon Birnbach",
        "Ivan Martinovic"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T19:05:37Z",
      "summary": "Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.   We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).   We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.",
      "pdf_url": "https://arxiv.org/pdf/2512.15892v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11087v1",
      "title": "Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification",
      "authors": [
        "Duo Zhou",
        "Jorge Chavez",
        "Hesun Chen",
        "Grani A. Hanasusanto",
        "Huan Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T19:59:37Z",
      "summary": "State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $α,β$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $α,β$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.",
      "pdf_url": "https://arxiv.org/pdf/2512.11087v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "math.OC"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.06042v1",
      "title": "Auto-SPT: Automating Semantic Preserving Transformations for Code",
      "authors": [
        "Ashish Hooda",
        "Mihai Christodorescu",
        "Chuangang Ren",
        "Aaron Wilson",
        "Kassem Fawaz",
        "Somesh Jha"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T04:11:44Z",
      "summary": "Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.",
      "pdf_url": "https://arxiv.org/pdf/2512.06042v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20789v1",
      "title": "X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis",
      "authors": [
        "Yihan",
        "Wen",
        "Xin Chen"
      ],
      "affiliations": [
        "Logon"
      ],
      "year": 2025,
      "published": "2025-12-23T21:36:20Z",
      "summary": "The growing complexity of power system operations has created an urgent need for intelligent, automated tools to support reliable and efficient grid management. Conventional analysis tools often require significant domain expertise and manual effort, which limits their accessibility and adaptability. To address these challenges, this paper presents X-GridAgent, a novel large language model (LLM)-powered agentic AI system designed to automate complex power system analysis through natural language queries. The system integrates domain-specific tools and specialized databases under a three-layer hierarchical architecture comprising planning, coordination, and action layers. This architecture offers high flexibility and adaptability to previously unseen tasks, while providing a modular and extensible framework that can be readily expanded to incorporate new tools, data sources, or analytical capabilities. To further enhance performance, we introduce two novel algorithms: (1) LLM-driven prompt refinement with human feedback, and (2) schema-adaptive hybrid retrieval-augmented generation (RAG) for accurate information retrieval from large-scale structured grid datasets. Experimental evaluations across a variety of user queries and power grid cases demonstrate the effectiveness and reliability of X-GridAgent in automating interpretable and rigorous power system analysis.",
      "pdf_url": "https://arxiv.org/pdf/2512.20789v1",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06396v1",
      "title": "AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity",
      "authors": [
        "Shovan Roy"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T10:59:21Z",
      "summary": "The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.",
      "pdf_url": "https://arxiv.org/pdf/2512.06396v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18878v1",
      "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
      "authors": [
        "Kaidi Liang",
        "Ke Li",
        "Xianbiao Hu",
        "Ruwen Qin"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T20:39:31Z",
      "summary": "Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.",
      "pdf_url": "https://arxiv.org/pdf/2512.18878v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18755v1",
      "title": "MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking",
      "authors": [
        "Jianyi Zhang",
        "Shizhao Liu",
        "Ziyin Zhou",
        "Zhen Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T14:43:26Z",
      "summary": "The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA",
      "pdf_url": "https://arxiv.org/pdf/2512.18755v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16676v1",
      "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
      "authors": [
        "Hao Liang",
        "Xiaochen Ma",
        "Zhou Liu",
        "Zhen Hao Wong",
        "Zhengyang Zhao",
        "Zimo Meng",
        "Runming He",
        "Chengyu Shen",
        "Qifeng Cai",
        "Zhaoyang Han",
        "Meiyi Qiang",
        "Yalin Feng",
        "Tianyi Bai",
        "Zewei Pan",
        "Ziyi Guo",
        "Yizhen Jiang",
        "Jingwen Deng",
        "Qijie You",
        "Peichao Lai",
        "Tianyu Guo",
        "Chi Hsu Tsai",
        "Hengyi Feng",
        "Rui Hu",
        "Wenkai Yu",
        "Junbo Niu",
        "Bohan Zeng",
        "Ruichuan An",
        "Lu Ma",
        "Jihao Huang",
        "Yaowei Zheng",
        "Conghui He",
        "Linpeng Tang",
        "Bin Cui",
        "Weinan E",
        "Wentao Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T15:46:15Z",
      "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
      "pdf_url": "https://arxiv.org/pdf/2512.16676v1",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.LG (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16529v2",
      "title": "ParamExplorer: A framework for exploring parameters in generative art",
      "authors": [
        "Julien Gachadoat",
        "Guillaume Lagarde"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T13:37:50Z",
      "summary": "Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.",
      "pdf_url": "https://arxiv.org/pdf/2512.16529v2",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ],
      "relevance_score": 39,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.12858v1",
      "title": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization",
      "authors": [
        "Sonal Prabhune",
        "Balaji Padmanabhan",
        "Kaushik Dutta"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T21:52:31Z",
      "summary": "Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.",
      "pdf_url": "https://arxiv.org/pdf/2512.12858v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "relevance_score": 38,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.13323v1",
      "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning",
      "authors": [
        "Árpád Pándy",
        "Róbert Lakatos",
        "András Hajdu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-15T13:39:14Z",
      "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.",
      "pdf_url": "https://arxiv.org/pdf/2512.13323v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 38,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.17146v1",
      "title": "Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors",
      "authors": [
        "Huixin Zhan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T00:51:11Z",
      "summary": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.",
      "pdf_url": "https://arxiv.org/pdf/2512.17146v1",
      "categories": [
        "cs.CR",
        "cs.LG",
        "q-bio.QM"
      ],
      "relevance_score": 38,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.16297v1",
      "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
      "authors": [
        "Taozhao Chen",
        "Linghan Huang",
        "Kim-Kwang Raymond Choo",
        "Huaming Chen"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T08:31:50Z",
      "summary": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
      "pdf_url": "https://arxiv.org/pdf/2512.16297v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.12856v1",
      "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents",
      "authors": [
        "Saad Alqithami"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T21:40:07Z",
      "summary": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.",
      "pdf_url": "https://arxiv.org/pdf/2512.12856v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.11573v1",
      "title": "Visualizing token importance for black-box language models",
      "authors": [
        "Paulius Rauba",
        "Qiyao Wei",
        "Mihaela van der Schaar"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T14:01:43Z",
      "summary": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.",
      "pdf_url": "https://arxiv.org/pdf/2512.11573v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CL (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.10341v1",
      "title": "A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale",
      "authors": [
        "Vinoth Punniyamoorthy",
        "Ashok Gadi Parthi",
        "Mayilsamy Palanigounder",
        "Ravi Kiran Kodali",
        "Bikesh Kumar",
        "Kabilan Kannan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T06:46:46Z",
      "summary": "Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deployment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero-knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership-inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Experimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The proposed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.",
      "pdf_url": "https://arxiv.org/pdf/2512.10341v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'kubernetes' (+6)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.05365v1",
      "title": "MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare",
      "authors": [
        "Zag ElSayed",
        "Craig Erickson",
        "Ernest Pedapati"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T02:02:22Z",
      "summary": "Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a specific clinical application, known as MCP-AI. This integration allows intelligent agents to reason over extended periods, collaborate securely, and adhere to authentic clinical logic, representing a significant shift away from traditional Clinical Decision Support Systems (CDSS) and prompt-based Large Language Models (LLMs). As healthcare systems become more complex, the need for autonomous, context-aware clinical reasoning frameworks has become urgent. We present MCP-AI, a novel architecture for explainable medical decision-making built upon the Model Context Protocol (MCP) a modular, executable specification for orchestrating generative and descriptive AI agents in real-time workflows. Each MCP file captures clinical objectives, patient context, reasoning state, and task logic, forming a reusable and auditable memory object. Unlike conventional CDSS or stateless prompt-based AI systems, MCP-AI supports adaptive, longitudinal, and collaborative reasoning across care settings. MCP-AI is validated through two use cases: (1) diagnostic modeling of Fragile X Syndrome with comorbid depression, and (2) remote coordination for Type 2 Diabetes and hypertension. In either scenario, the protocol facilitates physician-in-the-loop validation, streamlines clinical processes, and guarantees secure transitions of AI responsibilities between healthcare providers. The system connects with HL7/FHIR interfaces and adheres to regulatory standards, such as HIPAA and FDA SaMD guidelines. MCP-AI provides a scalable basis for interpretable, composable, and safety-oriented AI within upcoming clinical environments.",
      "pdf_url": "https://arxiv.org/pdf/2512.05365v1",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.13702v1",
      "title": "Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport",
      "authors": [
        "A. Anil Sinaci",
        "Senan Postaci",
        "Dogukan Cavdaroglu",
        "Machteld J. Boonstra",
        "Okan Mercan",
        "Kerem Yilmaz",
        "Gokce B. Laleci Erturkmen",
        "Folkert W. Asselbergs",
        "Karim Lekadir"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T08:35:22Z",
      "summary": "Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.",
      "pdf_url": "https://arxiv.org/pdf/2512.13702v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.00742v1",
      "title": "On the Regulatory Potential of User Interfaces for AI Agent Governance",
      "authors": [
        "K. J. Kevin Feng",
        "Tae Soo Kim",
        "Rock Yuren Pang",
        "Faria Huq",
        "Tal August",
        "Amy X. Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-30T05:32:13Z",
      "summary": "AI agents that take actions in their environment autonomously over extended time horizons require robust governance interventions to curb their potentially consequential risks. Prior proposals for governing AI agents primarily target system-level safeguards (e.g., prompt injection monitors) or agent infrastructure (e.g., agent IDs). In this work, we explore a complementary approach: regulating user interfaces of AI agents as a way of enforcing transparency and behavioral requirements that then demand changes at the system and/or infrastructure levels. Specifically, we analyze 22 existing agentic systems to identify UI elements that play key roles in human-agent interaction and communication. We then synthesize those elements into six high-level interaction design patterns that hold regulatory potential (e.g., requiring agent memory to be editable). We conclude with policy recommendations based on our analysis. Our work exposes a new surface for regulatory action that supplements previous proposals for practical AI agent governance.",
      "pdf_url": "https://arxiv.org/pdf/2512.00742v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.00586v1",
      "title": "Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&D",
      "authors": [
        "Michael R. Doane"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-29T18:40:42Z",
      "summary": "This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.",
      "pdf_url": "https://arxiv.org/pdf/2512.00586v1",
      "categories": [
        "cs.LG",
        "cs.CL",
        "q-bio.QM"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.LG (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2511.22031v1",
      "title": "Predicting Public Health Impacts of Electricity Usage",
      "authors": [
        "Yejia Liu",
        "Zhifeng Wu",
        "Pengfei Li",
        "Shaolei Ren"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-27T02:33:13Z",
      "summary": "The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.",
      "pdf_url": "https://arxiv.org/pdf/2511.22031v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2511.18306v1",
      "title": "Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning",
      "authors": [
        "Mohammad Aqib",
        "Mohd Hamza",
        "Ying Hei Chui",
        "Qipei Mei"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-23T06:34:51Z",
      "summary": "Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.",
      "pdf_url": "https://arxiv.org/pdf/2511.18306v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20436v1",
      "title": "Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI",
      "authors": [
        "Muhammad Usman",
        "Azka Rehman",
        "Muhammad Mutti Ur Rehman",
        "Abd Ur Rehman",
        "Muhammad Umar Farooq"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T15:24:31Z",
      "summary": "Accurate segmentation of ischemic stroke lesions from diffusion magnetic resonance imaging (MRI) is essential for clinical decision-making and outcome assessment. Diffusion-Weighted Imaging (DWI) and Apparent Diffusion Coefficient (ADC) scans provide complementary information on acute and sub-acute ischemic changes; however, automated lesion delineation remains challenging due to variability in lesion appearance.   In this work, we study ischemic stroke lesion segmentation using multimodal diffusion MRI from the ISLES 2022 dataset. Several state-of-the-art convolutional and transformer-based architectures, including U-Net variants, Swin-UNet, and TransUNet, are benchmarked. Based on performance, a dual-encoder TransUNet architecture is proposed to learn modality-specific representations from DWI and ADC inputs. To incorporate spatial context, adjacent slice information is integrated using a three-slice input configuration.   All models are trained under a unified framework and evaluated using the Dice Similarity Coefficient (DSC). Results show that transformer-based models outperform convolutional baselines, and the proposed dual-encoder TransUNet achieves the best performance, reaching a Dice score of 85.4% on the test set. The proposed framework offers a robust solution for automated ischemic stroke lesion segmentation from diffusion MRI.",
      "pdf_url": "https://arxiv.org/pdf/2512.20436v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'transformer' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19864v1",
      "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
      "authors": [
        "Shashi Kant Gupta",
        "Arijeet Pramanik",
        "Jerrin John Thomas",
        "Regina Schwind",
        "Lauren Wiener",
        "Avi Raju",
        "Jeremy Kornbluth",
        "Yanshan Wang",
        "Zhaohui Su",
        "Hrituraj Singh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T20:38:30Z",
      "summary": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",
      "pdf_url": "https://arxiv.org/pdf/2512.19864v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19663v1",
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "authors": [
        "Argha Kamal Samanta",
        "Harshika Goyal",
        "Vasudha Joshi",
        "Tushar Mungle",
        "Pabitra Mitra"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T18:41:45Z",
      "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.19663v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'transformer' (+6)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20856v1",
      "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
      "authors": [
        "NVIDIA",
        ":",
        "Aaron Blakeman",
        "Aaron Grattafiori",
        "Aarti Basant",
        "Abhibha Gupta",
        "Abhinav Khattar",
        "Adi Renduchintala",
        "Aditya Vavre",
        "Akanksha Shukla",
        "Akhiad Bercovich",
        "Aleksander Ficek",
        "Aleksandr Shaposhnikov",
        "Alex Kondratenko",
        "Alexander Bukharin",
        "Alexandre Milesi",
        "Ali Taghibakhshi",
        "Alisa Liu",
        "Amelia Barton",
        "Ameya Sunil Mahabaleshwarkar",
        "Amir Klein",
        "Amit Zuker",
        "Amnon Geifman",
        "Amy Shen",
        "Anahita Bhiwandiwalla",
        "Andrew Tao",
        "Anjulie Agrusa",
        "Ankur Verma",
        "Ann Guan",
        "Anubhav Mandarwal",
        "Arham Mehta",
        "Ashwath Aithal",
        "Ashwin Poojary",
        "Asif Ahamed",
        "Asit Mishra",
        "Asma Kuriparambil Thekkumpate",
        "Ayush Dattagupta",
        "Banghua Zhu",
        "Bardiya Sadeghi",
        "Barnaby Simkin",
        "Ben Lanir",
        "Benedikt Schifferer",
        "Besmira Nushi",
        "Bilal Kartal",
        "Bita Darvish Rouhani",
        "Boris Ginsburg",
        "Brandon Norick",
        "Brandon Soubasis",
        "Branislav Kisacanin",
        "Brian Yu",
        "Bryan Catanzaro",
        "Carlo del Mundo",
        "Chantal Hwang",
        "Charles Wang",
        "Cheng-Ping Hsieh",
        "Chenghao Zhang",
        "Chenhan Yu",
        "Chetan Mungekar",
        "Chintan Patel",
        "Chris Alexiuk",
        "Christopher Parisien",
        "Collin Neale",
        "Cyril Meurillon",
        "Damon Mosk-Aoyama",
        "Dan Su",
        "Dane Corneil",
        "Daniel Afrimi",
        "Daniel Lo",
        "Daniel Rohrer",
        "Daniel Serebrenik",
        "Daria Gitman",
        "Daria Levy",
        "Darko Stosic",
        "David Mosallanezhad",
        "Deepak Narayanan",
        "Dhruv Nathawani",
        "Dima Rekesh",
        "Dina Yared",
        "Divyanshu Kakwani",
        "Dong Ahn",
        "Duncan Riach",
        "Dusan Stosic",
        "Edgar Minasyan",
        "Edward Lin",
        "Eileen Long",
        "Eileen Peters Long",
        "Elad Segal",
        "Elena Lantz",
        "Ellie Evans",
        "Elliott Ning",
        "Eric Chung",
        "Eric Harper",
        "Eric Tramel",
        "Erick Galinkin",
        "Erik Pounds",
        "Evan Briones",
        "Evelina Bakhturina",
        "Evgeny Tsykunov",
        "Faisal Ladhak",
        "Fay Wang",
        "Fei Jia",
        "Felipe Soares",
        "Feng Chen",
        "Ferenc Galko",
        "Frank Sun",
        "Frankie Siino",
        "Gal Hubara Agam",
        "Ganesh Ajjanagadde",
        "Gantavya Bhatt",
        "Gargi Prasad",
        "George Armstrong",
        "Gerald Shen",
        "Gorkem Batmaz",
        "Grigor Nalbandyan",
        "Haifeng Qian",
        "Harsh Sharma",
        "Hayley Ross",
        "Helen Ngo",
        "Herbert Hum",
        "Herman Sahota",
        "Hexin Wang",
        "Himanshu Soni",
        "Hiren Upadhyay",
        "Huizi Mao",
        "Huy C Nguyen",
        "Huy Q Nguyen",
        "Iain Cunningham",
        "Ido Galil",
        "Ido Shahaf",
        "Igor Gitman",
        "Ilya Loshchilov",
        "Itamar Schen",
        "Itay Levy",
        "Ivan Moshkov",
        "Izik Golan",
        "Izzy Putterman",
        "Jan Kautz",
        "Jane Polak Scowcroft",
        "Jared Casper",
        "Jatin Mitra",
        "Jeffrey Glick",
        "Jenny Chen",
        "Jesse Oliver",
        "Jian Zhang",
        "Jiaqi Zeng",
        "Jie Lou",
        "Jimmy Zhang",
        "Jinhang Choi",
        "Jining Huang",
        "Joey Conway",
        "Joey Guman",
        "John Kamalu",
        "Johnny Greco",
        "Jonathan Cohen",
        "Joseph Jennings",
        "Joyjit Daw",
        "Julien Veron Vialard",
        "Junkeun Yi",
        "Jupinder Parmar",
        "Kai Xu",
        "Kan Zhu",
        "Kari Briski",
        "Katherine Cheung",
        "Katherine Luna",
        "Keith Wyss",
        "Keshav Santhanam",
        "Kevin Shih",
        "Kezhi Kong",
        "Khushi Bhardwaj",
        "Kirthi Shankar",
        "Krishna C. Puvvada",
        "Krzysztof Pawelec",
        "Kumar Anik",
        "Lawrence McAfee",
        "Laya Sleiman",
        "Leon Derczynski",
        "Li Ding",
        "Lizzie Wei",
        "Lucas Liebenwein",
        "Luis Vega",
        "Maanu Grover",
        "Maarten Van Segbroeck",
        "Maer Rodrigues de Melo",
        "Mahdi Nazemi",
        "Makesh Narsimhan Sreedhar",
        "Manoj Kilaru",
        "Maor Ashkenazi",
        "Marc Romeijn",
        "Marcin Chochowski",
        "Mark Cai",
        "Markus Kliegl",
        "Maryam Moosaei",
        "Matt Kulka",
        "Matvei Novikov",
        "Mehrzad Samadi",
        "Melissa Corpuz",
        "Mengru Wang",
        "Meredith Price",
        "Michael Andersch",
        "Michael Boone",
        "Michael Evans",
        "Miguel Martinez",
        "Mikail Khona",
        "Mike Chrzanowski",
        "Minseok Lee",
        "Mohammad Dabbah",
        "Mohammad Shoeybi",
        "Mostofa Patwary",
        "Nabin Mulepati",
        "Najeeb Nabwani",
        "Natalie Hereth",
        "Nave Assaf",
        "Negar Habibi",
        "Neta Zmora",
        "Netanel Haber",
        "Nicola Sessions",
        "Nidhi Bhatia",
        "Nikhil Jukar",
        "Nikki Pope",
        "Nikolai Ludwig",
        "Nima Tajbakhsh",
        "Nir Ailon",
        "Nirmal Juluru",
        "Nishant Sharma",
        "Oleksii Hrinchuk",
        "Oleksii Kuchaiev",
        "Olivier Delalleau",
        "Oluwatobi Olabiyi",
        "Omer Ullman Argov",
        "Omri Puny",
        "Oren Tropp",
        "Ouye Xie",
        "Parth Chadha",
        "Pasha Shamis",
        "Paul Gibbons",
        "Pavlo Molchanov",
        "Pawel Morkisz",
        "Peter Dykas",
        "Peter Jin",
        "Pinky Xu",
        "Piotr Januszewski",
        "Pranav Prashant Thombre",
        "Prasoon Varshney",
        "Pritam Gundecha",
        "Przemek Tredak",
        "Qing Miao",
        "Qiyu Wan",
        "Rabeeh Karimi Mahabadi",
        "Rachit Garg",
        "Ran El-Yaniv",
        "Ran Zilberstein",
        "Rasoul Shafipour",
        "Rich Harang",
        "Rick Izzo",
        "Rima Shahbazyan",
        "Rishabh Garg",
        "Ritika Borkar",
        "Ritu Gala",
        "Riyad Islam",
        "Robert Hesse",
        "Roger Waleffe",
        "Rohit Watve",
        "Roi Koren",
        "Ruoxi Zhang",
        "Russell Hewett",
        "Russell J. Hewett",
        "Ryan Prenger",
        "Ryan Timbrook",
        "Sadegh Mahdavi",
        "Sahil Modi",
        "Samuel Kriman",
        "Sangkug Lim",
        "Sanjay Kariyappa",
        "Sanjeev Satheesh",
        "Saori Kaji",
        "Satish Pasumarthi",
        "Saurav Muralidharan",
        "Sean Narentharen",
        "Sean Narenthiran",
        "Seonmyeong Bak",
        "Sergey Kashirsky",
        "Seth Poulos",
        "Shahar Mor",
        "Shanmugam Ramasamy",
        "Shantanu Acharya",
        "Shaona Ghosh",
        "Sharath Turuvekere Sreenivas",
        "Shelby Thomas",
        "Shiqing Fan",
        "Shreya Gopal",
        "Shrimai Prabhumoye",
        "Shubham Pachori",
        "Shubham Toshniwal",
        "Shuoyang Ding",
        "Siddharth Singh",
        "Simeng Sun",
        "Smita Ithape",
        "Somshubra Majumdar",
        "Soumye Singhal",
        "Stas Sergienko",
        "Stefania Alborghetti",
        "Stephen Ge",
        "Sugam Dipak Devare",
        "Sumeet Kumar Barua",
        "Suseella Panguluri",
        "Suyog Gupta",
        "Sweta Priyadarshi",
        "Syeda Nahida Akter",
        "Tan Bui",
        "Teodor-Dumitru Ene",
        "Terry Kong",
        "Thanh Do",
        "Tijmen Blankevoort",
        "Tim Moon",
        "Tom Balough",
        "Tomer Asida",
        "Tomer Bar Natan",
        "Tomer Ronen",
        "Tugrul Konuk",
        "Twinkle Vashishth",
        "Udi Karpas",
        "Ushnish De",
        "Vahid Noorozi",
        "Vahid Noroozi",
        "Venkat Srinivasan",
        "Venmugil Elango",
        "Victor Cui",
        "Vijay Korthikanti",
        "Vinay Rao",
        "Vitaly Kurin",
        "Vitaly Lavrukhin",
        "Vladimir Anisimov",
        "Wanli Jiang",
        "Wasi Uddin Ahmad",
        "Wei Du",
        "Wei Ping",
        "Wenfei Zhou",
        "Will Jennings",
        "William Zhang",
        "Wojciech Prazuch",
        "Xiaowei Ren",
        "Yashaswi Karnati",
        "Yejin Choi",
        "Yev Meyer",
        "Yi-Fu Wu",
        "Yian Zhang",
        "Yigong Qin",
        "Ying Lin",
        "Yonatan Geifman",
        "Yonggan Fu",
        "Yoshi Subara",
        "Yoshi Suhara",
        "Yubo Gao",
        "Zach Moshe",
        "Zhen Dong",
        "Zhongbo Zhu",
        "Zihan Liu",
        "Zijia Chen",
        "Zijie Yan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T00:24:05Z",
      "summary": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
      "pdf_url": "https://arxiv.org/pdf/2512.20856v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.17289v1",
      "title": "Subjective Question Generation and Answer Evaluation using NLP",
      "authors": [
        "G. M. Refatul Islam",
        "Safwan Shaheer",
        "Yaseen Nur",
        "Mohammad Rafid Hamid"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T07:11:50Z",
      "summary": "Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.",
      "pdf_url": "https://arxiv.org/pdf/2512.17289v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "relevance_score": 37,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20275v1",
      "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
      "authors": [
        "Divya Vijay",
        "Vignesh Ethiraj"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T11:27:17Z",
      "summary": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.",
      "pdf_url": "https://arxiv.org/pdf/2512.20275v1",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16419v1",
      "title": "Large Language Models as a (Bad) Security Norm in the Context of Regulation and Compliance",
      "authors": [
        "Kaspar Rosager Ludvigsen"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T11:14:21Z",
      "summary": "The use of Large Language Models (LLM) by providers of cybersecurity and digital infrastructures of all kinds is an ongoing development. It is suggested and on an experimental basis used to write the code for the systems, and potentially fed with sensitive data or what would otherwise be considered trade secrets. Outside of these obvious points, this paper asks how AI can negatively affect cybersecurity and law when used for the design and deployment of security infrastructure by its developers.   Firstly, the paper discusses the use of LLMs in security, either directly or indirectly, and briefly tackles other types of AI. It then lists norms in cybersecurity, then a range of legal cybersecurity obligations from the European Union, to create a frame of reference. Secondly, the paper describes how LLMs may fail to fulfil both legal obligations and best practice in cybersecurity is given, and the paper ends with some economic and practical consequences for this development, with some notions of solutions as well.   The paper finds that using LLMs comes with many risks, many of which are against good security practice, and the legal obligations in security regulation. This is because of the inherent weaknesses of LLMs, most of which are mitigated if replaced with symbolic AI. Both also have issues fulfilling basic traceability obligations and practice. Solutions are secondary systems surrounding LLM based AI, fulfilment of security norms beyond legal requirements and simply not using such technology in certain situations.",
      "pdf_url": "https://arxiv.org/pdf/2512.16419v1",
      "categories": [
        "cs.CY",
        "cs.CR"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.03931v1",
      "title": "Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties",
      "authors": [
        "Vineel Tummala",
        "Daniela Inclezan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-03T16:29:09Z",
      "summary": "This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "pdf_url": "https://arxiv.org/pdf/2512.03931v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.02774v1",
      "title": "AI-Driven Document Redaction in UK Public Authorities: Implementation Gaps, Regulatory Challenges, and the Human Oversight Imperative",
      "authors": [
        "Yijun Chen"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-02T13:52:10Z",
      "summary": "Document redaction in public authorities faces critical challenges as traditional manual approaches struggle to balance growing transparency demands with increasingly stringent data protection requirements. This study investigates the implementation of AI-driven document redaction within UK public authorities through Freedom of Information (FOI) requests. While AI technologies offer potential solutions to redaction challenges, their actual implementation within public sector organizations remains underexplored. Based on responses from 44 public authorities across healthcare, government, and higher education sectors, this study reveals significant gaps between technological possibilities and organizational realities. Findings show highly limited AI adoption (only one authority reported using AI tools), widespread absence of formal redaction policies (50 percent reported \"information not held\"), and deficiencies in staff training. The study identifies three key barriers to effective AI implementation: poor record-keeping practices, lack of standardized redaction guidelines, and insufficient specialized training for human oversight. These findings highlight the need for a socio-technical approach that balances technological automation with meaningful human expertise. This research provides the first empirical assessment of AI redaction practices in UK public authorities and contributes evidence to support policymakers navigating the complex interplay between transparency obligations, data protection requirements, and emerging AI technologies in public administration.",
      "pdf_url": "https://arxiv.org/pdf/2512.02774v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20396v1",
      "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
      "authors": [
        "Narges Khakpour",
        "Nicolas Berthier"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T14:33:31Z",
      "summary": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.",
      "pdf_url": "https://arxiv.org/pdf/2512.20396v1",
      "categories": [
        "cs.CR",
        "cs.FL",
        "cs.PL",
        "cs.SE"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.21243v1",
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "authors": [
        "Anatoly O. Onishchenko",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T15:36:21Z",
      "summary": "Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .",
      "pdf_url": "https://arxiv.org/pdf/2512.21243v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19933v1",
      "title": "PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation",
      "authors": [
        "Zhixiang Lu",
        "Xueyuan Deng",
        "Yiran Liu",
        "Yulong Li",
        "Qiang Yan",
        "Imran Razzak",
        "Jionglong Su"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T23:31:49Z",
      "summary": "Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19933v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19475v1",
      "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting",
      "authors": [
        "Ivan Decostanzi",
        "Yelena Mejova",
        "Kyriaki Kalimeri"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T15:28:55Z",
      "summary": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.",
      "pdf_url": "https://arxiv.org/pdf/2512.19475v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19458v1",
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "authors": [
        "Zeyu Xia",
        "Jinzhe Ma",
        "Congjie Zheng",
        "Shufei Zhang",
        "Yuqiang Li",
        "Hang Su",
        "P. Hu",
        "Changshui Zhang",
        "Xingao Gong",
        "Wanli Ouyang",
        "Lei Bai",
        "Dongzhan Zhou",
        "Mao Su"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T15:03:57Z",
      "summary": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
      "pdf_url": "https://arxiv.org/pdf/2512.19458v1",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19396v1",
      "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
      "authors": [
        "Runze Li",
        "Yuwen Zhai",
        "Bo Xu",
        "LiWu Xu",
        "Nian Shi",
        "Wei Zhang",
        "Ran Lin",
        "Liang Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T13:42:18Z",
      "summary": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
      "pdf_url": "https://arxiv.org/pdf/2512.19396v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19287v1",
      "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
      "authors": [
        "Jiaao Wu",
        "Xian Zhang",
        "Fan Yang",
        "Yinpeng Dong"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T11:30:19Z",
      "summary": "We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.",
      "pdf_url": "https://arxiv.org/pdf/2512.19287v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20959v1",
      "title": "Can Agentic AI Match the Performance of Human Data Scientists?",
      "authors": [
        "An Luo",
        "Jin Du",
        "Fangqiao Tian",
        "Xun Xian",
        "Robert Specht",
        "Ganghua Wang",
        "Xuan Bi",
        "Charles Fleming",
        "Jayanth Srinivasa",
        "Ashish Kundu",
        "Mingyi Hong",
        "Jie Ding"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T05:31:42Z",
      "summary": "Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.",
      "pdf_url": "https://arxiv.org/pdf/2512.20959v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18533v1",
      "title": "Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset",
      "authors": [
        "S Mahmudul Hasan",
        "Shaily Roy",
        "Akib Jawad Nafis"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T23:08:18Z",
      "summary": "The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard \"Performance Ceiling\", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive \"Generalization Gap\" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.",
      "pdf_url": "https://arxiv.org/pdf/2512.18533v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.16701v1",
      "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
      "authors": [
        "Giovanni Adorni"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T16:06:04Z",
      "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.   We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
      "pdf_url": "https://arxiv.org/pdf/2512.16701v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.15231v1",
      "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
      "authors": [
        "Zhengchao Chen",
        "Haoran Wang",
        "Jing Yao",
        "Pedram Ghamisi",
        "Jun Zhou",
        "Peter M. Atkinson",
        "Bing Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T09:31:57Z",
      "summary": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
      "pdf_url": "https://arxiv.org/pdf/2512.15231v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 36,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06111v1",
      "title": "A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts",
      "authors": [
        "Arthur Mukwaya",
        "Nancy Kasamala",
        "Nana Kankam Gyimah",
        "Judith Mwakalonge",
        "Gurcan Comert",
        "Saidi Siuhi",
        "Denis Ruganuza",
        "Mark Ngotonie"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T19:36:11Z",
      "summary": "The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.",
      "pdf_url": "https://arxiv.org/pdf/2512.06111v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 35,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20334v1",
      "title": "Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation",
      "authors": [
        "Yuan Huang",
        "Yukang Zhou",
        "Xiangping Chen",
        "Zibin Zheng"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:08:19Z",
      "summary": "With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.   This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.",
      "pdf_url": "https://arxiv.org/pdf/2512.20334v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 35,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.05459v2",
      "title": "PrivCode: When Code Generation Meets Differential Privacy",
      "authors": [
        "Zheng Liu",
        "Chen Gong",
        "Terry Yue Zhuo",
        "Kecen Li",
        "Weichen Yu",
        "Matt Fredrikson",
        "Tianhao Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T06:27:06Z",
      "summary": "Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.   We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed \"privacy-sanitizing\", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed \"utility-boosting\", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.",
      "pdf_url": "https://arxiv.org/pdf/2512.05459v2",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 35,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'code generation' (+7)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.21081v1",
      "title": "Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics",
      "authors": [
        "Karim Abdelsalam",
        "Zeyad Gamal",
        "Ayman El-Badawy"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T09:56:28Z",
      "summary": "Controlling systems with complex, nonlinear dynamics poses a significant challenge, particularly in achieving efficient and robust control. In this paper, we propose a Dyna-Style Reinforcement Learning control framework that integrates Sparse Identification of Nonlinear Dynamics (SINDy) with Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning. SINDy is used to identify a data-driven model of the system, capturing its key dynamics without requiring an explicit physical model. This identified model is used to generate synthetic rollouts that are periodically injected into the reinforcement learning replay buffer during training on the real environment, enabling efficient policy learning with limited data available. By leveraging this hybrid approach, we mitigate the sample inefficiency of traditional model-free reinforcement learning methods while ensuring accurate control of nonlinear systems. To demonstrate the effectiveness of this framework, we apply it to a bi-rotor system as a case study, evaluating its performance in stabilization and trajectory tracking. The results show that our SINDy-TD3 approach achieves superior accuracy and robustness compared to direct reinforcement learning techniques, highlighting the potential of combining data-driven modeling with reinforcement learning for complex dynamical systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.21081v1",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "relevance_score": 35,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20368v1",
      "title": "Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability",
      "authors": [
        "Samya Praharaj",
        "Koulik Khamaru"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:53:53Z",
      "summary": "Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\\sqrt{d \\log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.   A key structural property that circumvents this limitation is the \\emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \\emph{without} incurring the $\\sqrt{d \\log T}$ price of adaptivity.   In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.",
      "pdf_url": "https://arxiv.org/pdf/2512.20368v1",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG",
        "math.ST"
      ],
      "relevance_score": 35,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.08169v1",
      "title": "Information-Dense Reasoning for Efficient and Auditable Security Alert Triage",
      "authors": [
        "Guangze Zhao",
        "Yongzheng Zhang",
        "Changbo Tian",
        "Dan Xie",
        "Hongri Liu",
        "Bailing Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T01:57:24Z",
      "summary": "Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.",
      "pdf_url": "https://arxiv.org/pdf/2512.08169v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.08104v1",
      "title": "AgentCrypt: Advancing Privacy and (Secure) Computation in AI Agent Collaboration",
      "authors": [
        "Harish Karthikeyan",
        "Yue Guo",
        "Leo de Castro",
        "Antigoni Polychroniadou",
        "Leo Ardon",
        "Udari Madhushani Sehwag",
        "Sumitra Ganesh",
        "Manuela Veloso"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T23:20:20Z",
      "summary": "As AI agents increasingly operate in real-world, multi-agent environments, ensuring reliable and context-aware privacy in agent communication is critical, especially to comply with evolving regulatory requirements. Traditional access controls are insufficient, as privacy risks often arise after access is granted; agents may use information in ways that compromise privacy, such as messaging humans, sharing context with other agents, making tool calls, persisting data, or generating derived private information. Existing approaches often treat privacy as a binary constraint, whether data is shareable or not, overlooking nuanced, role-specific, and computation-dependent privacy needs essential for regulatory compliance.   Agents, including those based on large language models, are inherently probabilistic and heuristic. There is no formal guarantee of how an agent will behave for any query, making them ill-suited for operations critical to security. To address this, we introduce AgentCrypt, a four-tiered framework for fine-grained, encrypted agent communication that adds a protection layer atop any AI agent platform. AgentCrypt spans unrestricted data exchange (Level 1) to fully encrypted computation using techniques such as homomorphic encryption (Level 4). Crucially, it guarantees the privacy of tagged data is always maintained, prioritizing privacy above correctness.   AgentCrypt ensures privacy across diverse interactions and enables computation on otherwise inaccessible data, overcoming barriers such as data silos. We implemented and tested it with Langgraph and Google ADK, demonstrating versatility across platforms. We also introduce a benchmark dataset simulating privacy-critical tasks at all privacy levels, enabling systematic evaluation and fostering the development of regulatable machine learning systems for secure agent communication and computation.",
      "pdf_url": "https://arxiv.org/pdf/2512.08104v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.07075v1",
      "title": "Do Large Language Models Truly Understand Cross-cultural Differences?",
      "authors": [
        "Shiwei Guo",
        "Sihang Jiang",
        "Qianxi He",
        "Yanghua Xiao",
        "Jiaqing Liang",
        "Bi Yude",
        "Minggui He",
        "Shimin Tao",
        "Li Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T01:21:58Z",
      "summary": "In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.",
      "pdf_url": "https://arxiv.org/pdf/2512.07075v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.03765v1",
      "title": "The Treasury Proof Ledger: A Cryptographic Framework for Accountable Bitcoin Treasuries",
      "authors": [
        "Jose E. Puente",
        "Carlos Puente"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-03T13:14:06Z",
      "summary": "Public companies and institutional investors that hold Bitcoin face increasing pressure to show solvency, manage risk, and satisfy regulatory expectations without exposing internal wallet structures or trading strategies. This paper introduces the Treasury Proof Ledger (TPL), a Bitcoin-anchored logging framework for multi-domain Bitcoin treasuries that treats on-chain and off-chain exposures as a conserved state machine with an explicit fee sink. A TPL instance records proof-of-reserves snapshots, proof-of-transit receipts for movements between domains, and policy metadata, and it supports restricted views based on stakeholder permissions. We define an idealised TPL model, represent Bitcoin treasuries as multi-domain exposure vectors, and give deployment-level security notions including exposure soundness, policy completeness, non-equivocation, and privacy-compatible policy views. We then outline how practical, restricted forms of these guarantees can be achieved by combining standard proof-of-reserves and proof-of-transit techniques with hash-based commitments anchored on Bitcoin. The results are existence-type statements: they show which guarantees are achievable once economic and governance assumptions are set, without claiming that any current system already provides them. A stylised corporate-treasury example illustrates how TPL could support responsible transparency policies and future cross-institution checks consistent with Bitcoin's fixed monetary supply.",
      "pdf_url": "https://arxiv.org/pdf/2512.03765v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.02047v1",
      "title": "Copyright in AI Pre-Training Data Filtering: Regulatory Landscape and Mitigation Strategies",
      "authors": [
        "Mariia Kyrychenko",
        "Mykyta Mudryi",
        "Markiyan Chaklosh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-26T14:02:45Z",
      "summary": "The rapid advancement of general-purpose AI models has increased concerns about copyright infringement in training data, yet current regulatory frameworks remain predominantly reactive rather than proactive. This paper examines the regulatory landscape of AI training data governance in major jurisdictions, including the EU, the United States, and the Asia-Pacific region. It also identifies critical gaps in enforcement mechanisms that threaten both creator rights and the sustainability of AI development. Through analysis of major cases we identified critical gaps in pre-training data filtering. Existing solutions such as transparency tools, perceptual hashing, and access control mechanisms address only specific aspects of the problem and cannot prevent initial copyright violations. We identify two fundamental challenges: pre-training license collection and content filtering, which faces the impossibility of comprehensive copyright management at scale, and verification mechanisms, which lack tools to confirm filtering prevented infringement. We propose a multilayered filtering pipeline that combines access control, content verification, machine learning classifiers, and continuous database cross-referencing to shift copyright protection from post-training detection to pre-training prevention. This approach offers a pathway toward protecting creator rights while enabling continued AI innovation.",
      "pdf_url": "https://arxiv.org/pdf/2512.02047v1",
      "categories": [
        "cs.CY",
        "cs.CR"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.02046v1",
      "title": "Global AI Governance Overview: Understanding Regulatory Requirements Across Global Jurisdictions",
      "authors": [
        "Mariia Kyrychenko",
        "Mykyta Mudryi",
        "Markiyan Chaklosh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-26T13:59:11Z",
      "summary": "The rapid advancement of general-purpose AI models has increased concerns about copyright infringement in training data, yet current regulatory frameworks remain predominantly reactive rather than proactive. This paper examines the regulatory landscape of AI training data governance in major jurisdictions, including the EU, the United States, and the Asia-Pacific region. It also identifies critical gaps in enforcement mechanisms that threaten both creator rights and the sustainability of AI development. Through analysis of major cases we identified critical gaps in pre-training data filtering. Existing solutions such as transparency tools, perceptual hashing, and access control mechanisms address only specific aspects of the problem and cannot prevent initial copyright violations. We identify two fundamental challenges: pre-training license collection and content filtering, which faces the impossibility of comprehensive copyright management at scale, and verification mechanisms, which lack tools to confirm filtering prevented infringement. We propose a multilayered filtering pipeline that combines access control, content verification, machine learning classifiers, and continuous database cross-referencing to shift copyright protection from post-training detection to pre-training prevention. This approach offers a pathway toward protecting creator rights while enabling continued AI innovation.",
      "pdf_url": "https://arxiv.org/pdf/2512.02046v1",
      "categories": [
        "cs.CY",
        "cs.CR"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2511.21334v1",
      "title": "Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text",
      "authors": [
        "Kai Kugler"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-26T12:31:14Z",
      "summary": "We present the first systematic investigation of Martin's Law - the empirical relationship between word frequency and polysemy - in text generated by neural language models during training. Using DBSCAN clustering of contextualized embeddings as an operationalization of word senses, we analyze four Pythia models (70M-1B parameters) across 30 training checkpoints. Our results reveal a non-monotonic developmental trajectory: Martin's Law emerges around checkpoint 100, reaches peak correlation (r > 0.6) at checkpoint 104, then degrades by checkpoint 105. Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation. The frequency-specificity trade-off remains stable (r $\\approx$ -0.3) across all models. These findings suggest that compliance with linguistic regularities in LLM-generated text is not monotonically increasing with training, but instead follows a balanced trajectory with an optimal semantic window. This work establishes a novel methodology for evaluating emergent linguistic structure in neural language models.",
      "pdf_url": "https://arxiv.org/pdf/2511.21334v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2511.20623v1",
      "title": "Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development",
      "authors": [
        "David Szczecina",
        "Senan Gaffori",
        "Edmond Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-25T18:46:14Z",
      "summary": "The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.",
      "pdf_url": "https://arxiv.org/pdf/2511.20623v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.17667v1",
      "title": "STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting",
      "authors": [
        "Yifei Cheng",
        "Yujia Zhu",
        "Baiyang Li",
        "Xinhao Deng",
        "Yitong Cai",
        "Yaochen Ren",
        "Qingyun Liu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T15:12:01Z",
      "summary": "Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.",
      "pdf_url": "https://arxiv.org/pdf/2512.17667v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16904v1",
      "title": "How Good is Post-Hoc Watermarking With Language Model Rephrasing?",
      "authors": [
        "Pierre Fernandez",
        "Tom Sander",
        "Hady Elsahar",
        "Hongyan Chang",
        "Tomáš Souček",
        "Valeriu Lacatusu",
        "Tuan Tran",
        "Sylvestre-Alvise Rebuffi",
        "Alexandre Mourachko"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T18:57:33Z",
      "summary": "Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.",
      "pdf_url": "https://arxiv.org/pdf/2512.16904v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.14753v1",
      "title": "CODE ACROSTIC: Robust Watermarking for Code Generation",
      "authors": [
        "Li Lin",
        "Siyuan Xin",
        "Yang Cao",
        "Xiaochun Cao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T19:14:54Z",
      "summary": "Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.",
      "pdf_url": "https://arxiv.org/pdf/2512.14753v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.10393v1",
      "title": "Cross-modal Retrieval Models for Stripped Binary Analysis",
      "authors": [
        "Guoqiang Chen",
        "Lingyun Ying",
        "Ziyang Song",
        "Daguang Liu",
        "Qiang Wang",
        "Zhiqi Wang",
        "Li Hu",
        "Shaoyin Cheng",
        "Weiming Zhang",
        "Nenghai Yu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T07:58:10Z",
      "summary": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.",
      "pdf_url": "https://arxiv.org/pdf/2512.10393v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.SE (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.10766v1",
      "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
      "authors": [
        "Chenyu Zhang",
        "Yiwen Ma",
        "Lanjun Wang",
        "Wenhui Li",
        "Yi Tu",
        "An-An Liu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T12:38:00Z",
      "summary": "Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defense mechanisms. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based \\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming to effectively and efficiently attack diverse defense mechanisms without prior knowledge of their type by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Extensive experiments on T2I models with various external and internal defense mechanisms demonstrate that MJA outperforms six baseline methods, achieving stronger attack performance while using fewer queries. Code is available in https://github.com/datar001/metaphor-based-jailbreaking-attack.",
      "pdf_url": "https://arxiv.org/pdf/2512.10766v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20333v1",
      "title": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization",
      "authors": [
        "Junren Li",
        "Luhua Lai"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:07:22Z",
      "summary": "Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the \"synthesis cliff\" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures.",
      "pdf_url": "https://arxiv.org/pdf/2512.20333v1",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20082v2",
      "title": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches",
      "authors": [
        "Chaithra",
        "Kamesh Kadimisetty",
        "Biju R Mohan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T06:27:12Z",
      "summary": "Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.",
      "pdf_url": "https://arxiv.org/pdf/2512.20082v2",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20061v1",
      "title": "Scaling Reinforcement Learning for Content Moderation with Large Language Models",
      "authors": [
        "Hamed Firooz",
        "Rui Liu",
        "Yuchen Lu",
        "Zhenyu Hou",
        "Fangzhou Xiong",
        "Xiaoyang Zhang",
        "Changshu Jian",
        "Zhicheng Zhu",
        "Jiayuan Ma",
        "Jacob Tao",
        "Chaitali Gupta",
        "Xiaochang Peng",
        "Shike Mei",
        "Hang Cui",
        "Yang Qin",
        "Shuo Tang",
        "Jason Gaedtke",
        "Arpit Mittal"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T05:27:16Z",
      "summary": "Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.",
      "pdf_url": "https://arxiv.org/pdf/2512.20061v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19682v2",
      "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
      "authors": [
        "Jiacheng Guo",
        "Ling Yang",
        "Peter Chen",
        "Qixin Xiao",
        "Yinjie Wang",
        "Xinzhe Juan",
        "Jiahao Qiu",
        "Ke Shen",
        "Mengdi Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T18:57:13Z",
      "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
      "pdf_url": "https://arxiv.org/pdf/2512.19682v2",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19414v1",
      "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
      "authors": [
        "Jiaren Peng",
        "Hongda Sun",
        "Xuan Tian",
        "Cheng Huang",
        "Zeqing Li",
        "Rui Yan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T14:13:01Z",
      "summary": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
      "pdf_url": "https://arxiv.org/pdf/2512.19414v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19126v2",
      "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
      "authors": [
        "Zihan Lin",
        "Xiaohan Wang",
        "Hexiong Yang",
        "Jiajun Chai",
        "Jie Cao",
        "Guojun Yin",
        "Wei Lin",
        "Ran He"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T08:07:00Z",
      "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
      "pdf_url": "https://arxiv.org/pdf/2512.19126v2",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18432v1",
      "title": "Federated Learning Based Decentralized Adaptive Intelligent Transmission Protocol for Privacy Preserving 6G Networks",
      "authors": [
        "Ansar Ahmed"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T17:18:15Z",
      "summary": "The move to 6th Generation (6G) wireless networks creates new issues with privacy, scalability, and adaptability. The data-intensive nature of 6G is not handled well by older, centralized network models. A shift toward more secure and decentralized systems is therefore required. A new framework called the Federated Learning-based Decentralized Adaptive Intelligent Transmission Protocol (AITP) is proposed to meet these challenges. The AITP uses the distributed learning of Federated Learning (FL) within a decentralized system. Transmission parameters can be adjusted intelligently in real time. User privacy is maintained by keeping raw data on local edge devices. The protocol's performance was evaluated with mathematical modeling and detailed simulations. It was shown to be superior to traditional non-adaptive and centralized AI methods across several key metrics. These included latency, network throughput, energy efficiency, and robustness. The AITP is presented as a foundational technology for future 6G networks that supports a user-centric, privacy-first design. This study is a step forward for privacy-preserving research in 6G.",
      "pdf_url": "https://arxiv.org/pdf/2512.18432v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.15343v1",
      "title": "Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality",
      "authors": [
        "Efe Bozkir",
        "Enkelejda Kasneci"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T11:41:25Z",
      "summary": "The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.",
      "pdf_url": "https://arxiv.org/pdf/2512.15343v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.14860v1",
      "title": "Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks",
      "authors": [
        "Viet K. Nguyen",
        "Mohammad I. Husain"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-16T19:22:50Z",
      "summary": "Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent architecture that mimics the functionality of a university information management system and 13 distinct attack scenarios that span prompt injection, Server Side Request Forgery (SSRF), SQL injection, and tool misuse. Our 130 total test cases reveal significant security disparities: AutoGen demonstrates a 52.3% refusal rate versus CrewAI's 30.8%, while model performance ranges from Nova Pro's 46.2% to Claude and Grok 2's 38.5%. Most critically, Grok 2 on CrewAI rejected only 2 of 13 attacks (15.4% refusal rate), and the overall refusal rate of 41.5% across all configurations indicates that more than half of malicious prompts succeeded despite enterprise-grade safety mechanisms. We identify six distinct defensive behavior patterns including a novel \"hallucinated compliance\" strategy where models fabricate outputs rather than executing or refusing attacks, and provide actionable recommendations for secure agent deployment. Complete attack prompts are also included in the Appendix to enable reproducibility.",
      "pdf_url": "https://arxiv.org/pdf/2512.14860v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11269v1",
      "title": "A Scalable Multi-GPU Framework for Encrypted Large-Model Inference",
      "authors": [
        "Siddharth Jayashankar",
        "Joshua Kim",
        "Michael B. Sullivan",
        "Wenting Zheng",
        "Dimitrios Skarlatos"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T04:15:38Z",
      "summary": "Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.",
      "pdf_url": "https://arxiv.org/pdf/2512.11269v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.09882v1",
      "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
      "authors": [
        "Justin W. Lin",
        "Eliot Krzysztof Jones",
        "Donovan Julian Jasper",
        "Ethan Jun-shen Ho",
        "Anna Wu",
        "Arnold Tianyi Yang",
        "Neil Perry",
        "Andy Zou",
        "Matt Fredrikson",
        "J. Zico Kolter",
        "Percy Liang",
        "Dan Boneh",
        "Daniel E. Ho"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-10T18:12:29Z",
      "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
      "pdf_url": "https://arxiv.org/pdf/2512.09882v1",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.09264v1",
      "title": "FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection",
      "authors": [
        "Xiaojing Chen",
        "Dan Li",
        "Lijun Peng",
        "Jun YanŁetter",
        "Zhiqing Guo",
        "Junyang Chen",
        "Xiao Lan",
        "Zhongjie Ba",
        "Yunfeng DiaoŁetter"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-10T02:38:47Z",
      "summary": "The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \\textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.",
      "pdf_url": "https://arxiv.org/pdf/2512.09264v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.08185v1",
      "title": "A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties",
      "authors": [
        "Jinghao Wang",
        "Ping Zhang",
        "Carter Yagemann"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T02:28:15Z",
      "summary": "Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.08185v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "relevance_score": 34,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.14756v1",
      "title": "Examining Software Developers' Needs for Privacy Enforcing Techniques: A survey",
      "authors": [
        "Ioanna Theophilou",
        "Georgia M. Kapitsaki"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-15T13:20:14Z",
      "summary": "Data privacy legislation, such as GDPR and CCPA/CPRA, has rendered data privacy law compliance a requirement of all software systems. Developers need to implement various kinds of functionalities to cover law needs, including user rights and law principles. As data compliance is tightly coupled with legal knowledge, it is not always easy to perform such integrations in software systems. Prior studies have focused on developers' understanding of privacy principles, such as Privacy by Design, and have examined privacy techniques used in the software industry. Nevertheless, emerging developer needs that can assist in privacy law compliance have not been examined but are useful in understanding what development automation tools, such as Generative AI, need to cover to make the compliance process more straightforward and seamless within the development process. In this work, we present a survey that examines the above needs with the participation of 68 developers, while we have examined which factors affect practitioners' needs. Most developers express a need for more automated tools, while privacy experience increases practitioners' concerns for privacy tools. Our results can assist practitioners in better positioning their development activities within privacy law compliance and point to an urgent need for privacy facilitators.",
      "pdf_url": "https://arxiv.org/pdf/2512.14756v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.11986v1",
      "title": "Learning to Extract Context for Context-Aware LLM Inference",
      "authors": [
        "Minseon Kim",
        "Lucas Caccia",
        "Zhengyan Shi",
        "Matheus Pereira",
        "Marc-Alexandre Côté",
        "Xingdi Yuan",
        "Alessandro Sordoni"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T19:10:08Z",
      "summary": "User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.",
      "pdf_url": "https://arxiv.org/pdf/2512.11986v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.09562v1",
      "title": "Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values",
      "authors": [
        "Radoslaw Klimek",
        "Jakub Blazowski"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-10T11:57:08Z",
      "summary": "Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.",
      "pdf_url": "https://arxiv.org/pdf/2512.09562v1",
      "categories": [
        "cs.SE",
        "cs.IT"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.17538v1",
      "title": "Binding Agent ID: Unleashing the Power of AI Agents with accountability and credibility",
      "authors": [
        "Zibin Lin",
        "Shengli Zhang",
        "Guofu Liao",
        "Dacheng Tao",
        "Taotao Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T13:01:54Z",
      "summary": "Autonomous AI agents lack traceable accountability mechanisms, creating a fundamental dilemma where systems must either operate as ``downgraded tools'' or risk real-world abuse. This vulnerability stems from the limitations of traditional key-based authentication, which guarantees neither the operator's physical identity nor the agent's code integrity. To bridge this gap, we propose BAID (Binding Agent ID), a comprehensive identity infrastructure establishing verifiable user-code binding. BAID integrates three orthogonal mechanisms: local binding via biometric authentication, decentralized on-chain identity management, and a novel zkVM-based Code-Level Authentication protocol. By leveraging recursive proofs to treat the program binary as the identity, this protocol provides cryptographic guarantees for operator identity, agent configuration integrity, and complete execution provenance, thereby effectively preventing unauthorized operation and code substitution. We implement and evaluate a complete prototype system, demonstrating the practical feasibility of blockchain-based identity management and zkVM-based authentication protocol.",
      "pdf_url": "https://arxiv.org/pdf/2512.17538v1",
      "categories": [
        "cs.NI",
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15554v1",
      "title": "WuppieFuzz: Coverage-Guided, Stateful REST API Fuzzing",
      "authors": [
        "Thomas Rooijakkers",
        "Anne Nijsten",
        "Cristian Daniele",
        "Erieke Weitenberg",
        "Ringo Groenewegen",
        "Arthur Melissen"
      ],
      "affiliations": [
        "The Netherlands Organisation for Applied Scientific Research",
        "The Netherlands Organisation for Applied Scientific Research",
        "Radboud University, Nijmegen, The Netherlands",
        "The Netherlands Organisation for Applied Scientific Research",
        "The Netherlands Organisation for Applied Scientific Research",
        "The Netherlands Organisation for Applied Scientific Research"
      ],
      "year": 2025,
      "published": "2025-12-17T16:05:39Z",
      "summary": "Many business processes currently depend on web services, often using REST APIs for communication. REST APIs expose web service functionality through endpoints, allowing easy client interaction over the Internet. To reduce the security risk resulting from exposed endpoints, thorough testing is desired. Due to the generally vast number of endpoints, automated testing techniques, like fuzzing, are of interest.   This paper introduces WuppieFuzz, an open-source REST API fuzzer built on LibAFL, supporting white-box, grey-box and black-box fuzzing. Using an OpenAPI specification, it can generate an initial input corpus consisting of sequences of requests. These are mutated with REST-specific and LibAFL-provided mutators to explore different code paths in the software under test. Guided by the measured coverage, WuppieFuzz then selects which request sequences to send next to reach complex states in the software under test. In this process, it automates harness creation to reduce manual efforts often required in fuzzing. Different kinds of reporting are provided by the fuzzer to help fixing bugs.   We evaluated our tool on the Petstore API to assess the robustness of the white-box approach and the effectiveness of different power schedules. We further monitored endpoint and code coverage over time to measure the efficacy of the approach.",
      "pdf_url": "https://arxiv.org/pdf/2512.15554v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15414v1",
      "title": "Packed Malware Detection Using Grayscale Binary-to-Image Representations",
      "authors": [
        "Ehab Alkhateeb",
        "Ali Ghorbani",
        "Arash Habibi Lashkari"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T13:02:33Z",
      "summary": "Detecting packed executables is a critical step in malware analysis, as packing obscures the original code and complicates static inspection. This study evaluates both classical feature-based methods and deep learning approaches that transform binary executables into visual representations, specifically, grayscale byte plots, and employ convolutional neural networks (CNNs) for automated classification of packed and non-packed binaries. A diverse dataset of benign and malicious Portable Executable (PE) files, packed using various commercial and open-source packers, was curated to capture a broad spectrum of packing transformations and obfuscation techniques. Classical models using handcrafted Gabor jet features achieved intense discrimination at moderate computational cost. In contrast, CNNs based on VGG16 and DenseNet121 significantly outperformed them, achieving high detection performance with well-balanced precision, recall, and F1-scores. DenseNet121 demonstrated slightly higher precision and lower false positive rates, whereas VGG16 achieved marginally higher recall, indicating complementary strengths for practical deployment. Evaluation against unknown packers confirmed robust generalization, demonstrating that grayscale byte-plot representations combined with deep learning provide a useful and reliable approach for early detection of packed malware, enhancing malware analysis pipelines and supporting automated antivirus inspection.",
      "pdf_url": "https://arxiv.org/pdf/2512.15414v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.19730v1",
      "title": "ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures",
      "authors": [
        "Zhonghao Yang",
        "Cheng Luo",
        "Daojing He",
        "Yiming Li",
        "Yu Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T06:42:38Z",
      "summary": "Backdoor attacks pose a significant threat to the security and reliability of deep learning models. To mitigate such attacks, one promising approach is to learn to extract features from the target model and use these features for backdoor detection. However, we discover that existing learning-based neural backdoor detection methods do not generalize well to new architectures not seen during the learning phase. In this paper, we analyze the root cause of this issue and propose a novel black-box neural backdoor detection method called ArcGen. Our method aims to obtain architecture-invariant model features, i.e., aligned features, for effective backdoor detection. Specifically, in contrast to existing methods directly using model outputs as model features, we introduce an additional alignment layer in the feature extraction function to further process these features. This reduces the direct influence of architecture information on the features. Then, we design two alignment losses to train the feature extraction function. These losses explicitly require that features from models with similar backdoor behaviors but different architectures are aligned at both the distribution and sample levels. With these techniques, our method demonstrates up to 42.5% improvements in detection performance (e.g., AUC) on unseen model architectures. This is based on a large-scale evaluation involving 16,896 models trained on diverse datasets, subjected to various backdoor attacks, and utilizing different model architectures. Our code is available at https://github.com/SeRAlab/ArcGen.",
      "pdf_url": "https://arxiv.org/pdf/2512.19730v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.14422v1",
      "title": "Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset",
      "authors": [
        "Waqas Ahmed"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-16T14:07:22Z",
      "summary": "The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.",
      "pdf_url": "https://arxiv.org/pdf/2512.14422v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.08204v1",
      "title": "Evaluating Vulnerabilities of Connected Vehicles Under Cyber Attacks by Attack-Defense Tree",
      "authors": [
        "Muhammad Baqer Mollah",
        "Honggang Wang",
        "Hua Fang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T03:28:31Z",
      "summary": "Connected vehicles represent a key enabler of intelligent transportation systems, where vehicles are equipped with advanced communication, sensing, and computing technologies to interact not only with one another but also with surrounding infrastructures and the environment. Through continuous data exchange, such vehicles are capable of enhancing road safety, improving traffic efficiency, and ensuring more reliable mobility services. Further, when these capabilities are integrated with advanced automation technologies, the concept essentially evolves into connected and autonomous vehicles (CAVs). While connected vehicles primarily focus on seamless information sharing, autonomous vehicles are mainly dependent on advanced perception, decision-making, and control mechanisms to operate with minimal or without human intervention. However, as a result of connectivity, an adversary with malicious intentions might be able to compromise successfully by breaching the system components of CAVs. In this paper, we present an attack-tree based methodology for evaluating cyber security vulnerabilities in CAVs. In particular, we utilize the attack-defense tree formulation to systematically assess attack-leaf vulnerabilities, and before analyzing the vulnerability indices, we also define a measure of vulnerabilities, which is based on existing cyber security threats and corresponding defensive countermeasures.",
      "pdf_url": "https://arxiv.org/pdf/2512.08204v1",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.07342v2",
      "title": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning",
      "authors": [
        "Chen Gong",
        "Zheng Liu",
        "Kecen Li",
        "Tianhao Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T09:29:24Z",
      "summary": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.   To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.",
      "pdf_url": "https://arxiv.org/pdf/2512.07342v2",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.05321v1",
      "title": "A Practical Honeypot-Based Threat Intelligence Framework for Cyber Defence in the Cloud",
      "authors": [
        "Darren Malvern Chin",
        "Bilal Isfaq",
        "Simon Yusuf Enoch"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T23:39:25Z",
      "summary": "In cloud environments, conventional firewalls rely on predefined rules and manual configurations, limiting their ability to respond effectively to evolving or zero-day threats. As organizations increasingly adopt platforms such as Microsoft Azure, this static defense model exposes cloud assets to zero-day exploits, botnets, and advanced persistent threats. In this paper, we introduce an automated defense framework that leverages medium- to high-interaction honeypot telemetry to dynamically update firewall rules in real time. The framework integrates deception sensors (Cowrie), Azure-native automation tools (Monitor, Sentinel, Logic Apps), and MITRE ATT&CK-aligned detection within a closed-loop feedback mechanism. We developed a testbed to automatically observe adversary tactics, classify them using the MITRE ATT&CK framework, and mitigate network-level threats automatically with minimal human intervention.   To assess the framework's effectiveness, we defined and applied a set of attack- and defense-oriented security metrics. Building on existing adaptive defense strategies, our solution extends automated capabilities into cloud-native environments. The experimental results show an average Mean Time to Block of 0.86 seconds - significantly faster than benchmark systems - while accurately classifying over 12,000 SSH attempts across multiple MITRE ATT&CK tactics. These findings demonstrate that integrating deception telemetry with Azure-native automation reduces attacker dwell time, enhances SOC visibility, and provides a scalable, actionable defense model for modern cloud infrastructures.",
      "pdf_url": "https://arxiv.org/pdf/2512.05321v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.05069v1",
      "title": "Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection",
      "authors": [
        "Mohammad Arif Rasyidi",
        "Omar Alhussein",
        "Sami Muhaidat",
        "Ernesto Damiani"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T18:29:05Z",
      "summary": "Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement, measurement approach, variational and non-variational formulations, and latent-space regularization. Experiments across three benchmark NIDS datasets show that HQC autoencoders can match or exceed classical performance in their best configurations, although they exhibit higher sensitivity to architectural decisions. Under zero-day evaluation, well-configured HQC models provide stronger and more stable generalization than classical and supervised baselines. Simulated gate-noise experiments reveal early performance degradation, indicating the need for noise-aware HQC designs. These results provide the first data-driven characterization of HQC autoencoder behavior for network intrusion detection and outline key factors that govern their practical viability. All experiment code and configurations are available at https://github.com/arasyi/hqcae-network-intrusion-detection.",
      "pdf_url": "https://arxiv.org/pdf/2512.05069v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "quant-ph"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.04338v1",
      "title": "One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises",
      "authors": [
        "Biagio Montaruli",
        "Luca Compagna",
        "Serena Elisa Ponta",
        "Davide Balzarotti"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-03T23:53:56Z",
      "summary": "The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).   We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.   We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.   Overall, we uncovered 346 malicious packages, now reported to the community.",
      "pdf_url": "https://arxiv.org/pdf/2512.04338v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20967v1",
      "title": "Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions",
      "authors": [
        "Linggao Kong",
        "Yuedong Xu",
        "Lei Jiao",
        "Chuan Xu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T05:47:27Z",
      "summary": "As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \\emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\\mathcal{O}(\\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\\%.",
      "pdf_url": "https://arxiv.org/pdf/2512.20967v1",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20077v1",
      "title": "Fault Injection Attacks on Machine Learning-based Quantum Computer Readout Error Correction",
      "authors": [
        "Anthony Etim",
        "Jakub Szefer"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T06:19:14Z",
      "summary": "Machine-learning (ML) classifiers are increasingly used in quantum computing systems to improve multi-qubit readout discrimination and to mitigate correlated readout errors. These ML classifiers are an integral component of today's quantum computer's control and readout stacks. This paper is the first to analyze the susceptibility of such ML classifiers to physical fault-injection which can result in generation of incorrect readout results from quantum computers. The study targets 5-qubit (thus 32-class) readout error-correction model. Using the ChipWhisperer Husky for physical voltage glitching, this work leverages an automated algorithm for scanning the fault injection parameter search space to find various successful faults in all the layers of the target ML model. Across repeated trials, this work finds that fault susceptibility is strongly layer-dependent: early-layers demonstrate higher rates of misprediction when faults are triggered in them, whereas later layers have smaller misprediction rates. This work further characterizes the resulting readout failures at the bitstring level using Hamming-distance and per-bit flip statistics, showing that single-shot glitches can induce structured readout corruption rather than purely random noise. These results motivate treating ML-based quantum computer readout and readout correction as a security-critical component of quantum systems and highlight the need for lightweight, deployment-friendly fault detection and redundancy mechanisms in the quantum computer readout pipelines.",
      "pdf_url": "https://arxiv.org/pdf/2512.20077v1",
      "categories": [
        "quant-ph",
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18517v1",
      "title": "Exploring Runtime Evolution in Android: A Cross-Version Analysis and Its Implications for Memory Forensics",
      "authors": [
        "Babangida Bappah",
        "Lauren G Bristol",
        "Lamine Noureddine",
        "Sideeq Bello",
        "Umar Farooq",
        "Aisha Ali-Gombe"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T21:59:23Z",
      "summary": "Userland memory forensics has become a critical component of smartphone investigations and incident response, enabling the recovery of volatile evidence such as deleted messages from end-to-end encrypted apps and cryptocurrency transactions. However, these forensics tools, particularly on Android, face significant challenges in adapting to different versions and maintaining reliability over time due to the constant evolution of low-level structures critical for evidence recovery and reconstruction. Structural changes, ranging from simple offset modifications to complete architectural redesigns, pose substantial maintenance and adaptability issues for forensic tools that rely on precise structure interpretation. Thus, this paper presents the first systematic study of Android Runtime (ART) structural evolution and its implications for memory forensics. We conduct an empirical analysis of critical Android runtime structures, examining their evolution across six versions for four different architectures. Our findings reveal that over 73.2% of structure members underwent positional changes, significantly affecting the adaptability and reliability of memory forensic tools. Further analysis of core components such as Runtime, Thread, and Heap structures highlights distinct evolution patterns and their impact on critical forensic operations, including thread state enumeration, memory mapping, and object reconstruction. These results demonstrate that traditional approaches relying on static structure definitions and symbol-based methods, while historically reliable, are increasingly unsustainable on their own. We recommend that memory forensic tools in general and Android in particular evolve toward hybrid approaches that retain the validation strength of symbolic methods while integrating automated structure inference, version-aware parsing, and redundant analysis strategies.",
      "pdf_url": "https://arxiv.org/pdf/2512.18517v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.16080v1",
      "title": "Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities",
      "authors": [
        "Tianyi Ma"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T01:56:11Z",
      "summary": "In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.",
      "pdf_url": "https://arxiv.org/pdf/2512.16080v1",
      "categories": [
        "cs.CR",
        "q-fin.TR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08856v1",
      "title": "Can the GPC standard eliminate consent banners in the EU?",
      "authors": [
        "Sebastian Zimmeck",
        "Harshvardhan J. Pandit",
        "Frederik Zuiderveen Borgesius",
        "Cristiana Teixeira Santos",
        "Konrad Kollnig",
        "Robin Berjon"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T17:49:48Z",
      "summary": "In the EU, the General Data Protection Regulation and the ePrivacy Directive mandate informed consent for behavioural advertising and use of tracking technologies. However, the ubiquity of consent banners and popups has led to widespread consent fatigue and questions regarding the effectiveness of these mechanisms in protecting users' data. In contrast, users in California and other US jurisdictions can utilize Global Privacy Control (GPC), a browser-based privacy signal that automatically broadcasts a legally binding opt-out request to websites. In this paper we explore whether, and to what extent, GPC can be adapted to the EU legal framework to mitigate consent fatigue and improve privacy protections for EU residents.   We analyse GPC as a technical specification standardized at the World Wide Web Consortium and examine its standing under current EU data protection law. Generally, GPC can be mapped to the various legal bases for processing under the GDPR. However, our evaluation also identifies friction between the GPC specification and EU data protection law as it stands. These discrepancies are resolvable and present an opportunity for EU legislators and regulators to interpret GPC in alignment with EU data protection requirements, particularly, considering the European Commission's recent Digital Omnibus proposal. We conclude that while GPC is not a silver bullet, its adoption -- supported by clear authoritative guidance and specification updates -- can offer a pragmatic path toward more automated and effective data protection in the EU.",
      "pdf_url": "https://arxiv.org/pdf/2512.08856v1",
      "categories": [
        "cs.CY",
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08289v1",
      "title": "MIRAGE: Misleading Retrieval-Augmented Generation via Black-box and Query-agnostic Poisoning Attacks",
      "authors": [
        "Tailun Chen",
        "Yu He",
        "Yan Wang",
        "Shuo Shao",
        "Haolun Zheng",
        "Zhihao Liu",
        "Jinfeng Li",
        "Yuefeng Chen",
        "Zhixuan Chu",
        "Zhan Qin"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T06:38:16Z",
      "summary": "Retrieval-Augmented Generation (RAG) systems enhance LLMs with external knowledge but introduce a critical attack surface: corpus poisoning. While recent studies have demonstrated the potential of such attacks, they typically rely on impractical assumptions, such as white-box access or known user queries, thereby underestimating the difficulty of real-world exploitation. In this paper, we bridge this gap by proposing MIRAGE, a novel multi-stage poisoning pipeline designed for strict black-box and query-agnostic environments. Operating on surrogate model feedback, MIRAGE functions as an automated optimization framework that integrates three key mechanisms: it utilizes persona-driven query synthesis to approximate latent user search distributions, employs semantic anchoring to imperceptibly embed these intents for high retrieval visibility, and leverages an adversarial variant of Test-Time Preference Optimization (TPO) to maximize persuasion. To rigorously evaluate this threat, we construct a new benchmark derived from three long-form, domain-specific datasets. Extensive experiments demonstrate that MIRAGE significantly outperforms existing baselines in both attack efficacy and stealthiness, exhibiting remarkable transferability across diverse retriever-LLM configurations and highlighting the urgent need for robust defense strategies.",
      "pdf_url": "https://arxiv.org/pdf/2512.08289v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.21041v1",
      "title": "When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design",
      "authors": [
        "Zijian Li",
        "Luzhen Tang",
        "Mengyu Xia",
        "Xinyu Li",
        "Naping Chen",
        "Dragan Gašević",
        "Yizhou Fan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T08:10:02Z",
      "summary": "With generative artificial intelligence driving the growth of dialogic data in education, automated coding is a promising direction for learning analytics to improve efficiency. This surge highlights the need to understand the nuances of student-AI interactions, especially those rare yet crucial. However, automated coding may struggle to capture these rare codes due to imbalanced data, while human coding remains time-consuming and labour-intensive. The current study examined the potential of large language models (LLMs) to approximate or replace humans in deductive, theory-driven coding, while also exploring how human-AI collaboration might support such coding tasks at scale. We compared the coding performance of small transformer classifiers (e.g., BERT) and LLMs in two datasets, with particular attention to imbalanced head-tail distributions in dialogue codes. Our results showed that LLMs did not outperform BERT-based models and exhibited systematic errors and biases in deductive coding tasks. We designed and evaluated a human-AI collaborative workflow that improved coding efficiency while maintaining coding reliability. Our findings reveal both the limitations of LLMs -- especially their difficulties with semantic similarity and theoretical interpretations and the indispensable role of human judgment -- while demonstrating the practical promise of human-AI collaborative workflows for coding.",
      "pdf_url": "https://arxiv.org/pdf/2512.21041v1",
      "categories": [
        "cs.HC"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20203v1",
      "title": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair",
      "authors": [
        "Zhenlei Ye",
        "Xiaobing Sun",
        "Sicong Cao",
        "Lili Bo",
        "Bin Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T09:54:22Z",
      "summary": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.   To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.",
      "pdf_url": "https://arxiv.org/pdf/2512.20203v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18996v1",
      "title": "Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation",
      "authors": [
        "Chong Liu",
        "Ming Zhang",
        "Fei Li",
        "Hao Zhou",
        "Xiaoshuang Chen",
        "Ye Yuan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T03:24:11Z",
      "summary": "Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks",
      "pdf_url": "https://arxiv.org/pdf/2512.18996v1",
      "categories": [
        "cs.IR",
        "cs.SE"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.17455v1",
      "title": "An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys",
      "authors": [
        "Ronnie de Souza Santos",
        "Italo Santos",
        "Maria Teresa Baldassarre",
        "Cleyton Magalhaes",
        "Mairieli Wessel"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T11:17:05Z",
      "summary": "Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.",
      "pdf_url": "https://arxiv.org/pdf/2512.17455v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15422v1",
      "title": "Can AI Generate more Comprehensive Test Scenarios? Review on Automated Driving Systems Test Scenario Generation Methods",
      "authors": [
        "Ji Zhou",
        "Yongqi Zhao",
        "Yixian Hu",
        "Hexuan Li",
        "Zhengguo Gu",
        "Nan Xu",
        "Arno Eichberger"
      ],
      "affiliations": [
        "Institute of Automotive Engineering, Graz University of Technology, Graz, Austria",
        "Institute of Automotive Engineering, Graz University of Technology, Graz, Austria",
        "Institute of Automotive Engineering, Graz University of Technology, Graz, Austria",
        "Institute of Automotive Engineering, Graz University of Technology, Graz, Austria",
        "Institute of Automotive Engineering, Graz University of Technology, Graz, Austria",
        "National Key Laboratory of Automotive Chassis Integration and Bionics, Jilin university",
        "Institute of Automotive Engineering, Graz University of Technology, Graz, Austria"
      ],
      "year": 2025,
      "published": "2025-12-17T13:14:15Z",
      "summary": "Ensuring the safety and reliability of Automated Driving Systems (ADS) remains a critical challenge, as traditional verification methods such as large-scale on-road testing are prohibitively costly and time-consuming.To address this,scenario-based testing has emerged as a scalable and efficient alternative,yet existing surveys provide only partial coverage of recent methodological and technological advances.This review systematically analyzes 31 primary studies,and 10 surveys identified through a comprehensive search spanning 2015~2025;however,the in-depth methodological synthesis and comparative evaluation focus primarily on recent frameworks(2023~2025),reflecting the surge of Artificial Intelligent(AI)-assisted and multimodal approaches in this period.Traditional approaches rely on expert knowledge,ontologies,and naturalistic driving or accident data,while recent developments leverage generative models,including large language models,generative adversarial networks,diffusion models,and reinforcement learning frameworks,to synthesize diverse and safety-critical scenarios.Our synthesis identifies three persistent gaps:the absence of standardized evaluation metrics,limited integration of ethical and human factors,and insufficient coverage of multimodal and Operational Design Domain (ODD)-specific scenarios.To address these challenges,this review contributes a refined taxonomy that incorporates multimodal extensions,an ethical and safety checklist for responsible scenario design,and an ODD coverage map with a scenario-difficulty schema to enable transparent benchmarking.Collectively,these contributions provide methodological clarity for researchers and practical guidance for industry,supporting reproducible evaluation and accelerating the safe deployment of higher-level ADS.",
      "pdf_url": "https://arxiv.org/pdf/2512.15422v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 33,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18525v1",
      "title": "A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System",
      "authors": [
        "Miyuki T. Nakata"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T22:46:13Z",
      "summary": "Understanding learning as a dynamic process is challenging due to the interaction of multiple factors, including cognitive load, internal state change, and subjective evaluation. Existing approaches often address these elements in isolation, limiting the ability to describe learning phenomena within a unified and structurally explicit framework. This paper proposes a multi-layer formal descriptive framework for learning dynamics. Rather than offering a predictive or prescriptive model, the framework introduces a symbolic language composed of state variables, mappings, and layer-specific responsibilities, enabling consistent description of learning processes without commitment to specific functional forms or optimization objectives. This descriptive framework is intended to serve as a structural substrate for analyzing learning processes in human learners, and by extension, in adaptive and Al-assisted learning systems. A central design principle is the explicit separation of descriptive responsibilities across layers, distinguishing load generation, internal understanding transformation, observation, and evaluation. Within this structure, cognitive load is treated as a relational quantity arising from interactions between external input and internal organization, while subjective evaluation is modeled as a minimal regulatory interface responding to learning dynamics and environmental conditions. By emphasizing descriptive clarity and extensibility, the framework provides a common language for organizing existing theories and supporting future empirical and theoretical work.",
      "pdf_url": "https://arxiv.org/pdf/2512.18525v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.15196v1",
      "title": "Governing rapid technological change: Policy Delphi on the future of European AI governance",
      "authors": [
        "Atte Ojanen",
        "Johannes Anttila",
        "Thilo H. K. Thelitz",
        "Anna Bjork"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T08:46:54Z",
      "summary": "The rapid advancements in artificial intelligence (AI) present unique challenges for policymakers that seek to govern the technology. In this context, the Delphi method has become an established way to identify consensus and disagreement on emerging technological issues among experts in the field of futures studies and foresight. The aim of this article is twofold: first, it examines key tensions experts see in the development of AI governance in Europe, and second, it reflects on the Delphi method's capacity to inform anticipatory governance of emerging technologies like AI based on these insights. The analysis is based on the results of a two-round Policy Delphi study on the future of AI governance with European policymakers, researchers and NGOs, conducted in mid-2024. The Policy Delphi proved useful in revealing diverse perspectives on European AI governance, drawing out a consensus that future-proof AI regulation will likely depend more on practical implementation and enforcement of legislation than on its technical specifics or scope. Furthermore, the study identified a desirability-probability gap in AI governance: desirable policy directions, like greater citizen participation, were perceived as less probable and feasible. This highlights a tension between desirable regulatory oversight and the practical difficulty for regulation to keep up with technological change.",
      "pdf_url": "https://arxiv.org/pdf/2512.15196v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11933v1",
      "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance",
      "authors": [
        "Eren Kurshan",
        "Tucker Balch",
        "David Byrd"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T05:57:32Z",
      "summary": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.11933v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CE",
        "cs.MA",
        "q-fin.GN"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.09867v1",
      "title": "MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI",
      "authors": [
        "Fengli Wu",
        "Vaidehi Patil",
        "Jaehong Yoon",
        "Yue Zhang",
        "Mohit Bansal"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-10T17:55:06Z",
      "summary": "Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the \"right to be forgotten\". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.09867v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11892v1",
      "title": "Should AI Become an Intergenerational Civil Right?",
      "authors": [
        "Jon Crowcroft",
        "Rute C. Sofia",
        "Dirk Trossen",
        "Vassilis Tsaoussidis"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T20:22:16Z",
      "summary": "Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.   This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \\emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.   Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.",
      "pdf_url": "https://arxiv.org/pdf/2512.11892v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.NI"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06240v1",
      "title": "AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems",
      "authors": [
        "Chuanhao Nie",
        "Yunbo Liu",
        "Chao Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T01:37:24Z",
      "summary": "Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.",
      "pdf_url": "https://arxiv.org/pdf/2512.06240v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.00332v1",
      "title": "Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents",
      "authors": [
        "Daud Waqas",
        "Aaryamaan Golthi",
        "Erika Hayashida",
        "Huanzhi Mao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-29T05:44:37Z",
      "summary": "Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.",
      "pdf_url": "https://arxiv.org/pdf/2512.00332v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.03077v1",
      "title": "Irresponsible AI: big tech's influence on AI research and associated impacts",
      "authors": [
        "Alex Hernandez-Garcia",
        "Alexandra Volokhova",
        "Ezekiel Williams",
        "Dounia Shaaban Kabakibo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-27T22:02:27Z",
      "summary": "The accelerated development, deployment and adoption of artificial intelligence systems has been fuelled by the increasing involvement of big tech. This has been accompanied by increasing ethical concerns and intensified societal and environmental impacts. In this article, we review and discuss how these phenomena are deeply entangled. First, we examine the growing and disproportionate influence of big tech in AI research and argue that its drive for scaling and general-purpose systems is fundamentally at odds with the responsible, ethical, and sustainable development of AI. Second, we review key current environmental and societal negative impacts of AI and trace their connections to big tech and its underlying economic incentives. Finally, we argue that while it is important to develop technical and regulatory approaches to these challenges, these alone are insufficient to counter the distortion introduced by big tech's influence. We thus review and propose alternative strategies that build on the responsibility of implicated actors and collective action.",
      "pdf_url": "https://arxiv.org/pdf/2512.03077v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2511.21755v1",
      "title": "Who Owns the Knowledge? Copyright, GenAI, and the Future of Academic Publishing",
      "authors": [
        "Dmitry Kochetkov"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-24T10:34:38Z",
      "summary": "The integration of generative artificial intelligence (GenAI) and large language models (LLMs) into scientific research and higher education presents a paradigm shift, offering revolutionizing opportunities while simultaneously raising profound ethical, legal, and regulatory questions. This study examines the complex intersection of AI and science, with a specific focus on the challenges posed to copyright law and the principles of open science. The author argues that current regulatory frameworks in key jurisdictions like the United States, China, the European Union, and the United Kingdom, while aiming to foster innovation, contain significant gaps, particularly concerning the use of copyrighted works and open science outputs for AI training. Widely adopted licensing mechanisms, such as Creative Commons, fail to adequately address the nuances of AI training, and the pervasive lack of attribution within AI systems fundamentally challenges established notions of originality. This paper issues a call to action, contending that AI training should not be shielded under fair use exceptions. Instead, the author advocates for upholding authors' rights to refuse the use of their works for AI training and proposes that universities assume a leading role in shaping responsible AI governance. The conclusion is that a harmonized international legislative effort is urgently needed to ensure transparency, protect intellectual property, and prevent the emergence of an oligopolistic market structure that could prioritize commercial profit over scientific integrity and equitable knowledge production.",
      "pdf_url": "https://arxiv.org/pdf/2511.21755v1",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2511.20695v1",
      "title": "A Brief History of Digital Twin Technology",
      "authors": [
        "Yunqi Zhang",
        "Kuangyu Shi",
        "Biao Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-24T02:03:55Z",
      "summary": "Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.",
      "pdf_url": "https://arxiv.org/pdf/2511.20695v1",
      "categories": [
        "cs.AI",
        "cs.CY",
        "physics.med-ph"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19247v1",
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "authors": [
        "Do Minh Duc",
        "Quan Xuan Truong",
        "Nguyen Tat Dat",
        "Nguyen Van Vinh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T10:29:51Z",
      "summary": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
      "pdf_url": "https://arxiv.org/pdf/2512.19247v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CL (+6)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16553v1",
      "title": "Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild",
      "authors": [
        "Yumeng Wang",
        "Tianyu Fan",
        "Lingrui Xu",
        "Chao Huang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T13:57:28Z",
      "summary": "Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.",
      "pdf_url": "https://arxiv.org/pdf/2512.16553v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "relevance_score": 32,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16233v1",
      "title": "DAG Learning from Zero-Inflated Count Data Using Continuous Optimization",
      "authors": [
        "Noriaki Sato",
        "Marco Scutari",
        "Shuichi Kawano",
        "Rui Yamaguchi",
        "Seiya Imoto"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T06:26:43Z",
      "summary": "We address network structure learning from zero-inflated count data by casting each node as a zero-inflated generalized linear model and optimizing a smooth, score-based objective under a directed acyclic graph constraint. Our Zero-Inflated Continuous Optimization (ZICO) approach uses node-wise likelihoods with canonical links and enforces acyclicity through a differentiable surrogate constraint combined with sparsity regularization. ZICO achieves superior performance with faster runtimes on simulated data. It also performs comparably to or better than common algorithms for reverse engineering gene regulatory networks. ZICO is fully vectorized and mini-batched, enabling learning on larger variable sets with practical runtimes in a wide range of domains.",
      "pdf_url": "https://arxiv.org/pdf/2512.16233v1",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.15067v1",
      "title": "EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks",
      "authors": [
        "Zijiang Yan",
        "Yixiang Huang",
        "Jianhua Pei",
        "Hina Tabassum",
        "Luca Chiaraviglio"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T04:12:52Z",
      "summary": "The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.",
      "pdf_url": "https://arxiv.org/pdf/2512.15067v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2511.22737v1",
      "title": "Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being",
      "authors": [
        "Salman Jan",
        "Toqeer Ali Syed",
        "Gohar Ali",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum",
        "Ahmad Ali"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-27T20:08:12Z",
      "summary": "The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.",
      "pdf_url": "https://arxiv.org/pdf/2511.22737v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.07864v1",
      "title": "Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data",
      "authors": [
        "Muhammad Sukri Bin Ramli"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-26T14:58:03Z",
      "summary": "New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare \"mega-trades\" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in \"mega-trades\" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.",
      "pdf_url": "https://arxiv.org/pdf/2512.07864v1",
      "categories": [
        "cs.LG",
        "econ.EM",
        "econ.GN"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2511.20812v1",
      "title": "Strategic bid response under automated market power mitigation in electricity markets",
      "authors": [
        "Chiara Fusar Bassini",
        "Jacqueline Adelowo",
        "Priya L. Donti",
        "Lynn H. Kaack"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-25T19:57:20Z",
      "summary": "In auction markets that are prone to market power abuse, preventive mitigation of bid prices can be applied through automated mitigation procedures (AMP). Despite the widespread application of AMP in US electricity markets, there exists scarce evidence on how firms strategically react to such price-cap-and-penalty regulation: when the price cap rarely leads to penalty mitigation, it is difficult to distinguish whether AMP are an effective deterrent or simply too lax. We investigate their impact on the bids of generation firms, using 2019 data from the New York and New England electricity markets (NYISO, ISO-NE). We employ a regression discontinuity design, which exploits the fact that the price cap with penalty is only activated when a structural index (e.g., congestion, pivotality) exceeds a certain cutoff. By estimating the Local Average Treatment Effect (LATE) of screening activation, we can causally identify successful deterrence of anti-competitive behavior. Around 30-40% of the analyzed bidders per market exhibit a significant strategic response - corresponding to a decrease in maximum bid prices of 4-10 $/MWh to avoid the penalty. However, there is significant heterogeneity between firms, and the regulatory impact on the overall market is not statistically detectable, suggesting lax mitigation thresholds. Using a merit-order simulation, we estimate the welfare impact of more stringent thresholds to lie between 350 and 980 thousand dollars of increased buyer surplus per mitigated hour, with the associated number of mitigated hours being below 33 hours/year. Our results motivate the empirical calibration of mitigation thresholds to improve the efficiency of AMP regulation.",
      "pdf_url": "https://arxiv.org/pdf/2511.20812v1",
      "categories": [
        "econ.GN"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2511.19735v1",
      "title": "Integrating RCTs, RWD, AI/ML and Statistics: Next-Generation Evidence Synthesis",
      "authors": [
        "Shu Yang",
        "Margaret Gamalo",
        "Haoda Fu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-24T21:51:52Z",
      "summary": "Randomized controlled trials (RCTs) have been the cornerstone of clinical evidence; however, their cost, duration, and restrictive eligibility criteria limit power and external validity. Studies using real-world data (RWD), historically considered less reliable for establishing causality, are now recognized to be important for generating real-world evidence (RWE). In parallel, artificial intelligence and machine learning (AI/ML) are being increasingly used throughout the drug development process, providing scalability and flexibility but also presenting challenges in interpretability and rigor that traditional statistics do not face. This Perspective argues that the future of evidence generation will not depend on RCTs versus RWD, or statistics versus AI/ML, but on their principled integration. To this end, a causal roadmap is needed to clarify inferential goals, make assumptions explicit, and ensure transparency about tradeoffs. We highlight key objectives of integrative evidence synthesis, including transporting RCT results to broader populations, embedding AI-assisted analyses within RCTs, designing hybrid controlled trials, and extending short-term RCTs with long-term RWD. We also outline future directions in privacy-preserving analytics, uncertainty quantification, and small-sample methods. By uniting statistical rigor with AI/ML innovation, integrative approaches can produce robust, transparent, and policy-relevant evidence, making them a key component of modern regulatory science.",
      "pdf_url": "https://arxiv.org/pdf/2511.19735v1",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.15109v2",
      "title": "Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network",
      "authors": [
        "Zhuoran Li",
        "Zhen Gao",
        "Xinhua Liu",
        "Zheng Wang",
        "Xiaotian Zhou",
        "Lei Liu",
        "Yongpeng Wu",
        "Wei Feng",
        "Yongming Huang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T06:01:16Z",
      "summary": "The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.15109v2",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.IT"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.04367v1",
      "title": "AgentBay: A Hybrid Interaction Sandbox for Seamless Human-AI Intervention in Agentic Systems",
      "authors": [
        "Yun Piao",
        "Hongbo Min",
        "Hang Su",
        "Leilei Zhang",
        "Lei Wang",
        "Yue Yin",
        "Xiao Wu",
        "Zhejing Xu",
        "Liwei Qu",
        "Hang Li",
        "Xinxin Zeng",
        "Wei Tian",
        "Fei Yu",
        "Xiaowei Li",
        "Jiayi Jiang",
        "Tongxu Liu",
        "Hao Tian",
        "Yufei Que",
        "Xiaobing Tu",
        "Bing Suo",
        "Yuebing Li",
        "Xiangting Chen",
        "Zeen Zhao",
        "Jiaming Tang",
        "Wei Huang",
        "Xuguang Li",
        "Jing Zhao",
        "Jin Li",
        "Jie Shen",
        "Jinkui Ren",
        "Xiantao Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T01:31:00Z",
      "summary": "The rapid advancement of Large Language Models (LLMs) is catalyzing a shift towards autonomous AI Agents capable of executing complex, multi-step tasks. However, these agents remain brittle when faced with real-world exceptions, making Human-in-the-Loop (HITL) supervision essential for mission-critical applications. In this paper, we present AgentBay, a novel sandbox service designed from the ground up for hybrid interaction. AgentBay provides secure, isolated execution environments spanning Windows, Linux, Android, Web Browsers, and Code interpreters. Its core contribution is a unified session accessible via a hybrid control interface: An AI agent can interact programmatically via mainstream interfaces (MCP, Open Source SDK), while a human operator can, at any moment, seamlessly take over full manual control. This seamless intervention is enabled by Adaptive Streaming Protocol (ASP). Unlike traditional VNC/RDP, ASP is specifically engineered for this hybrid use case, delivering an ultra-low-latency, smoother user experience that remains resilient even in weak network environments. It achieves this by dynamically blending command-based and video-based streaming, adapting its encoding strategy based on network conditions and the current controller (AI or human). Our evaluation demonstrates strong results in security, performance, and task completion rates. In a benchmark of complex tasks, the AgentBay (Agent + Human) model achieved more than 48% success rate improvement. Furthermore, our ASP protocol reduces bandwidth consumption by up to 50% compared to standard RDP, and in end-to-end latency with around 5% reduction, especially under poor network conditions. We posit that AgentBay provides a foundational primitive for building the next generation of reliable, human-supervised autonomous systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.04367v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20996v1",
      "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control",
      "authors": [
        "Yuwei Du",
        "Jun Zhang",
        "Jie Feng",
        "Zhicheng Liu",
        "Jian Yuan",
        "Yong Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T06:48:04Z",
      "summary": "Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.",
      "pdf_url": "https://arxiv.org/pdf/2512.20996v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20985v1",
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": [
        "Salman Jan",
        "Hassan Ali Razzaqi",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T06:20:28Z",
      "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.",
      "pdf_url": "https://arxiv.org/pdf/2512.20985v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20974v1",
      "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
      "authors": [
        "Jingyang You",
        "Hanna Kurniawati"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T06:00:51Z",
      "summary": "Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.",
      "pdf_url": "https://arxiv.org/pdf/2512.20974v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20958v1",
      "title": "ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design",
      "authors": [
        "R Yadunandan",
        "Nimisha Ghosh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T05:29:35Z",
      "summary": "De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \\textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \\textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.",
      "pdf_url": "https://arxiv.org/pdf/2512.20958v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20948v1",
      "title": "Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study",
      "authors": [
        "Zhongren Dong",
        "Haotian Guo",
        "Weixiang Xu",
        "Huan Zhao",
        "Zixing Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T05:07:07Z",
      "summary": "Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.",
      "pdf_url": "https://arxiv.org/pdf/2512.20948v1",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20866v1",
      "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images",
      "authors": [
        "Haotian Lv",
        "Chao Li",
        "Jiangbo Dai",
        "Yuhui Zhang",
        "Zepeng Fan",
        "Yiqiu Tan",
        "Dawei Wang",
        "Binglei Xie"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T00:50:27Z",
      "summary": "To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.",
      "pdf_url": "https://arxiv.org/pdf/2512.20866v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20817v1",
      "title": "EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading",
      "authors": [
        "Kumar Satvik Chaudhary",
        "Chengshuai Zhao",
        "Fan Zhang",
        "Yung Hin Tse",
        "Garima Agrawal",
        "Yuli Deng",
        "Huan Liu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T22:33:54Z",
      "summary": "Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.",
      "pdf_url": "https://arxiv.org/pdf/2512.20817v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20491v2",
      "title": "Step-DeepResearch Technical Report",
      "authors": [
        "Chen Hu",
        "Haikuo Du",
        "Heng Wang",
        "Lin Lin",
        "Mingrui Chen",
        "Peng Liu",
        "Ruihang Miao",
        "Tianchi Yue",
        "Wang You",
        "Wei Ji",
        "Wei Yuan",
        "Wenjin Deng",
        "Xiaojian Yuan",
        "Xiaoyun Zhang",
        "Xiangyu Liu",
        "Xikai Liu",
        "Yanming Xu",
        "Yicheng Cao",
        "Yifei Zhang",
        "Yongyao Wang",
        "Yubo Shu",
        "Yurong Zhang",
        "Yuxiang Zhang",
        "Zheng Gong",
        "Zhichao Chang",
        "Binyan Li",
        "Dan Ma",
        "Furong Jia",
        "Hongyuan Wang",
        "Jiayu Liu",
        "Jing Bai",
        "Junlan Liu",
        "Manjiao Liu",
        "Na Wang",
        "Qiuping Wu",
        "Qinxin Du",
        "Shiwei Li",
        "Wen Sun",
        "Yifeng Gong",
        "Yonglin Chen",
        "Yuling Zhao",
        "Yuxuan Lin",
        "Ziqi Ren",
        "Zixuan Wang",
        "Aihu Zhang",
        "Brian Li",
        "Buyun Ma",
        "Kang An",
        "Li Xie",
        "Mingliang Li",
        "Pan Li",
        "Shidong Yang",
        "Xi Chen",
        "Xiaojia Liu",
        "Yuchu Luo",
        "Yuan Song",
        "YuanHao Ding",
        "Yuanwei Liang",
        "Zexi Li",
        "Zhaoning Zhang",
        "Zixin Zhang",
        "Binxing Jiao",
        "Daxin Jiang",
        "Jiansheng Chen",
        "Jing Li",
        "Xiangyu Zhang",
        "Yibo Zhu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T16:32:27Z",
      "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2512.20491v2",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20344v1",
      "title": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice",
      "authors": [
        "Yaowei Bai",
        "Ruiheng Zhang",
        "Yu Lei",
        "Xuhua Duan",
        "Jingfeng Yao",
        "Shuguang Ju",
        "Chaoyang Wang",
        "Wei Yao",
        "Yiwan Guo",
        "Guilin Zhang",
        "Chao Wan",
        "Qian Yuan",
        "Lei Chen",
        "Wenjuan Tang",
        "Biqiang Zhu",
        "Xinggang Wang",
        "Tao Sun",
        "Wei Zhou",
        "Dacheng Tao",
        "Yongchao Xu",
        "Chuansheng Zheng",
        "Huangxuan Zhao",
        "Bo Du"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:26:13Z",
      "summary": "A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.",
      "pdf_url": "https://arxiv.org/pdf/2512.20344v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19978v1",
      "title": "Regression of Functions by Quantum Neural Networks Circuits",
      "authors": [
        "Fernando M. de Paula Neto",
        "Lucas dos Reis Silva",
        "Paulo S. G. de Mattos Neto",
        "Felipe F. Fanchini"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T01:58:03Z",
      "summary": "The performance of quantum neural network models depends strongly on architectural decisions, including circuit depth, placement of parametrized operations, and data-encoding strategies. Selecting an effective architecture is challenging and closely related to the classical difficulty of choosing suitable neural-network topologies, which is computationally hard. This work investigates automated quantum-circuit construction for regression tasks and introduces a genetic-algorithm framework that discovers Reduced Regressor QNN architectures. The approach explores depth, parametrized gate configurations, and flexible data re-uploading patterns, formulating the construction of quantum regressors as an optimization process. The discovered circuits are evaluated against seventeen classical regression models on twenty-two nonlinear benchmark functions and four analytical functions. Although classical methods often achieve comparable results, they typically require far more parameters, whereas the evolved quantum models remain compact while providing competitive performance. We further analyze dataset complexity using twelve structural descriptors and show, across five increasingly challenging meta-learning scenarios, that these measures can reliably predict which quantum architecture will perform best. The results demonstrate perfect or near-perfect predictive accuracy in several scenarios, indicating that complexity metrics offer powerful and compact representations of dataset structure and can effectively guide automated model selection. Overall, this study provides a principled basis for meta-learning-driven quantum architecture design and advances the understanding of how quantum models behave in regression settings--a topic that has received limited exploration in prior work. These findings pave the way for more systematic and theoretically grounded approaches to quantum regression.",
      "pdf_url": "https://arxiv.org/pdf/2512.19978v1",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19799v1",
      "title": "PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research",
      "authors": [
        "Tingjia Miao",
        "Jiawen Dai",
        "Jingkun Liu",
        "Jinxin Tan",
        "Muhua Zhang",
        "Wenkai Jin",
        "Yuwen Du",
        "Tian Jin",
        "Xianghe Pang",
        "Zexi Liu",
        "Tu Guo",
        "Zhengliang Zhang",
        "Yunjie Huang",
        "Shuo Chen",
        "Rui Ye",
        "Yuzhi Zhang",
        "Linfeng Zhang",
        "Kun Chen",
        "Wei Wang",
        "Weinan E",
        "Siheng Chen"
      ],
      "affiliations": [
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "School of Physics and Astronomy, Shanghai Jiao Tong University",
        "School of Physics and Astronomy, Shanghai Jiao Tong University",
        "School of Physics and Astronomy, Shanghai Jiao Tong University",
        "School of Physics and Astronomy, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "School of Physics and Astronomy, Shanghai Jiao Tong University",
        "School of Physics and Astronomy, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "Institute of Theoretical Physics, Chinese Academy of Sciences",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "DP Technology",
        "DP Technology",
        "Institute of Theoretical Physics, Chinese Academy of Sciences",
        "School of Physics and Astronomy, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University"
      ],
      "year": 2025,
      "published": "2025-12-22T19:00:15Z",
      "summary": "Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19799v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19516v1",
      "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "authors": [
        "Xueming Yan",
        "Bo Yin",
        "Yaochu Jin"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T16:08:03Z",
      "summary": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
      "pdf_url": "https://arxiv.org/pdf/2512.19516v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19178v1",
      "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "authors": [
        "Jin Wang",
        "Kim Tien Ly",
        "Jacques Cloete",
        "Nikos Tsagarakis",
        "Ioannis Havoutis"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T09:12:48Z",
      "summary": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2512.19178v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18357v1",
      "title": "DACE For Railway Acronym Disambiguation",
      "authors": [
        "El Mokhtar Hribach",
        "Oussama Mechhour",
        "Mohammed Elmonstaser",
        "Yassine El Boudouri",
        "Othmane Kabal"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T12:56:06Z",
      "summary": "Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.",
      "pdf_url": "https://arxiv.org/pdf/2512.18357v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20823v1",
      "title": "NotSoTiny: A Large, Living Benchmark for RTL Code Generation",
      "authors": [
        "Razine Moundir Ghorab",
        "Emanuele Parisi",
        "Cristian Gutierrez",
        "Miquel Alberti-Binimelis",
        "Miquel Moreto",
        "Dario Garcia-Gasulla",
        "Gokcen Kestor"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T22:53:47Z",
      "summary": "LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.",
      "pdf_url": "https://arxiv.org/pdf/2512.20823v1",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18608v1",
      "title": "A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts",
      "authors": [
        "Prabigya Acharya",
        "Liza Shrestha"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T05:58:40Z",
      "summary": "Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2512.18608v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18256v1",
      "title": "MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification",
      "authors": [
        "Sirui Li",
        "Wangyue Lu",
        "Xiaorui Shi",
        "Ke Weng",
        "Haozhe Sun",
        "Minghe Yu",
        "Tiancheng Zhang",
        "Ge Yu",
        "Hengyu Liu",
        "Lun Du"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T07:39:19Z",
      "summary": "Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.",
      "pdf_url": "https://arxiv.org/pdf/2512.18256v1",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.17853v1",
      "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning",
      "authors": [
        "Ran Gong",
        "Xiaohan Zhang",
        "Jinghuan Shang",
        "Maria Vittoria Minniti",
        "Jigarkumar Patel",
        "Valerio Pepe",
        "Riedana Yan",
        "Ahmet Gundogdu",
        "Ivan Kapelyukh",
        "Ali Abbas",
        "Xiaoqiang Yan",
        "Harsh Patel",
        "Laura Herlant",
        "Karl Schmeckpeper"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T17:55:48Z",
      "summary": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .",
      "pdf_url": "https://arxiv.org/pdf/2512.17853v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16921v1",
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "authors": [
        "Qihao Liu",
        "Chengzhi Mao",
        "Yaojie Liu",
        "Alan Yuille",
        "Wen-Sheng Chu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T18:59:57Z",
      "summary": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
      "pdf_url": "https://arxiv.org/pdf/2512.16921v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16861v1",
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "authors": [
        "Zihan Zhou",
        "Animesh Garg",
        "Ajay Mandlekar",
        "Caelan Garrett"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T18:32:39Z",
      "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.16446v1",
      "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
      "authors": [
        "Enis Yalcin",
        "Joshua O'Hara",
        "Maria Stamatopoulou",
        "Chengxu Zhou",
        "Dimitrios Kanoulas"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T12:08:24Z",
      "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
      "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16275v1",
      "title": "GFLAN: Generative Functional Layouts",
      "authors": [
        "Mohamed Abouagour",
        "Eleftherios Garyfallidis"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T07:52:47Z",
      "summary": "Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.",
      "pdf_url": "https://arxiv.org/pdf/2512.16275v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16214v2",
      "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving",
      "authors": [
        "Jianming Liu",
        "Ren Zhu",
        "Jian Xu",
        "Kun Ding",
        "Xu-Yao Zhang",
        "Gaofeng Meng",
        "Cheng-Lin Liu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T06:02:50Z",
      "summary": "Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.",
      "pdf_url": "https://arxiv.org/pdf/2512.16214v2",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16149v1",
      "title": "ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs",
      "authors": [
        "Hao Chen",
        "Zhexin Hu",
        "Jiajun Chai",
        "Haocheng Yang",
        "Hang He",
        "Xiaohan Wang",
        "Wei Lin",
        "Luhang Wang",
        "Guojun Yin",
        "Zhuofeng zhao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T04:06:26Z",
      "summary": "Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .",
      "pdf_url": "https://arxiv.org/pdf/2512.16149v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.16036v1",
      "title": "Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education",
      "authors": [
        "Diane Myung-kyung Woodbridge",
        "Allyson Seba",
        "Freddie Seba",
        "Aydin Schwartz"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T23:39:19Z",
      "summary": "As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.",
      "pdf_url": "https://arxiv.org/pdf/2512.16036v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.15816v1",
      "title": "A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning",
      "authors": [
        "Daragh King",
        "Vasileios Koutavas",
        "Laura Kovacs"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T14:16:59Z",
      "summary": "Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2512.15816v1",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.LO"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.15435v1",
      "title": "Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat",
      "authors": [
        "Stefan Edelkamp"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T13:27:44Z",
      "summary": "In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.",
      "pdf_url": "https://arxiv.org/pdf/2512.15435v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 31,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.12400v1",
      "title": "Agentic AI for 6G: A New Paradigm for Autonomous RAN Security Compliance",
      "authors": [
        "Sotiris Chatzimiltis",
        "Mahdi Boloursaz Mashhadi",
        "Mohammad Shojafar",
        "Merouane Debbah",
        "Rahim Tafazolli"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-13T17:15:00Z",
      "summary": "Agentic AI systems are emerging as powerful tools for automating complex, multi-step tasks across various industries. One such industry is telecommunications, where the growing complexity of next-generation radio access networks (RANs) opens up numerous opportunities for applying these systems. Securing the RAN is a key area, particularly through automating the security compliance process, as traditional methods often struggle to keep pace with evolving specifications and real-time changes. In this article, we propose a framework that leverages LLM-based AI agents integrated with a retrieval-augmented generation (RAG) pipeline to enable intelligent and autonomous enforcement of security compliance. An initial case study demonstrates how an agent can assess configuration files for compliance with O-RAN Alliance and 3GPP standards, generate explainable justifications, and propose automated remediation if needed. We also highlight key challenges such as model hallucinations and vendor inconsistencies, along with considerations like agent security, transparency, and system trust. Finally, we outline future directions, emphasizing the need for telecom-specific LLMs and standardized evaluation frameworks.",
      "pdf_url": "https://arxiv.org/pdf/2512.12400v1",
      "categories": [
        "cs.NI"
      ],
      "relevance_score": 30,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Abstract keyword: 'autonomous' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.21039v1",
      "title": "Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection",
      "authors": [
        "Roopa Bukke",
        "Soumya Pandey",
        "Suraj Kumar",
        "Soumi Chattopadhyay",
        "Chandranath Adak"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T08:06:52Z",
      "summary": "The rapid proliferation of online misinformation poses significant risks to public trust, policy, and safety, necessitating reliable automated fake news detection. Existing methods often struggle with multimodal content, domain generalization, and explainability. We propose AMPEND-LS, an agentic multi-persona evidence-grounded framework with LLM-SLM synergy for multimodal fake news detection. AMPEND-LS integrates textual, visual, and contextual signals through a structured reasoning pipeline powered by LLMs, augmented with reverse image search, knowledge graph paths, and persuasion strategy analysis. To improve reliability, we introduce a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context, and a complementary SLM classifier to mitigate LLM uncertainty and hallucinations. Extensive experiments across three benchmark datasets demonstrate that AMPEND-LS consistently outperformed state-of-the-art baselines in accuracy, F1 score, and robustness. Qualitative case studies further highlight its transparent reasoning and resilience against evolving misinformation. This work advances the development of adaptive, explainable, and evidence-aware systems for safeguarding online information integrity.",
      "pdf_url": "https://arxiv.org/pdf/2512.21039v1",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "relevance_score": 30,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20107v1",
      "title": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
      "authors": [
        "Thanh-Tung Le",
        "Tuan Pham",
        "Tung Nguyen",
        "Deying Kong",
        "Xiaohui Xie",
        "Stephan Mandt"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T07:08:00Z",
      "summary": "Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.",
      "pdf_url": "https://arxiv.org/pdf/2512.20107v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 30,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)"
      ]
    },
    {
      "arxiv_id": "2512.18503v1",
      "title": "NASTaR: NovaSAR Automated Ship Target Recognition Dataset",
      "authors": [
        "Benyamin Hosseiny",
        "Kamirul Kamirul",
        "Odysseas Pappas",
        "Alin Achim"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T20:42:30Z",
      "summary": "Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://10.5523/bris, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.",
      "pdf_url": "https://arxiv.org/pdf/2512.18503v1",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "relevance_score": 30,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.18331v1",
      "title": "A two-stream network with global-local feature fusion for bone age assessment",
      "authors": [
        "Qiong Lou",
        "Han Yang",
        "Fang Lu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T11:56:21Z",
      "summary": "Bone Age Assessment (BAA) is a widely used clinical technique that can accurately reflect an individual's growth and development level, as well as maturity. In recent years, although deep learning has advanced the field of bone age assessment, existing methods face challenges in efficiently balancing global features and local skeletal details. This study aims to develop an automated bone age assessment system based on a two-stream deep learning architecture to achieve higher accuracy in bone age assessment. We propose the BoNet+ model incorporating global and local feature extraction channels. A Transformer module is introduced into the global feature extraction channel to enhance the ability in extracting global features through multi-head self-attention mechanism. A RFAConv module is incorporated into the local feature extraction channel to generate adaptive attention maps within multiscale receptive fields, enhancing local feature extraction capabilities. Global and local features are concatenated along the channel dimension and optimized by an Inception-V3 network. The proposed method has been validated on the Radiological Society of North America (RSNA) and Radiological Hand Pose Estimation (RHPE) test datasets, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively. These results are comparable to the state-of-the-art. The BoNet+ model reduces the clinical workload and achieves automatic, high-precision, and more objective bone age assessment.",
      "pdf_url": "https://arxiv.org/pdf/2512.18331v1",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "relevance_score": 30,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.17390v1",
      "title": "Psychological Factors Influencing University Students Trust in AI-Based Learning Assistants",
      "authors": [
        "Ezgi Dağtekin",
        "Ercan Erkalkan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T09:44:43Z",
      "summary": "Artificial intelligence (AI) based learning assistants and chatbots are increasingly integrated into higher education. While these tools are often evaluated in terms of technical performance, their successful and ethical use also depends on psychological factors such as trust, perceived risk, technology anxiety, and students general attitudes toward AI. This paper adopts a psychology oriented perspective to examine how university students form trust in AI based learning assistants. Drawing on recent literature in mental health, human AI interaction, and trust in automation, we propose a conceptual framework that organizes psychological predictors of trust into four groups: cognitive appraisals, affective reactions, social relational factors, and contextual moderators. A narrative review approach synthesizes empirical findings and derives research questions and hypotheses for future studies. The paper highlights that trust in AI is a psychological process shaped by individual differences and learning environments, with practical implications for instructors, administrators, and designers of educational AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.17390v1",
      "categories": [
        "cs.HC",
        "cs.CY"
      ],
      "relevance_score": 30,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'nist' (+10)"
      ]
    },
    {
      "arxiv_id": "2512.15483v1",
      "title": "Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs",
      "authors": [
        "Luca Torresi",
        "Pascal Friederich"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T14:35:31Z",
      "summary": "Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.",
      "pdf_url": "https://arxiv.org/pdf/2512.15483v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 30,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.09959v1",
      "title": "TRUCE: TRUsted Compliance Enforcement Service for Secure Health Data Exchange",
      "authors": [
        "Dae-young Kim",
        "Karuna Pande Joshi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T21:47:46Z",
      "summary": "Organizations are increasingly sharing large volumes of sensitive Personally Identifiable Information (PII), like health records, with each other to better manage their services. Protecting PII data has become increasingly important in today's digital age, and several regulations have been formulated to ensure the secure exchange and management of sensitive personal data. However, at times some of these regulations are at loggerheads with each other, like the Health Insurance Portability and Accountability Act (HIPAA) and Cures Act; and this adds complexity to the already challenging task of Health Data compliance. As public concern regarding sensitive data breaches grows, finding solutions that streamline compliance processes and enhance individual privacy is crucial. We have developed a novel TRUsted Compliance Enforcement (TRUCE) framework for secure data exchange which aims to automate compliance procedures and enhance trusted data management within organizations. The TRUCE framework reasons over contexts of data exchange and assesses the trust score of users and the veracity of data based on corresponding regulations. This framework, developed using approaches from AI/Knowledge representation and Semantic Web technologies, includes a trust management method that incorporates static ground truth, represented by regulations such as HIPAA, and dynamic ground truth, defined by an organization's policies. In this paper, we present our framework in detail along with the validation against the Health Insurance Portability and Accountability Act (HIPAA) Data Usage Agreement (DUA) on CDC Contact Tracing patient data, up to one million patient records. TRUCE service will streamline compliance efforts and ensure adherence to privacy regulations and can be used by organizations to manage compliance of large velocity data exchange in real time.",
      "pdf_url": "https://arxiv.org/pdf/2512.09959v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.07909v1",
      "title": "Agentic Artificial Intelligence for Ethical Cybersecurity in Uganda: A Reinforcement Learning Framework for Threat Detection in Resource-Constrained Environments",
      "authors": [
        "Ibrahim Adabara",
        "Bashir Olaniyi Sadiq",
        "Aliyu Nuhu Shuaibu",
        "Yale Ibrahim Danjuma",
        "Venkateswarlu Maninti",
        "Mutebi Joe"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T05:44:25Z",
      "summary": "Uganda's rapid digital transformation, supported by national strategies such as Vision 2040 and the Digital Transformation Roadmap, has expanded reliance on networked services while simultaneously increasing exposure to sophisticated cyber threats. In resource-constrained settings, commonly deployed rule-based intrusion detection systems lack the adaptability and ethical safeguards needed to address evolving attack patterns, leading to undetected breaches and excessive blocking of legitimate traffic. This study proposes an Agentic Artificial Intelligence (AAI) framework that integrates reinforcement learning, an explicit ethical governance layer, and human oversight to deliver adaptive and trustworthy cybersecurity. A CPU-optimized simulation environment was developed using a five-node network topology that mirrors key elements of Uganda's critical digital infrastructure and generates both benign and malicious traffic, including phishing, ransomware, and distributed denial-of-service attacks. A Q-learning agent, operating within clearly defined ethical constraints and subject to human auditability, was trained and evaluated against a traditional rule-based baseline. The AAI framework achieved a 100 percent detection rate, zero false positives, and full ethical compliance, compared with 70 percent detection and 15 percent false positives for the baseline system. These results demonstrate that agentic, ethically governed reinforcement learning can substantially improve cybersecurity effectiveness and fairness in CPU-only, resource-constrained environments, offering a practical pathway for operationalizing responsible AI in Uganda's national cybersecurity strategy.",
      "pdf_url": "https://arxiv.org/pdf/2512.07909v1",
      "categories": [
        "cs.CR",
        "cs.CY"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2511.22933v1",
      "title": "RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN",
      "authors": [
        "Onur Salan",
        "Burak Çırağ",
        "Onur Sever",
        "İbrahim Hökelek",
        "Ali Görçin",
        "Hakan Ali Çırpan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-28T07:20:25Z",
      "summary": "Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management. In this paper, a Retrieval-Augmented Generation (RAG)-empowered Large Language Model (ReLLM)-driven dynamic radio resource management framework for Open Radio Access Network (O-RAN) inspired 6G networks is proposed. The introduced methodology leverages the ReLLM framework to interpret both historical and real-time network data, enabling adaptive control of network slices. The ReLLM is founded on two specialized agents, one is responsible for proactively detecting service level agreement (SLA) violations by continuously monitoring and estimating slice-specific performance metrics, and the other one is responsible for dynamically reallocating physical resource blocks when the SLA violation probability exceeds a pre-defined threshold. The primary objective of this dual-agent design is to minimize unnecessary LLM inference calls while satisfying the SLA requirements of the slices, thereby improving computational and energy efficiency. The proposed ReLLM framework is implemented and validated on an end-to-end O-RAN testbed built upon open-source OpenAirInterface emulators. The experimental results demonstrate that the LLM approach with its reduced token consumption feature maintains a near-zero drop ratio for the low-priority slice while simultaneously satisfying acceptable latency performance for the high-priority slice. The ReLLM-driven design improves reliability and SLA compliance, confirming its practicality for real-world O-RAN testbeds and its potential applicability to future 6G networks.",
      "pdf_url": "https://arxiv.org/pdf/2511.22933v1",
      "categories": [
        "eess.SP"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)"
      ]
    },
    {
      "arxiv_id": "2511.22746v1",
      "title": "Epistemic Fragility in Large Language Models: Prompt Framing Systematically Modulates Misinformation Correction",
      "authors": [
        "Sekoul Krastev",
        "Hilary Sweatman",
        "Anni Sternisko",
        "Steve Rathje"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-27T20:32:47Z",
      "summary": "As large language models (LLMs) rapidly displace traditional expertise, their capacity to correct misinformation has become a core concern. We investigate the idea that prompt framing systematically modulates misinformation correction - something we term 'epistemic fragility'. We manipulated prompts by open-mindedness, user intent, user role, and complexity. Across ten misinformation domains, we generated 320 prompts and elicited 2,560 responses from four frontier LLMs, which were coded for strength of misinformation correction and rectification strategy use. Analyses showed that creative intent, expert role, and closed framing led to a significant reduction in correction likelihood and effectiveness of used strategy. We also found striking model differences: Gemini 2.5 Pro had 74% lower odds of strong correction than Claude Sonnet 4.5. These findings highlight epistemic fragility as an important structural property of LLMs, challenging current guardrails and underscoring the need for alignment strategies that prioritize epistemic integrity over conversational compliance.",
      "pdf_url": "https://arxiv.org/pdf/2511.22746v1",
      "categories": [
        "cs.HC"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'large language model' (+8)"
      ]
    },
    {
      "arxiv_id": "2512.20176v1",
      "title": "Optimistic TEE-Rollups: A Hybrid Architecture for Scalable and Verifiable Generative AI Inference on Blockchain",
      "authors": [
        "Aaron Chan",
        "Alex Ding",
        "Frank Chen",
        "Alan Wu",
        "Bruce Zhang",
        "Arther Tian"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T09:16:41Z",
      "summary": "The rapid integration of Large Language Models (LLMs) into decentralized physical infrastructure networks (DePIN) is currently bottlenecked by the Verifiability Trilemma, which posits that a decentralized inference system cannot simultaneously achieve high computational integrity, low latency, and low cost. Existing cryptographic solutions, such as Zero-Knowledge Machine Learning (ZKML), suffer from superlinear proving overheads (O(k NlogN)) that render them infeasible for billionparameter models. Conversely, optimistic approaches (opML) impose prohibitive dispute windows, preventing real-time interactivity, while recent \"Proof of Quality\" (PoQ) paradigms sacrifice cryptographic integrity for subjective semantic evaluation, leaving networks vulnerable to model downgrade attacks and reward hacking. In this paper, we introduce Optimistic TEE-Rollups (OTR), a hybrid verification protocol that harmonizes these constraints. OTR leverages NVIDIA H100 Confidential Computing Trusted Execution Environments (TEEs) to provide sub-second Provisional Finality, underpinned by an optimistic fraud-proof mechanism and stochastic Zero-Knowledge spot-checks to mitigate hardware side-channel risks. We formally define Proof of Efficient Attribution (PoEA), a consensus mechanism that cryptographically binds execution traces to hardware attestations, thereby guaranteeing model authenticity. Extensive simulations demonstrate that OTR achieves 99% of the throughput of centralized baselines with a marginal cost overhead of $0.07 per query, maintaining Byzantine fault tolerance against rational adversaries even in the presence of transient hardware vulnerabilities.",
      "pdf_url": "https://arxiv.org/pdf/2512.20176v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.19215v1",
      "title": "Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation",
      "authors": [
        "Junyao Ye",
        "Zhen Li",
        "Xi Tang",
        "Shouhuai Xu",
        "Deqing Zou",
        "Zhongsheng Yuan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T09:54:52Z",
      "summary": "Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.",
      "pdf_url": "https://arxiv.org/pdf/2512.19215v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.19153v1",
      "title": "University Rents Enabling Corporate Innovation: Mapping Academic Researcher Coding and Discursive Labour in the R Language Ecosystem",
      "authors": [
        "Xiaolan Cai",
        "Mathieu O'Neil",
        "Stefano Zacchiroli"
      ],
      "affiliations": [
        "IP Paris, LTCI, ACES, INFRES",
        "IP Paris, LTCI, ACES, INFRES",
        "IP Paris, LTCI, ACES, INFRES"
      ],
      "year": 2025,
      "published": "2025-12-22T08:50:21Z",
      "summary": "This article explores the role of unrecognised labour in corporate innovation systems via an analysis of researcher coding and discursive contributions to R, one of the largest statistical software ecosystems. Studies of online platforms typically focus on how platform affordances constrain participants' actions, and profit from their labour. We innovate by connecting the labour performed inside digital platforms to the professional employment of participants. Our case study analyses 8,924 R package repositories on GitHub, examining commits and communications. Our quantitative findings show that researchers, alongside non-affiliated contributors, are the most frequent owners of R package repositories and their most active contributors. Researchers are more likely to hold official roles compared to the average, and to engage in collaborative problem-solving and support work during package development. This means there is, underneath the 'recognised' category of star researchers who transition between academia and industry and secure generous funding, an 'unrecognised' category of researchers who not only create and maintain key statistical infrastructure, but also provide support to industry employees, for no remuneration. Our qualitative findings show how this unrecognised labour affects practitioners. Finally, our analysis of the ideology and practice of free, libre and open source software (FLOSS) shows how this ideology and practice legitimate the use of 'university rents' by Big Tech.",
      "pdf_url": "https://arxiv.org/pdf/2512.19153v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18560v1",
      "title": "Proof of Authenticity of General IoT Information with Tamper-Evident Sensors and Blockchain",
      "authors": [
        "Kenji Saito"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T01:36:24Z",
      "summary": "Sensor data in IoT (Internet of Things) systems is vulnerable to tampering or falsification when transmitted through untrusted services. This is critical because such data increasingly underpins real-world decisions in domains such as logistics, healthcare, and other critical infrastructure. We propose a general method for secure sensor-data logging in which tamper-evident devices periodically sign readouts, link data using redundant hash chains, and submit cryptographic evidence to a blockchain-based service via Merkle trees to ensure verifiability even under data loss. Our approach enables reliable and cost-effective validation of sensor data across diverse IoT systems, including disaster response and other humanitarian applications, without relying on the integrity of intermediate systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.18560v1",
      "categories": [
        "cs.CR",
        "cs.CY"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.17310v1",
      "title": "Cryptanalysis of Pseudorandom Error-Correcting Codes",
      "authors": [
        "Tianrui Wang",
        "Anyu Wang",
        "Tianshuo Cong",
        "Delong Ran",
        "Jinyuan Liu",
        "Xiaoyun Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T07:48:04Z",
      "summary": "Pseudorandom error-correcting codes (PRC) is a novel cryptographic primitive proposed at CRYPTO 2024. Due to the dual capability of pseudorandomness and error correction, PRC has been recognized as a promising foundational component for watermarking AI-generated content. However, the security of PRC has not been thoroughly analyzed, especially with concrete parameters or even in the face of cryptographic attacks. To fill this gap, we present the first cryptanalysis of PRC. We first propose three attacks to challenge the undetectability and robustness assumptions of PRC. Among them, two attacks aim to distinguish PRC-based codewords from plain vectors, and one attack aims to compromise the decoding process of PRC. Our attacks successfully undermine the claimed security guarantees across all parameter configurations. Notably, our attack can detect the presence of a watermark with overwhelming probability at a cost of $2^{22}$ operations. We also validate our approach by attacking real-world large generative models such as DeepSeek and Stable Diffusion. To mitigate our attacks, we further propose three defenses to enhance the security of PRC, including parameter suggestions, implementation suggestions, and constructing a revised key generation algorithm. Our proposed revised key generation function effectively prevents the occurrence of weak keys. However, we highlight that the current PRC-based watermarking scheme still cannot achieve a 128-bit security under our parameter suggestions due to the inherent configurations of large generative models, such as the maximum output length of large language models.",
      "pdf_url": "https://arxiv.org/pdf/2512.17310v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15919v1",
      "title": "Analysing Multidisciplinary Approaches to Fight Large-Scale Digital Influence Operations",
      "authors": [
        "David Arroyo",
        "Rafael Mata Milla",
        "Marc Almeida Ros",
        "Nikolaos Lykousas",
        "Ivan Homoliak",
        "Constantinos Patsakis",
        "Fran Casino"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T19:31:24Z",
      "summary": "Crime as a Service (CaaS) has evolved from isolated criminal incidents to a broad spectrum of illicit activities, including social media manipulation, foreign information manipulation and interference (FIMI), and the sale of disinformation toolkits. This article analyses how threat actors exploit specialised infrastructures ranging from proxy and VPN services to AI-driven generative models to orchestrate large-scale opinion manipulation. Moreover, it discusses how these malicious operations monetise the virality of social networks, weaponise dual-use technologies, and leverage user biases to amplify polarising narratives. In parallel, it examines key strategies for detecting, attributing, and mitigating such campaigns by highlighting the roles of blockchain-based content verification, advanced cryptographic proofs, and cross-disciplinary collaboration. Finally, the article highlights that countering disinformation demands an integrated framework that combines legal, technological, and societal efforts to address a rapidly adapting and borderless threat",
      "pdf_url": "https://arxiv.org/pdf/2512.15919v1",
      "categories": [
        "cs.CY",
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.12904v1",
      "title": "OptHQC: Optimize HQC for High-Performance Post-Quantum Cryptography",
      "authors": [
        "Ben Dong",
        "Hui Feng",
        "Qian Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-15T01:07:57Z",
      "summary": "As post-quantum cryptography (PQC) becomes increasingly critical for securing future communication systems, the performance overhead introduced by quantum-resistant algorithms presents a major computing challenge. HQC (Hamming Quasi-Cyclic) is a newly standardized code-based PQC scheme designed to replace classical key exchange methods. In this paper, we propose OptHQC, an optimized implementation of the HQC scheme to deliver high-performance cryptographic operations. Our approach provides a comprehensive analysis of each computational blocks in HQC and introduces optimizations across all three stages: key generation, encryption, and decryption. We first exploit data-level sparsity in vector multiplication to accelerate polynomial operations during vector generation. We then leverage instruction-level acceleration (e.g., AVX2) in hash computation to further improve performance. Last, we transform multiplication into lookup table indexing and optimize memory access patterns in syndrome computation and error vector recovery, which are the most computationally intensive operations in HQC. Overall, OptHQC achieves an average 55% speedup over the reference HQC implementation on CPU.",
      "pdf_url": "https://arxiv.org/pdf/2512.12904v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.12568v1",
      "title": "Intelligent Adaptive Federated Byzantine Agreement for Robust Blockchain Consensus",
      "authors": [
        "Erdhi Widyarto Nugroho",
        "R. Rizal Isnanto",
        "Luhur Bayuaji"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T06:25:52Z",
      "summary": "The Federated Byzantine Agreement (FBA) achieves rapid consensus by relying on overlapping quorum slices. But this architecture leads to a high dependence on the availability of validators when about one fourth of validators go down, the classical FBA can lose liveness or fail to reach agreement. We thus come up with an Adaptive FBA architecture that can reconfigure quorum slices intelligently based on real time validator reputation to overcome this drawback. Our model includes trust scores computed from EigenTrust and a sliding window behavioral assessment to determine the reliability of validators.   We have built the intelligent adaptive FBA model and conducted tests in a Stellar based setting. Results of real life experiments reveal that the system is stable enough to keep consensus when more than half of the validators (up to 62 percent) are disconnected, which is a great extension of the failure threshold of a classical FBA. A fallback mode allows the network to be functional with as few as three validators, thus showing a significant robustness enhancement. Besides, a comparative study with the existing consensus protocols shows that Adaptive FBA can be an excellent choice for the next generation of blockchain systems, especially for constructing a resilient blockchain infrastructure.",
      "pdf_url": "https://arxiv.org/pdf/2512.12568v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.09958v1",
      "title": "When Quantum Federated Learning Meets Blockchain in 6G Networks",
      "authors": [
        "Dinh C. Nguyen",
        "Md Bokhtiar Al Zami",
        "Ratun Rahman",
        "Shaba Shaon",
        "Tuy Tan Nguyen",
        "Fatemeh Afghah"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T21:44:18Z",
      "summary": "Quantum federated learning (QFL) is emerging as a key enabler for intelligent, secure, and privacy-preserving model training in next-generation 6G networks. By leveraging the computational advantages of quantum devices, QFL offers significant improvements in learning efficiency and resilience against quantum-era threats. However, future 6G environments are expected to be highly dynamic, decentralized, and data-intensive, which necessitates moving beyond traditional centralized federated learning frameworks. To meet this demand, blockchain technology provides a decentralized, tamper-resistant infrastructure capable of enabling trustless collaboration among distributed quantum edge devices. This paper presents QFLchain, a novel framework that integrates QFL with blockchain to support scalable and secure 6G intelligence. In this work, we investigate four key pillars of \\textit{QFLchain} in the 6G context: (i) communication and consensus overhead, (ii) scalability and storage overhead, (iii) energy inefficiency, and (iv) security vulnerability. A case study is also presented, demonstrating potential advantages of QFLchain, based on simulation, over state-of-the-art approaches in terms of training performance.",
      "pdf_url": "https://arxiv.org/pdf/2512.09958v1",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08918v1",
      "title": "Improved Pseudorandom Codes from Permuted Puzzles",
      "authors": [
        "Miranda Christ",
        "Noah Golowich",
        "Sam Gunn",
        "Ankur Moitra",
        "Daniel Wichs"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T18:53:42Z",
      "summary": "Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.   In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.   We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs.",
      "pdf_url": "https://arxiv.org/pdf/2512.08918v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08858v1",
      "title": "NecoFuzz: Effective Fuzzing of Nested Virtualization via Fuzz-Harness Virtual Machines",
      "authors": [
        "Reima Ishii",
        "Takaaki Fukai",
        "Takahiro Shinagawa"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T17:50:32Z",
      "summary": "Nested virtualization is now widely supported by major cloud vendors, allowing users to leverage virtualization-based technologies in the cloud. However, supporting nested virtualization significantly increases host hypervisor complexity and introduces a new attack surface in cloud platforms. While many prior studies have explored hypervisor fuzzing, none has explicitly addressed nested virtualization due to the challenge of generating effective virtual machine (VM) instances with a vast state space as fuzzing inputs.   We present NecoFuzz, the first fuzzing framework that systematically targets nested virtualization-specific logic in hypervisors. NecoFuzz synthesizes executable fuzz-harness VMs with internal states near the boundary between valid and invalid, guided by an approximate model of hardware-assisted virtualization specifications. Since vulnerabilities in nested virtualization often stem from incorrect handling of unexpected VM states, this specification-guided, boundary-oriented generation significantly improves coverage of security-critical code across different hypervisors.   We implemented NecoFuzz on Intel VT-x and AMD-V by extending AFL++ to support fuzz-harness VMs. NecoFuzz achieved 84.7% and 74.2% code coverage for nested virtualization-specific code on Intel VT-x and AMD-V, respectively, and uncovered six previously unknown vulnerabilities across three hypervisors, including two assigned CVEs.",
      "pdf_url": "https://arxiv.org/pdf/2512.08858v1",
      "categories": [
        "cs.OS",
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08403v2",
      "title": "DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components",
      "authors": [
        "Yupei Li",
        "Li Wang",
        "Yuxiang Wang",
        "Lei Wang",
        "Rizhao Cai",
        "Jie Shi",
        "Björn W. Schuller",
        "Zhizheng Wu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T09:36:38Z",
      "summary": "Audio deepfake detection has recently garnered public concern due to its implications for security and reliability. Traditional deep learning methods have been widely applied to this task but often lack generalisability when confronted with newly emerging spoofing techniques and more tasks such as spoof attribution recognition rather than simple binary classification. In principle, Large Language Models (LLMs) are considered to possess the needed generalisation capabilities. However, previous research on Audio LLMs (ALLMs) indicates a generalization bottleneck in audio deepfake detection performance, even when sufficient data is available. Consequently, this study investigates the model architecture and examines the effects of the primary components of ALLMs, namely the audio encoder and the text-based LLM. Our experiments demonstrate that the careful selection and combination of audio encoders and text-based LLMs are crucial for unlocking the deepfake detection potential of ALLMs. We further propose an ALLM structure capable of generalizing deepfake detection abilities to out-of-domain spoofing tests and other deepfake tasks, such as spoof positioning and spoof attribution recognition. Our proposed model architecture achieves state-of-the-art (SOTA) performance across multiple datasets, including ASVSpoof2019, InTheWild, and Demopage, with accuracy reaching up to 95.76% on average, and exhibits competitive capabilities in other deepfake detection tasks such as attribution, and localisation compared to SOTA audio understanding models. Data and codes are provided in supplementary materials.",
      "pdf_url": "https://arxiv.org/pdf/2512.08403v2",
      "categories": [
        "cs.SD"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)"
      ]
    },
    {
      "arxiv_id": "2512.08320v1",
      "title": "Developing a Strong CPS Defender: An Evolutionary Approach",
      "authors": [
        "Qingyuan Hu",
        "Christopher M. Poskitt",
        "Jun Sun",
        "Yuqi Chen"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T07:33:26Z",
      "summary": "Cyber-physical systems (CPSs) are used extensively in critical infrastructure, underscoring the need for anomaly detection systems that are able to catch even the most motivated attackers. Traditional anomaly detection techniques typically do `one-off' training on datasets crafted by experts or generated by fuzzers, potentially limiting their ability to generalize to unseen and more subtle attack strategies. Stopping at this point misses a key opportunity: a defender can actively challenge the attacker to find more nuanced attacks, which in turn can lead to more effective detection capabilities. Building on this concept, we propose Evo-Defender, an evolutionary framework that iteratively strengthens CPS defenses through a dynamic attacker-defender interaction. Evo-Defender includes a smart attacker that employs guided fuzzing to explore diverse, non-redundant attack strategies, while the self-evolving defender uses incremental learning to adapt to new attack patterns. We implement Evo-Defender on two realistic CPS testbeds: the Tennessee Eastman process and a Robotic Arm Assembly Workstation, injecting over 600 attack scenarios. In end-to-end attack detection experiments, Evo-Defender achieves up to 2.7% higher performance than state-of-the-art baselines on unseen scenarios, while utilizing training data more efficiently for faster and more robust detection.",
      "pdf_url": "https://arxiv.org/pdf/2512.08320v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.07033v1",
      "title": "Managed TLS Under Migration: Authentication Authority Across CDN and Hosting Transitions",
      "authors": [
        "Daniyal Ganiuly",
        "Nurzhau Bolatbek",
        "Assel Smaiyl"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T22:52:52Z",
      "summary": "Managed TLS has become a common approach for deploying HTTPS, with platforms generating and storing private keys and automating certificate issuance on behalf of domain operators. This model simplifies operational management but shifts control of authentication material from the domain owner to the platform. The implications of this shift during provider transitions remain insufficiently examined. This study investigates how managed TLS platforms behave when a domain is moved away from the platform that originally issued and stored its certificate. A controlled measurement environment was used to monitor multiple platforms after migration. Each platform was observed for the full remaining lifetime of the certificate that had been active during delegation. The measurements show that platforms continue to serve the same certificate until it expires, even after DNS resolvers direct traffic toward new infrastructure. No platform revoked, replaced, or retired the certificate, and no new certificate was issued after delegation ended. Direct connections to the previous platform continued to complete TLS handshakes with the stale certificate, which confirms that authentication capability persisted independently of DNS state. These findings indicate that authentication authority remains with the previous platform for the entire lifetime of certificates issued during the delegation period. The gap between DNS control and control of authentication material introduces a window in which multiple environments can authenticate the same domain. As managed TLS adoption grows, clearer mechanisms for key retirement and certificate invalidation are needed to ensure that the authentication authority follows operational authority during transitions.",
      "pdf_url": "https://arxiv.org/pdf/2512.07033v1",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.06467v1",
      "title": "Formalisation of Security for Federated Learning with DP and Attacker Advantage in IIIf for Satellite Swarms -- Extended Version",
      "authors": [
        "Florian Kammüller"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T15:09:52Z",
      "summary": "In distributed applications, like swarms of satellites, machine learning can be efficiently applied even on small devices by using Federated Learning (FL). This allows to reduce the learning complexity by transmitting only updates to the general model in the server in the form of differences in stochastic gradient descent. FL naturally supports differential privacy but new attacks, so called Data Leakage from Gradient (DLG) have been discovered recently. There has been work on defenses against DLG but there is a lack of foundation and rigorous evaluation of their security. In the current work, we extend existing work on a formal notion of Differential Privacy for Federated Learning distributed dynamic systems and relate it to the notion of the attacker advantage. This formalisation is carried out within the Isabelle Insider and Infrastructure framework (IIIf) allowing the machine supported verification of theory and applications within the proof assistant Isabelle. Satellite swarm systems are used as a motivating use case but also as a validation case study.",
      "pdf_url": "https://arxiv.org/pdf/2512.06467v1",
      "categories": [
        "cs.CR",
        "cs.LO"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.06155v1",
      "title": "Sift or Get Off the PoC: Applying Information Retrieval to Vulnerability Research with SiftRank",
      "authors": [
        "Caleb Gross"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T21:09:32Z",
      "summary": "Security research is fundamentally a problem of resource constraint and consequent prioritization. There is simply too much attack surface and too little time and energy to spend analyzing it all. The most effective security researchers are often those who are most skilled at intuitively deciding which part of an expansive attack surface to investigate. We demonstrate that this problem of selecting the most promising option from among many possibilities can be reframed as an information retrieval problem, and solved using document ranking techniques with LLMs performing the heavy lifting as general-purpose rankers. We present SiftRank, a ranking algorithm achieving O(n) complexity through three key mechanisms: listwise ranking using an LLM to order documents in small batches of approximately 10 items at a time; inflection-based convergence detection that adaptively terminates ranking when score distributions have stabilized; and iterative refinement that progressively focuses ranking effort on the most relevant documents. Unlike existing reranking approaches that require a separate first-stage retrieval step to narrow datasets to approximately 100 candidates, SiftRank operates directly on thousands of items, with each document evaluated across multiple randomized batches to mitigate inconsistent judgments by an LLM. We demonstrate practical effectiveness on N-day vulnerability analysis, successfully identifying a vulnerability-fixing function among 2,197 changed functions in a stripped binary firmware patch within 99 seconds at an inference cost of $0.82. Our approach enables scalable security prioritization for problems that are generally constrained by manual analysis, requiring only standard LLM API access without specialized infrastructure, embedding, or domain-specific fine-tuning. An open-source implementation of SiftRank may be found at https://github.com/noperator/siftrank.",
      "pdf_url": "https://arxiv.org/pdf/2512.06155v1",
      "categories": [
        "cs.CR",
        "cs.IR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.05551v1",
      "title": "Automated Code Review Assignments: An Alternative Perspective of Code Ownership on GitHub",
      "authors": [
        "Jai Lal Lulla",
        "Raula Gaikovina Kula",
        "Christoph Treude"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T09:14:22Z",
      "summary": "Code ownership is central to ensuring accountability and maintaining quality in large-scale software development. Yet, as external threats such as software supply chain attacks on project health and quality assurance increase, mechanisms for assigning and enforcing responsibility have become increasingly critical. In 2017, GitHub introduced the CODEOWNERS feature, which automatically designates reviewers for specific files to strengthen accountability and protect critical parts of the codebase. Despite its potential, little is known about how CODEOWNERS is actually adopted and practiced. We present the first large-scale empirical study of CODEOWNERS usage across over 844,000 pull requests with 1.9 million comments and over 2 million reviews. We identify 10,287 code owners to track their review activities. Results indicate that codeowners tend to adhere the rules specified in the CODEOWNERS file, exhibit similar collaborative behaviours to traditional metrics of ownership, but tend to contribute to a smoother and faster PR workflow over time. Finally, using regression discontinuity design (RDD) analysis, we find that repositories adopting CODEOWNERS experience shifts in review dynamics, as ownership redistributes review responsibilities away from core developers. Our results position CODEOWNERS as a promising yet underutilized mechanism for improving software governance and resilience. We discuss how projects can leverage this alternative ownership method as a perspective to enhance security, accountability, and workflow efficiency in open-source development.",
      "pdf_url": "https://arxiv.org/pdf/2512.05551v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.04855v1",
      "title": "A Novel Trust-Based DDoS Cyberattack Detection Model for Smart Business Environments",
      "authors": [
        "Oghenetejiri Okporokpo",
        "Funminiyi Olajide",
        "Nemitari Ajienka",
        "Xiaoqi Ma"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T14:37:55Z",
      "summary": "As the frequency and complexity of Distributed Denial-of-Service (DDoS) attacks continue to increase, the level of threats posed to Smart Internet of Things (SIoT) business environments have also increased. These environments generally have several interconnected SIoT systems and devices that are integral to daily operations, usually depending on cloud infrastructure and real-time data analytics, which require continuous availability and secure data exchange. Conventional detection mechanisms, while useful in static or traditional network environments, often are inadequate in responding to the needs of these dynamic and diverse SIoT networks. In this paper, we introduce a novel trust-based DDoS detection model tailored to meet the unique requirements of smart business environments. The proposed model incorporates a trust evaluation engine that continuously monitors node behaviour, calculating trust scores based on packet delivery ratio, response time, and anomaly detection. These trust metrics are then aggregated by a central trust-based repository that uses inherent trust values to identify traffic patterns indicative of DDoS attacks. By integrating both trust scores and central trust-based outputs, the trust calculation is enhanced, ensuring that threats are accurately identified and addressed in real-time. The model demonstrated a significant improvement in detection accuracy, and a low false-positive rate with enhanced scalability and adaptability under TCP SYN, Ping Flood, and UDP Flood attacks. The results show that a trust-based approach provides an effective, lightweight alternative for securing resource-constrained business IoT environments.",
      "pdf_url": "https://arxiv.org/pdf/2512.04855v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.20715v1",
      "title": "SoK: Speedy Secure Finality",
      "authors": [
        "Yash Saraswat",
        "Abhimanyu Nag"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T19:25:02Z",
      "summary": "While Ethereum has successfully achieved dynamic availability together with safety, a fundamental delay remains between transaction execution and immutable finality. In Ethereum's current Gasper protocol, this latency is on the order of 15 minutes, exposing the network to ex ante reorganization attacks, enabling MEV extraction, and limiting the efficiency of economic settlement. These limitations have motivated a growing body of work on Speedy Secure Finality (SSF), which aims to minimize confirmation latency without weakening formal security guarantees.   This paper surveys the state of the art in fast finality protocol design. We introduce the core theoretical primitives underlying this space, including reorganization resilience and the generalized sleepy model, and trace their development from Goldfish to RLMD-GHOST. We then analyze the communication and aggregation bottlenecks faced by single-slot finality protocols in large validator settings. Finally, we survey the 3-slot finality (3SF) protocol as a practical synthesis that balances fast finality with the engineering constraints of the Ethereum network.",
      "pdf_url": "https://arxiv.org/pdf/2512.20715v1",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.20705v1",
      "title": "Anota: Identifying Business Logic Vulnerabilities via Annotation-Based Sanitization",
      "authors": [
        "Meng Wang",
        "Philipp Görz",
        "Joschua Schilling",
        "Keno Hassler",
        "Liwei Guo",
        "Thorsten Holz",
        "Ali Abbasi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T19:08:56Z",
      "summary": "Detecting business logic vulnerabilities is a critical challenge in software security. These flaws come from mistakes in an application's design or implementation and allow attackers to trigger unintended application behavior. Traditional fuzzing sanitizers for dynamic analysis excel at finding vulnerabilities related to memory safety violations but largely fail to detect business logic vulnerabilities, as these flaws require understanding application-specific semantic context. Recent attempts to infer this context, due to their reliance on heuristics and non-portable language features, are inherently brittle and incomplete. As business logic vulnerabilities constitute a majority (27/40) of the most dangerous software weaknesses in practice, this is a worrying blind spot of existing tools. In this paper, we tackle this challenge with ANOTA, a novel human-in-the-loop sanitizer framework. ANOTA introduces a lightweight, user-friendly annotation system that enables users to directly encode their domain-specific knowledge as lightweight annotations that define an application's intended behavior. A runtime execution monitor then observes program behavior, comparing it against the policies defined by the annotations, thereby identifying deviations that indicate vulnerabilities. To evaluate the effectiveness of ANOTA, we combine ANOTA with a state-of-the-art fuzzer and compare it against other popular bug finding methods compatible with the same targets. The results show that ANOTA+FUZZER outperforms them in terms of effectiveness. More specifically, ANOTA+FUZZER can successfully reproduce 43 known vulnerabilities, and discovered 22 previously unknown vulnerabilities (17 CVEs assigned) during the evaluation. These results demonstrate that ANOTA provides a practical and effective approach for uncovering complex business logic flaws often missed by traditional security testing techniques.",
      "pdf_url": "https://arxiv.org/pdf/2512.20705v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.20234v1",
      "title": "Achieving Flexible and Secure Authentication with Strong Privacy in Decentralized Networks",
      "authors": [
        "Bin Xie",
        "Rui Song",
        "Xuyuan Cai"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T10:49:05Z",
      "summary": "Anonymous credentials (ACs) are a crucial cryptographic tool for privacy-preserving authentication in decentralized networks, allowing holders to prove eligibility without revealing their identity. However, a major limitation of standard ACs is the disclosure of the issuer's identity, which can leak sensitive contextual information about the holder. Issuer-hiding ACs address this by making a credential's origin indistinguishable among a set of approved issuers. Despite this advancement, existing solutions suffer from practical limitations that hinder their deployment in decentralized environments: unflexible credential models that restrict issuer and holder autonomy, flawed revocation mechanisms that compromise security, and weak attribute hiding that fails to meet data minimization principles. This paper introduces a new scheme called IRAC to overcome these challenges. We propose a flexible credential model that employs vector commitments with a padding strategy to unify credentials from heterogeneous issuers, enabling privacy-preserving authentication without enforcing a global static attribute set or verifier-defined policies. Furthermore, we design a secure decentralized revocation mechanism where holders prove non-revocation by demonstrating their credential's hash lies within a gap in the issuer's sorted revocation list, effectively decoupling revocation checks from verifier policies while maintaining issuer anonymity. IRAC also strengthens attribute hiding by utilizing zk-SNARKs and vector commitments, allowing holders to prove statements about their attributes without disclosing the attributes themselves or the credential structure. Security analysis and performance evaluations demonstrate its practical feasibility for decentralized networks, where presenting a credential can be finished in 1s.",
      "pdf_url": "https://arxiv.org/pdf/2512.20234v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.19606v1",
      "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
      "authors": [
        "George Karfakis",
        "Faraz Tahmasebi",
        "Binglu Chen",
        "Lime Yao",
        "Saptarshi Mitra",
        "Tianyue Pan",
        "Hyoukjun Kwon",
        "Puneet Gupta"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T17:42:51Z",
      "summary": "RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.   Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.",
      "pdf_url": "https://arxiv.org/pdf/2512.19606v1",
      "categories": [
        "cs.PF",
        "cs.DC"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)"
      ]
    },
    {
      "arxiv_id": "2512.19189v1",
      "title": "Configuration Work: Four Consequences of LLMs-in-use",
      "authors": [
        "Gabriel Alcaras",
        "Donato Ricci"
      ],
      "affiliations": [
        "médialab",
        "médialab"
      ],
      "year": 2025,
      "published": "2025-12-22T09:27:05Z",
      "summary": "This article examines what it means to use Large Language Models in everyday work. Drawing on a seven-month longitudinal qualitative study, we argue that LLMs do not straightforwardly automate or augment tasks. We propose the concept of configuration work to describe the labor through which workers make a generic system usable for a specific professional task. Configuration work materializes in four intertwined consequences. First, workers must discretize their activity, breaking it into units that the system can process. Second, operating the system generates cluttering, as prompting, evaluating, and correcting responses add scattered layers of work that get in the way of existing routines. Third, users gradually attune their practices and expectations to the machine's generic rigidity, making sense of the system's limits and finding space for it within their practices. Fourth, as LLMs absorb repetitive tasks, they desaturate the texture of work, shifting activity toward logistical manipulation of outputs and away from forms of engagement that sustain a sense of accomplishment. Taken together, these consequences suggest that LLMs reshape work through the individualized labor required to configure a universal, task-agnostic system within situated professional ecologies.",
      "pdf_url": "https://arxiv.org/pdf/2512.19189v1",
      "categories": [
        "cs.CY"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Title keyword: 'llm' (+8)"
      ]
    },
    {
      "arxiv_id": "2512.20860v1",
      "title": "pokiSEC: A Multi-Architecture, Containerized Ephemeral Malware Detonation Sandbox",
      "authors": [
        "Alejandro Avina",
        "Yashas Hariprasad",
        "Naveen Kumar Chaudhary"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T00:38:40Z",
      "summary": "Dynamic malware analysis requires executing untrusted binaries inside strongly isolated, rapidly resettable environments. In practice, many detonation workflows remain tied to heavyweight hypervisors or dedicated bare-metal labs, limiting portability and automation. This challenge has intensified with the adoption of ARM64 developer hardware (e.g., Apple Silicon), where common open-source sandbox recipes and pre-built environments frequently assume x86_64 hosts and do not translate cleanly across architectures. This paper presents pokiSEC, a lightweight, ephemeral malware detonation sandbox that packages the full virtualization and access stack inside a Docker container. pokiSEC integrates QEMU with hardware acceleration (KVM when available) and exposes a browser-based workflow that supports bring-your-own Windows disk images. The key contribution is a Universal Entrypoint that performs runtime host-architecture detection and selects validated hypervisor configurations (machine types, acceleration modes, and device profiles), enabling a single container image and codebase to launch Windows guests on both ARM64 and x86_64 hosts. We validate pokiSEC on Apple Silicon (ARM64) and Ubuntu (AMD64), demonstrating interactive performance suitable for analyst workflows and consistent teardown semantics via ephemeral container lifecycles.",
      "pdf_url": "https://arxiv.org/pdf/2512.20860v1",
      "categories": [
        "cs.CR",
        "cs.OS"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.20535v1",
      "title": "ARBITER: AI-Driven Filtering for Role-Based Access Control",
      "authors": [
        "Michele Lorenzo",
        "Idilio Drago",
        "Dario Salvadori",
        "Fabio Romolo Vayr"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T17:25:51Z",
      "summary": "Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \\our, a system designed to provide RBAC in RAG systems. \\our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \\our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\\% accuracy and 89\\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.",
      "pdf_url": "https://arxiv.org/pdf/2512.20535v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.15815v1",
      "title": "Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive",
      "authors": [
        "Valeria Granata",
        "Francois Liot",
        "Xing Wang",
        "Steen Lysgaard",
        "Ivano E. Castelli",
        "Tejs Vegge",
        "Nicola Marzari",
        "Giovanni Pizzi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T14:15:10Z",
      "summary": "Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).",
      "pdf_url": "https://arxiv.org/pdf/2512.15815v1",
      "categories": [
        "cs.DB",
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.05707v1",
      "title": "Evaluating Concept Filtering Defenses against Child Sexual Abuse Material Generation by Text-to-Image Models",
      "authors": [
        "Ana-Maria Cretu",
        "Klim Kireev",
        "Amro Abdalla",
        "Wisdom Obinna",
        "Raphael Meier",
        "Sarah Adel Bargal",
        "Elissa M. Redmiles",
        "Carmela Troncoso"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T13:34:05Z",
      "summary": "We evaluate the effectiveness of child filtering to prevent the misuse of text-to-image (T2I) models to create child sexual abuse material (CSAM). First, we capture the complexity of preventing CSAM generation using a game-based security definition. Second, we show that current detection methods cannot remove all children from a dataset. Third, using an ethical proxy for CSAM (a child wearing glasses, hereafter, CWG), we show that even when only a small percentage of child images are left in the training dataset, there exist prompting strategies that generate CWG from a child-filtered T2I model using only a few more queries than when the model is trained on the unfiltered data. Fine-tuning the filtered model on child images further reduces the additional query overhead. We also show that reintroducing a concept is possible via fine-tuning even if filtering is perfect. Our results demonstrate that current filtering methods offer limited protection to closed-weight models and no protection to open-weight models, while reducing the generality of the model by hindering the generation of child-related concepts or changing their representation. We conclude by outlining challenges in conducting evaluations that establish robust evidence on the impact of AI safety mitigations for CSAM.",
      "pdf_url": "https://arxiv.org/pdf/2512.05707v1",
      "categories": [
        "cs.CR"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CR (+8)"
      ]
    },
    {
      "arxiv_id": "2512.05358v1",
      "title": "BGPFuzz: Automated Configuration Fuzzing of the Border Gateway Protocol",
      "authors": [
        "Chenlu Zhang",
        "Amirmohammad Pasdar",
        "Van-Thuan Pham"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T01:53:14Z",
      "summary": "Telecommunications networks rely on configurations to define routing behavior, especially in the Border Gateway Protocol (BGP), where misconfigurations can lead to severe outages and security breaches, as demonstrated by the 2021 Facebook outage. Unlike existing approaches that rely on synthesis or verification, our work offers a cost-effective method for identifying misconfigurations resulting from BGP's inherent complexity or vendor-specific implementations. We present BGPFuzz, a structure-aware and stateful fuzzing framework that systematically mutates BGP configurations and evaluates their effects in virtualized network. Without requiring predefined correctness properties as in static analysis, BGPFuzz detects anomalies through runtime oracles that capture practical symptoms such as session resets, blackholing, and traffic redirection. Our experiments show that BGPFuzz can reliably reproduce and detect known failures, including max-prefix violations and sub-prefix hijacks.",
      "pdf_url": "https://arxiv.org/pdf/2512.05358v1",
      "categories": [
        "cs.SE"
      ],
      "relevance_score": 28,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.SE (+8)"
      ]
    },
    {
      "arxiv_id": "2512.18239v1",
      "title": "Emergent Learner Agency in Implicit Human-AI Collaboration: How AI Personas Reshape Creative-Regulatory Interaction",
      "authors": [
        "Yueqiao Jin",
        "Roberto Martinez-Maldonado",
        "Dragan Gašević",
        "Lixiang Yan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T06:40:44Z",
      "summary": "Generative AI is increasingly embedded in collaborative learning, yet little is known about how AI personas shape learner agency when AI teammates are present but not disclosed. This mechanism study examines how supportive and contrarian AI personas reconfigure emergent learner agency, discourse patterns, and experiences in implicit human-AI creative collaboration. A total of 224 university students were randomly assigned to 97 online triads in one of three conditions: human-only control, hybrid teams with a supportive AI, or hybrid teams with a contrarian AI. Participants completed an individual-group-individual movie-plot writing task; the 10-minute group chat was coded using a creative-regulatory framework. We combined transition network analysis, theory-driven sequential pattern mining, and Gaussian mixture clustering to model structural, temporal, and profile-level manifestations of agency, and linked these to cognitive load, psychological safety, teamwork satisfaction, and embedding-based creative performance. Contrarian AI produced challenge- and reflection-rich discourse structures and motifs indicating productive friction, whereas supportive AI fostered agreement-centred trajectories and smoother convergence. Clustering showed AI agents concentrated in challenger profiles, with reflective regulation uniquely human. While no systematic differences emerged in cognitive load or creative gains, contrarian AI consistently reduced teamwork satisfaction and psychological safety. The findings reveal a design tension between leveraging cognitive conflict and maintaining affective safety and ownership in hybrid human-AI teams.",
      "pdf_url": "https://arxiv.org/pdf/2512.18239v1",
      "categories": [
        "cs.HC"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2512.13260v1",
      "title": "From Educational Analytics to AI Governance: Transferable Lessons from Complex Systems Interventions",
      "authors": [
        "Hugo Roger Paz"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-15T12:16:57Z",
      "summary": "Both student retention in higher education and artificial intelligence governance face a common structural challenge: the application of linear regulatory frameworks to complex adaptive systems. Risk-based approaches dominate both domains, yet systematically fail because they assume stable causal pathways, predictable actor responses, and controllable system boundaries. This paper extracts transferable methodological principles from CAPIRE (Curriculum, Archetypes, Policies, Interventions & Research Environment), an empirically validated framework for educational analytics that treats student dropout as an emergent property of curricular structures, institutional rules, and macroeconomic shocks. Drawing on longitudinal data from engineering programmes and causal inference methods, CAPIRE demonstrates that well-intentioned interventions routinely generate unintended consequences when system complexity is ignored. We argue that five core principles developed within CAPIRE - temporal observation discipline, structural mapping over categorical classification, archetype-based heterogeneity analysis, causal mechanism identification, and simulation-based policy design - transfer directly to the challenge of governing AI systems. The isomorphism is not merely analogical: both domains exhibit non-linearity, emergence, feedback loops, strategic adaptation, and path dependence. We propose Complex Systems AI Governance (CSAIG) as an integrated framework that operationalises these principles for regulatory design, shifting the central question from \"how risky is this AI system?\" to \"how does this intervention reshape system dynamics?\" The contribution is twofold: demonstrating that empirical lessons from one complex systems domain can accelerate governance design in another, and offering a concrete methodological architecture for complexity-aware AI regulation.",
      "pdf_url": "https://arxiv.org/pdf/2512.13260v1",
      "categories": [
        "cs.CY"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2512.12924v1",
      "title": "Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forward Validation Framework for Market Microstructure Signals",
      "authors": [
        "Gagan Deep",
        "Akash Deep",
        "William Lamptey"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-15T02:20:42Z",
      "summary": "We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdown -2.76%) and market-neutral characteristics (beta = 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020-2024) while underperforming in stable markets (-0.16%, 2015-2019). We report statistically insignificant aggregate results (p-value 0.34) to demonstrate a reproducible, honest validation protocol that prioritizes interpretability and extends naturally to advanced hypothesis generators, including large language models. The key empirical finding reveals that daily OHLCV-based microstructure signals require elevated information arrival and trading activity to function effectively. The framework provides complete mathematical specifications and open-source implementation, establishing a template for rigorous trading system evaluation that addresses the reproducibility crisis in quantitative finance research. For researchers, practitioners, and regulators, this work demonstrates that interpretable algorithmic trading strategies can be rigorously validated without sacrificing transparency or regulatory compliance.",
      "pdf_url": "https://arxiv.org/pdf/2512.12924v1",
      "categories": [
        "q-fin.TR",
        "q-fin.CP",
        "stat.ML"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2512.10913v1",
      "title": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies",
      "authors": [
        "Mohammad Rezoanul Hoque",
        "Md Meftahul Ferdaus",
        "M. Kabir Hassan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T18:42:19Z",
      "summary": "Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making.",
      "pdf_url": "https://arxiv.org/pdf/2512.10913v1",
      "categories": [
        "q-fin.CP"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06597v2",
      "title": "When Does Regulation by Insurance Work? The Case of Frontier AI",
      "authors": [
        "Cristian Trout"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T23:45:06Z",
      "summary": "No one doubts the utility of insurance for its ability to spread risk or streamline claims management; much debated is when and how insurance uptake can improve welfare by reducing harm, despite moral hazard. Proponents and dissenters of \"regulation by insurance\" have now documented a number of cases of insurers succeeding or failing to have such a net regulatory effect (in contrast with a net hazard effect). Collecting these examples together and drawing on an extensive economics literature, this Article develops a principled framework for evaluating insurance uptake's effect in a given context. The presence of certain distortions - including judgment-proofness, competitive dynamics, and behavioral biases - creates potential for a net regulatory effect. How much of that potential gets realized then depends on the type of policyholder, type of risk, type of insurer, and the structure of the insurance market. The analysis suggests regulation by insurance can be particularly effective for catastrophic non-product accidents where market mechanisms provide insufficient discipline and psychological biases are strongest. As a demonstration, the framework is applied to the frontier AI industry, revealing significant potential for a net regulatory effect but also the need for policy intervention to realize that potential. One option is a carefully designed mandate that encourages forming a specialized insurer or mutual, focuses on catastrophic rather than routine risks, and bars pure captives.",
      "pdf_url": "https://arxiv.org/pdf/2512.06597v2",
      "categories": [
        "cs.CY"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2512.06515v1",
      "title": "ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models",
      "authors": [
        "Somnath Banerjee",
        "Sayan Layek",
        "Sayantan Adak",
        "Mykola Pechenizkiy",
        "Animesh Mukherjee",
        "Rima Hazra"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-06T18:00:37Z",
      "summary": "Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned \"harm vector\" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.",
      "pdf_url": "https://arxiv.org/pdf/2512.06515v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.08978v1",
      "title": "Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT",
      "authors": [
        "Ruud Huijts",
        "Koen Suilen"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-04T12:41:32Z",
      "summary": "To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.   Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.   Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.   The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.   The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it.",
      "pdf_url": "https://arxiv.org/pdf/2512.08978v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.00428v1",
      "title": "Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana",
      "authors": [
        "Jiachuan Peng",
        "Kyle Lam",
        "Jianing Qiu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-29T10:05:44Z",
      "summary": "We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.",
      "pdf_url": "https://arxiv.org/pdf/2512.00428v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2511.22211v1",
      "title": "AI Regulation in Telecommunications: A Cross-Jurisdictional Legal Study",
      "authors": [
        "Avinash Agarwal",
        "Peeyush Agarwal",
        "Manisha J. Nene"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-27T08:30:12Z",
      "summary": "As Artificial Intelligence (AI) becomes increasingly embedded in critical digital infrastructure, including telecommunications, its integration introduces new risks that existing regulatory frameworks are ill-prepared to address. This paper conducts a comparative legal study of policy instruments across ten countries, examining how telecom, cybersecurity, data protection, and AI laws approach AI-related risks in infrastructure. The study finds that regulatory responses remain siloed, with minimal coordination across these domains. Most frameworks still prioritize traditional cybersecurity and data protection concerns, offering limited recognition of AI-specific vulnerabilities such as model drift, opaque decision-making, and algorithmic bias. Telecommunications regulations, in particular, exhibit little integration of AI considerations, despite AI systems increasingly supporting critical network operations. The paper identifies a governance gap where oversight remains fragmented and reactive, while AI reshapes the digital infrastructure. It provides a foundation for more coherent and anticipatory regulatory strategies spanning technological and institutional boundaries.",
      "pdf_url": "https://arxiv.org/pdf/2511.22211v1",
      "categories": [
        "cs.CY"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2511.21135v1",
      "title": "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation",
      "authors": [
        "Ziyi Chen",
        "Yingnan Guo",
        "Zedong Chu",
        "Minghua Luo",
        "Yanfen Shen",
        "Mingchao Sun",
        "Junjun Hu",
        "Shichao Xie",
        "Kuan Yang",
        "Pei Shi",
        "Zhining Gu",
        "Lu Liu",
        "Honglin Han",
        "Xiaolong Wu",
        "Mu Xu",
        "Yu Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-26T07:36:01Z",
      "summary": "Embodied navigation that adheres to social norms remains an open research challenge. Our \\textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical \"brain-action\" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/",
      "pdf_url": "https://arxiv.org/pdf/2511.21135v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.17864v2",
      "title": "Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN",
      "authors": [
        "Balram Singh",
        "Ram Prakash Sharma",
        "Somnath Dey"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T18:11:15Z",
      "summary": "Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.",
      "pdf_url": "https://arxiv.org/pdf/2512.17864v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.21201v1",
      "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
      "authors": [
        "Yu He",
        "Da Huang",
        "Zhenyang Liu",
        "Zixiao Gu",
        "Qiang Sun",
        "Guangnan Ye",
        "Yanwei Fu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T14:28:17Z",
      "summary": "Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.",
      "pdf_url": "https://arxiv.org/pdf/2512.21201v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20589v1",
      "title": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
      "authors": [
        "İbrahim Oğuz Çetinkaya",
        "Sajad Khodadadian",
        "Taylan G. Topçu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T18:36:07Z",
      "summary": "As systems engineering (SE) objectives evolve from design and operation of monolithic systems to complex System of Systems (SoS), the discipline of Mission Engineering (ME) has emerged which is increasingly being accepted as a new line of thinking for the SE community. Moreover, mission environments are uncertain, dynamic, and mission outcomes are a direct function of how the mission assets will interact with this environment. This proves static architectures brittle and calls for analytically rigorous approaches for ME. To that end, this paper proposes an intelligent mission coordination methodology that integrates digital mission models with Reinforcement Learning (RL), that specifically addresses the need for adaptive task allocation and reconfiguration. More specifically, we are leveraging a Digital Engineering (DE) based infrastructure that is composed of a high-fidelity digital mission model and agent-based simulation; and then we formulate the mission tactics management problem as a Markov Decision Process (MDP), and employ an RL agent trained via Proximal Policy Optimization. By leveraging the simulation as a sandbox, we map the system states to actions, refining the policy based on realized mission outcomes. The utility of the RL-based intelligent mission coordinator is demonstrated through an aerial firefighting case study. Our findings indicate that the RL-based intelligent mission coordinator not only surpasses baseline performance but also significantly reduces the variability in mission performance. Thus, this study serves as a proof of concept demonstrating that DE-enabled mission simulations combined with advanced analytical tools offer a mission-agnostic framework for improving ME practice; which can be extended to more complicated fleet design and selection problems in the future from a mission-first perspective.",
      "pdf_url": "https://arxiv.org/pdf/2512.20589v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "eess.SY",
        "math.OC"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20188v1",
      "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
      "authors": [
        "Teqiang Zou",
        "Hongliang Zeng",
        "Yuxuan Nong",
        "Yifan Li",
        "Kehui Liu",
        "Haotian Yang",
        "Xinyang Ling",
        "Xin Li",
        "Lianyang Ma"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T09:28:20Z",
      "summary": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.",
      "pdf_url": "https://arxiv.org/pdf/2512.20188v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20173v1",
      "title": "Offline Safe Policy Optimization From Heterogeneous Feedback",
      "authors": [
        "Ze Gong",
        "Pradeep Varakantham",
        "Akshat Kumar"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T09:07:53Z",
      "summary": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.",
      "pdf_url": "https://arxiv.org/pdf/2512.20173v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.20092v1",
      "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
      "authors": [
        "Yiming Du",
        "Baojun Wang",
        "Yifan Xiang",
        "Zhaowei Wang",
        "Wenyu Huang",
        "Boyang Xue",
        "Bin Liang",
        "Xingshan Zeng",
        "Fei Mi",
        "Haoli Bai",
        "Lifeng Shang",
        "Jeff Z. Pan",
        "Yuxin Jiang",
        "Kam-Fai Wong"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T06:37:29Z",
      "summary": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
      "pdf_url": "https://arxiv.org/pdf/2512.20092v1",
      "categories": [
        "cs.CL"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.CL (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19317v1",
      "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
      "authors": [
        "A. A. Gde Yogi Pramana",
        "Jason Ray",
        "Anthony Jaya",
        "Michael Wijaya"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T12:07:33Z",
      "summary": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19317v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.13750v1",
      "title": "The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs",
      "authors": [
        "Ezieddin Elmahjub"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-15T05:39:30Z",
      "summary": "Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.",
      "pdf_url": "https://arxiv.org/pdf/2512.13750v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11771v1",
      "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
      "authors": [
        "Kai Yao",
        "Marc Juarez"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T18:33:14Z",
      "summary": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.",
      "pdf_url": "https://arxiv.org/pdf/2512.11771v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.11893v1",
      "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI",
      "authors": [
        "Haocheng Lin"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T20:25:24Z",
      "summary": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.",
      "pdf_url": "https://arxiv.org/pdf/2512.11893v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.08026v1",
      "title": "Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching",
      "authors": [
        "Caroline N. Leach",
        "Mitchell A. Klusty",
        "Samuel E. Armstrong",
        "Justine C. Pickarski",
        "Kristen L. Hankins",
        "Emily B. Collier",
        "Maya Shah",
        "Aaron D. Mullen",
        "V. K. Cody Bumgardner"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T20:35:51Z",
      "summary": "Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.",
      "pdf_url": "https://arxiv.org/pdf/2512.08026v1",
      "categories": [
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18384v1",
      "title": "Datasets for machine learning and for assessing the intelligence level of automatic patent search systems",
      "authors": [
        "Boris Genin",
        "Alexander Gorbunov",
        "Dmitry Zolkin",
        "Igor Nekrasov"
      ],
      "affiliations": [
        "Federal Institute of Industrial Property, Berezhkovskaya nab. 30-1, Moscow, Russian Federation",
        "Federal Institute of Industrial Property, Berezhkovskaya nab. 30-1, Moscow, Russian Federation",
        "Federal Institute of Industrial Property, Berezhkovskaya nab. 30-1, Moscow, Russian Federation",
        "Federal Institute of Industrial Property, Berezhkovskaya nab. 30-1, Moscow, Russian Federation"
      ],
      "year": 2025,
      "published": "2025-12-20T14:51:57Z",
      "summary": "The key to success in automating prior art search in patent research using artificial intelligence lies in developing large datasets for machine learning and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for machine learning, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for machine learning. To evaluate machine learning outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search.",
      "pdf_url": "https://arxiv.org/pdf/2512.18384v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.AI (+6)"
      ]
    },
    {
      "arxiv_id": "2512.18269v1",
      "title": "Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System",
      "authors": [
        "Se-Young Jang",
        "Su-Yeon Yoon",
        "Jae-Woong Jung",
        "Dong-Hun Lee",
        "Seong-Hun Choi",
        "Soo-Kyung Jun",
        "Yu-Bin Kim",
        "Young-Seon Ju",
        "Kyounggon Kim"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T08:15:34Z",
      "summary": "With the accelerating pace of digital transformation and the widespread adoption of online platforms, both social and technical concerns regarding dark patterns-user interface designs that undermine users' ability to make informed and rational choices-have become increasingly prominent. As corporate online platforms grow more sophisticated in their design strategies, there is a pressing need for proactive and real-time detection technologies that go beyond the predominantly reactive approaches employed by regulatory authorities. In this paper, we propose a visual dark pattern detection framework that improves both detection accuracy and real-time performance. To this end, we constructed a proprietary visual object detection dataset by manually collecting 4,066 UI/UX screenshots containing dark patterns from 194 websites across six major industrial sectors in South Korea and abroad. The collected images were annotated with five representative UI components commonly associated with dark patterns: Button, Checkbox, Input Field, Pop-up, and QR Code. This dataset has been publicly released to support further research and development in the field. To enable real-time detection, this study adopted the YOLOv12x object detection model and applied transfer learning to optimize its performance for visual dark pattern recognition. Experimental results demonstrate that the proposed approach achieves a high detection accuracy of 92.8% in terms of mAP@50, while maintaining a real-time inference speed of 40.5 frames per second (FPS), confirming its effectiveness for practical deployment in online environments. Furthermore, to facilitate future research and contribute to technological advancements, the dataset constructed in this study has been made publicly available at https://github.com/B4E2/B4E2-DarkPattern-YOLO-DataSet.",
      "pdf_url": "https://arxiv.org/pdf/2512.18269v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 26,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'regulatory' (+6)"
      ]
    },
    {
      "arxiv_id": "2512.19744v1",
      "title": "DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation",
      "authors": [
        "Gustavo Coelho Haase",
        "Paulo Henrique Dourado da Silva"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T01:32:32Z",
      "summary": "We present DeepBridge, an 80K-line Python library that unifies multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. DeepBridge offers: (i) 5 validation suites (fairness with 15 metrics, robustness with weakness detection, uncertainty via conformal prediction, resilience with 5 drift types, hyperparameter sensitivity), (ii) automatic EEOC/ECOA/GDPR verification, (iii) multi-format reporting system (interactive/static HTML, PDF, JSON), (iv) HPM-KD framework for knowledge distillation with meta-learning, and (v) scalable synthetic data generation via Dask. Through 6 case studies (credit scoring, hiring, healthcare, mortgage, insurance, fraud) we demonstrate that DeepBridge: reduces validation time by 89% (17 min vs. 150 min with fragmented tools), automatically detects fairness violations with complete coverage (10/10 features vs. 2/10 from existing tools), generates audit-ready reports in minutes. HPM-KD demonstrates consistent superiority across compression ratios 2.3--7x (CIFAR100): +1.00--2.04pp vs. Direct Training (p<0.05), confirming that Knowledge Distillation is effective at larger teacher-student gaps. Usability study with 20 participants shows SUS score 87.5 (top 10%, ``excellent''), 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open-source under MIT license at https://github.com/deepbridge/deepbridge, with complete documentation at https://deepbridge.readthedocs.io",
      "pdf_url": "https://arxiv.org/pdf/2512.19744v1",
      "categories": [
        "cs.LG",
        "stat.AP"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.14980v2",
      "title": "Softly Constrained Denoisers for Diffusion Models",
      "authors": [
        "Victor M. Yeom-Song",
        "Severi Rissanen",
        "Arno Solin",
        "Samuel Kaski",
        "Mingfei Sun"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T00:35:45Z",
      "summary": "Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.",
      "pdf_url": "https://arxiv.org/pdf/2512.14980v2",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.10582v1",
      "title": "Topology-Guided Quantum GANs for Constrained Graph Generation",
      "authors": [
        "Tobias Rohe",
        "Markus Baumann",
        "Michael Poppel",
        "Gerhard Stenzel",
        "Maximilian Zorn",
        "Claudia Linnhoff-Popien"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T12:22:18Z",
      "summary": "Quantum computing (QC) promises theoretical advantages, benefiting computational problems that would not be efficiently classically simulatable. However, much of this theoretical speedup depends on the quantum circuit design solving the problem. We argue that QC literature has yet to explore more domain specific ansatz-topologies, instead of relying on generic, one-size-fits-all architectures. In this work, we show that incorporating task-specific inductive biases -- specifically geometric priors -- into quantum circuit design can enhance the performance of hybrid Quantum Generative Adversarial Networks (QuGANs) on the task of generating geometrically constrained K4 graphs. We evaluate a portfolio of entanglement topologies and loss-function designs to assess their impact on both statistical fidelity and compliance with geometric constraints, including the Triangle and Ptolemaic inequalities. Our results show that aligning circuit topology with the underlying problem structure yields substantial benefits: the Triangle-topology QuGAN achieves the highest geometric validity among quantum models and matches the performance of classical Generative Adversarial Networks (GAN). Additionally, we showcase how specific architectural choices, such as entangling gate types, variance regularization and output-scaling govern the trade-off between geometric consistency and distributional accuracy, thus emphasizing the value of structured, task-aware quantum ansatz-topologies.",
      "pdf_url": "https://arxiv.org/pdf/2512.10582v1",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.06973v2",
      "title": "Learning Robust and Correct Controllers Guided by Feasibility-Aware Signal Temporal Logic via BarrierNet",
      "authors": [
        "Shuo Liu",
        "Wenliang Liu",
        "Wei Xiao",
        "Calin A. Belta"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T19:52:27Z",
      "summary": "Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.",
      "pdf_url": "https://arxiv.org/pdf/2512.06973v2",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.03059v1",
      "title": "Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL",
      "authors": [
        "Jiaju Qi",
        "Lei Lei",
        "Thorsteinn Jonsson",
        "Dusit Niyato"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-25T20:00:02Z",
      "summary": "The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.",
      "pdf_url": "https://arxiv.org/pdf/2512.03059v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2511.17150v1",
      "title": "DiffRefiner: Coarse to Fine Trajectory Planning via Diffusion Refinement with Semantic Interaction for End to End Autonomous Driving",
      "authors": [
        "Liuhan Yin",
        "Runkun Ju",
        "Guodong Guo",
        "Erkang Cheng"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-21T11:16:00Z",
      "summary": "Unlike discriminative approaches in autonomous driving that predict a fixed set of candidate trajectories of the ego vehicle, generative methods, such as diffusion models, learn the underlying distribution of future motion, enabling more flexible trajectory prediction. However, since these methods typically rely on denoising human-crafted trajectory anchors or random noise, there remains significant room for improvement. In this paper, we propose DiffRefiner, a novel two-stage trajectory prediction framework. The first stage uses a transformer-based Proposal Decoder to generate coarse trajectory predictions by regressing from sensor inputs using predefined trajectory anchors. The second stage applies a Diffusion Refiner that iteratively denoises and refines these initial predictions. In this way, we enhance the performance of diffusion-based planning by incorporating a discriminative trajectory proposal module, which provides strong guidance for the generative refinement process. Furthermore, we design a fine-grained denoising decoder to enhance scene compliance, enabling more accurate trajectory prediction through enhanced alignment with the surrounding environment. Experimental results demonstrate that DiffRefiner achieves state-of-the-art performance, attaining 87.4 EPDMS on NAVSIM v2, and 87.1 DS along with 71.4 SR on Bench2Drive, thereby setting new records on both public benchmarks. The effectiveness of each component is validated via ablation studies as well.",
      "pdf_url": "https://arxiv.org/pdf/2511.17150v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.11298v1",
      "title": "SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks",
      "authors": [
        "Hao Zhou",
        "Suman Sourav",
        "Binbin Chen",
        "Ke Yu"
      ],
      "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "Aalborg University",
        "Singapore University of Technology and Design",
        "Beijing University of Posts and Telecommunications"
      ],
      "year": 2025,
      "published": "2025-12-12T05:47:39Z",
      "summary": "Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.",
      "pdf_url": "https://arxiv.org/pdf/2512.11298v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.21268v1",
      "title": "ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision",
      "authors": [
        "Weiqi Li",
        "Zehao Zhang",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T16:24:18Z",
      "summary": "Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.",
      "pdf_url": "https://arxiv.org/pdf/2512.21268v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20741v1",
      "title": "ASCHOPLEX encounters Dafne: a federated continuous learning project for the generalizability of the Choroid Plexus automatic segmentation",
      "authors": [
        "Valentina Visani",
        "Marco Pinamonti",
        "Valentina Sammassimo",
        "Manuela Moretto",
        "Mattia Veronese",
        "Agnese Tamanti",
        "Francesca Benedetta Pizzini",
        "Massimiliano Calabrese",
        "Marco Castellaro",
        "Francesco Santini"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T19:54:58Z",
      "summary": "The Choroid Plexus (ChP) is a highly vascularized brain structure that plays a critical role in several physiological processes. ASCHOPLEX, a deep learning-based segmentation toolbox with an integrated fine-tuning stage, provides accurate ChP delineations on non-contrast-enhanced T1-weighted MRI scans; however, its performance is hindered by inter-dataset variability. This study introduces the first federated incremental learning approach for automated ChP segmentation from 3D T1-weighted brain MRI, by integrating an enhanced version of ASCHOPLEX within the Dafne (Deep Anatomical Federated Network) framework. A comparative evaluation is conducted to assess whether federated incremental learning through Dafne improves model generalizability across heterogeneous imaging conditions, relative to the conventional fine-tuning strategy employed by standalone ASCHOPLEX. The experimental cohort comprises 2,284 subjects, including individuals with Multiple Sclerosis as well as healthy controls, collected from five independent MRI datasets. Results indicate that the fine-tuning strategy provides high performance on homogeneous data (e.g., same MRI sequence, same cohort of subjects), but limited generalizability when the data variability is high (e.g., multiple MRI sequences, multiple and new cohorts of subjects). By contrast, the federated incremental learning variant of ASCHOPLEX constitutes a robust alternative consistently achieving higher generalizability and more stable performance across diverse acquisition settings.",
      "pdf_url": "https://arxiv.org/pdf/2512.20741v1",
      "categories": [
        "eess.IV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20557v1",
      "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
      "authors": [
        "Shengchao Zhou",
        "Yuxin Chen",
        "Yuying Ge",
        "Wei Huang",
        "Jiehong Lin",
        "Ying Shan",
        "Xiaojuan Qi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T17:56:36Z",
      "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
      "pdf_url": "https://arxiv.org/pdf/2512.20557v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20513v1",
      "title": "Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow",
      "authors": [
        "Tyler Clark",
        "Christine Evers",
        "Jonathon Hare"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T17:02:17Z",
      "summary": "Recurrent off-policy deep reinforcement learning models achieve state-of-the-art performance but are often sidelined due to their high computational demands. In response, we introduce RISE (Recurrent Integration via Simplified Encodings), a novel approach that can leverage recurrent networks in any image-based off-policy RL setting without significant computational overheads via using both learnable and non-learnable encoder layers. When integrating RISE into leading non-recurrent off-policy RL algorithms, we observe a 35.6% human-normalized interquartile mean (IQM) performance improvement across the Atari benchmark. We analyze various implementation strategies to highlight the versatility and potential of our proposed framework.",
      "pdf_url": "https://arxiv.org/pdf/2512.20513v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20501v1",
      "title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
      "authors": [
        "Gorjan Radevski"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T16:46:58Z",
      "summary": "This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.   Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.   Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.   Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.   Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.   Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.   These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.",
      "pdf_url": "https://arxiv.org/pdf/2512.20501v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20479v1",
      "title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
      "authors": [
        "Yiming Zhao",
        "Yuanpeng Gao",
        "Yuxuan Luo",
        "Jiwei Duan",
        "Shisong Lin",
        "Longfei Xiong",
        "Zhouhui Lian"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T16:13:55Z",
      "summary": "AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.",
      "pdf_url": "https://arxiv.org/pdf/2512.20479v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20417v1",
      "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
      "authors": [
        "Pedro Domingos",
        "João Pereira",
        "Vasco Lopes",
        "João Neves",
        "David Semedo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T15:01:05Z",
      "summary": "Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.",
      "pdf_url": "https://arxiv.org/pdf/2512.20417v1",
      "categories": [
        "cs.CV",
        "cs.MA"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20414v1",
      "title": "Topological resolution of conical intersection seams and the coupled cluster bifurcation via mixed Hodge modules",
      "authors": [
        "Prasoon Saurabh"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T14:58:23Z",
      "summary": "The rigorous description of Conical Intersections (CIs) remains the central challenge of non-adiabatic quantum chemistry. While the ``Yarkony Seam'' -- the $(3N-8)$-dimensional manifold of degeneracy -- is well-understood geometrically, its accurate characterization by high-level electronic structure methods is plagued by numerical instabilities. Specifically, standard Coupled Cluster (CC) theory suffers from root bifurcations near Ground State CIs, rendering the ``Gold Standard'' of chemistry inapplicable where it is needed most. Here, we present \\textbf{QuMorpheus}, an open-source computational package that resolves these singularities by implementing a topological framework based on Dissipative Mixed Hodge Modules (DMHM) [P. Saurabh, arXiv:2512.19487 (2025)]. By algorithmically mapping the CC polynomial equations to a spectral sheaf, we compute the exact Monodromy ($μ$) invariants of the intersection. We demonstrate that this automated algebraic geometry approach correctly identifies the physical ground state topology in the Köhn-Tajti model and resolves the intersection seams of realistic chemical systems, including Ethylene and the Chloronium ion ($\\mathrm{H_2Cl^+}$). Furthermore, we apply QuMorpheus to the photoisomerization of Previtamin D, proving that the experimentally observed Woodward-Hoffmann selection rules are a direct consequence of a topological ``Monodromy Wall'' ($μ=1, γ=π$) rather than purely energetic barriers. This establishes a general software solution to the ``Yarkony Problem,'' enabling the robust, automated mapping of global intersection seams in complex molecular systems. The topological stability of these intersections allows for the control protocols discussed in Ref.[P. Saurabh, Submitted to Phys. Rev. X (2025)].",
      "pdf_url": "https://arxiv.org/pdf/2512.20414v1",
      "categories": [
        "physics.chem-ph",
        "math-ph",
        "physics.comp-ph",
        "quant-ph"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20374v1",
      "title": "CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images",
      "authors": [
        "Yujia Fu",
        "Zhiyu Dong",
        "Tianwen Qian",
        "Chenye Zheng",
        "Danian Ji",
        "Linhai Zhuo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:58:12Z",
      "summary": "Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.",
      "pdf_url": "https://arxiv.org/pdf/2512.20374v1",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20306v1",
      "title": "Structured Visualization Design Knowledge for Grounding Generative Reasoning and Situated Feedback",
      "authors": [
        "Péter Ferenc Gyarmati",
        "Dominik Moritz",
        "Torsten Möller",
        "Laura Koesten"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T12:17:10Z",
      "summary": "Automated visualization design navigates a tension between symbolic systems and generative models. Constraint solvers enforce structural and perceptual validity, but the rules they require are difficult to author and too rigid to capture situated design knowledge. Large language models require no formal rules and can reason about contextual nuance, but they prioritize popular conventions over empirically grounded best practices. We address this tension by proposing a cataloging scheme that structures visualization design knowledge as natural-language guidelines with semantically typed metadata. This allows experts to author knowledge that machines can query. An expert study ($N=18$) indicates that practitioners routinely adapt heuristics to situational factors such as audience and communicative intent. To capture this reasoning, guideline sections specify not only advice but also the contexts where it applies, exceptions that invalidate it, and the sources from which it derives. We demonstrate the scheme's expressiveness by cataloging 744 guidelines drawn from cognitive science, accessibility standards, data journalism, and research on rhetorical aspects of visual communication. We embed guideline sections in a vector space, opening the knowledge itself to structural analysis. This reveals conflicting advice across sources and transferable principles between domains. Rather than replacing constraint-based tools, our scheme provides what they lack: situated guidance that generative systems can retrieve to ground their reasoning, users can verify against cited sources, and experts can author as knowledge evolves.",
      "pdf_url": "https://arxiv.org/pdf/2512.20306v1",
      "categories": [
        "cs.HC"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20148v1",
      "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "authors": [
        "Robert van de Ven",
        "Trim Bresilla",
        "Bram Nelissen",
        "Ard Nieuwenhuizen",
        "Eldert J. van Henten",
        "Gert Kootstra"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T08:19:55Z",
      "summary": "Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\\leq95\\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.",
      "pdf_url": "https://arxiv.org/pdf/2512.20148v1",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20096v1",
      "title": "Information-directed sampling for bandits: a primer",
      "authors": [
        "Annika Hirling",
        "Giorgio Nicoletti",
        "Antonio Celani"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T06:49:33Z",
      "summary": "The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.",
      "pdf_url": "https://arxiv.org/pdf/2512.20096v1",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19970v1",
      "title": "Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis",
      "authors": [
        "Surya Jayakumar",
        "Kieran Sullivan",
        "John McLaughlin",
        "Christine O'Meara",
        "Indrakshi Dey"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T01:32:32Z",
      "summary": "This study introduces a novel data-driven framework and the first-ever county-scale application of Spatio-Temporal Graph Neural Networks (STGNN) to forecast composite sustainability indices from herd-level operational records. The methodology employs a novel, end-to-end pipeline utilizing a Variational Autoencoder (VAE) to augment Irish Cattle Breeding Federation (ICBF) datasets, preserving joint distributions while mitigating sparsity. A first-ever pillar-based scoring formulation is derived via Principal Component Analysis, identifying Reproductive Efficiency, Genetic Management, Herd Health, and Herd Management, to construct weighted composite indices. These indices are modelled using a novel STGNN architecture that explicitly encodes geographic dependencies and non-linear temporal dynamics to generate multi-year forecasts for 2026-2030.",
      "pdf_url": "https://arxiv.org/pdf/2512.19970v1",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19677v1",
      "title": "Detecting Coordinated Activities Through Temporal, Multiplex, and Collaborative Analysis",
      "authors": [
        "Letizia Iannucci",
        "Elisa Muratore",
        "Antonis Matakos",
        "Mikko Kivelä"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T18:53:43Z",
      "summary": "In the era of widespread online content consumption, effective detection of coordinated efforts is crucial for mitigating potential threats arising from information manipulation. Despite advances in isolating inauthentic and automated actors, the actions of individual accounts involved in influence campaigns may not stand out as anomalous if analyzed independently of the coordinated group. Given the collaborative nature of information operations, coordinated campaigns are better characterized by evidence of similar temporal behavioral patterns that extend beyond coincidental synchronicity across a group of accounts. We propose a framework to model complex coordination patterns across multiple online modalities. This framework utilizes multiplex networks to first decompose online activities into different interaction layers, and subsequently aggregate evidence of online coordination across the layers. In addition, we propose a time-aware collaboration model to capture patterns of online coordination for each modality. The proposed time-aware model builds upon the node-normalized collaboration model and accounts for repetitions of coordinated actions over different time intervals by employing an exponential decay temporal kernel. We validate our approach on multiple datasets featuring different coordinated activities. Our results demonstrate that a multiplex time-aware model excels in the identification of coordinating groups, outperforming previously proposed methods in coordinated activity detection.",
      "pdf_url": "https://arxiv.org/pdf/2512.19677v1",
      "categories": [
        "cs.SI",
        "cs.CY"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19485v1",
      "title": "Enabling Evolutionary Therapy in Metastatic Cancer Lacking Serum Biomarkers",
      "authors": [
        "Eva Molnárová",
        "Ties A. Mulders",
        "Marcela Spee-Dropková",
        "Louise M. Spekking",
        "Sepinoud Azimi",
        "Irene Grossmann",
        "Anne-Marie C. Dingemans",
        "Kateřina Staňková"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T15:40:55Z",
      "summary": "Evolutionary therapy (ET) aims to steer tumor evolution by adjusting treatment timing and dosing to control rather than eradicate tumor burden. Clinical use requires reliable monitoring of tumor dynamics to inform mathematical models that guide therapy. In cancers such as metastatic castrate-resistant prostate cancer and relapsed platinum-sensitive ovarian cancer, ET models are informed by serial serum biomarkers. For cancers lacking reliable biomarkers, such as metastatic non-small cell lung cancer (NSCLC), radiographic imaging remains the primary method for treatment response assessment, typically using RECIST 1.1 criteria. RECIST, which tracks a few lesions with one-dimensional (1D) measurements and defines progression relative to the nadir, the smallest tumor burden recorded after treatment, was not designed to support ET. It may miss early regrowth, underrepresent tumor burden, and obscure disease trends. Using a virtual NSCLC patient model, we demonstrate that lesion selection and measurement dimensionality strongly affect progression detection. Two-dimensional metrics provide modest improvement, but only 3D volumetric measurements accurately capture both tumor burden and its dynamics, which are key requirements for ET. To support ET in cancers lacking biomarkers, response assessment must evolve beyond RECIST by integrating volumetric imaging, automated segmentation, and potentially liquid biopsies, alongside redefining progression criteria to enable adaptive, patient-centered treatments.",
      "pdf_url": "https://arxiv.org/pdf/2512.19485v1",
      "categories": [
        "q-bio.PE"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19361v1",
      "title": "Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation",
      "authors": [
        "Isshaan Singh",
        "Divyansh Chawla",
        "Anshu Garg",
        "Shivin Mangal",
        "Pallavi Gupta",
        "Khushi Agarwal",
        "Nimrat Singh Khalsa",
        "Nandan Patel"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T12:59:48Z",
      "summary": "The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19361v1",
      "categories": [
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19270v1",
      "title": "Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization",
      "authors": [
        "Zhaoyang Liu",
        "Weitao Zhou",
        "Junze Wen",
        "Cheng Jing",
        "Qian Cheng",
        "Kun Jiang",
        "Diange Yang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T11:07:18Z",
      "summary": "Collecting large-scale naturalistic driving data is essential for training robust autonomous driving planners. However, real-world datasets often contain a substantial amount of repetitive and low-value samples, which lead to excessive storage costs and bring limited benefits to policy learning. To address this issue, we propose an information-theoretic data pruning method that effectively reduces the training data volume without compromising model performance. Our approach evaluates the trajectory distribution information entropy of driving data and iteratively selects high-value samples that preserve the statistical characteristics of the original dataset in a model-agnostic manner. From a theoretical perspective, we show that maximizing trajectory entropy effectively constrains the Kullback-Leibler divergence between the pruned subset and the original data distribution, thereby maintaining generalization ability. Comprehensive experiments on the NuPlan benchmark with a large-scale imitation learning framework demonstrate that the proposed method can reduce the dataset size by up to 40% while maintaining closed-loop performance. This work provides a lightweight and theoretically grounded approach for scalable data management and efficient policy learning in autonomous driving systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19270v1",
      "categories": [
        "cs.RO"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.19133v1",
      "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
      "authors": [
        "Pengxuan Yang",
        "Ben Lu",
        "Zhongpu Xia",
        "Chao Han",
        "Yinfeng Gao",
        "Teng Zhang",
        "Kun Zhan",
        "XianPeng Lang",
        "Yupeng Zheng",
        "Qichao Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T08:27:44Z",
      "summary": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).",
      "pdf_url": "https://arxiv.org/pdf/2512.19133v1",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20475v1",
      "title": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing",
      "authors": [
        "Maulana Bisyir Azhari",
        "Donghun Han",
        "Je In You",
        "Sungjun Park",
        "David Hyunchul Shim"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T16:12:10Z",
      "summary": "The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.",
      "pdf_url": "https://arxiv.org/pdf/2512.20475v1",
      "categories": [
        "cs.RO"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'autonomous' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.16223v1",
      "title": "NGCaptcha: A CAPTCHA Bridging the Past and the Future",
      "authors": [
        "Ziqi Ding",
        "Shangzhi Xu",
        "Wei Song",
        "Yuekang Li"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T06:14:54Z",
      "summary": "CAPTCHAs are widely employed for distinguishing humans from automated bots online. However, current vision based CAPTCHAs face escalating security risks: traditional attacks continue to bypass many deployed CAPTCHA schemes, and recent breakthroughs in AI, particularly large scale vision models, enable machine solvers to significantly outperform humans on many CAPTCHA tasks, undermining their original design assumptions. To address these issues, we introduce NGCAPTCHA, a Next Generation CAPTCHA framework that integrates a lightweight client side proof of work (PoW) mechanism with an AI resistant visual recognition challenge. In NGCAPTCHA, a browser must first complete a small hash based PoW before any challenge is displayed, throttling large scale automated attempts by increasing their computational cost. Once the PoW is solved, the user is presented with a human friendly yet model resistant image selection task that exploits perceptual cues current vision systems still struggle with. This hybrid design combines computational friction with AI robust visual discrimination, substantially raising the barrier for automated bots while keeping the verification process fast and effortless for legitimate users.",
      "pdf_url": "https://arxiv.org/pdf/2512.16223v1",
      "categories": [
        "cs.CY"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.12778v1",
      "title": "VeBPF Many-Core Architecture for Network Functions in FPGA-based SmartNICs and IoT",
      "authors": [
        "Zaid Tahir",
        "Ahmed Sanaullah",
        "Sahan Bandara",
        "Ulrich Drepper",
        "Martin Herbordt"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T17:34:05Z",
      "summary": "FPGA-based SmartNICs and IoT devices integrating soft-processors for network function execution have emerged to address the limited hardware reconfigurability of DPUs and MCUs. However, existing FPGA-based solutions lack a highly configurable many-core architecture specialized for network packet processing. This work presents VeBPF many-core architecture, a resource-optimized and highly configurable many-core architecture composed of custom VeBPF (Verilog eBPF) CPU cores designed for FPGA-based packet processing. The VeBPF cores are eBPF ISA compliant and implemented in Verilog HDL for seamless integration with existing FPGA IP blocks and subsystems.   The proposed many-core architecture enables parallel execution of multiple eBPF rules across multiple VeBPF cores, achieving low-latency packet processing. The architecture is fully parameterizable, allowing the number of VeBPF cores and eBPF rules to scale according to application requirements and available FPGA resources. eBPF rules can be dynamically updated at run time without requiring FPGA reconfiguration, enabling flexible and adaptive network processing.   The design incorporates hardware and computer architecture optimizations that support deployment across a wide range of platforms, from low-end FPGA-based IoT devices to high-end FPGA-based SmartNICs. In addition, we present automated testing and simulation frameworks developed using open-source tools such as Python and Cocotb. The VeBPF cores, many-core architecture, control software libraries, and simulation infrastructure are released as open source to support further research in FPGA-based many-core systems, eBPF acceleration, SmartNICs, IoT, and network security.",
      "pdf_url": "https://arxiv.org/pdf/2512.12778v1",
      "categories": [
        "cs.CE"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.11194v1",
      "title": "Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models",
      "authors": [
        "Divya Kothandaraman",
        "Jaclyn Pytlarz"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-12T00:50:38Z",
      "summary": "Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.   To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.",
      "pdf_url": "https://arxiv.org/pdf/2512.11194v1",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.18531v1",
      "title": "Pushing the limits of one-dimensional NMR spectroscopy for automated structure elucidation using artificial intelligence",
      "authors": [
        "Frank Hu",
        "Jonathan M. Tubb",
        "Dimitris Argyropoulos",
        "Sergey Golotvin",
        "Mikhail Elyashberg",
        "Grant M. Rotskoff",
        "Matthew W. Kanan",
        "Thomas E. Markland"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T22:56:49Z",
      "summary": "One-dimensional NMR spectroscopy is one of the most widely used techniques for the characterization of organic compounds and natural products. For molecules with up to 36 non-hydrogen atoms, the number of possible structures has been estimated to range from $10^{20} - 10^{60}$. The task of determining the structure (formula and connectivity) of a molecule of this size using only its one-dimensional $^1$H and/or $^{13}$C NMR spectrum, i.e. de novo structure generation, thus appears completely intractable. Here we show how it is possible to achieve this task for systems with up to 40 non-hydrogen atoms across the full elemental coverage typically encountered in organic chemistry (C, N, O, H, P, S, Si, B, and the halogens) using a deep learning framework, thus covering a vast portion of the drug-like chemical space. Leveraging insights from natural language processing, we show that our transformer-based architecture predicts the correct molecule with 55.2% accuracy within the first 15 predictions using only the $^1$H and $^{13}$C NMR spectra, thus overcoming the combinatorial growth of the chemical space while also being extensible to experimental data via fine-tuning.",
      "pdf_url": "https://arxiv.org/pdf/2512.18531v1",
      "categories": [
        "physics.chem-ph",
        "cs.LG"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Category: cs.LG (+5)"
      ]
    },
    {
      "arxiv_id": "2512.18528v1",
      "title": "WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring",
      "authors": [
        "Moses Kiprono"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T22:49:43Z",
      "summary": "Chronic wounds, including diabetic foot ulcers which affect up to one-third of people with diabetes, impose a substantial clinical and economic burden, with U.S. healthcare costs exceeding 25 billion dollars annually. Current wound assessment remains predominantly subjective, leading to inconsistent classification and delayed interventions. We present WoundNet-Ensemble, an Internet of Medical Things system leveraging a novel ensemble of three complementary deep learning architectures: ResNet-50, the self-supervised Vision Transformer DINOv2, and Swin Transformer, for automated classification of six clinically distinct wound types. Our system achieves 99.90 percent ensemble accuracy on a comprehensive dataset of 5,175 wound images spanning diabetic foot ulcers, pressure ulcers, venous ulcers, thermal burns, pilonidal sinus wounds, and fungating malignant tumors. The weighted fusion strategy demonstrates a 3.7 percent improvement over previous state-of-the-art methods. Furthermore, we implement a longitudinal wound healing tracker that computes healing rates, severity scores, and generates clinical alerts. This work demonstrates a robust, accurate, and clinically deployable tool for modernizing wound care through artificial intelligence, addressing critical needs in telemedicine and remote patient monitoring. The implementation and trained models will be made publicly available to support reproducibility.",
      "pdf_url": "https://arxiv.org/pdf/2512.18528v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.18197v1",
      "title": "Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results",
      "authors": [
        "Yilei Wu",
        "Yichi Zhang",
        "Zijian Dong",
        "Fang Ji",
        "An Sen Tan",
        "Gifford Tan",
        "Sizhao Tang",
        "Huijuan Chen",
        "Zijiao Chen",
        "Eric Kwun Kei Ng",
        "Jose Bernal",
        "Hang Min",
        "Ying Xia",
        "Ines Vati",
        "Liz Cooper",
        "Xiaoyu Hu",
        "Yuchen Pei",
        "Yutao Ma",
        "Victor Nozais",
        "Ami Tsuchida",
        "Pierre-Yves Hervé",
        "Philippe Boutinaud",
        "Marc Joliot",
        "Junghwa Kang",
        "Wooseung Kim",
        "Dayeon Bak",
        "Rachika E. Hamadache",
        "Valeriia Abramova",
        "Xavier Lladó",
        "Yuntao Zhu",
        "Zhenyu Gong",
        "Xin Chen",
        "John McFadden",
        "Pek Lan Khong",
        "Roberto Duarte Coello",
        "Hongwei Bran Li",
        "Woon Puay Koh",
        "Christopher Chen",
        "Joanna M. Wardlaw",
        "Maria del C. Valdés Hernández",
        "Juan Helen Zhou"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T03:45:14Z",
      "summary": "Perivascular spaces (PVS), when abnormally enlarged and visible in magnetic resonance imaging (MRI) structural sequences, are important imaging markers of cerebral small vessel disease and potential indicators of neurodegenerative conditions. Despite their clinical significance, automatic enlarged PVS (EPVS) segmentation remains challenging due to their small size, variable morphology, similarity with other pathological features, and limited annotated datasets. This paper presents the EPVS Challenge organized at MICCAI 2024, which aims to advance the development of automated algorithms for EPVS segmentation across multi-site data. We provided a diverse dataset comprising 100 training, 50 validation, and 50 testing scans collected from multiple international sites (UK, Singapore, and China) with varying MRI protocols and demographics. All annotations followed the STRIVE protocol to ensure standardized ground truth and covered the full brain parenchyma. Seven teams completed the full challenge, implementing various deep learning approaches primarily based on U-Net architectures with innovations in multi-modal processing, ensemble strategies, and transformer-based components. Performance was evaluated using dice similarity coefficient, absolute volume difference, recall, and precision metrics. The winning method employed MedNeXt architecture with a dual 2D/3D strategy for handling varying slice thicknesses. The top solutions showed relatively good performance on test data from seen datasets, but significant degradation of performance was observed on the previously unseen Shanghai cohort, highlighting cross-site generalization challenges due to domain shift. This challenge establishes an important benchmark for EPVS segmentation methods and underscores the need for the continued development of robust algorithms that can generalize in diverse clinical settings.",
      "pdf_url": "https://arxiv.org/pdf/2512.18197v1",
      "categories": [
        "q-bio.QM",
        "cs.CV",
        "eess.IV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.17263v1",
      "title": "AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning",
      "authors": [
        "Dong Zifei",
        "Wu Wenjie",
        "Hao Jinkui",
        "Chen Tianqi",
        "Weng Ziqiao",
        "Zhou Bo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T06:27:14Z",
      "summary": "Robust anatomical segmentation of chest X-rays (CXRs) remains challenging due to the scarcity of comprehensive annotations and the substantial variability of real-world acquisition conditions. We propose AnyCXR, a unified framework that enables generalizable multi-organ segmentation across arbitrary CXR projection angles using only synthetic supervision. The method combines a Multi-stage Domain Randomization (MSDR) engine, which generates over 100,000 anatomically faithful and highly diverse synthetic radiographs from 3D CT volumes, with a Conditional Joint Annotation Regularization (CAR) learning strategy that leverages partial and imperfect labels by enforcing anatomical consistency in a latent space. Trained entirely on synthetic data, AnyCXR achieves strong zero-shot generalization on multiple real-world datasets, providing accurate delineation of 54 anatomical structures in PA, lateral, and oblique views. The resulting segmentation maps support downstream clinical tasks, including automated cardiothoracic ratio estimation, spine curvature assessment, and disease classification, where the incorporation of anatomical priors improves diagnostic performance. These results demonstrate that AnyCXR establishes a scalable and reliable foundation for anatomy-aware CXR analysis and offers a practical pathway toward reducing annotation burdens while improving robustness across diverse imaging conditions.",
      "pdf_url": "https://arxiv.org/pdf/2512.17263v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.15895v1",
      "title": "The Next Generation Fornax Survey (NGFS).VIII. A Support Vector Machine Approach for Disentangling Globular Clusters from other Sources",
      "authors": [
        "Yasna Ordenes-Briceño",
        "Thomas H. Puzia",
        "Paul Eigenthaler",
        "Matias Blaña",
        "Juan P. Carvajal",
        "Matthew A. Taylor",
        "Bryan W. Miller",
        "Rohan Rahatgaonkar",
        "Evelyn J. Johnston",
        "Prasanta K. Nayak",
        "Gaspar Galaz"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T19:06:55Z",
      "summary": "Wide-field, multi-band surveys now detect millions of unresolved sources in nearby galaxy clusters, yet separating globular clusters (GCs) from foreground stars and background galaxies remains challenging. Scalable, automated classification is therefore essential to convert the forthcoming data from facilities such as the Vera C. Rubin/LSST, the Roman and Euclid into robust constraints on galaxy assembly. We introduce a supervised classification method to separate GCs, stars, and galaxies based on their locations in color-color diagrams. The main objective is to recover a clean GC sample for future scientific analysis. The method exploits broad spectral energy distribution coverage, deep photometry, and is optimized for next-generation survey volumes. We use the central 3deg2 of the Next Generation Fornax Survey (NGFS), which images the Fornax cluster in u'g'i'JKs. We build a Support Vector Machine (SVM; svm.SVC, scikit-learn) using 15 features: all color combinations and basic morphological parameters. Spectroscopically confirmed sources define the training classes. Color pairs connecting near-UV/optical/near-IR. The full 15 feature model achieves 97.3% accuracy and a pruned 7 feature model built from the most informative, least correlated features achieves 96.6% accuracy. Misclassifications amount 8.4% and 10.4%, respectively. Omitting the u' or/and near-IR bands degrades performance. Emulating LSST filters with NGFS u'g'i' and DES r'z'Y shows that u' and Y bands are crucial, but models lacking NIR remain suboptimal. Combining broad SED coverage with simple morphological parameters enables precise, scalable separation of unresolved sources. Including NIR bands significantly improves GC classification, and joining LSST with forthcoming Euclid and Roman data will further enhance machine-learning frameworks.",
      "pdf_url": "https://arxiv.org/pdf/2512.15895v1",
      "categories": [
        "astro-ph.GA"
      ],
      "relevance_score": 25,
      "relevance_reasons": [
        "Published in 2025 (+20)",
        "Abstract keyword: 'automated' (+5)"
      ]
    },
    {
      "arxiv_id": "2512.20000v1",
      "title": "Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models",
      "authors": [
        "Zhenhao Li",
        "Shaohan Yi",
        "Zheng Liu",
        "Leonartinus Gao",
        "Minh Ngoc Le",
        "Ambrose Ling",
        "Zhuoran Wang",
        "Md Amirul Islam",
        "Zhixiang Chi",
        "Yuanhao Yu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T02:52:18Z",
      "summary": "Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.",
      "pdf_url": "https://arxiv.org/pdf/2512.20000v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.18474v1",
      "title": "When Robots Say No: The Empathic Ethical Disobedience Benchmark",
      "authors": [
        "Dmytro Kuzmenko",
        "Nadiya Shvai"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-20T19:35:08Z",
      "summary": "Robots must balance compliance with safety and social expectations as blind obedience can cause harm, while over-refusal erodes trust. Existing safe reinforcement learning (RL) benchmarks emphasize physical hazards, while human-robot interaction trust studies are small-scale and hard to reproduce. We present the Empathic Ethical Disobedience (EED) Gym, a standardized testbed that jointly evaluates refusal safety and social acceptability. Agents weigh risk, affect, and trust when choosing to comply, refuse (with or without explanation), clarify, or propose safer alternatives. EED Gym provides different scenarios, multiple persona profiles, and metrics for safety, calibration, and refusals, with trust and blame models grounded in a vignette study. Using EED Gym, we find that action masking eliminates unsafe compliance, while explanatory refusals help sustain trust. Constructive styles are rated most trustworthy, empathic styles -- most empathic, and safe RL methods improve robustness but also make agents more prone to overly cautious behavior. We release code, configurations, and reference policies to enable reproducible evaluation and systematic human-robot interaction research on refusal and trust. At submission time, we include an anonymized reproducibility package with code and configs, and we commit to open-sourcing the full repository after the paper is accepted.",
      "pdf_url": "https://arxiv.org/pdf/2512.18474v1",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.12707v1",
      "title": "From Linear Risk to Emergent Harm: Complexity as the Missing Core of AI Governance",
      "authors": [
        "Hugo Roger Paz"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-14T14:19:21Z",
      "summary": "Risk-based AI regulation has become the dominant paradigm in AI governance, promising proportional controls aligned with anticipated harms. This paper argues that such frameworks often fail for structural reasons: they implicitly assume linear causality, stable system boundaries, and largely predictable responses to regulation. In practice, AI operates within complex adaptive socio-technical systems in which harm is frequently emergent, delayed, redistributed, and amplified through feedback loops and strategic adaptation by system actors. As a result, compliance can increase while harm is displaced or concealed rather than eliminated. We propose a complexity-based framework for AI governance that treats regulation as intervention rather than control, prioritises dynamic system mapping over static classifications, and integrates causal reasoning and simulation for policy design under uncertainty. The aim is not to eliminate uncertainty, but to enable robust system stewardship through monitoring, learning, and iterative revision of governance interventions.",
      "pdf_url": "https://arxiv.org/pdf/2512.12707v1",
      "categories": [
        "cs.CY"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.10634v1",
      "title": "Field Reconstruction for High-Frequency Electromagnetic Exposure Assessment Based on Deep Learning",
      "authors": [
        "Miao Cao",
        "Zicheng Liu",
        "Bazargul Matkerim",
        "Tongning Wu",
        "Changyou Li",
        "Yali Zong",
        "Bo Qi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T13:35:06Z",
      "summary": "Fifth-generation (5G) communication systems, operating in higher frequency bands from 3 to 300 GHz, provide unprecedented bandwidth to enable ultra-high data rates and low-latency services. However, the use of millimeter-wave frequencies raises public health concerns regarding prolonged electromagnetic radiation (EMR) exposure. Above 6 GHz, the incident power density (IPD) is used instead of the specific absorption rate (SAR) for exposure assessment, owing to the shallow penetration depth of millimeter waves. This paper proposes a hybrid field reconstruction framework that integrates classical electromagnetic algorithms with deep learning to evaluate the IPD of wireless communication devices operating at 30 GHz, thereby determining compliance with established RF exposure limits. An initial estimate of the electric field on the evaluation plane is obtained using a classical reconstruction algorithm, followed by refinement through a neural network model that learns the mapping between the initial and accurate values. A multi-antenna dataset, generated via full-wave simulation, is used for training and testing. The impacts of training strategy, initial-value algorithm, reconstruction distance, and measurement sampling density on model performance are analyzed. Results show that the proposed method significantly improves reconstruction accuracy, achieving an average relative error of 4.57% for electric field reconstruction and 2.97% for IPD estimation on the test dataset. Additionally, the effects of practical uncertainty factors, including probe misalignment, inter-probe coupling, and measurement noise, are quantitatively assessed.",
      "pdf_url": "https://arxiv.org/pdf/2512.10634v1",
      "categories": [
        "physics.app-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.07114v1",
      "title": "Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots",
      "authors": [
        "Jue Wang",
        "Mingsong Jiang",
        "Luis A. Ramirez",
        "Bilige Yang",
        "Mujun Zhang",
        "Esteban Figueroa",
        "Wenzhong Yan",
        "Rebecca Kramer-Bottiglio"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-08T02:52:42Z",
      "summary": "Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.",
      "pdf_url": "https://arxiv.org/pdf/2512.07114v1",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2511.18088v1",
      "title": "A Unified Multi-Dynamics Framework for Perception-Oriented Modeling in Tendon-Driven Continuum Robots",
      "authors": [
        "Ibrahim Alsarraj",
        "Yuhao Wang",
        "Abdalla Swikir",
        "Cesare Stefanini",
        "Dezhen Song",
        "Zhanchi Wang",
        "Ke Wu"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-22T15:05:44Z",
      "summary": "Tendon-driven continuum robots offer intrinsically safe and contact-rich interactions owing to their kinematic redundancy and structural compliance. However, their perception often depends on external sensors, which increase hardware complexity and limit scalability. This work introduces a unified multi-dynamics modeling framework for tendon-driven continuum robotic systems, exemplified by a spiral-inspired robot named Spirob. The framework integrates motor electrical dynamics, motor-winch dynamics, and continuum robot dynamics into a coherent system model. Within this framework, motor signals such as current and angular displacement are modeled to expose the electromechanical signatures of external interactions, enabling perception grounded in intrinsic dynamics. The model captures and validates key physical behaviors of the real system, including actuation hysteresis and self-contact at motion limits. Building on this foundation, the framework is applied to environmental interaction: first for passive contact detection, verified experimentally against simulation data; then for active contact sensing, where control and perception strategies from simulation are successfully applied to the real robot; and finally for object size estimation, where a policy learned in simulation is directly deployed on hardware. The results demonstrate that the proposed framework provides a physically grounded way to interpret interaction signatures from intrinsic motor signals in tendon-driven continuum robots.",
      "pdf_url": "https://arxiv.org/pdf/2511.18088v1",
      "categories": [
        "cs.RO"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2511.17730v1",
      "title": "Safety and Risk Pathways in Cooperative Generative Multi-Agent Systems: A Telecom Perspective",
      "authors": [
        "Zeinab Nezami",
        "Shehr Bano",
        "Abdelaziz Salama",
        "Maryam Hafeez",
        "Syed Ali Raza Zaidi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-11-21T19:29:19Z",
      "summary": "Generative multiagent systems are rapidly emerging as transformative tools for scalable automation and adaptive decisionmaking in telecommunications. Despite their promise, these systems introduce novel risks that remain underexplored, particularly when agents operate asynchronously across layered architectures. This paper investigates key safety pathways in telecomfocused Generative MultiAgent Systems (GMAS), emphasizing risks of miscoordination and semantic drift shaped by persona diversity. We propose a modular safety evaluation framework that integrates agentlevel checks on code quality and compliance with systemlevel safety metrics. Using controlled simulations across 32 persona sets, five questions, and multiple iterative runs, we demonstrate progressive improvements in analyzer penalties and AllocatorCoder consistency, alongside persistent vulnerabilities such as policy drift and variability under specific persona combinations. Our findings provide the first domaingrounded evidence that persona design, coding style, and planning orientation directly influence the stability and safety of telecom GMAS, highlighting both promising mitigation strategies and open risks for future deployment.",
      "pdf_url": "https://arxiv.org/pdf/2511.17730v1",
      "categories": [
        "eess.SY"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20827v1",
      "title": "Towards City-Scale Quantum Timing: Wireless Synchronization via Quantum Hubs",
      "authors": [
        "Mohammad Taghi Dabiri",
        "Mazen Hasna",
        "Rula Ammuri",
        "Saif Al-Kuwari",
        "Khalid Qaraqe"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T23:02:47Z",
      "summary": "This paper presents a novel wireless quantum synchronization framework tailored for city-scale deployment using entangled photon pairs and passive corner cube retroreflector (CCR) arrays. A centralized quantum hub emits entangled photons, directing one toward a target device and the other toward a local reference unit. The target, equipped with a planar CCR array, reflects the incoming photon without active circuitry, enabling secure round-trip quantum measurements for sub-nanosecond synchronization and localization. We develop a comprehensive analytical model that captures key physical-layer phenomena, including Gaussian beam spread, spatial misalignment, atmospheric turbulence, and probabilistic photon generation. A closed-form expression is derived for the single-photon detection probability under Gamma-Gamma fading, and its distribution is used to model photon arrival events and synchronization error. Moreover, we analyze the impact of background photons, SPAD detector jitter, and quantum generation randomness on synchronization accuracy and outage probability. Simulation results confirm the accuracy of the analytical models and reveal key trade-offs among beam waist, CCR array size, and background light. The proposed architecture offers a low-power, infrastructure-free solution for secure timing in next-generation smart cities.",
      "pdf_url": "https://arxiv.org/pdf/2512.20827v1",
      "categories": [
        "eess.SP",
        "quant-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20716v1",
      "title": "Flux rope formation through flux cancellation of sheared coronal arcades in a 3D convectively-driven MHD simulation",
      "authors": [
        "Sondre Vik Furuseth",
        "Guillaume Aulanier"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T19:27:59Z",
      "summary": "Context. Space weather and its potential negative consequences for life on Earth has received increasing attention in recent decades. Particularly predicting CME onset has become important from a security perspective. To predict CMEs, one must first understand the dynamics leading to pre-eruptive magnetic field configurations such as flux ropes.   Aims. In this study, we investigate the realistic formation of coronal flux ropes above the solar photosphere. The aim is to find if and how flux ropes can form there, and how the formation is related to flux cancellation at the photosphere.   Methods. We run a convective non-symmetric 3D radiative MHD simulation with the code Bifrost. A linear force-free field with sheared coronal arcades is slowly inserted in the 24Mmx24Mmx30Mm simulation box. After this, the self-consistent stochastic plasma flows of the convection zone drive several small-scale flux cancellations and magnetic reconnection, without external influence. Lagrangian markers called corks are used to track the dynamic evolution of the magnetic field. Results. Over a period of 2.5 h, a flux rope is generated with photospheric footpoints separated by up to 12Mm. The flux rope forms gradually through several individual events, such as slipping reconnection, U-loop emergence, and thick-photosphere tether-cutting reconnection.   Conclusions. Flux ropes can be formed in the solar atmosphere solely driven by convection and flux cancellations at the photosphere. However, not all flux cancellations contribute to the build-up of the flux rope, and some coronal reconnection events that do are not clearly related to flux cancellation. The formation process of flux ropes from coronal sheared arcades driven by convection is therefore more complex than in the original smooth flux cancellation model. But the end result is qualitatively the same. Flux cancellation works. A flux rope is formed.",
      "pdf_url": "https://arxiv.org/pdf/2512.20716v1",
      "categories": [
        "astro-ph.SR"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.18961v1",
      "title": "Integrated Ring-based Quantum Key Distribution with Weak Measurement Enhanced Fiber-Optic Sensing Disturbance Magnitude and Location",
      "authors": [
        "Weiqian Zhao",
        "Wenzhao Huang",
        "Zifu Su",
        "Fangyuan Li",
        "Qirong Jiang",
        "Cheng Yuan",
        "Yafei Yu",
        "Jindong Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T02:22:38Z",
      "summary": "The deep integration of quantum communication and fiber-optic sensing is pivotal for the development of next-generation multifunctional and highly reliable secure information infrastructure. Here, we present a Sagnac-loop integrated system (SLIS) that, for the first time, combines ring-based quantum key distribution (QKD) with fiber-based weak measurement (WM) enhanced sensing and disturbance localization capabilities. In the event of communication interruption due to external disturbances, the SLIS seamlessly switches to perception system, employing interference measurement and WM techniques to monitor channel disturbances. By integrating null-frequencies localization (NFL) mode, the system precisely determines the disturbance location, enabling rapid identification of security vulnerabilities along the link. Experimental results demonstrate that, over a 30 km Sagnac loop channel, the SLIS achieves a raw key generation rate of 22.4 kbps with stable operation and clear scalability toward network expansion. In terms of perception performance, the SLIS exhibits strong capability for both dynamic and quasi-static disturbances. For dynamic perturbations, the system detects transient impacts and PZT-driven frequency variations down to 100 Hz, and enables long-distance localization via NFL alignment, with improved localization performance as the disturbance position moves farther away along the loop. For quasi-static disturbances, gravitational changes as small as 100 g are resolved, corresponding to a time-delay variation of 9.81 as. This work provides a novel technical pathway toward self-diagnosing, robust quantum networks through integrated communication and sensing functionalities.",
      "pdf_url": "https://arxiv.org/pdf/2512.18961v1",
      "categories": [
        "quant-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.10176v1",
      "title": "Simultaneous Classical and Quantum Communications: Recent Progress and Three Challenges",
      "authors": [
        "Phuc V. Trinh",
        "Shinya Sugiura",
        "Carlo Ottaviani",
        "Chao Xu",
        "Lajos Hanzo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T00:27:08Z",
      "summary": "A critical aspect of next-generation wireless networks is the integration of quantum communications to guard against quantum computing threats to classical networks. Despite successful experimental demonstrations, integrating quantum communications into the classical infrastructure faces substantial challenges, including high costs, compatibility issues, and extra hardware deployment to accommodate both classical and quantum communication equipment. To mitigate these challenges, we explore novel protocols that enable simultaneous classical and quantum communications, relying on a single set of transceivers to jointly modulate and decode classical and quantum information onto the same signal. Additionally, we emphasize extending quantum communication capabilities beyond traditional optical bands into the terahertz, even possibly to millimeter-wave and microwave frequencies, thereby broadening the potential horizon of quantum-secure applications. Finally, we identify open problems that must be addressed to facilitate practical implementation.",
      "pdf_url": "https://arxiv.org/pdf/2512.10176v1",
      "categories": [
        "quant-ph",
        "eess.SP"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.09700v1",
      "title": "LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery",
      "authors": [
        "Seon-Hoon Kim",
        "Hyeji Sim",
        "Youeyun Jung",
        "Ok-Chul Jung",
        "Yerin Kim"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-10T14:48:58Z",
      "summary": "Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.",
      "pdf_url": "https://arxiv.org/pdf/2512.09700v1",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.11891v1",
      "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
      "authors": [
        "Songqiao Hu",
        "Zeyi Liu",
        "Shuang Liu",
        "Jun Cen",
        "Zihan Meng",
        "Xiao He"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-09T16:53:44Z",
      "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.",
      "pdf_url": "https://arxiv.org/pdf/2512.11891v1",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20637v1",
      "title": "Cross-Domain Elephant Flow Detection: A Unified Machine Learning Approach with Application-Aware and Security Features",
      "authors": [
        "Tabidah Usmani",
        "Sara Zahid",
        "Amna Javaid"
      ],
      "affiliations": [
        "National University of Computer and Emerging Sciences",
        "National University of Computer and Emerging Sciences",
        "National University of Computer and Emerging Sciences"
      ],
      "year": 2025,
      "published": "2025-12-05T16:42:12Z",
      "summary": "Network traffic classification, particularly elephant flow detection, faces significant challenges when deployed across heterogeneous network environments. While existing approaches demonstrate high accuracy within single domains, they suffer from poor generalization due to domain shift phenomena. This paper presents a unified machine learning framework for cross domain elephant flow detection that incorporates application aware and security features to enhance robustness across diverse network environments. Our approach addresses the critical gap in existing literature by evaluating model performance across three distinct domains: Campus networks, UNSW-NB15, and CIC-IDS2018 datasets. This paper proposes a unified pipeline that employs adaptive thresholding, comprehensive feature engineering, and cross-domain evaluation to quantify and mitigate domain shift effects. Experimental results demonstrate significant performance variations across domains (F1-scores ranging from 0.37 to 0.97), highlighting the importance of cross-domain validation. The unified model achieves an overall cross-validation F1 score of 0.99 while maintaining interpretability through feature importance analysis. Our findings reveal that while size based features dominate elephant flow detection (33.80% importance for total bytes), application-aware and security features contribute to improved classification accuracy and provide valuable insights for network management and security applications.",
      "pdf_url": "https://arxiv.org/pdf/2512.20637v1",
      "categories": [
        "cs.NI"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.05420v1",
      "title": "Verifier-initiated quantum message-authentication via quantum zero-knowledge proofs",
      "authors": [
        "Wusheng Wang",
        "Masahito Hayashi"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-05T04:40:34Z",
      "summary": "On-demand authentication is critical for scalable quantum systems, yet current approaches require the signer to initiate communication, creating unnecessary overhead. We introduce a new method where the verifier can request authentication only when needed, improving efficiency for quantum networks and blockchain applications. Our approach adapts the concept of zero-knowledge proofs widely used in classical cryptography to quantum settings, ensuring that verification reveals nothing about secret keys. We develop a general framework that converts any suitable quantum proof into a verifier-driven signature protocol and present a concrete implementation based on quantum measurements. The protocol achieves strong security guarantees, including resistance to forgery and privacy against curious verifiers, without relying on computational hardness assumptions and with qubit technologies. This work delivers the first general verifier-initiated quantum signature scheme with formal security, paving the way for scalable, secure authentication in future quantum infrastructures and decentralized systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.05420v1",
      "categories": [
        "quant-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.21302v1",
      "title": "AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents",
      "authors": [
        "Yue Cao",
        "Yingyao Wang",
        "Pi Bu",
        "Jingxuan Xing",
        "Wei Jiang",
        "Zekun Zhu",
        "Junpeng Ma",
        "Sashuai Zhou",
        "Tong Lu",
        "Jun Song",
        "Yu Cheng",
        "Yuning Jiang",
        "Bo Zheng"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T17:40:42Z",
      "summary": "Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.",
      "pdf_url": "https://arxiv.org/pdf/2512.21302v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.21276v1",
      "title": "GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation",
      "authors": [
        "Snehal Singh Tomar",
        "Alexandros Graikos",
        "Arjun Krishna",
        "Dimitris Samaras",
        "Klaus Mueller"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T16:46:04Z",
      "summary": "Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.",
      "pdf_url": "https://arxiv.org/pdf/2512.21276v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.21185v1",
      "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
      "authors": [
        "Tanghui Jia",
        "Dongyu Yan",
        "Dehao Hao",
        "Yang Li",
        "Kaiyi Zhang",
        "Xianyi He",
        "Lanjiong Li",
        "Jinnan Chen",
        "Lutao Jiang",
        "Qishen Yin",
        "Long Quan",
        "Ying-Cong Chen",
        "Li Yuan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T14:08:38Z",
      "summary": "In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.",
      "pdf_url": "https://arxiv.org/pdf/2512.21185v1",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.21149v1",
      "title": "Equilibrium investment under dynamic preference uncertainty",
      "authors": [
        "Luca De Gennaro Aquino",
        "Sascha Desmettre",
        "Yevhen Havrylenko",
        "Mogens Steffensen"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T12:33:38Z",
      "summary": "We study a continuous-time portfolio choice problem for an investor whose state-dependent preferences are determined by an exogenous factor that evolves as an Itô diffusion process. Since risk attitudes at the end of the investment horizon are uncertain, terminal wealth is evaluated under a set of utility functions corresponding to all possible future preference states. These utilities are first converted into certainty equivalents at their respective levels of terminal risk aversion and then (nonlinearly) aggregated over the conditional distribution of future states, yielding an inherently time-inconsistent optimization criterion. We approach this problem by developing a general equilibrium framework for such state-dependent preferences and characterizing subgame-perfect equilibrium investment policies through an extended Hamilton-Jacobi-Bellman system. This system gives rise to a coupled nonlinear partial integro-differential equation for the value functions associated with each state. We then specialize the model to a tractable constant relative risk aversion specification in which the preference factor follows an arithmetic Brownian motion. In this setting, the equilibrium policy admits a semi-explicit representation that decomposes into a standard myopic demand and a novel preference-hedging component that captures incentives to hedge against anticipated changes in risk aversion. Numerical experiments illustrate how features of the preference dynamics -- most notably the drift of the preference process and the correlation between preference shocks and asset returns -- jointly determine the sign and magnitude of the hedging demand and the evolution of the equilibrium risky investment over time.",
      "pdf_url": "https://arxiv.org/pdf/2512.21149v1",
      "categories": [
        "q-fin.MF",
        "math.OC"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.21105v1",
      "title": "Volatile Organic Compounds for Stress Detection: A Scoping Review and Exploratory Feasibility Study with Low-Cost Sensors",
      "authors": [
        "Nicolai Plintz",
        "Marcus Vetter",
        "Dirk Ifenthaler"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T11:08:55Z",
      "summary": "Volatile organic compounds (VOCs) represent a novel but underexplored modality for emotion recognition. This paper presents a systematic evidence synthesis and exploratory investigation of VOC-based affective computing using low-cost sensors. Study 1, a systematic scoping review following PRISMA-ScR guidelines, analyzed 16 studies from 610 records across breath, sweat, skin, and urine biosources. Evidence indicates that stress and affective states are reflected in VOC signatures (aldehydes, ketones, fatty acids, sulfur compounds), though with considerable heterogeneity. Current research relies predominantly on laboratory-grade GC-MS or PTR-MS, while wearable sensors provide pattern-level outputs without compound-specific identification - a critical gap for practical systems. Study 2 (n=25) investigated whether low-cost TVOC sensors (BME688, ENS160) combined with physiological monitoring (HR, HRV, GSR) can detect laboratory-induced stress. Exploratory analysis revealed that high cardiovascular reactors exhibited elevated TVOC during arithmetic stress (d=1.38), though requiring replication in larger samples. Substantial interindividual variability emerged (CV>80%), with coupling patterns moderated by baseline emission levels and temporal lags of 30-80 seconds. Random Forest-based multimodal classification achieved 77.3% accuracy (5-fold CV). SHAP analysis indicated VOC sensors contributed 24.9% of model performance. Leave-one-subject-out validation yielded 65.3% accuracy, highlighting the need for individual calibration. This work provides three contributions: (1) comprehensive mapping of VOC biomarker evidence and technological gaps, (2) initial demonstration that low-cost sensors can capture stress-related VOC patterns in multimodal fusion, and (3) identification of key implementation challenges. Findings require replication in larger samples (n>=50).",
      "pdf_url": "https://arxiv.org/pdf/2512.21105v1",
      "categories": [
        "cs.HC"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20940v1",
      "title": "ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments",
      "authors": [
        "Shuhao Ye",
        "Sitong Mao",
        "Yuxiang Cui",
        "Xuan Yu",
        "Shichao Zhai",
        "Wen Chen",
        "Shunbo Zhou",
        "Rong Xiong",
        "Yue Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T04:53:03Z",
      "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.",
      "pdf_url": "https://arxiv.org/pdf/2512.20940v1",
      "categories": [
        "cs.RO"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20918v1",
      "title": "Welfare at Risk: Distributional impact of policy interventions",
      "authors": [
        "Costas Lambros",
        "Emerson Melo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-24T03:49:38Z",
      "summary": "This paper proposes a framewrok for analyzing how the welfare effects of policy interventions are distributed across individuals when those effects are unobserved. Rather than focusing solely on average outcomes, the approach uses readily available information on average welfare responses to uncover meaningful patterns in how gains and losses are distributed across different populations. The framework is built around the concept of superquantile and applies to a broad class of models with unobserved individual heterogeneity. It enables policymakers to identify which groups are most adversely affected by a policy and to evaluate trade-offs between efficiency and equity. We illustrate the approach in three widely studied economic settings: price changes and compensated variation, treatment allocation with self-selection, and the cost-benefit analysis of social programs. In this latter application, we show how standard tools from the marginal treatment effect and generalized Roy model literature are useful for implementing our bounds for both the overall population and for individuals who participate in the program.",
      "pdf_url": "https://arxiv.org/pdf/2512.20918v1",
      "categories": [
        "econ.EM",
        "econ.TH"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20748v1",
      "title": "Fixed-time control with prescribed performance for path following of underwater gliders",
      "authors": [
        "Hanzhi Yang",
        "Nina Mahmoudian"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T20:11:25Z",
      "summary": "Underwater gliders are increasingly deployed in challenging missions - such as hurricane-season observations and long-endurance environmental monitoring - where strong currents and turbulence pose significant risks to navigation safety. To address these practical challenges, this paper presents a fixed-time prescribed performance control scheme for the 3D path following of underwater gliders subject to model uncertainties and environmental disturbances. The primary contribution is the integration of a finite-time performance function within a fixed-time control framework. This synthesis ensures that the tracking errors are constrained within prescribed performance bounds and converge to a compact set within a fixed time, independent of initial conditions. A second key contribution is the development of a fixed-time sliding mode disturbance observer that provides accurate finite-time estimation of lumped disturbances, enhancing the system's robustness. Integrated with an iLOS guidance law, the proposed controller enables precise and safe waypoint following. Numerical simulations demonstrate that the proposed method outperforms conventional sliding mode and prescribed performance controllers in tracking accuracy, convergence speed, and control effort smoothness, validating its efficacy for robust underwater navigation.",
      "pdf_url": "https://arxiv.org/pdf/2512.20748v1",
      "categories": [
        "eess.SY",
        "cs.RO",
        "math.OC"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20448v1",
      "title": "Enriching Earth Observation labeled data with Quantum Conditioned Diffusion Models",
      "authors": [
        "Francesco Mauro",
        "Francesca De Falco",
        "Lorenzo Papa",
        "Andrea Ceschini",
        "Alessandro Sebastianelli",
        "Paolo Gamba",
        "Massimo Panella",
        "Silvia Ullo"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T15:40:31Z",
      "summary": "The rapid adoption of diffusion models (DMs) in the Earth Observation (EO) domain has unlocked new generative capabilities aimed at producing new samples, whose statistical properties closely match real imagery, for tasks such as synthesizing missing data, augmenting scarce labeled datasets, and improving image reconstruction. This is particularly relevant in EO, where labeled data are often costly to obtain and limited in availability. However, classical DMs still face significant computational limitations, requiring hundreds to thousands of inference steps, as well as difficulties in capturing the intricate spatial and spectral correlations characteristic of EO data. Recent research in Quantum Machine Learning (QML), including initial attempts of Quantum Generative Models, offers a fundamentally different approach to overcome these challenges. Motivated by these considerations, we introduce the Quanvolutional Conditioned U-Net (QCU-Net), a hybrid quantum--classical architecture that applies quantum operations within a conditioned diffusion framework using a novel quanvolutional feature-extraction approach, for generating synthetic labeled EO imagery. Extensive experiments on the EuroSAT RGB dataset demonstrate that our QCU-Net achieves superior results. Notably, it reduces the Fréchet Inception Distance by 64%, lowers the Kernel Inception Distance by 76%, and yields higher semantic accuracy. Ablation studies further reveal that strategically positioning quantum layers and employing entangling variational circuits enhance model performance and convergence. This work represents the first successful adaptation of class-conditioned quantum diffusion modeling in the EO domain, paving the way for quantum-enhanced remote sensing imagery synthesis.",
      "pdf_url": "https://arxiv.org/pdf/2512.20448v1",
      "categories": [
        "quant-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20437v1",
      "title": "Quantum Bayesian Optimization for the Automatic Tuning of Lorenz-96 as a Surrogate Climate Model",
      "authors": [
        "Paul J. Christiansen",
        "Daniel Ohl de Mello",
        "Cedric Brügmann",
        "Steffen Hien",
        "Felix Herbort",
        "Martin Kiffner",
        "Lorenzo Pastori",
        "Veronika Eyring",
        "Mierk Schwabe"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T15:26:24Z",
      "summary": "In this work, we propose a hybrid quantum-inspired heuristic for automatically tuning the Lorenz-96 model -- a simple proxy to describe atmospheric dynamics, yet exhibiting chaotic behavior. Building on the history matching framework by Lguensat et al. (2023), we fully automate the tuning process with a new convergence criterion and propose replacing classical Gaussian process emulators with quantum counterparts. We benchmark three quantum kernel architectures, distinguished by their quantum feature map circuits. A dimensionality argument implies, in principle, an increased expressivity of the quantum kernels over their classical competitors. For each kernel type, we perform an extensive hyperparameter optimization of our tuning algorithm. We confirm the validity of a quantum-inspired approach based on statevector simulation by numerically demonstrating the superiority of two studied quantum kernels over the canonical classical RBF kernel. Finally, we discuss the pathway towards real quantum hardware, mainly driven by a transition to shot-based simulations and evaluating quantum kernels via randomized measurements, which can mitigate the effect of gate errors. The very low qubit requirements and moderate circuit depths, together with a minimal number of trainable circuit parameters, make our method particularly NISQ-friendly.",
      "pdf_url": "https://arxiv.org/pdf/2512.20437v1",
      "categories": [
        "quant-ph",
        "physics.ao-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20369v1",
      "title": "EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge",
      "authors": [
        "Xiaoxuan Guo",
        "Hengyan Huang",
        "Jiayi Zhou",
        "Renhe Sun",
        "Jian Liu",
        "Haonan Cheng",
        "Long Ye",
        "Qin Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:54:02Z",
      "summary": "Recent advances in generative audio models have enabled high-fidelity environmental sound synthesis, raising serious concerns for audio security. The ESDD 2026 Challenge therefore addresses environmental sound deepfake detection under unseen generators (Track 1) and black-box low-resource detection (Track 2) conditions. We propose EnvSSLAM-FFN, which integrates a frozen SSLAM self-supervised encoder with a lightweight FFN back-end. To effectively capture spoofing artifacts under severe data imbalance, we fuse intermediate SSLAM representations from layers 4-9 and adopt a class-weighted training objective. Experimental results show that the proposed system consistently outperforms the official baselines on both tracks, achieving Test Equal Error Rates (EERs) of 1.20% and 1.05%, respectively.",
      "pdf_url": "https://arxiv.org/pdf/2512.20369v1",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20340v1",
      "title": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
      "authors": [
        "Qingdong He",
        "Xueqin Chen",
        "Yanjie Pan",
        "Peng Tang",
        "Pengcheng Xu",
        "Zhenye Gan",
        "Chengjie Wang",
        "Xiaobin Hu",
        "Jiangning Zhang",
        "Yabiao Wang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:15:31Z",
      "summary": "Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.",
      "pdf_url": "https://arxiv.org/pdf/2512.20340v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20339v1",
      "title": "MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model",
      "authors": [
        "Ye Tao",
        "Xuenan Xu",
        "Wen Wu",
        "Shuai Wang",
        "Mengyue Wu",
        "Chao Zhang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T13:14:06Z",
      "summary": "Text-guided audio editing aims to modify specific acoustic events while strictly preserving non-target content. Despite recent progress, existing approaches remain fundamentally limited. Training-free methods often suffer from signal degradation caused by diffusion inversion, while training-based methods, although achieving higher generation quality, are severely constrained by the scarcity of high-quality paired data and task formulations that cover only a narrow subset of editing operations. In addition, standard architectures typically decouple text and audio processing, limiting the ability to align instructions with specific acoustic contexts.   To address these challenges, we propose MMEdit, an audio-language-model-driven framework for unified audio editing. We systematically extend task definitions to cover a comprehensive range of editing operations, including addition, replacement, removal, reordering, and attribute modification. Furthermore, we design a scalable data synthesis pipeline to construct large-scale paired datasets with fine-grained event-level annotations. To capture complex editing semantics, we integrate a Qwen2-Audio encoder with an MMDiT-based generator, enabling precise cross-modal alignment and localized editing.   Experimental results demonstrate that our method achieves superior editing localization accuracy, robust instruction following, and high fidelity in non-edited regions.",
      "pdf_url": "https://arxiv.org/pdf/2512.20339v1",
      "categories": [
        "cs.SD"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20034v1",
      "title": "VSA:Visual-Structural Alignment for UI-to-Code",
      "authors": [
        "Xian Wu",
        "Ming Zhang",
        "Zhiyu Fang",
        "Fei Li",
        "Bin Wang",
        "Yong Jiang",
        "Hao Zhou"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T03:55:45Z",
      "summary": "The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \\textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.",
      "pdf_url": "https://arxiv.org/pdf/2512.20034v1",
      "categories": [
        "cs.IR"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20008v1",
      "title": "From Optimization to Learning: Dual-Approach Resource Allocation for Over-the-Air Edge Computing Under Execution Uncertainty",
      "authors": [
        "Tuo Wu",
        "Xiazhi Lai",
        "Shihang Lu",
        "Zihao Chen",
        "Xiaotong Zhao",
        "Yuanhao Cui"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T03:05:29Z",
      "summary": "The exponential proliferation of mobile devices and data-intensive applications in future wireless networks imposes substantial computational burdens on resource-constrained devices, thereby fostering the emergence of over-the-air computation (AirComp) as a transformative paradigm for edge intelligence.} To enhance the efficiency and scalability of AirComp systems, this paper proposes a comprehensive dual-approach framework that systematically transitions from traditional mathematical optimization to deep reinforcement learning (DRL) for resource allocation under execution uncertainty. Specifically, we establish a rigorous system model capturing execution uncertainty via Gamma-distributed computational workloads, resulting in challenging nonlinear optimization problems involving complex Gamma functions. For single-user scenarios, we design advanced block coordinate descent (BCD) and majorization-maximization (MM) algorithms, which yield semi-closed-form solutions with provable performance guarantees. However, conventional optimization approaches become computationally intractable in dynamic multi-user environments due to inter-user interference and resource contention. To this end, we introduce a Deep Q-Network (DQN)-based DRL framework capable of adaptively learning optimal policies through environment interaction. Our dual methodology effectively bridges analytical tractability with adaptive intelligence, leveraging optimization for foundational insight and learning for real-time adaptability. Extensive numerical results corroborate the performance gains achieved via increased edge server density and validate the superiority of our optimization-to-learning paradigm in next-generation AirComp systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.20008v1",
      "categories": [
        "eess.SP"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19923v1",
      "title": "Synthesis of a high intensity, superthermal muonium beam for gravity and laser spectroscopy experiments",
      "authors": [
        "Jesse Zhang",
        "Aldo Antognini",
        "Marek Bartkowiak",
        "Klaus Kirch",
        "Andreas Knecht",
        "Damian Goeldi",
        "David Taqqu",
        "Robert Waddy",
        "Frederik Wauters",
        "Paul Wegmann",
        "Anna Soter"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T22:59:40Z",
      "summary": "The universality of free fall, a cornerstone of Einstein's theory of gravity, has so far only been tested with neutral composite states of first-generation Standard Model (SM) particles, such as atoms or neutrons, and, most recently, antihydrogen. Extending these gravitational measurements to other sectors of the SM requires the formation of neutral bound states using higher-generation, unstable particles. Muonium, the bound state of an antimuon ($μ^+$) and an electron ($e^-$), offers the possibility to probe gravity with second-generation (anti)leptons, in the absence of the strong interaction. However, the short $μ^+$ lifetime ($τ_μ\\approx 2.2~μ$s) and the existing diffuse thermal muonium sources rendered such measurements unfeasible. Here, we report the synthesis of a high-brightness muonium beam, extracted from a thin layer of superfluid helium by exploiting its chemical potential and unique transport properties. The mean longitudinal velocity (${v}\\approx 2180~\\rm{m/s}$) and narrow distribution (${Δv}< 150 ~\\rm{m/s}$) of the atoms characterise a superthermal beam, while yields are similar to the highest intensity diffuse sources. This new beam is expected to enable muonium interferometry and a percent-level measurement of its gravitational acceleration, providing the first direct test of the Weak Equivalence Principle with second-generation (anti)matter. Its unprecedented brightness also opens the way to sub-kHz 1S-2S spectroscopy, enabling precise determination of the muon mass and stringent tests of bound-state quantum electrodynamics.",
      "pdf_url": "https://arxiv.org/pdf/2512.19923v1",
      "categories": [
        "physics.atom-ph",
        "gr-qc",
        "hep-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19680v1",
      "title": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
      "authors": [
        "Xinyao Liao",
        "Qiyuan He",
        "Kai Xu",
        "Xiaoye Qu",
        "Yicong Li",
        "Wei Wei",
        "Angela Yao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T18:54:30Z",
      "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.",
      "pdf_url": "https://arxiv.org/pdf/2512.19680v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19632v1",
      "title": "Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment",
      "authors": [
        "Da Tan",
        "Michael Beck",
        "Christopher P. Bidinosti",
        "Robert H. Gulden",
        "Christopher J. Henry"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T18:07:08Z",
      "summary": "The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.",
      "pdf_url": "https://arxiv.org/pdf/2512.19632v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19629v2",
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "authors": [
        "Jiaqi Peng",
        "Wenzhe Cai",
        "Yuqiang Yang",
        "Tai Wang",
        "Yuan Shen",
        "Jiangmiao Pang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T18:03:08Z",
      "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation. We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io.",
      "pdf_url": "https://arxiv.org/pdf/2512.19629v2",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19563v1",
      "title": "On Network-Aware Semantic Communication and Edge-Cloud Collaborative Intelligence Systems",
      "authors": [
        "Murdadha Nasif",
        "Ahmed Refaey Hussein"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T16:44:32Z",
      "summary": "Semantic communication and edge-cloud collaborative intelligence are increasingly recognized as foundational enablers for next-generation intelligent services operating under stringent bandwidth, latency, and resource constraints. By shifting the communication objective from bit-perfect delivery toward the transmission of task-relevant semantic representations, semantic communication enables adaptive tradeoffs among communication overhead, inference accuracy, computational load, and end-to-end latency. This survey provides a comprehensive and system-level synthesis of recent advances in semantic communication at the edge-cloud interface, encompassing architectural models for collaborative intelligence, representation learning and semantic abstraction techniques, network-aware and resource-adaptive semantic encoding strategies, and learning-driven optimization and orchestration mechanisms. Beyond efficiency considerations, the survey situates semantic communication within practical operational contexts, including security, trust, resilience, and scalability, drawing connections to zero-trust networking, physical-layer security, and emerging edge-cloud control paradigms. Finally, open challenges and research directions are identified, highlighting the role of semantic communication as a key building block for AI-native networking and 6G-ready intelligent systems.",
      "pdf_url": "https://arxiv.org/pdf/2512.19563v1",
      "categories": [
        "cs.NI"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19539v1",
      "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
      "authors": [
        "Kaiwen Zhang",
        "Liming Jiang",
        "Angtian Wang",
        "Jacob Zhiyuan Fang",
        "Tiancheng Zhi",
        "Qing Yan",
        "Hao Kang",
        "Xin Lu",
        "Xingang Pan"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T16:23:24Z",
      "summary": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
      "pdf_url": "https://arxiv.org/pdf/2512.19539v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19402v1",
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "authors": [
        "Yujie Zhao",
        "Hongwei Fan",
        "Di Chen",
        "Shengcong Chen",
        "Liliang Chen",
        "Xiaoqi Li",
        "Guanghui Ren",
        "Hao Dong"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T13:53:25Z",
      "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
      "pdf_url": "https://arxiv.org/pdf/2512.19402v1",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19347v1",
      "title": "OMP: One-step Meanflow Policy with Directional Alignment",
      "authors": [
        "Han Fang",
        "Yize Huang",
        "Yuheng Zhao",
        "Paul Weng",
        "Xiao Li",
        "Yutong Ban"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T12:45:35Z",
      "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.",
      "pdf_url": "https://arxiv.org/pdf/2512.19347v1",
      "categories": [
        "cs.RO"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19302v1",
      "title": "Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing",
      "authors": [
        "Xu Zhang",
        "Junyao Ge",
        "Yang Zheng",
        "Kaitai Guo",
        "Jimin Liang"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T11:46:42Z",
      "summary": "Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.",
      "pdf_url": "https://arxiv.org/pdf/2512.19302v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19225v1",
      "title": "Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI",
      "authors": [
        "Beyza Zayim",
        "Aissiou Ikram",
        "Boukhiar Naima"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-22T10:05:37Z",
      "summary": "Breast cancer remains the most common cancer among women and is a leading cause of female mortality. Dynamic contrast-enhanced MRI (DCE-MRI) is a powerful imaging tool for evaluating breast tumors, yet the field lacks a standardized benchmark for analyzing treatment responses and guiding personalized care. We participated in the MAMA-MIA Challenge's Primary Tumor Segmentation task and this work presents a proposed selective, phase-aware training framework for the nnU-Net architecture, emphasizing quality-focused data selection to strengthen model robustness and generalization. We employed the No New Net (nnU-Net) framework with a selective training strategy that systematically analyzed the impact of image quality and center-specific variability on segmentation performance. Controlled experiments on the DUKE, NACT, ISPY1, and ISPY2 datasets revealed that including ISPY scans with motion artifacts and reduced contrast impaired segmentation performance, even with advanced preprocessing, such as contrast-limited adaptive histogram equalization (CLAHE). In contrast, training on DUKE and NACT data, which exhibited clearer contrast and fewer motion artifacts despite varying resolutions, with early phase images (0000-0002) provided more stable training conditions. Our results demonstrate the importance of phase-sensitive and quality-aware training strategies in achieving reliable segmentation performance in heterogeneous clinical datasets, highlighting the limitations of the expansion of naive datasets and motivating the need for future automation of quality-based data selection strategies.",
      "pdf_url": "https://arxiv.org/pdf/2512.19225v1",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.18918v1",
      "title": "Needles in a haystack: using forensic network science to uncover insider trading",
      "authors": [
        "Gian Jaeger",
        "Wang Ngai Yeung",
        "Renaud Lambiotte"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T23:34:15Z",
      "summary": "Although the automation and digitisation of anti-financial crime investigation has made significant progress in recent years, detecting insider trading remains a unique challenge, partly due to the limited availability of labelled data. To address this challenge, we propose using a data-driven networks approach that flags groups of corporate insiders who report coordinated transactions that are indicative of insider trading. Specifically, we leverage data on 2.9 million trades reported to the U.S. Securities and Exchange Commission (SEC) by company insiders (C-suite executives, board members and major shareholders) between 2014 and 2024. Our proposed algorithm constructs weighted edges between insiders based on the temporal similarity of their trades over the 10-year timeframe. Within this network we then uncover trends that indicate insider trading by focusing on central nodes and anomalous subgraphs. To highlight the validity of our approach we evaluate our findings with reference to two null models, generated by running our algorithm on synthetic empirically calibrated and shuffled datasets. The results indicate that our approach can be used to detect pairs or clusters of insiders whose behaviour suggests insider trading and/or market manipulation.",
      "pdf_url": "https://arxiv.org/pdf/2512.18918v1",
      "categories": [
        "cs.SI",
        "physics.data-an",
        "q-fin.CP"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.18087v1",
      "title": "AI Assisted Next Gen Outdoor Optical Networks: Camera Sensing for Monitoring and User Localization",
      "authors": [
        "Meysam Ghanbari",
        "Mohammad Taghi Dabiri",
        "Rula Ammuri",
        "Mazen Hasna",
        "Khalid Qaraqe"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-19T21:47:25Z",
      "summary": "We consider outdoor optical access points (OAPs), which, enabled by recent advances in metasurface technology, have attracted growing interest. While OAPs promise high data rates and strong physical-layer security, practical deployments still expose vulnerabilities and misuse patterns that necessitate a dedicated monitoring layer - the focus of this work. We therefore propose a user positioning and monitoring system that infers locations from spatial intensity measurements on a photodetector (PD) array. Specifically, our hybrid approach couples an optics-informed forward model and sparse, model-based inversion with a lightweight data-driven calibration stage, yielding high accuracy at low computational cost. This design preserves the interpretability and stability of model-based reconstruction while leveraging learning to absorb residual nonidealities and device-specific distortions. Under identical hardware and training conditions (both with 5 x 10^5 samples), the hybrid method attains consistently lower mean-squared error than a generic deep-learning baseline while using substantially less training time and compute. Accuracy improves with array resolution and saturates around 60 x 60-80 x 80, indicating a favorable accuracy-complexity trade-off for real-time deployment. The resulting position estimates can be cross-checked with real-time network logs to enable continuous monitoring, anomaly detection (e.g., potential eavesdropping), and access control in outdoor optical access networks.",
      "pdf_url": "https://arxiv.org/pdf/2512.18087v1",
      "categories": [
        "eess.SP"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.10232v1",
      "title": "Permutation-Equivariant Learning for Dynamic Security Assessment of Power System Frequency Response",
      "authors": [
        "Francisco Zelaya-Arrazabal",
        "Sebastian Martinez-Lizana",
        "Hector Pulgar-Painemal",
        "Jin Zhao"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-11T02:37:16Z",
      "summary": "This paper presents a hybrid model-AI framework for real-time dynamic security assessment of frequency stability in power systems. The proposed method rapidly estimates key frequency parameters under a dynamic set of disturbances, which are continuously updated based on operating conditions and unit commitment. To achieve this, the framework builds on a modal-based formulation of the system frequency response (SFR), which leverages the system's eigenstructure to predict key frequency stability metrics. A Deep Sets-inspired neural network is employed to estimate the complex modal coefficients required by the modal-based SFR approach, formulated as a permutation-equivariant learning problem. This enables fast and accurate prediction of the frequency nadir and its timing across different operating conditions and disturbances. The framework achieves scalability by reusing precomputed modal structures and updating only the disturbance-specific coefficients. It demonstrates strong generalization capabilities without requiring an extensive set of operating scenarios during training or the widespread deployment of phasor measurement units (PMUs). The method is validated on the IEEE 39-bus and 118-bus systems, showing superior accuracy, robustness, and computational efficiency compared to purely data-driven approaches.",
      "pdf_url": "https://arxiv.org/pdf/2512.10232v1",
      "categories": [
        "eess.SY"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.06800v1",
      "title": "Cloud Revolution: Tracing the Origins and Rise of Cloud Computing",
      "authors": [
        "Deepa Gurung",
        "S M Zia Ur Rashid",
        "Zain ul Abdeen",
        "Suman Rath"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-07T11:29:39Z",
      "summary": "The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.",
      "pdf_url": "https://arxiv.org/pdf/2512.06800v1",
      "categories": [
        "cs.DC"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.20271v1",
      "title": "Automated Training of Learned Database Components with Generative AI",
      "authors": [
        "Angjela Davitkova",
        "Sebastian Michel"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-23T11:24:52Z",
      "summary": "The use of deep learning for database optimization has gained significant traction, offering improvements in indexing, cardinality estimation, and query optimization. However, acquiring high-quality training data remains a significant challenge. This paper explores the possibility of using generative models, such as GPT, to synthesize training data for learned database components. We present an initial feasibility study investigating their ability to produce realistic query distributions and execution plans for database workloads. Additionally, we discuss key challenges, such as data scalability and labeling, along with potential solutions. The initial results suggest that generative models can effectively augment training datasets, improving the adaptability of learned database techniques.",
      "pdf_url": "https://arxiv.org/pdf/2512.20271v1",
      "categories": [
        "cs.DB"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.18582v1",
      "title": "Wireless Copilot: An AI-Powered Partner for Navigating Next-Generation Wireless Complexity",
      "authors": [
        "Haoxiang Luo",
        "Ruichen Zhang",
        "Yinqiu Liu",
        "Gang Sun",
        "Hongfang Yu",
        "Dusit Niyato",
        "Shiwen Mao",
        "Dong In Kim"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T03:58:34Z",
      "summary": "The sixth-generation (6G) of wireless networks introduces a level of operational complexity that exceeds the limits of traditional automation and manual oversight. This paper introduces the \"Wireless Copilot\", an AI-powered technical assistant designed to function as a collaborative partner for human network designers, engineers, and operators. We posit that by integrating Large Language Models (LLMs) with a robust cognitive framework. It will surpass the existing AI tools and interact with wireless devices, transmitting the user's intentions into the actual network execution process. Then, Wireless Copilot can translate high-level human intent into precise, optimized, and verifiable network actions. This framework bridges the gap between human expertise and machine-scale complexity, enabling more efficient, intelligent, and trustworthy management of 6G systems. Wireless Copilot will be a novel layer between the wireless infrastructure and the network operators. Moreover, we explore Wireless Copilot's methodology and analyze its application in Low-Altitude Wireless Networks (LAWNets) assisting 6G networking, including network design, configuration, evaluation, and optimization. Additionally, we present a case study on intent-based LAWNets resource allocation, demonstrating its superior adaptability compared to others. Finally, we outline future research directions toward creating a comprehensive human-AI collaborative ecosystem for the 6G era.",
      "pdf_url": "https://arxiv.org/pdf/2512.18582v1",
      "categories": [
        "cs.NI"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.19764v1",
      "title": "Visual Event Detection over AI-Edge LEO Satellites with AoI Awareness",
      "authors": [
        "Chathuranga M. Wijerathna Basnayaka",
        "Haeyoung Lee",
        "Pandelis Kourtessis",
        "John M. Senior",
        "Vishalya P. Sooriarachchi",
        "Dushantha Nalin K. Jayakody",
        "Marko Beko",
        "Seokjoo Shin"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-21T00:13:02Z",
      "summary": "Non terrestrial networks (NTNs), particularly low Earth orbit (LEO) satellite systems, play a vital role in supporting future mission critical applications such as disaster relief. Recent advances in artificial intelligence (AI)-native communications enable LEO satellites to act as intelligent edge nodes capable of on board learning and task oriented inference. However, the limited link budget, coupled with severe path loss and fading, significantly constrains reliable downlink transmission. This paper proposes a deep joint source-channel coding (DJSCC)-based downlink scheme for AI-native LEO networks, optimized for goal-oriented visual inference. In the DJSCC approach, only semantically meaningful features are extracted and transmitted, whereas conventional separate source-channel coding (SSCC) transmits the original image data. To evaluate information freshness and visual event detection performance, this work introduces the age of misclassified information (AoMI) metric and a threshold based AoI analysis that measures the proportion of users meeting application specific timeliness requirements. Simulation results show that the proposed DJSCC scheme provides higher inference accuracy, lower average AoMI, and greater threshold compliance than the conventional SSCC baseline, enabling semantic communication in AI native LEO satellite networks for 6G and beyond.",
      "pdf_url": "https://arxiv.org/pdf/2512.19764v1",
      "categories": [
        "cs.IT",
        "eess.SP",
        "physics.app-ph"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.16201v1",
      "title": "Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation",
      "authors": [
        "Sarosij Bose",
        "Ravi K. Rajendran",
        "Biplob Debnath",
        "Konstantinos Karydis",
        "Amit K. Roy-Chowdhury",
        "Srimat Chakradhar"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-18T05:48:21Z",
      "summary": "Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.",
      "pdf_url": "https://arxiv.org/pdf/2512.16201v1",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.16027v1",
      "title": "SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments",
      "authors": [
        "Shuaidong Ji",
        "Mahdi Bamdad",
        "Francisco Cruz"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T23:19:06Z",
      "summary": "Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.",
      "pdf_url": "https://arxiv.org/pdf/2512.16027v1",
      "categories": [
        "cs.RO"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    },
    {
      "arxiv_id": "2512.15126v2",
      "title": "3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding",
      "authors": [
        "Yupeng Zhu",
        "Xiongzhen Zhang",
        "Ye Chen",
        "Bingbing Ni"
      ],
      "affiliations": [],
      "year": 2025,
      "published": "2025-12-17T06:38:07Z",
      "summary": "3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.   To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.",
      "pdf_url": "https://arxiv.org/pdf/2512.15126v2",
      "categories": [
        "cs.CV"
      ],
      "relevance_score": 20,
      "relevance_reasons": [
        "Published in 2025 (+20)"
      ]
    }
  ]
}