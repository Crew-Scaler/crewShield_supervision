{
  "arxiv_id": "2512.01326",
  "title": "Securing Large Language Models (LLMs) from Prompt Injection Attacks",
  "authors": [
    "Omar Farooq Khan Suri",
    "John McCrae"
  ],
  "affiliation": "Omar Farooq Khan Suri",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.01326",
  "pdf_url": "https://arxiv.org/pdf/2512.01326.pdf",
  "relevance_score": 88,
  "query_id": "B4",
  "pages": 7,
  "key_topics": [
    "LLM security",
    "prompt injection"
  ],
  "summary": "Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-..."
}