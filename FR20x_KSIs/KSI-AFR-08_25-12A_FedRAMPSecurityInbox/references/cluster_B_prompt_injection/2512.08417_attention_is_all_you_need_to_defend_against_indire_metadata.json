{
  "arxiv_id": "2512.08417",
  "title": "Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs",
  "authors": [
    "Yinan Zhong",
    "Qianhao Miao",
    "Yanjiao Chen",
    "Jiangyi Deng",
    "Yushi Cheng",
    "Wenyuan Xu"
  ],
  "affiliation": "Yinan Zhong",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.08417",
  "pdf_url": "https://arxiv.org/pdf/2512.08417.pdf",
  "relevance_score": 82,
  "query_id": "B4",
  "pages": 7,
  "key_topics": [
    "LLM security",
    "prompt injection",
    "agent systems"
  ],
  "summary": "Large Language Models (LLMs) have been integrated into many applications (e.g., web agents) to perform more sophisticated tasks. However, LLM-empowered applications are vulnerable to Indirect Prompt I..."
}