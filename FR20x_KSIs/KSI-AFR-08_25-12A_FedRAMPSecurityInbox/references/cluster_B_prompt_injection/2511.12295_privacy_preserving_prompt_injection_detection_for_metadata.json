{
  "arxiv_id": "2511.12295",
  "title": "Privacy-Preserving Prompt Injection Detection for LLMs Using Federated Learning and Embedding-Based NLP Classification",
  "authors": [
    "Hasini Jayathilaka"
  ],
  "affiliation": "Hasini Jayathilaka",
  "published_date": "2025-11",
  "url": "https://arxiv.org/abs/2511.12295",
  "pdf_url": "https://arxiv.org/pdf/2511.12295.pdf",
  "relevance_score": 76,
  "query_id": "B4",
  "pages": 7,
  "key_topics": [
    "LLM security",
    "prompt injection",
    "threat modeling"
  ],
  "summary": "Prompt injection attacks are an emerging threat to large language models (LLMs), enabling malicious users to manipulate outputs through carefully designed inputs. Existing detection approaches often r..."
}