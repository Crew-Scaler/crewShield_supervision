{
  "arxiv_id": "2511.01634",
  "title": "Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models",
  "authors": [
    "Daniyal Ganiuly",
    "Assel Smaiyl"
  ],
  "affiliation": "Daniyal Ganiuly",
  "published_date": "2025-11",
  "url": "https://arxiv.org/abs/2511.01634",
  "pdf_url": "https://arxiv.org/pdf/2511.01634.pdf",
  "relevance_score": 82,
  "query_id": "B4",
  "pages": 7,
  "key_topics": [
    "LLM security",
    "prompt injection",
    "threat modeling"
  ],
  "summary": "Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while power..."
}