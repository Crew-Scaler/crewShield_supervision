{
  "arxiv_id": "2512.25026v1",
  "title": "Modeling Language as a Sequence of Thoughts",
  "authors": [
    "Nasim Borazjanizadeh",
    "James McClelland"
  ],
  "affiliation": "Nasim Borazjanizadeh",
  "published_date": "2025-12",
  "url": "https://arxiv.org/abs/2512.25026v1",
  "relevance_score": 65,
  "key_topics": [],
  "summary": "Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittl"
}