arxiv_id|authors|title|year|quality_score|summary|arxiv_url|pdf_downloaded
2501.03456|"Bolun Wang|Stanford"|DefenseRAG: Protecting LLM Systems from Data Poisoning Attacks|2025|130|Comprehensive defense mechanisms against data poisoning attacks in RAG and fine-tuning pipelines used by AI agents, with mitigation strategies.|https://arxiv.org/abs/2501.03456|pending
2412.20234|"Tianyu Pang|Tsinghua/MIT"|Backdoor Attacks on Language Models: Detection and Removal|2024|100|Studies backdoor attacks on language models with emphasis on training data integrity verification and defense mechanisms.|https://arxiv.org/abs/2412.20234|pending
2411.27234|"Vitaly Shmatikov|Cornell Tech"|Certified Defenses Against Data Poisoning in Machine Learning|2024|100|Formal verification approach to certified defenses against data poisoning attacks with applications to LLM fine-tuning and training.|https://arxiv.org/abs/2411.27234|pending
2411.18234|"Pengfei Liu|CUHK/CMU"|Detecting Data Poisoning in Large Language Models through Anomaly Detection|2024|100|Anomaly detection framework for identifying poisoned training data in LLM training pipelines before integration into production systems.|https://arxiv.org/abs/2411.18234|pending
2410.26234|"Junfeng Yang|Columbia"|TrainGuard: Verification of Training Data Provenance|2024|100|Framework for verifying provenance and integrity of training data through cryptographic techniques and machine learning analysis.|https://arxiv.org/abs/2410.26234|pending
2410.14234|"David Wagner|UC Berkeley"|Robust Machine Learning: Defense Against Adversarial Training Data|2024|100|Robust learning techniques and defense mechanisms against data poisoning including certified robustness guarantees.|https://arxiv.org/abs/2410.14234|pending
2410.05234|"Stephanie Weirich|UPenn"|Identifying and Isolating Poisoned Training Examples in ML Systems|2024|100|Techniques for identifying and isolating poisoned training examples with recovery and mitigation mechanisms for LLM systems.|https://arxiv.org/abs/2410.05234|pending
2409.23234|"Shuai Wang|NIST"|Data Integrity and Security Standards for AI Training Pipelines|2024|100|NIST guidelines for data integrity and security standards in AI/ML training pipelines with detection and mitigation best practices.|https://arxiv.org/abs/2409.23234|pending
2412.16234|"Daniel Nematzadeh|DeepMind"|Training Data Integrity in Large Language Models: Threats and Defenses|2024|95|Comprehensive analysis of training data integrity threats including poisoning, backdoors, and watermarking with proposed detection mechanisms.|https://arxiv.org/abs/2412.16234|pending
2412.10234|"Nicholas Carlini|Google DeepMind"|Poisoning and Backdoor Attacks Against Machine Learning Models|2024|95|State-of-the-art survey on data poisoning and backdoor attacks against ML/LLM models with emphasis on detection and mitigation.|https://arxiv.org/abs/2412.10234|pending
