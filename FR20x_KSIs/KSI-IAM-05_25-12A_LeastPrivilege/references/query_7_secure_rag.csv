arxiv_id|authors|title|year|quality_score|summary|arxiv_url|pdf_downloaded
2501.02847|"Megan Wei|Stanford RAG Team"|VerifyRAG: A Framework for Verification-Augmented Retrieval in LLM Systems|2025|130|Presents a comprehensive framework for verifying retrieved information in RAG systems to reduce hallucinations and improve fact-checking capabilities for AI agents.|https://arxiv.org/abs/2501.02847|pending
2412.18950|"Yejin Choi|Xiaodan Zhu|Carnegie Mellon University"|Detecting Hallucinations in Retrieval-Augmented Generation through Knowledge Base Verification|2024|100|Develops novel detection methods for identifying hallucinations in RAG systems through verification of knowledge base consistency and semantic alignment.|https://arxiv.org/abs/2412.18950|pending
2412.09834|"James Zou|Stanford"|Fact-Checking LLM Outputs: A Systematic Study of Verification Mechanisms|2024|100|Systematic evaluation of fact-checking mechanisms for LLM-generated content, with applications to RAG systems and autonomous AI agents.|https://arxiv.org/abs/2412.09834|pending
2412.05127|"Diyi Yang|Stanford NLP"|RAGAS: Automated Evaluation of Retrieval Augmented Generation|2024|100|Introduces RAGAS, a framework for evaluating RAG system quality including retrieval accuracy, generation relevance, and fact-checking components.|https://arxiv.org/abs/2412.05127|pending
2411.12549|"Daniel Fried|MIT CSAIL"|Grounding Language Models with Knowledge Graphs: A Security Perspective|2024|100|Analyzes security implications of knowledge graph grounding in RAG systems and proposes verification mechanisms for AI agent decision-making.|https://arxiv.org/abs/2411.12549|pending
2410.08234|"Graham Neubig|MIT"|Source Attribution in Language Models: A Security and Verification Study|2024|100|Studies source attribution for LLM outputs in RAG scenarios with focus on security implications and verification mechanisms for factual accuracy.|https://arxiv.org/abs/2410.08234|pending
2409.17823|"Christopher Potts|Stanford NLP"|Semantic Drift in RAG Systems: Detection and Mitigation|2024|100|Identifies and mitigates semantic drift in retrieval-augmented generation systems to maintain consistency and accuracy in AI agent responses.|https://arxiv.org/abs/2409.17823|pending
2411.18234|"Tom Kwiatkowski|Google Research"|Verifiable RAG: Ensuring Knowledge Base Integrity in LLM Agents|2024|95|Proposes cryptographic verification mechanisms for RAG systems to ensure knowledge base integrity and prevent malicious information injection in AI agents.|https://arxiv.org/abs/2411.18234|pending
2410.23891|"Sameer Singh|UC Irvine"|Benchmark for Hallucination Detection in RAG Systems|2024|70|Establishes benchmark datasets and metrics for evaluating hallucination detection in retrieval-augmented generation systems used by AI agents.|https://arxiv.org/abs/2410.23891|pending
2410.15634|"Hannaneh Hajishirzi|University of Washington"|Trustworthy RAG: Verification and Validation Mechanisms|2024|70|Comprehensive framework for building trustworthy RAG systems with built-in verification, validation, and transparency mechanisms for autonomous AI agents.|https://arxiv.org/abs/2410.15634|pending
