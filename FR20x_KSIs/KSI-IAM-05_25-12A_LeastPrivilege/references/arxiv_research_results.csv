Query|arxiv_id|authors|title|year|quality_score|summary|arxiv_url|pdf_downloaded|published_date
Query 10: Privilege Escalation & Service Account Monitoring|2510.13893v1|Olga E. Sorokoletova, Francesco Giarrusso, Vincenzo Suriani...|Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection|2025|100|Jailbreaking techniques pose a significant threat to the safety of Large Language Models (LLMs). Existing defenses typically focus on single-turn attacks, lack coverage across languages, and rely on l|http://arxiv.org/abs/2510.13893v1|yes|2025-10-14
Query 10: Privilege Escalation & Service Account Monitoring|2509.04191v1|Omri Sgan Cohen, Ehud Malul, Yair Meidan...|KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and Runtime Logs Analysis|2025|100|The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native applications has introduced significant security challenges, such as misconfigured resources and overly permissive configurat|http://arxiv.org/abs/2509.04191v1|yes|2025-09-04
Query 10: Privilege Escalation & Service Account Monitoring|2506.20503v1|Edoardo Di Paolo, Fabio De Gaspari, Angelo Spognardi|BotHash: Efficient and Training-Free Bot Detection Through Approximate Nearest Neighbor|2025|100|Online Social Networks (OSNs) are a cornerstone in modern society, serving as platforms for diverse content consumption by millions of users each day. However, the challenge of ensuring the accuracy o|http://arxiv.org/abs/2506.20503v1|yes|2025-06-25
Query 11: Policy-as-Code & Governance for AI Agents|2512.07462v2|Trung-Kiet Huynh, Duy-Minh Dao-Sy, Thanh-Bang Cao...|Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics|2025|100|As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound impl|http://arxiv.org/abs/2512.07462v2|yes|2025-12-08
Query 11: Policy-as-Code & Governance for AI Agents|2512.05951v2|Teofil Bodea, Masanori Misono, Julian Pritzi...|Trusted AI Agents in the Cloud|2025|100|AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these ag|http://arxiv.org/abs/2512.05951v2|yes|2025-12-05
Query 11: Policy-as-Code & Governance for AI Agents|2512.04416v2|Zhou Liu, Zhaoyang Han, Guochen Yan...|DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows|2025|100|Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emer|http://arxiv.org/abs/2512.04416v2|yes|2025-12-04
Query 11: Policy-as-Code & Governance for AI Agents|2512.00742v1|K. J. Kevin Feng, Tae Soo Kim, Rock Yuren Pang...|On the Regulatory Potential of User Interfaces for AI Agent Governance|2025|100|AI agents that take actions in their environment autonomously over extended time horizons require robust governance interventions to curb their potentially consequential risks. Prior proposals for gov|http://arxiv.org/abs/2512.00742v1|yes|2025-11-30
Query 11: Policy-as-Code & Governance for AI Agents|2511.22619v2|Boyuan Chen, Sitong Fang, Jiaming Ji...|AI Deception: Risks, Dynamics, and Controls|2025|100|As intelligence increases, so does its shadow. AI deception, in which systems induce false beliefs to secure self-beneficial outcomes, has evolved from a speculative concern to an empirically demonstr|http://arxiv.org/abs/2511.22619v2|yes|2025-11-27
Query 11: Policy-as-Code & Governance for AI Agents|2511.06262v1|Siming Zhao, Qi Li|GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening|2025|100|Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized c|http://arxiv.org/abs/2511.06262v1|yes|2025-11-09
Query 11: Policy-as-Code & Governance for AI Agents|2511.03248v2|Junhao Li, Jiahao Chen, Zhou Feng...|Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation Framework|2025|100|Recent advances in multi-modal Large Language Models (M-LLMs) have demonstrated a powerful ability to synthesize implicit information from disparate sources, including images and text. These resourcef|http://arxiv.org/abs/2511.03248v2|yes|2025-11-05
Query 11: Policy-as-Code & Governance for AI Agents|2510.26212v1|Yifeng Cai, Ziming Wang, Zhaomeng Deng...|Who Grants the Agent Power? Defending Against Instruction Injection via Task-Centric Access Control|2025|100|AI agents capable of GUI understanding and Model Context Protocol are increasingly deployed to automate mobile tasks. However, their reliance on over-privileged, static permissions creates a critical |http://arxiv.org/abs/2510.26212v1|yes|2025-10-30
Query 11: Policy-as-Code & Governance for AI Agents|2510.24383v1|Juraj Mavračić|Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents|2025|100|Policy Cards are introduced as a machine-readable, deployment-layer standard for expressing operational, regulatory, and ethical constraints for AI agents. The Policy Card sits with the agent and enab|http://arxiv.org/abs/2510.24383v1|yes|2025-10-28
Query 11: Policy-as-Code & Governance for AI Agents|2510.21117v2|Agostino Capponi, Alfio Gliozzo, Chunghyun Han...|DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance|2025|100|This paper presents a first empirical study of agentic AI as autonomous decision-makers in decentralized governance. Using more than 3K proposals from major protocols, we build an agentic AI voter tha|http://arxiv.org/abs/2510.21117v2|yes|2025-10-24
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.13337v1|Si Qi Goh, Yongsen Zheng, Ziyao Liu...|FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs|2025|100|Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficien|http://arxiv.org/abs/2512.13337v1|yes|2025-12-15
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.08261v1|Yibowen Zhao, Yinan Zhang, Zhixiang Su...|Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes|2025|100|Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awarenes|http://arxiv.org/abs/2512.08261v1|yes|2025-12-09
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.08026v1|Caroline N. Leach, Mitchell A. Klusty, Samuel E. Armstrong...|Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching|2025|100|Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (A|http://arxiv.org/abs/2512.08026v1|yes|2025-12-08
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.06781v1|Sima Jafarikhah, Daniel Thompson, Eva Deans...|From Description to Score: Can LLMs Quantify Vulnerabilities?|2025|100|Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study inv|http://arxiv.org/abs/2512.06781v1|yes|2025-12-07
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.06659v1|Vaishali Vinay|The Evolution of Agentic AI in Cybersecurity: From Single LLM Reasoners to Multi-Agent Systems and Autonomous Pipelines|2025|100|Cybersecurity has become one of the earliest adopters of agentic AI, as security operations centers increasingly rely on multi-step reasoning, tool-driven analysis, and rapid decision-making under pre|http://arxiv.org/abs/2512.06659v1|yes|2025-12-07
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.06105v1|Junwen Zheng, Xinran Xu, Li Rong Wang...|Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation|2025|100|Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability r|http://arxiv.org/abs/2512.06105v1|yes|2025-12-05
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.05908v1|Amirkia Rafiei Oskooei, S. Selcan Yukcu, Mehmet Cevheri Bozoglan...|Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures|2025|130|Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first ide|http://arxiv.org/abs/2512.05908v1|yes|2025-12-05
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.05365v1|Zag ElSayed, Craig Erickson, Ernest Pedapati|MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare|2025|100|Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a |http://arxiv.org/abs/2512.05365v1|yes|2025-12-05
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.05065v2|James Flemings, Ren Yi, Octavian Suciu...|Personalizing Agent Privacy Decisions via Logical Entailment|2025|100|Personal language model-based agents are becoming more widespread for completing tasks on behalf of users; however, this raises serious privacy questions regarding whether these models will appropriat|http://arxiv.org/abs/2512.05065v2|yes|2025-12-04
Query 12: Explainable AI (XAI) in Security Operations & Trust|2512.04841v1|Wei Zhao, Zhe Li, Jun Sun|SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security|2025|100|Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the |http://arxiv.org/abs/2512.04841v1|yes|2025-12-04
