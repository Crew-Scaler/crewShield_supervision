arxiv_id|authors|title|year|quality_score|summary|arxiv_url|pdf_downloaded|query_name
2502.11347|Unknown; DeepSeek Team|Evaluating the Performance of the DeepSeek Model in Confidential Computing Environment|2025|130|First performance evaluation of DeepSeek model in confidential computing environment. Addresses security concerns regarding model confidentiality and data privacy in cloud-based LLM services. Compares CPU-based TEEs, standard CPU execution, and GPU-accelerated platforms.|https://arxiv.org/abs/2502.11347|PENDING|Query 4: Confidential Computing & TEE
2507.16226|Unknown; Circuit Design Team|Distilled Large Language Model in Confidential Computing Environment for System-on-Chip Design|2025|130|Addresses how LLMs in circuit design require protection of training data and model confidentiality through TEEs. Focuses on intellectual property security for system-on-chip applications.|https://arxiv.org/abs/2507.16226|PENDING|Query 4: Confidential Computing & TEE
2508.19870|Unknown; Liu et al|Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey|2025|130|Introduces LoRATEE framework for secure multi-tenant LoRA-based LLM inference using TEEs. Covers deployment of LLMs within TEEs on edge devices with SGX enclaves for isolated processing.|https://arxiv.org/abs/2508.19870|Yes|Query 4: Confidential Computing & TEE
2504.08508|Unknown|An Early Experience with Confidential Computing Architecture for On-Device Model Protection|2025|130|Explores TEEs as practical solution for protecting proprietary models on-device. Accepted to 8th Workshop on System Software for Trusted Execution (SysTEX 2025). Addresses architectural constraints of current TEE solutions.|https://arxiv.org/abs/2504.08508|PENDING|Query 4: Confidential Computing & TEE
2410.13752|Unknown|Privacy-Preserving Decentralized AI with Confidential Computing|2024|70|Research on zkML and confidential computing approaches for privacy-preserving AI. Discusses computational and latency overhead challenges for LLM applications and emerging solutions.|https://arxiv.org/abs/2410.13752|PENDING|Query 4: Confidential Computing & TEE
2409.03992|Unknown|Confidential Computing on NVIDIA Hopper GPUs: A Performance Benchmark Study|2024|70|Evaluates performance impact of enabling TEEs on NVIDIA Hopper GPUs for LLM inference tasks. Benchmarks show overhead remains below 7% for typical queries with larger models experiencing near-zero overhead.|https://arxiv.org/abs/2409.03992|PENDING|Query 4: Confidential Computing & TEE
2509.18886|Unknown|Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs|2025|130|Comprehensive analysis of confidential LLM inference performance and cost across CPU-based and GPU-based trusted execution environments with detailed comparative metrics.|https://arxiv.org/abs/2509.18886|PENDING|Query 4: Confidential Computing & TEE
2409.19134|Unknown|Confidential Prompting: Protecting User Prompts from Cloud LLM Providers|2024|70|Addresses privacy protection of user prompts in cloud LLM services through confidential computing approaches. Protects sensitive user instructions from cloud provider inspection.|https://arxiv.org/abs/2409.19134|PENDING|Query 4: Confidential Computing & TEE
2409.04040|Unknown|A First Look At Efficient And Secure On-Device LLM Inference Against KV Leakage|2024|70|Explores secure on-device LLM inference with protection against key-value cache leakage attacks. Addresses memory isolation requirements for sensitive LLM operations.|https://arxiv.org/abs/2409.04040|PENDING|Query 4: Confidential Computing & TEE
2510.19979|Unknown|SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment|2025|130|Hybrid framework leveraging heterogeneous TEE-GPU architecture to isolate privacy-critical components while offloading compute-intensive operations to untrusted accelerators. Solves TEE performance limitations.|https://arxiv.org/abs/2510.19979|PENDING|Query 4: Confidential Computing & TEE
2506.23260|Unknown; Threat Modeling Team|From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows|2025|130|Unified end-to-end threat model for LLM-agent ecosystems cataloging over 30 attack techniques. Covers input manipulation, model compromise, system attacks, and privacy issues across agent workflows.|https://arxiv.org/abs/2506.23260|PENDING|Query 5: Prompt Injection & Input Validation
2505.04806|Unknown; Red Team Research|Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs|2025|130|Systematic evaluation of prompt injection and jailbreak vulnerabilities. Roleplay exploitation achieved 89.6% ASR, logic trap attacks 81.4%, encoding tricks 76.2%. Comprehensive attack surface analysis.|https://arxiv.org/abs/2505.04806|PENDING|Query 5: Prompt Injection & Input Validation
2504.11168|Unknown; Defense Research Team|Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails|2025|130|Research on bypassing existing defense mechanisms for prompt injection and jailbreak detection. Documents techniques for circumventing current LLM safeguards and guardrails.|https://arxiv.org/abs/2504.11168|PENDING|Query 5: Prompt Injection & Input Validation
2511.15759|Unknown; Security Framework Team|Securing AI Agents Against Prompt Injection Attacks: A Comprehensive Benchmark and Defense Framework|2025|130|Comprehensive benchmark and defense framework for securing AI agents. Uses content filtering, hierarchical guardrails, multi-stage verification. Reduces ASR from 73.2% to 8.7% while maintaining 94.3% baseline performance.|https://arxiv.org/abs/2511.15759|PENDING|Query 5: Prompt Injection & Input Validation
2507.13169|Unknown; Advanced Threats Team|Prompt Injection 2.0: Hybrid AI Threats|2025|130|Novel perspective on prompt injection attacks as hybrid threats combining multiple attack vectors. Addresses emerging attack patterns and composite threat scenarios.|https://arxiv.org/abs/2507.13169|PENDING|Query 5: Prompt Injection & Input Validation
2509.14285|Unknown; Multi-Agent Defense Team|A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks|2025|130|Multi-agent defense framework with distributed detection achieving 0% ASR across 55+ unique adversarial cases while maintaining 100% baseline performance. Complete mitigation demonstrated.|https://arxiv.org/abs/2509.14285|PENDING|Query 5: Prompt Injection & Input Validation
2502.00580|Unknown; Defense Mechanisms Team|Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation|2025|130|Defense mechanism using prompt evaluation to mitigate best-of-N jailbreaking attacks. Targets sophisticated multi-attempt attack patterns.|https://arxiv.org/abs/2502.00580|PENDING|Query 5: Prompt Injection & Input Validation
2410.07283|Unknown|Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems|2024|70|Novel Prompt Infection attack where malicious prompts self-replicate across interconnected agents like computer viruses. Poses severe threats including data theft, scams, misinformation, and system-wide disruption.|https://arxiv.org/abs/2410.07283|PENDING|Query 5: Prompt Injection & Input Validation
2505.05849|Unknown; Red Team Framework Team|AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents|2025|130|Black-box red-teaming framework for testing indirect prompt injection vulnerabilities in LLM-based agents. Systematic methodology for discovering agent attack vectors.|https://arxiv.org/abs/2505.05849|PENDING|Query 5: Prompt Injection & Input Validation
2505.08807|Unknown; IoT Security Team|Security of Internet of Agents: Attacks and Countermeasures|2025|130|Addresses agent memorization of sensitive data showing extraction of PII including phone numbers and email via crafted query patterns. Documents attack vectors for data exfiltration from agents.|https://arxiv.org/abs/2505.08807|PENDING|Query 6: Data Classification & Sensitivity
2505.02077|Unknown; Multi-Agent Security Team|Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents|2025|130|Discusses novel threats in collaborative multi-agent environments including cascading privacy leaks and jailbreak proliferation across agent boundaries. Identifies emerging threat classes.|https://arxiv.org/abs/2505.02077|PENDING|Query 6: Data Classification & Sensitivity
2510.06445|Unknown; Agentic Security Survey Team|A Survey on Agentic Security: Applications, Threats and Defenses|2025|130|Comprehensive survey noting simple prompt injection attacks can leak personal data observed by agents during task execution. Complete taxonomy of agentic security challenges.|https://arxiv.org/abs/2510.06445|PENDING|Query 6: Data Classification & Sensitivity
2509.10018|Unknown; Privacy Preservation Team|GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Mechanism|2024|70|Multi-agent system with Anonymizing Mechanism for Privacy Preservation using knowledge and logic-based approaches. Domain rule enforcement and disproof mechanisms for data protection.|https://arxiv.org/abs/2509.10018|PENDING|Query 6: Data Classification & Sensitivity
2510.23883|Unknown; Threat Taxonomy Team|Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges|2025|130|Comprehensive survey with novel taxonomy of threats specific to agentic AI and discussion of amplified security risks from agent autonomy. First complete agentic threat classification.|https://arxiv.org/abs/2510.23883|PENDING|Query 6: Data Classification & Sensitivity
2506.04133|Unknown; TRiSM Framework Team|TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems|2025|130|Structured analysis of Trust, Risk, and Security Management for agentic systems. Four pillars: Identity, Data, Workload, Network. Proposes novel risk taxonomy for agent autonomy.|https://arxiv.org/abs/2506.04133|PENDING|Query 6: Data Classification & Sensitivity
2508.10043|Unknown; Network Security Team|Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System|2025|130|Threat modeling and risk analysis framework for network monitoring agentic AI systems. Documents network-specific threat vectors and mitigation strategies.|https://arxiv.org/abs/2508.10043|PENDING|Query 6: Data Classification & Sensitivity
2509.14956|Unknown; Sentinel Architecture Team|Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems|2024|70|Novel architectural framework with Sentinel Agents functioning as distributed security layer integrating semantic analysis for real-time threat detection.|https://arxiv.org/abs/2509.14956|PENDING|Query 6: Data Classification & Sensitivity
2510.16219|Unknown; Collaboration Security Team|SentinelNet: Safeguarding Multi-Agent Collaboration Through Secure Communication|2025|130|Framework for safeguarding multi-agent collaboration with secure communication protocols and trust verification mechanisms for inter-agent interactions.|https://arxiv.org/abs/2510.16219|PENDING|Query 6: Data Classification & Sensitivity
2502.18545|Unknown; PII Detection Team|PII-Bench: Evaluating Query-Aware Privacy Protection Systems|2025|130|Framework for evaluating query-aware privacy protection systems. Identifies which PII elements are essential to queries versus collateral exposure. Novel approach to privacy evaluation.|https://arxiv.org/abs/2502.18545|PENDING|Query 6: Data Classification & Sensitivity
