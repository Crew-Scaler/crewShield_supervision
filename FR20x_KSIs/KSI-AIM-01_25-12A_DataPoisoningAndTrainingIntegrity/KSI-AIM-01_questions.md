**KSI-AIM-01-Q01:** What procedures detect data poisoning attacks corrupting AI training datasets before model training? Document: (a) poisoning threat model (which training data sources attackable?), (b) detection mechanisms (statistical anomaly detection, adversarial example identification), (c) attack types defended (label flipping, feature corruption, backdoor poisoning), (d) research findings (22-27% accuracy degradation from poisoning detected), (e) response procedures (data quarantine, retraining from clean data).

**KSI-AIM-01-Q02:** How do you validate training data provenance ensuring logs/data used for model training are uncontaminated? Explain: (a) data source auditing (documenting all training sources), (b) integrity checks (checksums, format validation, anomaly detection), (c) supplier assessment (if third-party data, security evaluation), (d) continuous validation during retraining, (e) audit trail proving data integrity to compliance auditors.

**KSI-AIM-01-Q03:** What Byzantine-resilient aggregation techniques protect distributed training when participating data sources are compromised? Document: (a) distributed training architecture (multi-site contributions), (b) Byzantine tolerance threshold (how many poisoned sources tolerated?), (c) aggregation algorithm (median, trimmed mean, robust statistics), (d) validation (robust aggregation maintains model accuracy?), (e) research foundation (15-20% accuracy recovery from poisoning possible).

**KSI-AIM-01-Q04:** How do you detect implicit data poisoning where attackers generate malicious logs influencing model retraining? Describe: (a) attack scenario (adversary generates crafted logs with specific patterns), (b) detection approach (identifying unnatural log characteristics), (c) retraining validation (testing new models on clean held-out data), (d) model comparison (detecting accuracy drops from poisoning), (e) automated quarantine if poisoning suspected during retraining.

**KSI-AIM-01-Q05:** What is your clean data validation process before including AI activity logs in training datasets? Provide: (a) filtering criteria (outlier detection, format validation, known attack pattern identification), (b) class balance management (prevent majority class bias, research finding: 43% F1 improvement with proper handling), (c) human review procedures (borderline cases), (d) traceability (all training data decisions logged), (e) periodic re-validation as threat landscape evolves.

**KSI-AIM-01-Q06:** For LLM-based systems used in recovery documentation or decision support, what testing demonstrates that the LLM models have not been poisoned with false or misleading information during trainingâ€”particularly false incident procedures, hallucinated recovery steps, or systematically incorrect recovery recommendations that could cause failures during actual disaster recovery?

**KSI-AIM-01-Q07:** What mechanisms validate the integrity of training data pipelines that supply models used in AI agent communication generation and ensure that poisoned data cannot compromise the models generating communications? Explain: (a) data lineage tracking for training datasets (cryptographic provenance from source through processing), (b) integrity checks at each pipeline stage (validation, anomaly detection for poisoning indicators), (c) detection of poisoned data that could cause agents to generate malicious or inauthentic communications, (d) procedures for cryptographic verification that training data remains uncontaminated throughout the ML pipeline, (e) integration with communication integrity validation when agents are retrained.
