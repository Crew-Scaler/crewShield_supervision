{
  "stateful_inference": {
    "description": "Stateful AI inference failures - KV cache corruption, context state loss, model version inconsistency",
    "query": "(abs:\"stateful inference\" OR abs:\"KV cache\" OR abs:\"model state\") AND (abs:\"failure\" OR abs:\"corruption\" OR abs:\"recovery\")",
    "total_found": 37,
    "filtered_count": 25,
    "papers_2025": 20,
    "papers_2024": 5,
    "us_affiliated": 0,
    "papers": [
      {
        "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing",
        "authors": [
          "Lishuo Deng",
          "Shaojie Xu",
          "Jinwu Chen",
          "Changwei Yan",
          "Jiajie Wang",
          "Zhe Jiang",
          "Weiwei Shan"
        ],
        "published": "2025-12-03",
        "year": 2025,
        "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.",
        "pdf_url": "https://arxiv.org/pdf/2512.03608v1",
        "entry_id": "http://arxiv.org/abs/2512.03608v1",
        "categories": [
          "cs.AR",
          "cs.AI",
          "cs.ET"
        ],
        "us_affiliated": false
      },
      {
        "title": "CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights",
        "authors": [
          "Mohaiminul Al Nahian",
          "Abeer Matar A. Almalky",
          "Gamana Aragonda",
          "Ranyang Zhou",
          "Sabbir Ahmed",
          "Dmitry Ponomarev",
          "Li Yang",
          "Shaahin Angizi",
          "Adnan Siraj Rakin"
        ],
        "published": "2025-11-27",
        "year": 2025,
        "summary": "Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead.",
        "pdf_url": "https://arxiv.org/pdf/2511.22681v1",
        "entry_id": "http://arxiv.org/abs/2511.22681v1",
        "categories": [
          "cs.CR"
        ],
        "us_affiliated": false
      },
      {
        "title": "AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers",
        "authors": [
          "Boxun Xu",
          "Yu Wang",
          "Zihu Wang",
          "Peng Li"
        ],
        "published": "2025-11-20",
        "year": 2025,
        "summary": "Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.",
        "pdf_url": "https://arxiv.org/pdf/2511.16047v1",
        "entry_id": "http://arxiv.org/abs/2511.16047v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models",
        "authors": [
          "Yuhua Jiang",
          "Shuang Cheng",
          "Yan Ding",
          "Feifei Gao",
          "Biqing Qi"
        ],
        "published": "2025-11-18",
        "year": 2025,
        "summary": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.",
        "pdf_url": "https://arxiv.org/pdf/2511.14148v1",
        "entry_id": "http://arxiv.org/abs/2511.14148v1",
        "categories": [
          "cs.RO",
          "cs.AI",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Whose Narrative is it Anyway? A KV Cache Manipulation Attack",
        "authors": [
          "Mukkesh Ganesh",
          "Kaushik Iyer",
          "Arun Baalaaji Sankar Ananthan"
        ],
        "published": "2025-11-16",
        "year": 2025,
        "summary": "The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces \"History Swapping,\" a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.",
        "pdf_url": "https://arxiv.org/pdf/2511.12752v1",
        "entry_id": "http://arxiv.org/abs/2511.12752v1",
        "categories": [
          "cs.CR",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing",
        "authors": [
          "Mingyu Sung",
          "Vikas Palakonda",
          "Suhwan Im",
          "Sunghwan Moon",
          "Il-Min Kim",
          "Sangseok Yun",
          "Jae-Mo Kang"
        ],
        "published": "2025-11-06",
        "year": 2025,
        "summary": "Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy.",
        "pdf_url": "https://arxiv.org/pdf/2511.04002v1",
        "entry_id": "http://arxiv.org/abs/2511.04002v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism",
        "authors": [
          "Wendong Xu",
          "Chujie Chen",
          "He Xiao",
          "Kuan Li",
          "Jing Xiong",
          "Chen Zhang",
          "Wenyong Zhou",
          "Chaofan Tao",
          "Yang Bai",
          "Bei Yu",
          "Ngai Wong"
        ],
        "published": "2025-11-05",
        "year": 2025,
        "summary": "Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.",
        "pdf_url": "https://arxiv.org/pdf/2511.11617v1",
        "entry_id": "http://arxiv.org/abs/2511.11617v1",
        "categories": [
          "cs.DC"
        ],
        "us_affiliated": false
      },
      {
        "title": "Batch Speculative Decoding Done Right",
        "authors": [
          "Ranran Haoran Zhang",
          "Soumik Dey",
          "Ashirbad Mishra",
          "Hansi Wu",
          "Binbin Li",
          "Rui Zhang"
        ],
        "published": "2025-10-26",
        "year": 2025,
        "summary": "Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3$\\times$ throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.",
        "pdf_url": "https://arxiv.org/pdf/2510.22876v1",
        "entry_id": "http://arxiv.org/abs/2510.22876v1",
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity",
        "authors": [
          "Pratik Poudel"
        ],
        "published": "2025-10-23",
        "year": 2025,
        "summary": "The Key-Value (KV) cache is integral to efficient autoregressive inference in large language models (LLMs), yet its unbounded growth in stateful multi-turn scenarios presents major challenges. This paper examines the interplay between KV cache management strategies, the architectural context limits of models like meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of positional encodings. Through empirical analysis using a stateful benchmarking framework, we show that LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence. Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a cache by removing non-contiguous tokens can scramble these signals and lead to degenerative outputs. We further show that simple strategies preserving contiguous context blocks (e.g., keeping an initial \"gist\") can yield more coherent generations than complex or positionally disruptive ones. We advocate for eviction techniques that respect architectural limits, preserve positional structure, and view \"cache health\" holistically beyond mere size.",
        "pdf_url": "https://arxiv.org/pdf/2511.04686v1",
        "entry_id": "http://arxiv.org/abs/2511.04686v1",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models",
        "authors": [
          "Elias Hossain",
          "Swayamjit Saha",
          "Somshubhra Roy",
          "Ravi Prasad"
        ],
        "published": "2025-10-20",
        "year": 2025,
        "summary": "Even when prompts and parameters are secured, transformer language models remain vulnerable because their key-value (KV) cache during inference constitutes an overlooked attack surface. This paper introduces Malicious Token Injection (MTI), a modular framework that systematically perturbs cached key vectors at selected layers and timesteps through controlled magnitude and frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A theoretical analysis quantifies how these perturbations propagate through attention, linking logit deviations to the Frobenius norm of corruption and softmax Lipschitz dynamics. Empirical results show that MTI significantly alters next-token distributions and downstream task performance across GPT-2 and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic reasoning pipelines. These findings identify cache integrity as a critical yet underexplored vulnerability in current LLM deployments, positioning cache corruption as a reproducible and theoretically grounded threat model for future robustness and security research.",
        "pdf_url": "https://arxiv.org/pdf/2510.17098v1",
        "entry_id": "http://arxiv.org/abs/2510.17098v1",
        "categories": [
          "cs.CR",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models",
        "authors": [
          "Jeongjae Lee",
          "Jong Chul Ye"
        ],
        "published": "2025-09-30",
        "year": 2025,
        "summary": "While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.",
        "pdf_url": "https://arxiv.org/pdf/2509.25774v2",
        "entry_id": "http://arxiv.org/abs/2509.25774v2",
        "categories": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering",
        "authors": [
          "Minsoo Kim",
          "Arnav Kundu",
          "Han-Byul Kim",
          "Richa Dixit",
          "Minsik Cho"
        ],
        "published": "2025-09-22",
        "year": 2025,
        "summary": "Modern large language models (LLMs) extend context lengths to millions of tokens, enabling coherent, personalized responses grounded in long conversational histories. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly becomes the bottleneck in resource-constrained environments. An active line of research for reducing memory bottleneck is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting the KV cache after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to failure cases in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x compression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient multi-turn interaction under strict resource limits. Our code is available at https://github.com/apple/ml-epicache.",
        "pdf_url": "https://arxiv.org/pdf/2509.17396v3",
        "entry_id": "http://arxiv.org/abs/2509.17396v3",
        "categories": [
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication",
        "authors": [
          "Ankit Bhardwaj",
          "Weiyang Wang",
          "Jeremy Carin",
          "Adam Belay",
          "Manya Ghobadi"
        ],
        "published": "2025-07-17",
        "year": 2025,
        "summary": "This paper presents Checkmate, a system that enables per-iteration checkpointing in DNN training without any training slowdown. The traditional approach to checkpointing requires a pause in training to copy model states to a separate location, allowing the state to be restored in the event of failure. This approach fundamentally has a tradeoff between the frequency of checkpoints and the cost of a failure. We avoid this tradeoff; our key insight is that in data-parallel training, all information necessary to create a checkpoint already exists in the network as gradients. Our core contribution is a new multicast abstraction that simultaneously delivers gradients to a separate CPU-based shadow cluster. The shadow maintains a checkpoint by applying those gradients to a copy of the model. Our evaluation shows that Checkmate performs per-iteration checkpointing with training throughput comparable to an ideal no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent checkpointing compared to state-of-the-art checkpointing systems, resulting in 80% to 97.1% reduction in repeated work per failure. At the same checkpointing frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other systems.",
        "pdf_url": "https://arxiv.org/pdf/2507.13522v1",
        "entry_id": "http://arxiv.org/abs/2507.13522v1",
        "categories": [
          "cs.DC"
        ],
        "us_affiliated": false
      },
      {
        "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
        "authors": [
          "Chien Van Nguyen",
          "Ruiyi Zhang",
          "Hanieh Deilamsalehy",
          "Puneet Mathur",
          "Viet Dac Lai",
          "Haoliang Wang",
          "Jayakumar Subramanian",
          "Ryan A. Rossi",
          "Trung Bui",
          "Nikos Vlassis",
          "Franck Dernoncourt",
          "Thien Huu Nguyen"
        ],
        "published": "2025-07-11",
        "year": 2025,
        "summary": "We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into subquadratic architectures. Transformers faces severe computational and memory bottlenecks with long sequences due to the quadratic complexity of softmax attention and the growing Key-Value (KV) cache that makes inference memory-bound by context length. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving model quality. Unlike prior linearization methods constrained by fixed, non-adaptive structures, Lizard augments the architecture with compact, learnable modules that enable adaptive memory control and robust length generalization. Moreover, we introduce a hardwareaware algorithm that solves numerical instability in gated attention to accelerate training. Extensive experiments show that Lizard achieves near-lossless recovery of its teacher model's performance, significantly outperforming previous methods by up to 9.4 - 24.5 points on the 5-shot MMLU benchmark and demonstrating superior associative recall.",
        "pdf_url": "https://arxiv.org/pdf/2507.09025v3",
        "entry_id": "http://arxiv.org/abs/2507.09025v3",
        "categories": [
          "cs.CL",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
        "authors": [
          "Zefan Cai",
          "Wen Xiao",
          "Hanshi Sun",
          "Cheng Luo",
          "Yikai Zhang",
          "Ke Wan",
          "Yucheng Li",
          "Yeyang Zhou",
          "Li-Wen Chang",
          "Jiuxiang Gu",
          "Zhen Dong",
          "Anima Anandkumar",
          "Abedelkadir Asi",
          "Junjie Hu"
        ],
        "published": "2025-05-30",
        "year": 2025,
        "summary": "Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.",
        "pdf_url": "https://arxiv.org/pdf/2505.24133v3",
        "entry_id": "http://arxiv.org/abs/2505.24133v3",
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness",
        "authors": [
          "Hanyu Duan",
          "Yi Yang",
          "Ahmed Abbasi",
          "Kar Yan Tam"
        ],
        "published": "2025-05-16",
        "year": 2025,
        "summary": "This paper introduces Ready2Unlearn, a learning-time optimization approach designed to facilitate future unlearning processes. Unlike the majority of existing unlearning efforts that focus on designing unlearning algorithms, which are typically implemented reactively when an unlearning request is made during the model deployment phase, Ready2Unlearn shifts the focus to the training phase, adopting a \"forward-looking\" perspective. Building upon well-established meta-learning principles, Ready2Unlearn proactively trains machine learning models with unlearning readiness, such that they are well prepared and can handle future unlearning requests in a more efficient and principled manner. Ready2Unlearn is model-agnostic and compatible with any gradient ascent-based machine unlearning algorithms. We evaluate the method on both vision and language tasks under various unlearning settings, including class-wise unlearning and random data unlearning. Experimental results show that by incorporating such preparedness at training time, Ready2Unlearn produces an unlearning-ready model state, which offers several key advantages when future unlearning is required, including reduced unlearning time, improved retention of overall model capability, and enhanced resistance to the inadvertent recovery of forgotten data. We hope this work could inspire future efforts to explore more proactive strategies for equipping machine learning models with built-in readiness towards more reliable and principled machine unlearning.",
        "pdf_url": "https://arxiv.org/pdf/2505.10845v1",
        "entry_id": "http://arxiv.org/abs/2505.10845v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient Long-Context LLMs",
        "authors": [
          "Xin Liu",
          "Xudong Wang",
          "Pei Liu",
          "Guoming Tang"
        ],
        "published": "2025-03-13",
        "year": 2025,
        "summary": "The linear growth of key-value (KV) cache memory and quadratic computational in attention mechanisms complexity pose significant bottlenecks for large language models (LLMs) in long-context processing. While existing KV cache optimization methods address these challenges through token pruning or feature merging, they often incur irreversible information loss or require costly parameter retraining. To this end, we propose ZSMerge, a dynamic KV cache compression framework designed for efficient cache management, featuring three key operations: (1) fine-grained memory allocation guided by multi-dimensional token importance metrics at head-level granularity, (2) a residual merging mechanism that preserves critical context through compensated attention scoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM architectures without requiring retraining. ZSMerge significantly enhances memory efficiency and inference speed with negligible performance degradation across LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression ratio for key-value cache retention (reducing memory footprint to 5\\% of baseline) while sustaining comparable generation quality, coupled with triple throughput gains at extreme 54k-token contexts that eliminate out-of-memory failures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
        "pdf_url": "https://arxiv.org/pdf/2503.10714v3",
        "entry_id": "http://arxiv.org/abs/2503.10714v3",
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "AIBrix: Towards Scalable, Cost-Effective Large Language Model Inference Infrastructure",
        "authors": [
          "The AIBrix Team",
          "Jiaxin Shan",
          "Varun Gupta",
          "Le Xu",
          "Haiyang Shi",
          "Jingyuan Zhang",
          "Ning Wang",
          "Linhui Xu",
          "Rong Kang",
          "Tongping Liu",
          "Yifei Zhang",
          "Yiqing Zhu",
          "Shuowei Jin",
          "Gangmuk Lim",
          "Binbin Chen",
          "Zuzhi Chen",
          "Xiao Liu",
          "Xin Chen",
          "Kante Yin",
          "Chak-Pong Chung",
          "Chenyu Jiang",
          "Yicheng Lu",
          "Jianjun Chen",
          "Caixue Lin",
          "Wu Xiang",
          "Rui Shi",
          "Liguang Xie"
        ],
        "published": "2025-02-22",
        "year": 2025,
        "summary": "We introduce AIBrix, a cloud-native, open-source framework designed to optimize and simplify large-scale LLM deployment in cloud environments. Unlike traditional cloud-native stacks, AIBrix follows a co-design philosophy, ensuring every layer of the infrastructure is purpose-built for seamless integration with inference engines like vLLM. AIBrix introduces several key innovations to reduce inference costs and enhance performance including high-density LoRA management for dynamic adapter scheduling, LLM-specific autoscalers, and prefix-aware, load-aware routing. To further improve efficiency, AIBrix incorporates a distributed KV cache, boosting token reuse across nodes, leading to a 50% increase in throughput and a 70% reduction in inference latency. AIBrix also supports unified AI runtime which streamlines model management while maintaining vendor-agnostic engine compatibility. For large-scale multi-node inference, AIBrix employs hybrid orchestration -- leveraging Kubernetes for coarse-grained scheduling and Ray for fine-grained execution -- to balance efficiency and flexibility. Additionally, an SLO-driven GPU optimizer dynamically adjusts resource allocations, optimizing heterogeneous serving to maximize cost efficiency while maintaining service guarantees. Finally, AIBrix enhances system reliability with AI accelerator diagnostic tools, enabling automated failure detection and mock-up testing to improve fault resilience. AIBrix is available at https://github.com/vllm-project/aibrix.",
        "pdf_url": "https://arxiv.org/pdf/2504.03648v1",
        "entry_id": "http://arxiv.org/abs/2504.03648v1",
        "categories": [
          "cs.DC",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer",
        "authors": [
          "Euntae Choi",
          "Sumin Song",
          "Woosang Lim",
          "Sungjoo Yoo"
        ],
        "published": "2025-02-17",
        "year": 2025,
        "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code is available at https://github.com/ songsm921/RCP.",
        "pdf_url": "https://arxiv.org/pdf/2502.15779v2",
        "entry_id": "http://arxiv.org/abs/2502.15779v2",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Optimizing Urban Service Allocation with Time-Constrained Restless Bandits",
        "authors": [
          "Yi Mao",
          "Andrew Perrault"
        ],
        "published": "2025-01-27",
        "year": 2025,
        "summary": "Municipal inspections are an important part of maintaining the quality of goods and services. In this paper, we approach the problem of intelligently scheduling service inspections to maximize their impact, using the case of food establishment inspections in Chicago as a case study. The Chicago Department of Public Health (CDPH) inspects thousands of establishments each year, with a substantial fail rate (over 3,000 failed inspection reports in 2023). To balance the objectives of ensuring adherence to guidelines, minimizing disruption to establishments, and minimizing inspection costs, CDPH assigns each establishment an inspection window every year and guarantees that they will be inspected exactly once during that window. Meanwhile, CDPH also promises surprise public health inspections for unexpected food safety emergencies or complaints. These constraints create a challenge for a restless multi-armed bandit (RMAB) approach, for which there are no existing methods. We develop an extension to Whittle index-based systems for RMABs that can guarantee action window constraints and frequencies, and furthermore can be leveraged to optimize action window assignments themselves. Briefly, we combine MDP reformulation and integer programming-based lookahead to maximize the impact of inspections subject to constraints. A neural network-based supervised learning model is developed to model state transitions of real Chicago establishments using public CDPH inspection records, which demonstrates 10% AUC improvements compared with directly predicting establishments' failures. Our experiments not only show up to 24% (in simulation) or 33% (on real data) objective improvements resulting from our approach and robustness to surprise inspections, but also give insight into the impact of scheduling constraints.",
        "pdf_url": "https://arxiv.org/pdf/2502.00045v2",
        "entry_id": "http://arxiv.org/abs/2502.00045v2",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.CE",
          "cs.CY"
        ],
        "us_affiliated": false
      },
      {
        "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models",
        "authors": [
          "Bo Lv",
          "Quan Zhou",
          "Xuanang Ding",
          "Yan Wang",
          "Zeming Ma"
        ],
        "published": "2024-09-17",
        "year": 2024,
        "summary": "The bottleneck associated with the key-value(KV) cache presents a significant challenge during the inference processes of large language models. While depth pruning accelerates inference, it requires extensive recovery training, which can take up to two weeks. On the other hand, width pruning retains much of the performance but offers slight speed gains. To tackle these challenges, we propose KVPruner to improve model efficiency while maintaining performance. Our method uses global perplexity-based analysis to determine the importance ratio for each block and provides multiple strategies to prune non-essential KV channels within blocks. Compared to the original model, KVPruner reduces runtime memory usage by 50% and boosts throughput by over 35%. Additionally, our method requires only two hours of LoRA fine-tuning on small datasets to recover most of the performance.",
        "pdf_url": "https://arxiv.org/pdf/2409.11057v1",
        "entry_id": "http://arxiv.org/abs/2409.11057v1",
        "categories": [
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "A deeper look at depth pruning of LLMs",
        "authors": [
          "Shoaib Ahmed Siddiqui",
          "Xin Dong",
          "Greg Heinrich",
          "Thomas Breuel",
          "Jan Kautz",
          "David Krueger",
          "Pavlo Molchanov"
        ],
        "published": "2024-07-23",
        "year": 2024,
        "summary": "Large Language Models (LLMs) are not only resource-intensive to train but even more costly to deploy in production. Therefore, recent work has attempted to prune blocks of LLMs based on cheap proxies for estimating block importance, effectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b models without any significant degradation of downstream metrics. In this paper, we explore different block importance metrics by considering adaptive metrics such as Shapley value in addition to static ones explored in prior work. We show that adaptive metrics exhibit a trade-off in performance between tasks i.e., improvement on one task may degrade performance on the other due to differences in the computed block influences. Furthermore, we extend this analysis from a complete block to individual self-attention and feed-forward layers, highlighting the propensity of the self-attention layers to be more amendable to pruning, even allowing removal of upto 33% of the self-attention layers without incurring any performance degradation on MMLU for Mistral 7b (significant reduction in costly maintenance of KV-cache). Finally, we look at simple performance recovery techniques to emulate the pruned layers by training lightweight additive bias or low-rank linear adapters. Performance recovery using emulated updates avoids performance degradation for the initial blocks (up to 5% absolute improvement on MMLU), which is either competitive or superior to the learning-based technique.",
        "pdf_url": "https://arxiv.org/pdf/2407.16286v1",
        "entry_id": "http://arxiv.org/abs/2407.16286v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Retrieval Head Mechanistically Explains Long-Context Factuality",
        "authors": [
          "Wenhao Wu",
          "Yizhong Wang",
          "Guangxuan Xiao",
          "Hao Peng",
          "Yao Fu"
        ],
        "published": "2024-04-24",
        "year": 2024,
        "summary": "Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.",
        "pdf_url": "https://arxiv.org/pdf/2404.15574v1",
        "entry_id": "http://arxiv.org/abs/2404.15574v1",
        "categories": [
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "D\u00e9j\u00e0Vu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving",
        "authors": [
          "Foteini Strati",
          "Sara Mcallister",
          "Amar Phanishayee",
          "Jakub Tarnawski",
          "Ana Klimovic"
        ],
        "published": "2024-03-04",
        "year": 2024,
        "summary": "Distributed LLM serving is costly and often underutilizes hardware accelerators due to three key challenges: bubbles in pipeline-parallel deployments caused by the bimodal latency of prompt and token processing, GPU memory overprovisioning, and long recovery times in case of failures. In this paper, we propose D\u00e9j\u00e0Vu, a system to address all these challenges using a versatile and efficient KV cache streaming library (D\u00e9j\u00e0VuLib). Using D\u00e9j\u00e0VuLib, we propose and implement efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance. We highlight the efficacy of these solutions on a range of large models across cloud deployments.",
        "pdf_url": "https://arxiv.org/pdf/2403.01876v1",
        "entry_id": "http://arxiv.org/abs/2403.01876v1",
        "categories": [
          "cs.DC"
        ],
        "us_affiliated": false
      },
      {
        "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
        "authors": [
          "Yuxin Wang",
          "Yuhan Chen",
          "Zeyu Li",
          "Xueze Kang",
          "Yuchu Fang",
          "Yeju Zhou",
          "Yang Zheng",
          "Zhenheng Tang",
          "Xin He",
          "Rui Guo",
          "Xin Wang",
          "Qiang Wang",
          "Amelie Chi Zhou",
          "Xiaowen Chu"
        ],
        "published": "2024-01-31",
        "year": 2024,
        "summary": "Serving systems for Large Language Models (LLMs) are often optimized to improve quality of service (QoS) and throughput. However, due to the lack of open-source LLM serving workloads, these systems are frequently evaluated under unrealistic workload assumptions. Consequently, performance may degrade when systems are deployed in real-world scenarios. This work presents BurstGPT, an LLM serving workload with 10.31 million traces from regional Azure OpenAI GPT services over 213 days. BurstGPT captures LLM serving characteristics from user, model and system perspectives: (1) User request concurrency: burstiness variations of requests in Azure OpenAI GPT services, revealing diversified concurrency patterns in different services and model types. (2) User conversation patterns: counts and intervals within conversations for service optimizations. (3) Model response lengths: auto-regressive serving processes of GPT models, showing statistical relations between requests and their responses. (4) System response failures: failures of conversation and API services, showing intensive resource needs and limited availability of LLM services in Azure. The details of the characteristics can serve multiple purposes in LLM serving optimizations, such as system evaluation and trace provisioning. In our demo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines in efficiency, stability, or reliability in realistic LLM serving. We identify that the generalization of KV cache management, scheduling and disaggregation optimizations can be improved under realistic workload evaluations. BurstGPT is publicly available now at https://github.com/HPMLL/BurstGPT and is widely used to develop prototypes of LLM serving frameworks in the industry.",
        "pdf_url": "https://arxiv.org/pdf/2401.17644v5",
        "entry_id": "http://arxiv.org/abs/2401.17644v5",
        "categories": [
          "cs.DC",
          "cs.PF"
        ],
        "us_affiliated": false
      }
    ]
  },
  "agent_orchestration": {
    "description": "Agent orchestration failures - multi-agent cascades, distributed deadlock, reasoning corruption",
    "query": "(abs:\"multi-agent\" OR abs:\"agent orchestration\" OR abs:\"distributed agents\") AND (abs:\"failure\" OR abs:\"cascade\" OR abs:\"deadlock\")",
    "total_found": 50,
    "filtered_count": 50,
    "papers_2025": 50,
    "papers_2024": 0,
    "us_affiliated": 0,
    "papers": [
      {
        "title": "Architectures for Building Agentic AI",
        "authors": [
          "S\u0142awomir Nowaczyk"
        ],
        "published": "2025-12-10",
        "year": 2025,
        "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",
        "pdf_url": "https://arxiv.org/pdf/2512.09458v1",
        "entry_id": "http://arxiv.org/abs/2512.09458v1",
        "categories": [
          "cs.AI",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change",
        "authors": [
          "Yong-Woon Kim"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.",
        "pdf_url": "https://arxiv.org/pdf/2512.08449v1",
        "entry_id": "http://arxiv.org/abs/2512.08449v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
        "authors": [
          "Shiva Gaire",
          "Srijan Gyawali",
          "Saroj Mishra",
          "Suman Niroula",
          "Dilip Thakur",
          "Umesh Yadav"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",
        "pdf_url": "https://arxiv.org/pdf/2512.08290v1",
        "entry_id": "http://arxiv.org/abs/2512.08290v1",
        "categories": [
          "cs.CR",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning",
        "authors": [
          "Giray \u00d6n\u00fcr",
          "Azita Dabiri",
          "Bart De Schutter"
        ],
        "published": "2025-12-08",
        "year": 2025,
        "summary": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.",
        "pdf_url": "https://arxiv.org/pdf/2512.07417v1",
        "entry_id": "http://arxiv.org/abs/2512.07417v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes",
        "authors": [
          "Rongjia Zhou",
          "Chengzhuo Li",
          "Carl Yang",
          "Jiaying Lu"
        ],
        "published": "2025-12-08",
        "year": 2025,
        "summary": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.",
        "pdf_url": "https://arxiv.org/pdf/2512.07081v1",
        "entry_id": "http://arxiv.org/abs/2512.07081v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
        "authors": [
          "Ming Ma",
          "Jue Zhang",
          "Fangkai Yang",
          "Yu Kang",
          "Qingwei Lin",
          "Tianming Yang",
          "Saravan Rajmohan",
          "Dongmei Zhang"
        ],
        "published": "2025-12-07",
        "year": 2025,
        "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
        "pdf_url": "https://arxiv.org/pdf/2512.06749v2",
        "entry_id": "http://arxiv.org/abs/2512.06749v2",
        "categories": [
          "cs.AI",
          "cs.SE"
        ],
        "us_affiliated": false
      },
      {
        "title": "Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions",
        "authors": [
          "Piercosma Bisconti",
          "Marcello Galisai",
          "Federico Pierucci",
          "Marcantonio Bracale",
          "Matteo Prandi"
        ],
        "published": "2025-12-02",
        "year": 2025,
        "summary": "This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.",
        "pdf_url": "https://arxiv.org/pdf/2512.02682v1",
        "entry_id": "http://arxiv.org/abs/2512.02682v1",
        "categories": [
          "cs.MA",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Decentralized Multi-Agent System with Trust-Aware Communication",
        "authors": [
          "Yepeng Ding",
          "Ahmed Twabi",
          "Junwei Yu",
          "Lingfeng Zhang",
          "Tohru Kondo",
          "Hiroyuki Sato"
        ],
        "published": "2025-12-02",
        "year": 2025,
        "summary": "The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.",
        "pdf_url": "https://arxiv.org/pdf/2512.02410v1",
        "entry_id": "http://arxiv.org/abs/2512.02410v1",
        "categories": [
          "cs.MA",
          "cs.CR"
        ],
        "us_affiliated": false
      },
      {
        "title": "Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration",
        "authors": [
          "Nan Sun",
          "Bo Mao",
          "Yongchang Li",
          "Chenxu Wang",
          "Di Guo",
          "Huaping Liu"
        ],
        "published": "2025-11-30",
        "year": 2025,
        "summary": "Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.",
        "pdf_url": "https://arxiv.org/pdf/2512.00797v1",
        "entry_id": "http://arxiv.org/abs/2512.00797v1",
        "categories": [
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension",
        "authors": [
          "Yue Jiang",
          "Haiwei Xue",
          "Minghao Han",
          "Mingcheng Li",
          "Xiaolu Hou",
          "Dingkang Yang",
          "Lihua Zhang",
          "Xu Zheng"
        ],
        "published": "2025-11-29",
        "year": 2025,
        "summary": "Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.",
        "pdf_url": "https://arxiv.org/pdf/2512.00582v1",
        "entry_id": "http://arxiv.org/abs/2512.00582v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "AgentShield: Make MAS more secure and efficient",
        "authors": [
          "Kaixiang Wang",
          "Zhaojiacheng Zhou",
          "Bunyod Suvonov",
          "Jiong Lou",
          "Jie LI"
        ],
        "published": "2025-11-28",
        "year": 2025,
        "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \\textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \\textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \\textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \\textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\\% recovery rate and reduces auditing overhead by over 70\\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.",
        "pdf_url": "https://arxiv.org/pdf/2511.22924v1",
        "entry_id": "http://arxiv.org/abs/2511.22924v1",
        "categories": [
          "cs.MA",
          "cs.AI",
          "cs.CR"
        ],
        "us_affiliated": false
      },
      {
        "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation",
        "authors": [
          "Haoyi Wang",
          "Licheng Luo",
          "Yiannis Kantaros",
          "Bruno Sinopoli",
          "Mingyu Cai"
        ],
        "published": "2025-11-27",
        "year": 2025,
        "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages\n  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.\n  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.",
        "pdf_url": "https://arxiv.org/pdf/2511.22685v1",
        "entry_id": "http://arxiv.org/abs/2511.22685v1",
        "categories": [
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions",
        "authors": [
          "Jingyi Chen",
          "Xiaoyan Guo",
          "Songqiang Chen",
          "Shing-Chi Cheung",
          "Jiasi Shen"
        ],
        "published": "2025-11-26",
        "year": 2025,
        "summary": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.",
        "pdf_url": "https://arxiv.org/pdf/2511.21380v1",
        "entry_id": "http://arxiv.org/abs/2511.21380v1",
        "categories": [
          "cs.SE"
        ],
        "us_affiliated": false
      },
      {
        "title": "Distributionally Robust Cascading Risk in Multi-Agent Rendezvous: Extended Analysis of Parameter-Induced Ambiguity",
        "authors": [
          "Vivek Pandey",
          "Nader Motee"
        ],
        "published": "2025-11-25",
        "year": 2025,
        "summary": "Ensuring safety in autonomous multi-agent systems during time-critical tasks such as rendezvous is a fundamental challenge, particularly under communication delays and uncertainty in system parameters. In this paper, we develop a theoretical framework to analyze the \\emph{distributionally robust risk of cascading failures} in multi-agent rendezvous, where system parameters lie within bounded uncertainty sets around nominal values. Using a time-delayed dynamical network as a benchmark model, we quantify how small deviations in these parameters impact collective safety. We introduce a \\emph{conditional distributionally robust functional}, grounded in a bivariate Gaussian model, to characterize risk propagation between agents. This yields a \\emph{closed-form risk expression} that captures the complex interaction between time delays, network structure, noise statistics, and failure modes. These expressions expose key sensitivity patterns and provide actionable insight for the design of robust and resilient multi-agent networks. Extensive simulations validate the theoretical results and demonstrate the effectiveness of our framework.",
        "pdf_url": "https://arxiv.org/pdf/2511.20914v1",
        "entry_id": "http://arxiv.org/abs/2511.20914v1",
        "categories": [
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "Hierarchical Spatio-Temporal Attention Network with Adaptive Risk-Aware Decision for Forward Collision Warning in Complex Scenarios",
        "authors": [
          "Haoran Hu",
          "Junren Shi",
          "Shuo Jiang",
          "Kun Cheng",
          "Xia Yang",
          "Changhao Piao"
        ],
        "published": "2025-11-25",
        "year": 2025,
        "summary": "Forward Collision Warning systems are crucial for vehicle safety and autonomous driving, yet current methods often fail to balance precise multi-agent interaction modeling with real-time decision adaptability, evidenced by the high computational cost for edge deployment and the unreliability stemming from simplified interaction models.To overcome these dual challenges-computational complexity and modeling insufficiency-along with the high false alarm rates of traditional static-threshold warnings, this paper introduces an integrated FCW framework that pairs a Hierarchical Spatio-Temporal Attention Network with a Dynamic Risk Threshold Adjustment algorithm. HSTAN employs a decoupled architecture (Graph Attention Network for spatial, cascaded GRU with self-attention for temporal) to achieve superior performance and efficiency, requiring only 12.3 ms inference time (73% faster than Transformer methods) and reducing the Average Displacement Error (ADE) to 0.73m (42.2% better than Social_LSTM) on the NGSIM dataset. Furthermore, Conformalized Quantile Regression enhances reliability by generating prediction intervals (91.3% coverage at 90% confidence), which the DTRA module then converts into timely warnings via a physics-informed risk potential function and an adaptive threshold mechanism inspired by statistical process control.Tested across multi-scenario datasets, the complete system demonstrates high efficacy, achieving an F1 score of 0.912, a low false alarm rate of 8.2%, and an ample warning lead time of 2.8 seconds, validating the framework's superior performance and practical deployment feasibility in complex environments.",
        "pdf_url": "https://arxiv.org/pdf/2511.19952v1",
        "entry_id": "http://arxiv.org/abs/2511.19952v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis",
        "authors": [
          "Wenxuan Mu",
          "Jinzhong Ning",
          "Di Zhao",
          "Yijia Zhang"
        ],
        "published": "2025-11-24",
        "year": 2025,
        "summary": "In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.",
        "pdf_url": "https://arxiv.org/pdf/2511.19083v1",
        "entry_id": "http://arxiv.org/abs/2511.19083v1",
        "categories": [
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Wireless Power Transfer and Intent-Driven Network Optimization in AAVs-assisted IoT for 6G Sustainable Connectivity",
        "authors": [
          "Yue Hu",
          "Xiaoming He",
          "Rui Yuan",
          "Shahid Mumtaz"
        ],
        "published": "2025-11-23",
        "year": 2025,
        "summary": "Autonomous Aerial Vehicle (AAV)-assisted Internet of Things (IoT) represents a collaborative architecture in which AAV allocate resources over 6G links to jointly enhance user-intent interpretation and overall network performance. Owing to this mutual dependence, improvements in intent inference and policy decisions on one component reinforce the efficiency of others, making highly reliable intent prediction and low-latency action execution essential. Although numerous approaches can model intent relationships, they encounter severe obstacles when scaling to high-dimensional action sequences and managing intensive on-board computation. We propose an Intent-Driven Framework for Autonomous Network Optimization comprising prediction and decision modules. First, implicit intent modeling is adopted to mitigate inaccuracies arising from ambiguous user expressions. For prediction, we introduce Hyperdimensional Transformer (HDT), which embeds data into a Hyperdimensional space via Hyperdimensional vector encoding and replaces standard matrix and attention operations with symbolic Hyperdimensional computations. For decision-making, where AAV must respond to user intent while planning trajectories, we design Double Actions based Multi-Agent Proximal Policy Optimization (DA-MAPPO). Building upon MAPPO, it samples actions through two independently parameterized networks and cascades the user-intent network into the trajectory network to maintain action dependencies. We evaluate our framework on a real IoT action dataset with authentic wireless data. Experimental results demonstrate that HDT and DA-MAPPO achieve superior performance across diverse scenarios.",
        "pdf_url": "https://arxiv.org/pdf/2511.18368v1",
        "entry_id": "http://arxiv.org/abs/2511.18368v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving",
        "authors": [
          "Ye Han",
          "Lijun Zhang",
          "Dejian Meng",
          "Zhuang Zhang"
        ],
        "published": "2025-11-21",
        "year": 2025,
        "summary": "In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.",
        "pdf_url": "https://arxiv.org/pdf/2511.16916v1",
        "entry_id": "http://arxiv.org/abs/2511.16916v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction",
        "authors": [
          "Guolin Huang",
          "Wenting Chen",
          "Jiaqi Yang",
          "Xinheng Lyu",
          "Xiaoling Luo",
          "Sen Yang",
          "Xiaohan Xing",
          "Linlin Shen"
        ],
        "published": "2025-11-20",
        "year": 2025,
        "summary": "Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.",
        "pdf_url": "https://arxiv.org/pdf/2511.16635v1",
        "entry_id": "http://arxiv.org/abs/2511.16635v1",
        "categories": [
          "cs.CV",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy",
        "authors": [
          "Tomoki Nakao",
          "Kazumi Kasaura",
          "Tadashi Kozuno"
        ],
        "published": "2025-11-19",
        "year": 2025,
        "summary": "We address the fundamental challenge of resolving symmetry-induced deadlocks in distributed multi-agent navigation by proposing a new hierarchical navigation method. When multiple agents interact, it is inherently difficult for them to autonomously break the symmetry of deciding how to pass each other. To tackle this problem, we introduce an approach that quantifies cooperative symmetry-breaking strategies using a topological invariant called the winding number, and learns the strategies themselves through reinforcement learning. Our method features a hierarchical policy consisting of a learning-based Planner, which plans topological cooperative strategies, and a model-based Controller, which executes them. Through reinforcement learning, the Planner learns to produce two types of parameters for the Controller: one is the topological cooperative strategy represented by winding numbers, and the other is a set of dynamic weights that determine which agent interaction to prioritize in dense scenarios where multiple agents cross simultaneously. The Controller then generates collision-free and efficient motions based on the strategy and weights provided by the Planner. This hierarchical structure combines the flexible decision-making ability of learning-based methods with the reliability of model-based approaches. Simulation and real-world robot experiments demonstrate that our method outperforms existing baselines, particularly in dense environments, by efficiently avoiding collisions and deadlocks while achieving superior navigation performance. The code for the experiments is available at https://github.com/omron-sinicx/WNumMPC.",
        "pdf_url": "https://arxiv.org/pdf/2511.15239v1",
        "entry_id": "http://arxiv.org/abs/2511.15239v1",
        "categories": [
          "cs.RO",
          "cs.MA"
        ],
        "us_affiliated": false
      },
      {
        "title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents",
        "authors": [
          "Henrik Bradland",
          "Morten Goodwin",
          "Vladimir I. Zadorozhny",
          "Per-Arne Andersen"
        ],
        "published": "2025-11-19",
        "year": 2025,
        "summary": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.",
        "pdf_url": "https://arxiv.org/pdf/2511.15074v1",
        "entry_id": "http://arxiv.org/abs/2511.15074v1",
        "categories": [
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions",
        "authors": [
          "Vidur Sinha",
          "Muhammed Ustaomeroglu",
          "Guannan Qu"
        ],
        "published": "2025-11-17",
        "year": 2025,
        "summary": "Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.",
        "pdf_url": "https://arxiv.org/pdf/2511.13103v1",
        "entry_id": "http://arxiv.org/abs/2511.13103v1",
        "categories": [
          "cs.LG",
          "cs.MA",
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs",
        "authors": [
          "Yunhao Chen",
          "Xin Wang",
          "Juncheng Li",
          "Yixu Wang",
          "Jie Li",
          "Yan Teng",
          "Yingchun Wang",
          "Xingjun Ma"
        ],
        "published": "2025-11-16",
        "year": 2025,
        "summary": "Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \\textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.",
        "pdf_url": "https://arxiv.org/pdf/2511.12710v1",
        "entry_id": "http://arxiv.org/abs/2511.12710v1",
        "categories": [
          "cs.CL",
          "cs.CR"
        ],
        "us_affiliated": false
      },
      {
        "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation",
        "authors": [
          "Yuxiang Zhou",
          "Jichang Li",
          "Yanhao Zhang",
          "Haonan Lu",
          "Guanbin Li"
        ],
        "published": "2025-11-15",
        "year": 2025,
        "summary": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.",
        "pdf_url": "https://arxiv.org/pdf/2511.12254v2",
        "entry_id": "http://arxiv.org/abs/2511.12254v2",
        "categories": [
          "cs.AI",
          "cs.IR"
        ],
        "us_affiliated": false
      },
      {
        "title": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance",
        "authors": [
          "Lifan Zheng",
          "Jiawei Chen",
          "Qinghong Yin",
          "Jingyuan Zhang",
          "Xinyi Zeng",
          "Yu Tian"
        ],
        "published": "2025-11-13",
        "year": 2025,
        "summary": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.",
        "pdf_url": "https://arxiv.org/pdf/2511.10400v1",
        "entry_id": "http://arxiv.org/abs/2511.10400v1",
        "categories": [
          "cs.MA",
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Evaluating Software Process Models for Multi-Agent Class-Level Code Generation",
        "authors": [
          "Wasique Islam Shafin",
          "Md Nakhla Rafi",
          "Zhenhao Li",
          "Tse-Hsun Chen"
        ],
        "published": "2025-11-12",
        "year": 2025,
        "summary": "Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\\% for GPT-4o-mini and -39.8\\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.",
        "pdf_url": "https://arxiv.org/pdf/2511.09794v1",
        "entry_id": "http://arxiv.org/abs/2511.09794v1",
        "categories": [
          "cs.SE"
        ],
        "us_affiliated": false
      },
      {
        "title": "Robust and Diverse Multi-Agent Learning via Rational Policy Gradient",
        "authors": [
          "Niklas Lauffer",
          "Ameesh Shah",
          "Micah Carroll",
          "Sanjit A. Seshia",
          "Stuart Russell",
          "Michael Dennis"
        ],
        "published": "2025-11-12",
        "year": 2025,
        "summary": "Adversarial optimization algorithms that explicitly search for flaws in agents' policies have been successfully applied to finding robust and diverse policies in multi-agent settings. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to self-sabotage, blocking the completion of tasks and halting further learning. To address this, we introduce Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational--that is, their policies are optimal with respect to some possible partner policy. To solve RPO, we develop Rational Policy Gradient (RPG), which trains agents to maximize their own reward in a modified version of the original game in which we use opponent shaping techniques to optimize the adversarial objective. RPG enables us to extend a variety of existing adversarial optimization algorithms that, no longer subject to the limitations of self-sabotage, can find adversarial examples, improve robustness and adaptability, and learn diverse policies. We empirically validate that our approach achieves strong performance in several popular cooperative and general-sum environments. Our project page can be found at https://rational-policy-gradient.github.io.",
        "pdf_url": "https://arxiv.org/pdf/2511.09535v1",
        "entry_id": "http://arxiv.org/abs/2511.09535v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "MACEval: A Multi-Agent Continual Evaluation Network for Large Models",
        "authors": [
          "Zijian Chen",
          "Yuze Sun",
          "Yuan Tian",
          "Wenjun Zhang",
          "Guangtao Zhai"
        ],
        "published": "2025-11-12",
        "year": 2025,
        "summary": "Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \\Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.",
        "pdf_url": "https://arxiv.org/pdf/2511.09139v1",
        "entry_id": "http://arxiv.org/abs/2511.09139v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents",
        "authors": [
          "Chih-Hsuan Yang",
          "Tanwi Mallick",
          "Le Chen",
          "Krishnan Raghavan",
          "Azton Wells",
          "Amal Gueroudji",
          "Ian T. Foster",
          "Rajeev Thakur"
        ],
        "published": "2025-11-11",
        "year": 2025,
        "summary": "Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.",
        "pdf_url": "https://arxiv.org/pdf/2511.10687v2",
        "entry_id": "http://arxiv.org/abs/2511.10687v2",
        "categories": [
          "cs.MA",
          "cs.AI",
          "cs.CL",
          "cs.GT"
        ],
        "us_affiliated": false
      },
      {
        "title": "A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems",
        "authors": [
          "Manonmani Sekar",
          "Nasim Nezamoddini"
        ],
        "published": "2025-11-11",
        "year": 2025,
        "summary": "Reconfigurable manufacturing systems (RMS) are critical for future market adjustment given their rapid adaptation to fluctuations in consumer demands, the introduction of new technological advances, and disruptions in linked supply chain sections. The adjustable hard settings of such systems require a flexible soft planning mechanism that enables realtime production planning and scheduling amid the existing complexity and variability in their configuration settings. This study explores the application of multi agent reinforcement learning (MARL) for dynamic scheduling in soft planning of the RMS settings. In the proposed framework, deep Qnetwork (DQN) agents trained in centralized training learn optimal job machine assignments in real time while adapting to stochastic events such as machine breakdowns and reconfiguration delays. The model also incorporates a negotiation with an attention mechanism to enhance state representation and improve decision focus on critical system features. Key DQN enhancements including prioritized experience replay, nstep returns, double DQN and soft target update are used to stabilize and accelerate learning. Experiments conducted in a simulated RMS environment demonstrate that the proposed approach outperforms baseline heuristics in reducing makespan and tardiness while improving machine utilization. The reconfigurable manufacturing environment was extended to simulate realistic challenges, including machine failures and reconfiguration times. Experimental results show that while the enhanced DQN agent is effective in adapting to dynamic conditions, machine breakdowns increase variability in key performance metrics such as makespan, throughput, and total tardiness. The results confirm the advantages of applying the MARL mechanism for intelligent and adaptive scheduling in dynamic reconfigurable manufacturing environments.",
        "pdf_url": "https://arxiv.org/pdf/2511.07707v1",
        "entry_id": "http://arxiv.org/abs/2511.07707v1",
        "categories": [
          "cs.MA",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A Multi-Agent DRL Approach",
        "authors": [
          "Dao Lan Vy Dinh",
          "Anh Nguyen Thi Mai",
          "Hung Tran",
          "Giang Quynh Le Vu",
          "Tu Dac Ho",
          "Zhenni Pan",
          "Vo Nhan Van",
          "Symeon Chatzinotas",
          "Dinh-Hieu Tran"
        ],
        "published": "2025-11-10",
        "year": 2025,
        "summary": "This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience perspective in the 6G network energy saving (NES) scenario. More specifically, we consider multiple ground base stations (GBSs) and each GBS has three different sectors/cells in the terrestrial networks, and multiple cells are turned off due to NES or incidents, e.g., disasters, hardware failures, or outages. To address this, we propose a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework to enable UAV-assisted communication by jointly optimizing UAV trajectories, transmission power, and user-UAV association under a sleeping ground base station (GBS) strategy. This framework aims to ensure the resilience of active users in the network and the long-term operability of UAVs. Specifically, it maximizes service coverage for users during power outages or NES zones, while minimizing the energy consumption of UAVs. Simulation results demonstrate that the proposed MADDPG policy consistently achieves high coverage ratio across different testing episodes, outperforming other baselines. Moreover, the MADDPG framework attains the lowest total energy consumption, with a reduction of approximately 24\\% compared to the conventional all GBS ON configuration, while maintaining a comparable user service rate. These results confirm the effectiveness of the proposed approach in achieving a superior trade-off between energy efficiency and service performance, supporting the development of sustainable and resilient UAV-assisted cellular networks.",
        "pdf_url": "https://arxiv.org/pdf/2511.07366v1",
        "entry_id": "http://arxiv.org/abs/2511.07366v1",
        "categories": [
          "cs.NI",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots",
        "authors": [
          "Marcel M\u00fcller"
        ],
        "published": "2025-11-10",
        "year": 2025,
        "summary": "This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions.\n  To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes.\n  Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.",
        "pdf_url": "https://arxiv.org/pdf/2511.07071v1",
        "entry_id": "http://arxiv.org/abs/2511.07071v1",
        "categories": [
          "cs.MA",
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms",
        "authors": [
          "Qibing Ren",
          "Zhijie Zheng",
          "Jiaxuan Guo",
          "Junchi Yan",
          "Lizhuang Ma",
          "Jing Shao"
        ],
        "published": "2025-11-09",
        "year": 2025,
        "summary": "In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.",
        "pdf_url": "https://arxiv.org/pdf/2511.06448v1",
        "entry_id": "http://arxiv.org/abs/2511.06448v1",
        "categories": [
          "cs.MA",
          "cs.AI",
          "cs.CL",
          "cs.SI"
        ],
        "us_affiliated": false
      },
      {
        "title": "MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems",
        "authors": [
          "Barak Or"
        ],
        "published": "2025-11-08",
        "year": 2025,
        "summary": "Ensuring cognitive stability in autonomous multi-agent systems (MAS) is a central challenge for large-scale, distributed AI. While existing observability tools monitor system outputs, they cannot quantify how rapidly agentic workflows recover once reasoning coherence has been lost. We adapt classical reliability metrics-Mean Time-to-Recovery (MTTR), Mean Time Between Failures (MTBF), and related ratios-into the cognitive domain, defining MTTR-A (Mean Time-to-Recovery for Agentic Systems) as a runtime measure of cognitive recovery latency. MTTR-A quantifies the time required for a MAS to detect reasoning drift and restore consistent operation, capturing the recovery of reasoning coherence rather than infrastructural repair.\n  A benchmark simulation using the AG~News corpus and the LangGraph orchestration framework was conducted, modeling recovery latencies across multiple reflex modes. Automated reflexes restored stability within approximately 6s on average, while human-approval interventions required about 12s. Across 200 runs, the median simulated MTTR-A was 6.21+-2.14s, MTBF=6.7+-2.14s, and NRR=0.08, demonstrating measurable runtime resilience across reflex strategies.\n  By formalizing recovery latency as a quantifiable property of distributed reasoning-and deriving reliability bounds linking recovery time and cognitive uptime-this work establishes a foundation for runtime dependability in agentic cognition, transforming cognitive recovery from an ad-hoc process into a standardized, interpretable performance",
        "pdf_url": "https://arxiv.org/pdf/2511.20663v2",
        "entry_id": "http://arxiv.org/abs/2511.20663v2",
        "categories": [
          "cs.MA",
          "cs.AI",
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
        "authors": [
          "Ishan Kavathekar",
          "Hemang Jain",
          "Ameya Rathod",
          "Ponnurangam Kumaraguru",
          "Tanuja Ganu"
        ],
        "published": "2025-11-07",
        "year": 2025,
        "summary": "Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 211 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective Robustness Score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.",
        "pdf_url": "https://arxiv.org/pdf/2511.05269v1",
        "entry_id": "http://arxiv.org/abs/2511.05269v1",
        "categories": [
          "cs.MA",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration",
        "authors": [
          "Narjes Nourzad",
          "Hanqing Yang",
          "Shiyu Chen",
          "Carlee Joe-Wong"
        ],
        "published": "2025-11-06",
        "year": 2025,
        "summary": "Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.",
        "pdf_url": "https://arxiv.org/pdf/2511.04646v1",
        "entry_id": "http://arxiv.org/abs/2511.04646v1",
        "categories": [
          "cs.AI",
          "cs.CL",
          "cs.LG",
          "cs.MA"
        ],
        "us_affiliated": false
      },
      {
        "title": "Shared Spatial Memory Through Predictive Coding",
        "authors": [
          "Zhengru Fang",
          "Yu Guo",
          "Jingjing Wang",
          "Yuang Zhang",
          "Haonan An",
          "Yinhai Wang",
          "Yuguang Fang"
        ],
        "published": "2025-11-06",
        "year": 2025,
        "summary": "Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners' locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.",
        "pdf_url": "https://arxiv.org/pdf/2511.04235v2",
        "entry_id": "http://arxiv.org/abs/2511.04235v2",
        "categories": [
          "cs.AI",
          "cs.CE"
        ],
        "us_affiliated": false
      },
      {
        "title": "Detecting Silent Failures in Multi-Agentic AI Trajectories",
        "authors": [
          "Divya Pathak",
          "Harshit Kumar",
          "Anuska Roy",
          "Felix George",
          "Mudit Verma",
          "Pratibha Moogi"
        ],
        "published": "2025-11-06",
        "year": 2025,
        "summary": "Multi-Agentic AI systems, powered by large language models (LLMs), are inherently non-deterministic and prone to silent failures such as drift, cycles, and missing details in outputs, which are difficult to detect. We introduce the task of anomaly detection in agentic trajectories to identify these failures and present a dataset curation pipeline that captures user behavior, agent non-determinism, and LLM variation. Using this pipeline, we curate and label two benchmark datasets comprising \\textbf{4,275 and 894} trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection methods on these datasets, we show that supervised (XGBoost) and semi-supervised (SVDD) approaches perform comparably, achieving accuracies up to 98% and 96%, respectively. This work provides the first systematic study of anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks, and insights to guide future research.",
        "pdf_url": "https://arxiv.org/pdf/2511.04032v1",
        "entry_id": "http://arxiv.org/abs/2511.04032v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Security Analysis of Agentic AI Communication Protocols: A Comparative Evaluation",
        "authors": [
          "Yedidel Louck",
          "Ariel Stulman",
          "Amit Dvir"
        ],
        "published": "2025-11-05",
        "year": 2025,
        "summary": "Multi-agent systems (MAS) powered by artificial intelligence (AI) are increasingly foundational to complex, distributed workflows. Yet, the security of their underlying communication protocols remains critically under-examined. This paper presents the first empirical, comparative security analysis of the official CORAL implementation and a high-fidelity, SDK-based ACP implementation, benchmarked against a literature-based evaluation of A2A. Using a 14 point vulnerability taxonomy, we systematically assess their defenses across authentication, authorization, integrity, confidentiality, and availability. Our results reveal a pronounced security dichotomy: CORAL exhibits a robust architectural design, particularly in its transport-layer message validation and session isolation, but suffers from critical implementation-level vulnerabilities, including authentication and authorization failures at its SSE gateway. Conversely, ACP's architectural flexibility, most notably its optional JWS enforcement, translates into high-impact integrity and confidentiality flaws. We contextualize these findings within current industry trends, highlighting that existing protocols remain insufficiently secure. As a path forward, we recommend a hybrid approach that combines CORAL's integrated architecture with ACP's mandatory per-message integrity guarantees, laying the groundwork for resilient, next-generation agent communications.",
        "pdf_url": "https://arxiv.org/pdf/2511.03841v1",
        "entry_id": "http://arxiv.org/abs/2511.03841v1",
        "categories": [
          "cs.CR"
        ],
        "us_affiliated": false
      },
      {
        "title": "PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework",
        "authors": [
          "Sina Montazeri",
          "Yunhe Feng",
          "Kewei Sha"
        ],
        "published": "2025-11-04",
        "year": 2025,
        "summary": "Open data repositories hold potential for evidence-based decision-making, yet are inaccessible to non-experts lacking expertise in dataset discovery, schema mapping, and statistical analysis. Large language models show promise for individual tasks, but end-to-end analytical workflows expose fundamental limitations: attention dilutes across growing contexts, specialized reasoning patterns interfere, and errors propagate undetected. We present PublicAgent, a multi-agent framework that addresses these limitations through decomposition into specialized agents for intent clarification, dataset discovery, analysis, and reporting. This architecture maintains focused attention within agent contexts and enables validation at each stage. Evaluation across five models and 50 queries derives five design principles for multi-agent LLM systems. First, specialization provides value independent of model strength--even the strongest model shows 97.5% agent win rates, with benefits orthogonal to model scale. Second, agents divide into universal (discovery, analysis) and conditional (report, intent) categories. Universal agents show consistent effectiveness (std dev 12.4%) while conditional agents vary by model (std dev 20.5%). Third, agents mitigate distinct failure modes--removing discovery or analysis causes catastrophic failures (243-280 instances), while removing report or intent causes quality degradation. Fourth, architectural benefits persist across task complexity with stable win rates (86-92% analysis, 84-94% discovery), indicating workflow management value rather than reasoning enhancement. Fifth, wide variance in agent effectiveness across models (42-96% for analysis) requires model-aware architecture design. These principles guide when and why specialization is necessary for complex analytical workflows while enabling broader access to public data through natural language interfaces.",
        "pdf_url": "https://arxiv.org/pdf/2511.03023v1",
        "entry_id": "http://arxiv.org/abs/2511.03023v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks",
        "authors": [
          "Mohsin Mahmud Topu",
          "Mahfuz Ahmed Anik",
          "Azmine Toushik Wasi",
          "Md Manjurul Ahsan"
        ],
        "published": "2025-11-04",
        "year": 2025,
        "summary": "Pavement infrastructure monitoring is challenged by complex spatial dependencies, changing environmental conditions, and non-linear deterioration across road networks. Traditional Pavement Management Systems (PMS) remain largely reactive, lacking real-time intelligence for failure prevention and optimal maintenance planning. To address this, we propose a unified Digital Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven pavement health monitoring and predictive maintenance. Pavement segments and spatial relations are modeled as graph nodes and edges, while real-time UAV, sensor, and LiDAR data stream into the DT. The inductive GNN learns deterioration patterns from graph-structured inputs to forecast distress and enable proactive interventions. Trained on a real-world-inspired dataset with segment attributes and dynamic connectivity, our model achieves an R2 of 0.3798, outperforming baseline regressors and effectively capturing non-linear degradation. We also develop an interactive dashboard and reinforcement learning module for simulation, visualization, and adaptive maintenance planning. This DT-GNN integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, positioning the approach as a foundation for proactive, intelligent, and sustainable pavement management, with future extensions toward real-world deployment, multi-agent coordination, and smart-city integration.",
        "pdf_url": "https://arxiv.org/pdf/2511.02957v1",
        "entry_id": "http://arxiv.org/abs/2511.02957v1",
        "categories": [
          "cs.LG",
          "cs.CE",
          "cs.ET",
          "cs.NE",
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms",
        "authors": [
          "Linxin Hou",
          "Qirui Wu",
          "Zhihang Qin",
          "Neil Banerjee",
          "Yongxin Guo",
          "Cecilia Laschi"
        ],
        "published": "2025-11-04",
        "year": 2025,
        "summary": "This paper presents a quantitative comparison between centralised and distributed multi-agent reinforcement learning (MARL) architectures for controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical budgets. Both approaches are based on the arm having $n$ number of controlled sections. The study systematically varies $n$ and evaluates the performance of the arm to reach a fixed target in three scenarios: default baseline condition, recovery from external disturbance, and adaptation to actuator failure. Quantitative metrics used for the evaluation are mean action magnitude, mean final distance, mean episode length, and success rate. The results show that there are no significant benefits of the distributed policy when the number of controlled sections $n\\le4$. In very simple systems, when $n\\le2$, the centralised policy outperforms the distributed one. When $n$ increases to $4< n\\le 12$, the distributed policy shows a high sample efficiency. In these systems, distributed policy promotes a stronger success rate, resilience, and robustness under local observability and yields faster convergence given the same sample size. However, centralised policies achieve much higher time efficiency during training as it takes much less time to train the same size of samples. These findings highlight the trade-offs between centralised and distributed policy in reinforcement learning-based control for soft robotic systems and provide actionable design guidance for future sim-to-real transfer in soft rod-like manipulators.",
        "pdf_url": "https://arxiv.org/pdf/2511.02192v1",
        "entry_id": "http://arxiv.org/abs/2511.02192v1",
        "categories": [
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design",
        "authors": [
          "Ran Xu",
          "Yupeng Qi",
          "Jingsen Feng",
          "Xu Chu"
        ],
        "published": "2025-10-31",
        "year": 2025,
        "summary": "In modern engineering practice, human engineers collaborate in specialized teams to design complex products, with each expert completing their respective tasks while communicating and exchanging results and data with one another. While this division of expertise is essential for managing multidisciplinary complexity, it demands substantial development time and cost. Recently, we introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer for computational fluid dynamics, and turbulence.ai, which can conduct end-to-end research in fluid mechanics draft publications and PhD theses. Building upon these foundations, we present Engineering.ai, a platform for teams of AI engineers in computational design. The framework employs a hierarchical multi-agent architecture where a Chief Engineer coordinates specialized agents consisting of Aerodynamics, Structural, Acoustic, and Optimization Engineers, each powered by LLM with domain-specific knowledge. Agent-agent collaboration is achieved through file-mediated communication for data provenance and reproducibility, while a comprehensive memory system maintains project context, execution history, and retrieval-augmented domain knowledge to ensure reliable decision-making across the workflow. The system integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis, enabling parallel multidisciplinary simulations while maintaining computational accuracy. The framework is validated through UAV wing optimization. This work demonstrates that agentic-AI-enabled AI engineers has the potential to perform complex engineering tasks autonomously. Remarkably, the automated workflow achieved a 100% success rate across over 400 parametric configurations, with zero mesh generation failures, solver convergence issues, or manual interventions required, validating that the framework is trustworthy.",
        "pdf_url": "https://arxiv.org/pdf/2511.00122v1",
        "entry_id": "http://arxiv.org/abs/2511.00122v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems",
        "authors": [
          "Fulin Lin",
          "Shaowen Chen",
          "Ruishan Fang",
          "Hongwei Wang",
          "Tao Lin"
        ],
        "published": "2025-10-30",
        "year": 2025,
        "summary": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing autonomy with operational complexity often leads to critical inefficiencies, such as excessive token consumption and failures arising from misinformation. Existing methods primarily focus on post-hoc failure attribution, lacking proactive, real-time interventions to enhance robustness and efficiency. To this end, we introduce SupervisorAgent, a lightweight and modular framework for runtime, adaptive supervision that operates without altering the base agent's architecture. Triggered by an LLM-free adaptive filter, SupervisorAgent intervenes at critical junctures to proactively correct errors, guide inefficient behaviors, and purify observations. On the challenging GAIA benchmark, SupervisorAgent reduces the token consumption of the Smolagent framework by an average of 29.45% without compromising its success rate. Extensive experiments across five additional benchmarks (math reasoning, code generation, and question answering) and various SoTA foundation models validate the broad applicability and robustness of our approach. The code is available at https://github.com/LINs-lab/SupervisorAgent.",
        "pdf_url": "https://arxiv.org/pdf/2510.26585v1",
        "entry_id": "http://arxiv.org/abs/2510.26585v1",
        "categories": [
          "cs.MA",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration",
        "authors": [
          "Huajie Tan",
          "Cheng Chi",
          "Xiansheng Chen",
          "Yuheng Ji",
          "Zhongxia Zhao",
          "Xiaoshuai Hao",
          "Yaoxu Lyu",
          "Mingyu Cao",
          "Junkai Zhao",
          "Huaihai Lyu",
          "Enshen Zhou",
          "Ning Chen",
          "Yankai Fu",
          "Cheng Peng",
          "Wei Guo",
          "Dong Liang",
          "Zhuo Chen",
          "Mengsi Lyu",
          "Chenrui He",
          "Yulong Ao",
          "Yonghua Lin",
          "Pengwei Wang",
          "Zhongyuan Wang",
          "Shanghang Zhang"
        ],
        "published": "2025-10-30",
        "year": 2025,
        "summary": "The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: https://flagopen.github.io/RoboOS/",
        "pdf_url": "https://arxiv.org/pdf/2510.26536v1",
        "entry_id": "http://arxiv.org/abs/2510.26536v1",
        "categories": [
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming",
        "authors": [
          "Adhyayan Veer Singh",
          "Aaron Shen",
          "Brian Law",
          "Ahmed Ismail",
          "Jonas Rohweder",
          "Sean O'Brien",
          "Kevin Zhu"
        ],
        "published": "2025-10-26",
        "year": 2025,
        "summary": "Correctness alone is insufficient: LLM-generated programs frequently satisfy unit tests while violating contest time or memory budgets. We present SwiftSolve, a complexity-aware multi-agent system for competitive programming that couples algorithmic planning with empirical profiling and complexity-guided repair. We frame competitive programming as a software environment where specialized agents act as programmers, each assuming roles such as planning, coding, profiling, and complexity analysis. A Planner proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on a fixed input-size schedule to record wall time and peak memory; and a Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a complexity class and dispatch targeted patches to either the Planner or Coder. Agents communicate via typed, versioned JSON; a controller enforces iteration caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10 Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate run-level success is 73.08% at 12.40 s mean. Failures are predominantly resource-bound, indicating inefficiency rather than logic errors. Against Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness (pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that profiling and complexity-guided replanning reduce inefficiency while preserving accuracy.",
        "pdf_url": "https://arxiv.org/pdf/2510.22626v1",
        "entry_id": "http://arxiv.org/abs/2510.22626v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories",
        "authors": [
          "Aaron Appelle",
          "Jerome P. Lynch"
        ],
        "published": "2025-10-23",
        "year": 2025,
        "summary": "Large-scale video generation models have demonstrated high visual realism in diverse contexts, spurring interest in their potential as general-purpose world simulators. Existing benchmarks focus on individual subjects rather than scenes with multiple interacting people. However, the plausibility of multi-agent dynamics in generated videos remains unverified. We propose a rigorous evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of pedestrian dynamics. For I2V, we leverage start frames from established datasets to enable comparison with a ground truth video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian densities and interactions. A key component is a method to reconstruct 2D bird's-eye view trajectories from pixel-space without known camera parameters. Our analysis reveals that leading models have learned surprisingly effective priors for plausible multi-agent behavior. However, failure modes like merging and disappearing people highlight areas for future improvement.",
        "pdf_url": "https://arxiv.org/pdf/2510.20182v1",
        "entry_id": "http://arxiv.org/abs/2510.20182v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems",
        "authors": [
          "Linfeng Liang",
          "Chenkai Tan",
          "Yao Deng",
          "Yingfeng Cai",
          "T. Y Chen",
          "Xi Zheng"
        ],
        "published": "2025-10-22",
        "year": 2025,
        "summary": "Autonomous Driving Systems (ADS) are safety-critical, where failures can be severe. While Metamorphic Testing (MT) is effective for fault detection in ADS, existing methods rely heavily on manual effort and lack automation. We present AutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that automates the extraction of Metamorphic Relations (MRs) from local traffic rules and the generation of valid follow-up test cases. AutoMT leverages LLMs to extract MRs from traffic rules in Gherkin syntax using a predefined ontology. A vision-language agent analyzes scenarios, and a search agent retrieves suitable MRs from a RAG-based database to generate follow-up cases via computer vision. Experiments show that AutoMT achieves up to 5 x higher test diversity in follow-up case generation compared to the best baseline (manual expert-defined MRs) in terms of validation rate, and detects up to 20.55% more behavioral violations. While manual MT relies on a fixed set of predefined rules, AutoMT automatically extracts diverse metamorphic relations that augment real-world datasets and help uncover corner cases often missed during in-field testing and data collection. Its modular architecture separating MR extraction, filtering, and test generation supports integration into industrial pipelines and potentially enables simulation-based testing to systematically cover underrepresented or safety-critical scenarios.",
        "pdf_url": "https://arxiv.org/pdf/2510.19438v1",
        "entry_id": "http://arxiv.org/abs/2510.19438v1",
        "categories": [
          "cs.SE"
        ],
        "us_affiliated": false
      },
      {
        "title": "Distributed Allocation and Resource Scheduling Algorithms Resilient to Link Failure",
        "authors": [
          "Mohammadreza Doostmohammadian",
          "Sergio Pequito"
        ],
        "published": "2025-10-21",
        "year": 2025,
        "summary": "Distributed resource allocation (DRA) is fundamental to modern networked systems, spanning applications from economic dispatch in smart grids to CPU scheduling in data centers. Conventional DRA approaches require reliable communication, yet real-world networks frequently suffer from link failures, packet drops, and communication delays due to environmental conditions, network congestion, and security threats.\n  We introduce a novel resilient DRA algorithm that addresses these critical challenges, and our main contributions are as follows: (1) guaranteed constraint feasibility at all times, ensuring resource-demand balance even during algorithm termination or network disruption; (2) robust convergence despite sector-bound nonlinearities at nodes/links, accommodating practical constraints like quantization and saturation; and (3) optimal performance under merely uniformly-connected networks, eliminating the need for continuous connectivity.\n  Unlike existing approaches that require persistent network connectivity and provide only asymptotic feasibility, our graph-theoretic solution leverages network percolation theory to maintain performance during intermittent disconnections. This makes it particularly valuable for mobile multi-agent systems where nodes frequently move out of communication range. Theoretical analysis and simulations demonstrate that our algorithm converges to optimal solutions despite heterogeneous time delays and substantial link failures, significantly advancing the reliability of distributed resource allocation in practical network environments.",
        "pdf_url": "https://arxiv.org/pdf/2510.18273v1",
        "entry_id": "http://arxiv.org/abs/2510.18273v1",
        "categories": [
          "eess.SY",
          "cs.DC",
          "cs.MA",
          "eess.SP",
          "math.OC"
        ],
        "us_affiliated": false
      },
      {
        "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding",
        "authors": [
          "Rishabh Jain",
          "Keisuke Okumura",
          "Michael Amir",
          "Amanda Prorok"
        ],
        "published": "2025-10-20",
        "year": 2025,
        "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF) problems in real-time remains challenging even for state-of-the-art planners. To this end, we develop a hybrid framework that integrates a learned heuristic derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a leading search-based algorithm, LaCAM. While prior work has explored learning-guided search in MAPF, such methods have historically underperformed. In contrast, our approach, termed LaGAT, outperforms both purely search-based and purely learning-based methods in dense scenarios. This is achieved through an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of interest, and a deadlock detection scheme to account for imperfect neural guidance. Our results demonstrate that, when carefully designed, hybrid search offers a powerful solution for tightly coupled, challenging multi-agent coordination problems.",
        "pdf_url": "https://arxiv.org/pdf/2510.17382v1",
        "entry_id": "http://arxiv.org/abs/2510.17382v1",
        "categories": [
          "cs.AI",
          "cs.LG",
          "cs.MA",
          "cs.RO"
        ],
        "us_affiliated": false
      }
    ]
  },
  "unbounded_computation": {
    "description": "Unbounded computation - infinite loops, token explosion, runaway costs, resource exhaustion",
    "query": "(abs:\"infinite loop\" OR abs:\"unbounded computation\" OR abs:\"runaway cost\") AND (abs:\"AI\" OR abs:\"machine learning\" OR abs:\"LLM\")",
    "total_found": 12,
    "filtered_count": 8,
    "papers_2025": 6,
    "papers_2024": 2,
    "us_affiliated": 0,
    "papers": [
      {
        "title": "A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments",
        "authors": [
          "Jialu Zhang",
          "Jialiang Gu",
          "Wangmeiyu Zhang",
          "Jos\u00e9 Pablo Cambronero",
          "John Kolesar",
          "Ruzica Piskac",
          "Daming Li",
          "Hanyuan Shi"
        ],
        "published": "2025-10-16",
        "year": 2025,
        "summary": "Online programming platforms such as Codeforces and LeetCode attract millions of users seeking to learn to program or refine their skills for industry interviews. A major challenge for these users is the Time Limit Exceeded (TLE) error, triggered when a program exceeds the execution time bound. Although designed as a performance safeguard, TLE errors are difficult to resolve: error messages provide no diagnostic insight, platform support is minimal, and existing debugging tools offer little help. As a result, many users abandon their submissions after repeated TLE failures.\n  This paper presents the first large-scale empirical study of TLE errors in online programming. We manually analyzed 1000 Codeforces submissions with TLE errors, classified their root causes, and traced how users attempted to fix them. Our analysis shows that TLE errors often arise not only from inefficient algorithms but also from infinite loops, improper data structure use, and inefficient I/O, challenging the conventional view that TLEs are purely performance issues.\n  Guided by these findings, we introduce Nettle, the first automated repair tool specifically designed for TLE errors, and Nettle-Eval, the first framework for evaluating TLE repairs. Integrating LLMs with targeted automated feedback generated by the compiler and test cases, Nettle produces small, correct code edits that eliminate TLEs while preserving functionality. Evaluated on the same 1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the platform's official checker, confirming the reliability of our framework.",
        "pdf_url": "https://arxiv.org/pdf/2510.14339v1",
        "entry_id": "http://arxiv.org/abs/2510.14339v1",
        "categories": [
          "cs.SE"
        ],
        "us_affiliated": false
      },
      {
        "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning",
        "authors": [
          "Jialu Du",
          "Guiyang Hou",
          "Yihui Fu",
          "Chen Wu",
          "Wenqi Zhang",
          "Yongliang Shen",
          "Weiming Lu"
        ],
        "published": "2025-10-09",
        "year": 2025,
        "summary": "While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like \"tricky\" and \"confused\" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.",
        "pdf_url": "https://arxiv.org/pdf/2510.07974v2",
        "entry_id": "http://arxiv.org/abs/2510.07974v2",
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework",
        "authors": [
          "Kerui Huang",
          "Shuhan Liu",
          "Xing Hu",
          "Tongtong Xu",
          "Lingfeng Bao",
          "Xin Xia"
        ],
        "published": "2025-09-17",
        "year": 2025,
        "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by prompting intermediate steps, improving accuracy and robustness in arithmetic, logic, and commonsense tasks. However, this benefit comes with high computational costs: longer outputs increase latency, memory usage, and KV-cache demands. These issues are especially critical in software engineering tasks where concise and deterministic outputs are required. To investigate these trade-offs, we conduct an empirical study based on code generation benchmarks. The results reveal that longer CoT does not always help. Excessive reasoning often causes truncation, accuracy drops, and latency up to five times higher, with failed outputs consistently longer than successful ones. These findings challenge the assumption that longer reasoning is inherently better and highlight the need for adaptive CoT control. Motivated by this, we propose SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with task-aware adaptive filtering, dynamically adjusting thresholds based on pre-inference outputs to reduce verbosity and computational overhead. We then evaluate SEER on three software engineering tasks and one math task. On average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation, and eliminates most infinite loops. These results demonstrate SEER as a practical method to make CoT-enhanced LLMs more efficient and robust, even under resource constraints.",
        "pdf_url": "https://arxiv.org/pdf/2509.14093v1",
        "entry_id": "http://arxiv.org/abs/2509.14093v1",
        "categories": [
          "cs.SE",
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit",
        "authors": [
          "Zihao Wei",
          "Liang Pang",
          "Jiahao Liu",
          "Jingcheng Deng",
          "Shicheng Xu",
          "Zenghao Duan",
          "Jingang Wang",
          "Fei Sun",
          "Xunliang Cai",
          "Huawei Shen",
          "Xueqi Cheng"
        ],
        "published": "2025-08-25",
        "year": 2025,
        "summary": "Large language models (LLMs) enhance complex reasoning tasks by scaling the individual thinking process. However, prior work shows that overthinking can degrade overall performance. Motivated by observed patterns in thinking length and content length, we categorize reasoning into three stages: insufficient exploration stage, compensatory reasoning stage, and reasoning convergence stage. Typically, LLMs produce correct answers in the compensatory reasoning stage, whereas reasoning convergence often triggers overthinking, causing increased resource usage or even infinite loops. Therefore, mitigating overthinking hinges on detecting the end of the compensatory reasoning stage, defined as the Reasoning Completion Point (RCP). RCP typically appears at the end of the first complete reasoning cycle and can be identified by querying the LLM sentence by sentence or monitoring the probability of an end-of-thinking token (e.g., \\texttt{</think>}), though these methods lack an efficient and precise balance. To improve this, we mine more sensitive and consistent RCP patterns and develop a lightweight thresholding strategy based on heuristic rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D) demonstrate that the proposed method reduces token consumption while preserving or enhancing reasoning accuracy.",
        "pdf_url": "https://arxiv.org/pdf/2508.17627v1",
        "entry_id": "http://arxiv.org/abs/2508.17627v1",
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "WMAS: A Multi-Agent System Towards Intelligent and Customized Wireless Networks",
        "authors": [
          "Jingchen Peng",
          "Dingli Yuan",
          "Boxiang Ren",
          "Jie Fan",
          "Hao Wu",
          "Lu Yang"
        ],
        "published": "2025-08-01",
        "year": 2025,
        "summary": "The fast development of Artificial Intelligence (AI) agents provides a promising way for the realization of intelligent and customized wireless networks. In this paper, we propose a Wireless Multi-Agent System (WMAS), which can provide intelligent and customized services for different user equipment (UEs). Note that orchestrating multiple agents carries the risk of malfunction, and multi-agent conversations may fall into infinite loops. It is thus crucial to design a conversation topology for WMAS that enables agents to complete UE task requests with high accuracy and low conversation overhead. To address this issue, we model the multi-agent conversation topology as a directed acyclic graph and propose a reinforcement learning-based algorithm to optimize the adjacency matrix of this graph. As such, WMAS is capable of generating and self-optimizing multi-agent conversation topologies, enabling agents to effectively and collaboratively handle a variety of task requests from UEs. Simulation results across various task types demonstrate that WMAS can achieve higher task performance and lower conversation overhead compared to existing multi-agent systems. These results validate the potential of WMAS to enhance the intelligence of future wireless networks.",
        "pdf_url": "https://arxiv.org/pdf/2508.00280v1",
        "entry_id": "http://arxiv.org/abs/2508.00280v1",
        "categories": [
          "cs.MA"
        ],
        "us_affiliated": false
      },
      {
        "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs",
        "authors": [
          "Yue Li",
          "Xiao Li",
          "Hao Wu",
          "Yue Zhang",
          "Fengyuan Xu",
          "Xiuzhen Cheng",
          "Sheng Zhong"
        ],
        "published": "2025-07-22",
        "year": 2025,
        "summary": "Large Language Models (LLMs) have become integral to automated code analysis, enabling tasks such as vulnerability detection and code comprehension. However, their integration introduces novel attack surfaces. In this paper, we identify and investigate a new class of prompt-based attacks, termed Copy-Guided Attacks (CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs. By injecting carefully crafted triggers into external code snippets, adversaries can induce the model to replicate malicious content during inference. This behavior enables two classes of vulnerabilities: inference length manipulation, where the model generates abnormally short or excessively long reasoning traces; and inference result manipulation, where the model produces misleading or incorrect conclusions. We formalize CGA as an optimization problem and propose a gradient-based approach to synthesize effective triggers. Empirical evaluation on state-of-the-art reasoning LLMs shows that CGA reliably induces infinite loops, premature termination, false refusals, and semantic distortions in code analysis tasks. While highly effective in targeted settings, we observe challenges in generalizing CGA across diverse prompts due to computational constraints, posing an open question for future research. Our findings expose a critical yet underexplored vulnerability in LLM-powered development pipelines and call for urgent advances in prompt-level defense mechanisms.",
        "pdf_url": "https://arxiv.org/pdf/2507.16773v1",
        "entry_id": "http://arxiv.org/abs/2507.16773v1",
        "categories": [
          "cs.CR"
        ],
        "us_affiliated": false
      },
      {
        "title": "DiffSpec: Differential Testing with LLMs using Natural Language Specifications and Code Artifacts",
        "authors": [
          "Nikitha Rao",
          "Elizabeth Gilbert",
          "Harrison Green",
          "Tahina Ramananandro",
          "Nikhil Swamy",
          "Claire Le Goues",
          "Sarah Fakhoury"
        ],
        "published": "2024-10-05",
        "year": 2024,
        "summary": "Differential testing can be an effective way to find bugs in software systems with multiple implementations that conform to the same specification, like compilers, network protocol parsers, or language runtimes. Specifications for such systems are often standardized in natural language documents, like Instruction Set Architecture (ISA) specifications or IETF RFC's. Large Language Models (LLMs) have demonstrated potential in both generating tests and handling large volumes of natural language text, making them well-suited for analyzing artifacts like specification documents, bug reports, and code implementations. In this work, we leverage natural language and code artifacts to guide LLMs to generate targeted tests that highlight meaningful behavioral differences between implementations, including those corresponding to bugs. We introduce DiffSpec, a framework for generating differential tests with LLMs using prompt chaining. We demonstrate DiffSpec's efficacy on two different (extensively tested) systems, eBPF runtimes and Wasm validators. Using DiffSpec, we generated 1901 differentiating tests, uncovering at least four distinct and confirmed bugs in eBPF, including a kernel memory leak, inconsistent behavior in jump instructions, undefined behavior when using the stack pointer, and tests with infinite loops that hang the verifier in ebpf-for-windows. We also found 299 differentiating tests in Wasm validators pointing to two confirmed and fixed bugs.",
        "pdf_url": "https://arxiv.org/pdf/2410.04249v3",
        "entry_id": "http://arxiv.org/abs/2410.04249v3",
        "categories": [
          "cs.SE"
        ],
        "us_affiliated": false
      },
      {
        "title": "A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares",
        "authors": [
          "Stav Cohen",
          "Ron Bitton",
          "Ben Nassi"
        ],
        "published": "2024-08-09",
        "year": 2024,
        "summary": "In this paper we argue that a jailbroken GenAI model can cause substantial harm to GenAI-powered applications and facilitate PromptWare, a new type of attack that flips the GenAI model's behavior from serving an application to attacking it. PromptWare exploits user inputs to jailbreak a GenAI model to force/perform malicious activity within the context of a GenAI-powered application. First, we introduce a naive implementation of PromptWare that behaves as malware that targets Plan & Execute architectures (a.k.a., ReAct, function calling). We show that attackers could force a desired execution flow by creating a user input that produces desired outputs given that the logic of the GenAI-powered application is known to attackers. We demonstrate the application of a DoS attack that triggers the execution of a GenAI-powered assistant to enter an infinite loop that wastes money and computational resources on redundant API calls to a GenAI engine, preventing the application from providing service to a user. Next, we introduce a more sophisticated implementation of PromptWare that we name Advanced PromptWare Threat (APwT) that targets GenAI-powered applications whose logic is unknown to attackers. We show that attackers could create user input that exploits the GenAI engine's advanced AI capabilities to launch a kill chain in inference time consisting of six steps intended to escalate privileges, analyze the application's context, identify valuable assets, reason possible malicious activities, decide on one of them, and execute it. We demonstrate the application of APwT against a GenAI-powered e-commerce chatbot and show that it can trigger the modification of SQL tables, potentially leading to unauthorized discounts on the items sold to the user.",
        "pdf_url": "https://arxiv.org/pdf/2408.05061v1",
        "entry_id": "http://arxiv.org/abs/2408.05061v1",
        "categories": [
          "cs.CR",
          "cs.AI"
        ],
        "us_affiliated": false
      }
    ]
  },
  "ai_observability": {
    "description": "AI observability challenges - silent failures, metrics blindness, quality degradation",
    "query": "(abs:\"silent failure\" OR abs:\"observability\" OR abs:\"monitoring\") AND (abs:\"AI\" OR abs:\"machine learning\" OR abs:\"agent\") AND (abs:\"quality\" OR abs:\"degradation\")",
    "total_found": 50,
    "filtered_count": 50,
    "papers_2025": 50,
    "papers_2024": 0,
    "us_affiliated": 0,
    "papers": [
      {
        "title": "Visual Heading Prediction for Autonomous Aerial Vehicles",
        "authors": [
          "Reza Ahmari",
          "Ahmad Mohammadi",
          "Vahid Hemmati",
          "Mohammed Mynuddin",
          "Parham Kebria",
          "Mahmoud Nabil Mahmoud",
          "Xiaohong Yuan",
          "Abdollah Homaifar"
        ],
        "published": "2025-12-10",
        "year": 2025,
        "summary": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506\u00b0 and a root mean squared error of 0.1957\u00b0, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration",
        "pdf_url": "https://arxiv.org/pdf/2512.09898v1",
        "entry_id": "http://arxiv.org/abs/2512.09898v1",
        "categories": [
          "cs.RO",
          "cs.AI",
          "cs.CV",
          "cs.MA",
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
        "authors": [
          "Justin W. Lin",
          "Eliot Krzysztof Jones",
          "Donovan Julian Jasper",
          "Ethan Jun-shen Ho",
          "Anna Wu",
          "Arnold Tianyi Yang",
          "Neil Perry",
          "Andy Zou",
          "Matt Fredrikson",
          "J. Zico Kolter",
          "Percy Liang",
          "Dan Boneh",
          "Daniel E. Ho"
        ],
        "published": "2025-12-10",
        "year": 2025,
        "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
        "pdf_url": "https://arxiv.org/pdf/2512.09882v1",
        "entry_id": "http://arxiv.org/abs/2512.09882v1",
        "categories": [
          "cs.AI",
          "cs.CR",
          "cs.CY"
        ],
        "us_affiliated": false
      },
      {
        "title": "Monitoring Deployed AI Systems in Health Care",
        "authors": [
          "Timothy Keyes",
          "Alison Callahan",
          "Abby S. Pandya",
          "Nerissa Ambers",
          "Juan M. Banda",
          "Miguel Fuentes",
          "Carlene Lugtu",
          "Pranav Masariya",
          "Srikar Nallan",
          "Connor O'Brien",
          "Thomas Wang",
          "Emily Alsentzer",
          "Jonathan H. Chen",
          "Dev Dash",
          "Matthew A. Eisenberg",
          "Patricia Garcia",
          "Nikesh Kotecha",
          "Anurang Revri",
          "Michael A. Pfeffer",
          "Nigam H. Shah",
          "Sneha S. Jain"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "Post-deployment monitoring of artificial intelligence (AI) systems in health care is essential to ensure their safety, quality, and sustained benefit-and to support governance decisions about which systems to update, modify, or decommission. Motivated by these needs, we developed a framework for monitoring deployed AI systems grounded in the mandate to take specific actions when they fail to behave as intended. This framework, which is now actively used at Stanford Health Care, is organized around three complementary principles: system integrity, performance, and impact. System integrity monitoring focuses on maximizing system uptime, detecting runtime errors, and identifying when changes to the surrounding IT ecosystem have unintended effects. Performance monitoring focuses on maintaining accurate system behavior in the face of changing health care practices (and thus input data) over time. Impact monitoring assesses whether a deployed system continues to have value in the form of benefit to clinicians and patients. Drawing on examples of deployed AI systems at our academic medical center, we provide practical guidance for creating monitoring plans based on these principles that specify which metrics to measure, when those metrics should be reviewed, who is responsible for acting when metrics change, and what concrete follow-up actions should be taken-for both traditional and generative AI. We also discuss challenges to implementing this framework, including the effort and cost of monitoring for health systems with limited resources and the difficulty of incorporating data-driven monitoring practices into complex organizations where conflicting priorities and definitions of success often coexist. This framework offers a practical template and starting point for health systems seeking to ensure that AI deployments remain safe and effective over time.",
        "pdf_url": "https://arxiv.org/pdf/2512.09048v1",
        "entry_id": "http://arxiv.org/abs/2512.09048v1",
        "categories": [
          "q-bio.OT",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference",
        "authors": [
          "Amit Bendkhale"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.",
        "pdf_url": "https://arxiv.org/pdf/2512.08860v1",
        "entry_id": "http://arxiv.org/abs/2512.08860v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
        "authors": [
          "Eranga Bandara",
          "Ross Gore",
          "Peter Foytik",
          "Sachin Shetty",
          "Ravi Mukkamala",
          "Abdul Rahman",
          "Xueping Liang",
          "Safdar H. Bouk",
          "Amin Hass",
          "Sachini Rajapakse",
          "Ng Wee Keong",
          "Kasun De Zoysa",
          "Aruna Withanage",
          "Nilaan Loganathan"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
        "pdf_url": "https://arxiv.org/pdf/2512.08769v1",
        "entry_id": "http://arxiv.org/abs/2512.08769v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "The SMART+ Framework for AI Systems",
        "authors": [
          "Laxmiraju Kandikatla",
          "Branislav Radeljic"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.",
        "pdf_url": "https://arxiv.org/pdf/2512.08592v1",
        "entry_id": "http://arxiv.org/abs/2512.08592v1",
        "categories": [
          "cs.AI",
          "cs.CY",
          "cs.HC",
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "Collaborative Intelligence for UAV-Satellite Network Slicing: Towards a Joint QoS-Energy-Fairness MADRL Optimization",
        "authors": [
          "Thanh-Dao Nguyen",
          "Ngoc-Tan Nguyen",
          "Thai-Duong Nguyen",
          "Nguyen Van Huynh",
          "Dinh-Hieu Tran",
          "Symeon Chatzinotas"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "Non terrestrial networks are critical for achieving global 6G coverage, yet efficient resource management in aerial and space environments remains challenging due to limited onboard power and dynamic operational conditions. Network slicing offers a promising solution for spectrum optimization in UAV based systems serving heterogeneous service demands. For that, this paper proposes a hierarchical network slicing framework for UAV satellite integrated networks supporting eMBB, URLLC, and mMTC services. Specifically, we formulate a joint optimization of UAV trajectory, transmission power, and spectrum allocation as a decentralized partially observable Markov decision process that ensures quality of service while minimizing energy consumption and maximizing resource fairness. To address the computational intractability and partial observability, we develop a multi agent deep reinforcement learning solution under the centralized training and decentralized execution paradigm. In the proposed system, UAV agents act as distributed actors coordinated by a shared critic operating with multi head attention mechanism at a low Earth orbit satellite. Experimental results then demonstrate that our approach outperforms existing methods by up to 33% in cumulative reward while achieving superior energy efficiency and fairness.",
        "pdf_url": "https://arxiv.org/pdf/2512.08322v1",
        "entry_id": "http://arxiv.org/abs/2512.08322v1",
        "categories": [
          "cs.NI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models",
        "authors": [
          "Haoran Wang",
          "Zhuohang Chen",
          "Guang Li",
          "Bo Ma",
          "Chuanghuang Li"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.",
        "pdf_url": "https://arxiv.org/pdf/2512.08145v1",
        "entry_id": "http://arxiv.org/abs/2512.08145v1",
        "categories": [
          "cs.RO",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Robust Agents in Open-Ended Worlds",
        "authors": [
          "Mikayel Samvelyan"
        ],
        "published": "2025-12-09",
        "year": 2025,
        "summary": "The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.",
        "pdf_url": "https://arxiv.org/pdf/2512.08139v1",
        "entry_id": "http://arxiv.org/abs/2512.08139v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Hidden Structural Variants in ALD NbN Superconducting Trilayers Revealed by Atomistic Analysis",
        "authors": [
          "Prachi Garg",
          "Danqing Wang",
          "Hong X. Tang",
          "Baishakhi Mazumder"
        ],
        "published": "2025-12-08",
        "year": 2025,
        "summary": "Microscopic inhomogeneity within superconducting films is a critical bottleneck hindering the performance and scalability of quantum circuits. All-nitride Josephson Junctions (JJs) have attracted substantial attention for their potential to provide enhanced coherence times and enable higher temperature operation. However, their performance is often limited by local variations caused by polymorphism, impurities, and interface quality. This work diagnoses atomic-scale limitations preventing superconducting NbN/AlN/NbN JJs from reaching their full potential. Electrical measurements reveal suppressed critical current density and soft onset of quasiparticle current. However, inverse proportionality between resistance and junction area confirms homogenous barrier thickness. This isolates structural and chemical variations in electrodes and barrier as the source of performance limitation. The observed characteristics are attributed to complex materials problems: NbN polymorphism, phase coexistence, and oxygen impurities. Using advanced microscopy and machine learning integrated approach, nanoscale inclusions of epsilon-Nb2N2 are found to coexist within dominant delta-NbN electrodes. DC performance of JJs may be affected by these defects, leading to unresolved supercurrent and soft transition to normal state. By identifying specific atomic scale defects, tracing its origin to initial film nucleation, and linking to its detrimental electrical signature, this work establishes a material-to-device correlation and provides targeted strategy for phase engineering towards reproducible, high coherence and scalable quantum devices.",
        "pdf_url": "https://arxiv.org/pdf/2512.07095v1",
        "entry_id": "http://arxiv.org/abs/2512.07095v1",
        "categories": [
          "quant-ph"
        ],
        "us_affiliated": false
      },
      {
        "title": "Tournament-Based Performance Evaluation and Systematic Misallocation: Why Forced Ranking Systems Produce Random Outcomes",
        "authors": [
          "Jeremy McEntire"
        ],
        "published": "2025-12-06",
        "year": 2025,
        "summary": "Tournament-based compensation schemes with forced distributions represent a widely adopted class of relative performance evaluation mechanisms in technology and corporate environments. These systems mandate within-team ranking and fixed distributional requirements (e.g., bottom 15% terminated, top 15% promoted), ostensibly to resolve principal-agent problems through mandatory differentiation. We demonstrate through agent-based simulation that this mechanism produces systematic classification errors independent of implementation quality. With 994 engineers across 142 teams of 7, random team assignment yields 32% error in termination and promotion decisions, misclassifying employees purely through composition variance. Under realistic conditions reflecting differential managerial capability, error rates reach 53%, with false positives and false negatives each exceeding correct classifications. Cross-team calibration (often proposed as remedy) transforms evaluation into influence contests where persuasive managers secure promotions independent of merit. Multi-period dynamics produce adverse selection as employees observe random outcomes, driving risk-averse behavior and high-performer exit. The efficient solution (delegating judgment to managers with hierarchical accountability) cannot be formalized within the legal and coordination constraints that necessitated forced ranking. We conclude that this evaluation mechanism persists not through incentive alignment but through satisfying demands for demonstrable process despite producing outcomes indistinguishable from random allocation. This demonstrates how formalization intended to reduce agency costs structurally increases allocation error.",
        "pdf_url": "https://arxiv.org/pdf/2512.06583v1",
        "entry_id": "http://arxiv.org/abs/2512.06583v1",
        "categories": [
          "econ.GN"
        ],
        "us_affiliated": false
      },
      {
        "title": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning",
        "authors": [
          "Ting-Ting Xie",
          "Yixin Zhang"
        ],
        "published": "2025-12-05",
        "year": 2025,
        "summary": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.",
        "pdf_url": "https://arxiv.org/pdf/2512.05576v1",
        "entry_id": "http://arxiv.org/abs/2512.05576v1",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition",
        "authors": [
          "Adam Lizerbram",
          "Shane Stevenson",
          "Iman Khadir",
          "Matthew Tu",
          "Samuel S. P. Shen"
        ],
        "published": "2025-12-04",
        "year": 2025,
        "summary": "Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.",
        "pdf_url": "https://arxiv.org/pdf/2512.05323v1",
        "entry_id": "http://arxiv.org/abs/2512.05323v1",
        "categories": [
          "cs.LG",
          "cs.AI",
          "stat.ML",
          "stat.OT"
        ],
        "us_affiliated": false
      },
      {
        "title": "Reflection-Satisfaction Tradeoff: Investigating Impact of Reflection on Student Engagement with AI-Generated Programming Hints",
        "authors": [
          "Heeryung Choi",
          "Tung Phung",
          "Mengyan Wu",
          "Adish Singla",
          "Christopher Brooks"
        ],
        "published": "2025-12-04",
        "year": 2025,
        "summary": "Generative AI tools, such as AI-generated hints, are increasingly integrated into programming education to offer timely, personalized support. However, little is known about how to effectively leverage these hints while ensuring autonomous and meaningful learning. One promising approach involves pairing AI-generated hints with reflection prompts, asking students to review and analyze their learning, when they request hints. This study investigates the interplay between AI-generated hints and different designs of reflection prompts in an online introductory programming course. We conducted a two-trial field experiment. In Trial 1, students were randomly assigned to receive prompts either before or after receiving hints, or no prompt at all. Each prompt also targeted one of three SRL phases: planning, monitoring, and evaluation. In Trial 2, we examined two types of prompt guidance: directed (offering more explicit and structured guidance) and open (offering more general and less constrained guidance). Findings show that students in the before-hint (RQ1), planning (RQ2), and directed (RQ3) prompt groups produced higher-quality reflections but reported lower satisfaction with AI-generated hints than those in other conditions. Immediate performance did not differ across conditions. This negative relationship between reflection quality and hint satisfaction aligns with previous work on student mental effort and satisfaction. Our results highlight the need to reconsider how AI models are trained and evaluated for education, as prioritizing user satisfaction can undermine deeper learning.",
        "pdf_url": "https://arxiv.org/pdf/2512.04630v1",
        "entry_id": "http://arxiv.org/abs/2512.04630v1",
        "categories": [
          "cs.CY"
        ],
        "us_affiliated": false
      },
      {
        "title": "Evaluating Long-Context Reasoning in LLM-Based WebAgents",
        "authors": [
          "Andy Chung",
          "Yichi Zhang",
          "Kaixiang Lin",
          "Aditya Rawal",
          "Qiaozi Gao",
          "Joyce Chai"
        ],
        "published": "2025-12-03",
        "year": 2025,
        "summary": "As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing personalized and contextually aware assistance. However, the performance of these agents in long context scenarios, particularly for action-taking WebAgents operating in realistic web environments, remains largely unexplored. This paper introduces a benchmark for evaluating long context reasoning capabilities of WebAgents through sequentially dependent subtasks that require retrieval and application of information from extended interaction histories. We develop a novel evaluation framework that simulates multi-session user interactions by injecting irrelevant task trajectories between dependent subtasks, creating contexts ranging from 25,000 to 150,000 tokens. Through extensive evaluation of four popular models, Claude-3.7, GPT-4.1, Llama 4, and o4-mini, we observe a dramatic performance degradation as context length increases, with success rates dropping from 40-50\\% in baseline conditions to less than 10\\% in long context scenarios. Our detailed error analysis reveals that agents primarily fail due to getting stuck in loops and losing track of original task objectives. We further propose an implicit RAG approach that provides modest improvements by generating task-relevant summaries, though fundamental limitations in long context reasoning persist. These findings highlight critical challenges for deploying WebAgents in realistic, long-term user interaction scenarios and provide insights for developing more robust agent architectures capable of maintaining coherent task execution across extended contexts.",
        "pdf_url": "https://arxiv.org/pdf/2512.04307v1",
        "entry_id": "http://arxiv.org/abs/2512.04307v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions",
        "authors": [
          "Paul Fuchs",
          "Julija Zavadlav"
        ],
        "published": "2025-12-03",
        "year": 2025,
        "summary": "Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.",
        "pdf_url": "https://arxiv.org/pdf/2512.03974v1",
        "entry_id": "http://arxiv.org/abs/2512.03974v1",
        "categories": [
          "physics.comp-ph",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "MechDetect: Detecting Data-Dependent Errors",
        "authors": [
          "Philipp Jung",
          "Nicholas Chandler",
          "Sebastian J\u00e4ger",
          "Felix Biessmann"
        ],
        "published": "2025-12-03",
        "year": 2025,
        "summary": "Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.",
        "pdf_url": "https://arxiv.org/pdf/2512.04138v1",
        "entry_id": "http://arxiv.org/abs/2512.04138v1",
        "categories": [
          "cs.LG",
          "cs.DB",
          "cs.IR"
        ],
        "us_affiliated": false
      },
      {
        "title": "Fault-Tolerant Control of Steam Temperature in HRSG Superheater under Actuator Fault Using a Sliding Mode Observer and PINN",
        "authors": [
          "Mojtaba Fanoodi",
          "Farzaneh Abdollahi",
          "Mahdi Aliyari Shoorehdeli"
        ],
        "published": "2025-12-03",
        "year": 2025,
        "summary": "This paper presents a novel fault-tolerant control framework for steam temperature regulation in Heat Recovery Steam Generators (HRSGs) subject to actuator faults. Addressing the critical challenge of valve degradation in superheater spray attemperators, we propose a synergistic architecture comprising three components: (1) a Sliding Mode Observer (SMO) for estimation of unmeasured thermal states, (2) a Physics-Informed Neural Network (PINN) for estimating multiplicative actuator faults using physical laws as constraints, and (3) a one-sided Sliding Mode Controller (SMC) that adapts to the estimated faults while minimizing excessive actuation.\n  The key innovation lies in the framework of closed-loop physics-awareness, where the PINN continuously informs both the observer and controller about fault severity while preserving thermodynamic consistency.\n  Rigorous uniform ultimate boundedness (UUB) is established via Lyapunov analysis under practical assumptions. Validated on real HRSG operational data, the framework demonstrates effective fault adaptation, reduced temperature overshoot, and maintains steam temperature within 1\u00b0C of the setpoint under valve effectiveness loss.\n  This work bridges control theory and physics-guided machine learning to deliver a practically deployable solution for power plant resilience, with extensions applicable to thermal systems subject to multiplicative faults.",
        "pdf_url": "https://arxiv.org/pdf/2512.03846v1",
        "entry_id": "http://arxiv.org/abs/2512.03846v1",
        "categories": [
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing",
        "authors": [
          "Emmanuel Akeweje",
          "Conall Kirk",
          "Chi-Wai Chan",
          "Denis Dowling",
          "Mimi Zhang"
        ],
        "published": "2025-12-03",
        "year": 2025,
        "summary": "Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.",
        "pdf_url": "https://arxiv.org/pdf/2512.06012v2",
        "entry_id": "http://arxiv.org/abs/2512.06012v2",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study",
        "authors": [
          "Johnny Peng",
          "Thanh Tung Khuat",
          "Ellen Otte",
          "Katarzyna Musial",
          "Bogdan Gabrys"
        ],
        "published": "2025-12-03",
        "year": 2025,
        "summary": "In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.",
        "pdf_url": "https://arxiv.org/pdf/2512.03460v1",
        "entry_id": "http://arxiv.org/abs/2512.03460v1",
        "categories": [
          "q-bio.QM",
          "cs.AI",
          "cs.CE",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding",
        "authors": [
          "Kiev Gama",
          "Filipe Calegario",
          "Victoria Jackson",
          "Alexander Nolte",
          "Luiz Augusto Morais",
          "Vinicius Garcia"
        ],
        "published": "2025-12-02",
        "year": 2025,
        "summary": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.",
        "pdf_url": "https://arxiv.org/pdf/2512.02750v1",
        "entry_id": "http://arxiv.org/abs/2512.02750v1",
        "categories": [
          "cs.SE",
          "cs.HC"
        ],
        "us_affiliated": false
      },
      {
        "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System",
        "authors": [
          "Martin Weiss",
          "Jesko Hecking-Harbusch",
          "Jochen Quante",
          "Matthias Woehrle"
        ],
        "published": "2025-12-02",
        "year": 2025,
        "summary": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.\n  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.\n  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.",
        "pdf_url": "https://arxiv.org/pdf/2512.02567v1",
        "entry_id": "http://arxiv.org/abs/2512.02567v1",
        "categories": [
          "cs.SE",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing",
        "authors": [
          "Shuvom Sadhuka",
          "Drew Prinster",
          "Clara Fannjiang",
          "Gabriele Scalia",
          "Aviv Regev",
          "Hanchen Wang"
        ],
        "published": "2025-12-02",
        "year": 2025,
        "summary": "Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.",
        "pdf_url": "https://arxiv.org/pdf/2512.03109v1",
        "entry_id": "http://arxiv.org/abs/2512.03109v1",
        "categories": [
          "cs.LG",
          "cs.AI",
          "stat.AP",
          "stat.ML"
        ],
        "us_affiliated": false
      },
      {
        "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
        "authors": [
          "Suzan Ece Ada",
          "Georg Martius",
          "Emre Ugur",
          "Erhan Oztop"
        ],
        "published": "2025-12-01",
        "year": 2025,
        "summary": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.",
        "pdf_url": "https://arxiv.org/pdf/2512.01987v2",
        "entry_id": "http://arxiv.org/abs/2512.01987v2",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "Accurate cosmological emulator for the probability distribution function of gravitational lensing of point sources",
        "authors": [
          "Tun\u00e7 T\u00fcrker",
          "Valerio Marra",
          "Tiago Castro",
          "Miguel Quartin",
          "Stefano Borgani"
        ],
        "published": "2025-12-01",
        "year": 2025,
        "summary": "We develop an accurate and computationally efficient emulator to model the gravitational lensing magnification probability distribution function (PDF), enabling robust cosmological inference of point sources such as supernovae and gravitational-wave observations. We construct a pipeline utilizing cosmological $N$-body simulations, creating past light cones to compute convergence and shear maps. Principal Component Analysis (PCA) is employed for dimensionality reduction, followed by an eXtreme Gradient Boosting (XGBoost) machine learning model to interpolate magnification PDFs across a broad cosmological parameter space ($\u03a9_m$, $\u03c3_8$, $w$, $h$) and redshift range ($0.2 \\le z \\le 6$). We identify the optimal number of PCA components to balance accuracy and stability. Our emulator, publicly released as ace_lensing, accurately reproduces lensing PDFs with a median Kullback-Leibler divergence of $0.007$. Validation on the test set confirmed that the model reliably reproduces the detailed shapes and statistical properties of the PDFs across the explored parameter range, showing no significant degradation for specific parameter combinations or redshifts. Future work will focus on incorporating baryonic physics through hydrodynamical simulations and expanding the training set to further enhance model accuracy and generalizability.",
        "pdf_url": "https://arxiv.org/pdf/2512.01607v1",
        "entry_id": "http://arxiv.org/abs/2512.01607v1",
        "categories": [
          "astro-ph.CO"
        ],
        "us_affiliated": false
      },
      {
        "title": "The Silence that Speaks: Neural Estimation via Communication Gaps",
        "authors": [
          "Shubham Aggarwal",
          "Dipankar Maity",
          "Tamer Ba\u015far"
        ],
        "published": "2025-11-30",
        "year": 2025,
        "summary": "Accurate remote state estimation is a fundamental component of many autonomous and networked dynamical systems, where multiple decision-making agents interact and communicate over shared, bandwidth-constrained channels. These communication constraints introduce an additional layer of complexity, namely, the decision of when to communicate. This results in a fundamental trade-off between estimation accuracy and communication resource usage. Traditional extensions of classical estimation algorithms (e.g., the Kalman filter) treat the absence of communication as 'missing' information. However, silence itself can carry implicit information about the system's state, which, if properly interpreted, can enhance the estimation quality even in the absence of explicit communication. Leveraging this implicit structure, however, poses significant analytical challenges, even in relatively simple systems. In this paper, we propose CALM (Communication-Aware Learning and Monitoring), a novel learning-based framework that jointly addresses the dual challenges of communication scheduling and estimator design. Our approach entails learning not only when to communicate but also how to infer useful information from periods of communication silence. We perform comparative case studies on multiple benchmarks to demonstrate that CALM is able to decode the implicit coordination between the estimator and the scheduler to extract information from the instances of 'silence' and enhance the estimation accuracy.",
        "pdf_url": "https://arxiv.org/pdf/2512.01056v1",
        "entry_id": "http://arxiv.org/abs/2512.01056v1",
        "categories": [
          "eess.SY",
          "cs.LG",
          "math.OC"
        ],
        "us_affiliated": false
      },
      {
        "title": "AI Agent for Source Finding by SoFiA-2 for SKA-SDC2",
        "authors": [
          "Xingchen Zhou",
          "Nan Li",
          "Peng Jia",
          "Yingfeng Liu",
          "Furen Deng",
          "Shuanghao Shu",
          "Ying Li",
          "Liang Cao",
          "Huanyuan Shan",
          "Ayodeji Ibitoye"
        ],
        "published": "2025-11-30",
        "year": 2025,
        "summary": "Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.",
        "pdf_url": "https://arxiv.org/pdf/2512.00769v1",
        "entry_id": "http://arxiv.org/abs/2512.00769v1",
        "categories": [
          "cs.LG",
          "astro-ph.GA"
        ],
        "us_affiliated": false
      },
      {
        "title": "Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?",
        "authors": [
          "Matt MacDermott",
          "Qiyao Wei",
          "Rada Djoneva",
          "Francis Rhys Ward"
        ],
        "published": "2025-11-28",
        "year": 2025,
        "summary": "AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \\emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \\emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.",
        "pdf_url": "https://arxiv.org/pdf/2512.00218v2",
        "entry_id": "http://arxiv.org/abs/2512.00218v2",
        "categories": [
          "cs.AI",
          "cs.CR"
        ],
        "us_affiliated": false
      },
      {
        "title": "Machine learning for violence prediction: a systematic review and critical appraisal",
        "authors": [
          "Stefaniya Kozhevnikova",
          "Denis Yukhnenko",
          "Giulio Scola",
          "Seena Fazel"
        ],
        "published": "2025-11-28",
        "year": 2025,
        "summary": "Purpose To conduct a systematic review of machine learning models for predicting violent behaviour by synthesising and appraising their validity, usefulness, and performance.\n  Methods We systematically searched nine bibliographic databases and Google Scholar up to September 2025 for development and/or validation studies on machine learning methods for predicting all forms of violent behaviour. We synthesised the results by summarising discrimination and calibration performance statistics and evaluated study quality by examining risk of bias and clinical utility.\n  Results We identified 38 studies reporting the development and validation of 40 models. Most studies reported Area Under the Curve (AUC) as the discrimination statistic with a range of 0.68-0.99. Only eight studies reported calibration performance, and three studies reported external validation. 31 studies had a high risk of bias, mainly in the analysis domain, and three studies had low risk of bias. The overall clinical utility of violence prediction models is poor, as indicated by risks of overfitting due to small samples, lack of transparent reporting, and low generalisability.\n  Conclusion Although black box machine learning models currently have limited applicability in clinical settings, they may show promise for identifying high-risk individuals. We recommend five key considerations for violence prediction modelling: (i) ensuring methodological quality (e.g. following guidelines) and interdisciplinary collaborations; (ii) using black box algorithms only for highly complex data; (iii) incorporating dynamic predictions to allow for risk monitoring; (iv) developing more trustworthy algorithms using explainable methods; and (v) applying causal machine learning approaches where appropriate.",
        "pdf_url": "https://arxiv.org/pdf/2511.23118v1",
        "entry_id": "http://arxiv.org/abs/2511.23118v1",
        "categories": [
          "stat.ME",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "An LLM-Assisted Multi-Agent Control Framework for Roll-to-Roll Manufacturing Systems",
        "authors": [
          "Jiachen Li",
          "Shihao Li",
          "Christopher Martin",
          "Zijun Chen",
          "Dongmei Chen",
          "Wei Li"
        ],
        "published": "2025-11-28",
        "year": 2025,
        "summary": "Roll-to-roll manufacturing requires precise tension and velocity control to ensure product quality, yet controller commissioning and adaptation remain time-intensive processes dependent on expert knowledge. This paper presents an LLM-assisted multi-agent framework that automates control system design and adaptation for R2R systems while maintaining safety. The framework operates through five phases: system identification from operational data, automated controller selection and tuning, sim-to-real adaptation with safety verification, continuous monitoring with diagnostic capabilities, and periodic model refinement. Experimental validation on a R2R system demonstrates successful tension regulation and velocity tracking under significant model uncertainty, with the framework achieving performance convergence through iterative adaptation. The approach reduces manual tuning effort while providing transparent diagnostic information for maintenance planning, offering a practical pathway for integrating AI-assisted automation in manufacturing control systems.",
        "pdf_url": "https://arxiv.org/pdf/2511.22975v1",
        "entry_id": "http://arxiv.org/abs/2511.22975v1",
        "categories": [
          "eess.SY"
        ],
        "us_affiliated": false
      },
      {
        "title": "Experts are all you need: A Composable Framework for Large Language Model Inference",
        "authors": [
          "Shrihari Sridharan",
          "Sourjya Roy",
          "Anand Raghunathan",
          "Kaushik Roy"
        ],
        "published": "2025-11-28",
        "year": 2025,
        "summary": "Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or \"experts\". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential \"plan--act--observe\" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.",
        "pdf_url": "https://arxiv.org/pdf/2511.22955v1",
        "entry_id": "http://arxiv.org/abs/2511.22955v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "AREA3D: Active Reconstruction Agent with Unified Feed-Forward 3D Perception and Vision-Language Guidance",
        "authors": [
          "Tianling Xu",
          "Shengzhe Gan",
          "Leslie Gu",
          "Yuelei Li",
          "Fangneng Zhan",
          "Hanspeter Pfister"
        ],
        "published": "2025-11-28",
        "year": 2025,
        "summary": "Active 3D reconstruction enables an agent to autonomously select viewpoints to efficiently obtain accurate and complete scene geometry, rather than passively reconstructing scenes from pre-collected images. However, existing active reconstruction methods often rely on hand-crafted geometric heuristics, which can lead to redundant observations without substantially improving reconstruction quality. To address this limitation, we propose AREA3D, an active reconstruction agent that leverages feed-forward 3D reconstruction models and vision-language guidance. Our framework decouples view-uncertainty modeling from the underlying feed-forward reconstructor, enabling precise uncertainty estimation without expensive online optimization. In addition, an integrated vision-language model provides high-level semantic guidance, encouraging informative and diverse viewpoints beyond purely geometric cues. Extensive experiments on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, particularly in the sparse-view regime. Code will be made available at: https://github.com/TianlingXu/AREA3D .",
        "pdf_url": "https://arxiv.org/pdf/2512.05131v1",
        "entry_id": "http://arxiv.org/abs/2512.05131v1",
        "categories": [
          "cs.CV",
          "cs.AI",
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "Mapping urban air quality using mobile and fixed low cost sensors: a model comparison",
        "authors": [
          "Yacine Mohamed Idir",
          "Olivier Orfila",
          "Patrice Chatellier",
          "Vincent Judalet",
          "Valentin Guaffre"
        ],
        "published": "2025-11-27",
        "year": 2025,
        "summary": "This study addresses the critical challenge of modeling and mapping urban air quality to ascertain pollutant concentrations in unmonitored locations. The advent of low-cost sensors, particularly those deployed in vehicular networks, presents novel datasets that hold the potential to enhance air quality modeling. This research conducts a comprehensive review of ten statistical models drawn from existing literature, using both fixed and mobile low-cost sensor data, alongside ancillary variables, within the urban confines of Nantes, France.\n  Employing a methodology that includes cross-validation of data from low-cost sensors and validation on fixed air quality monitoring stations, this paper evaluates the models' performance in scenarios of temporal interpolation and prediction. Our findings reveal a pronounced bias in the model outputs when reliant on low-cost sensor data compared to the verification data obtained from fixed stations. Furthermore, machine learning models demonstrated superior performance in predictive scenarios, suggesting their enhanced suitability for forecasting tasks.\n  The study conclusively indicates that reliance solely on data from low-cost mobile sensors compromises the reliability of air quality models, due to significant accuracy deficiencies. Consequently, we advocate for a directed focus towards the integration and calibration of low-cost sensor data with information from fixed monitoring stations. This approach, rather than an exclusive emphasis on the complexity of statistical modeling techniques, is pivotal for achieving the precision required for effective air quality management and policy-making.",
        "pdf_url": "https://arxiv.org/pdf/2511.22550v1",
        "entry_id": "http://arxiv.org/abs/2511.22550v1",
        "categories": [
          "stat.AP"
        ],
        "us_affiliated": false
      },
      {
        "title": "Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges",
        "authors": [
          "Guanxi Lu",
          "Hao Mark Chen",
          "Zhiqiang Que",
          "Wayne Luk",
          "Hongxiang Fan"
        ],
        "published": "2025-11-27",
        "year": 2025,
        "summary": "Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.",
        "pdf_url": "https://arxiv.org/pdf/2511.22483v1",
        "entry_id": "http://arxiv.org/abs/2511.22483v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Softly Symbolifying Kolmogorov-Arnold Networks",
        "authors": [
          "James Bagrow",
          "Josh Bongard"
        ],
        "published": "2025-11-27",
        "year": 2025,
        "summary": "Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.",
        "pdf_url": "https://arxiv.org/pdf/2512.07875v1",
        "entry_id": "http://arxiv.org/abs/2512.07875v1",
        "categories": [
          "cs.LG",
          "cs.NE",
          "physics.data-an",
          "stat.ML"
        ],
        "us_affiliated": false
      },
      {
        "title": "WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios",
        "authors": [
          "Eun Chang",
          "Zhuangqun Huang",
          "Yiwei Liao",
          "Sagar Ravi Bhavsar",
          "Amogh Param",
          "Tammy Stark",
          "Adel Ahmadyan",
          "Xiao Yang",
          "Jiaqi Wang",
          "Ahsan Abdullah",
          "Giang Nguyen",
          "Akil Iyer",
          "David Hall",
          "Elissa Li",
          "Shane Moon",
          "Nicolas Scheffer",
          "Kirmani Ahmed",
          "Babak Damavandi",
          "Rakesh Wanga",
          "Anuj Kumar",
          "Rohit Patel",
          "Xin Luna Dong"
        ],
        "published": "2025-11-27",
        "year": 2025,
        "summary": "We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.",
        "pdf_url": "https://arxiv.org/pdf/2511.22154v2",
        "entry_id": "http://arxiv.org/abs/2511.22154v2",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images",
        "authors": [
          "Kunpeng Zhang",
          "Hanwen Xu",
          "Sheng Wang"
        ],
        "published": "2025-11-26",
        "year": 2025,
        "summary": "Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed \"PathReasoning\", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.",
        "pdf_url": "https://arxiv.org/pdf/2511.21902v1",
        "entry_id": "http://arxiv.org/abs/2511.21902v1",
        "categories": [
          "cs.CV",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "A Dynamic Anti-Equinus Orthosis with Electromyography Sensor for Neuromuscular Rehabilitation",
        "authors": [
          "Manuel Terradillos Perea",
          "Olga Alonso Gonzalez",
          "Cristina Soguero Ruiz",
          "David Gutierrez"
        ],
        "published": "2025-11-26",
        "year": 2025,
        "summary": "The equinus foot is a neuromuscular condition that affects ankle dorsiflexion, impairing gait and reducing quality of life. This study presents EquiSay, a dynamic anti-equinus orthosis equipped with an anterior elastic tension system and an electromyography (EMG) sensor to quantify muscle activation, particularly of the tibialis anterior. EquiSay provides dynamic support that improves foot posture and natural movement while enabling real-time neuromuscular monitoring.\n  To address the limited availability of EMG data, the system incorporates a U-Net based model for generating synthetic EMG signals and a predictive framework for automatic calibration of minimum activation thresholds. Experimental results show improved dorsiflexion, increased patient satisfaction, and valuable clinical insights for rehabilitation planning. These findings highlight the potential of EquiSay as an assistive tool and as a platform for future AI-enhanced developments.",
        "pdf_url": "https://arxiv.org/pdf/2511.21484v1",
        "entry_id": "http://arxiv.org/abs/2511.21484v1",
        "categories": [
          "physics.med-ph"
        ],
        "us_affiliated": false
      },
      {
        "title": "Dual Preintegration for Relative State Estimation",
        "authors": [
          "Ruican Xia",
          "Hailong Pei"
        ],
        "published": "2025-11-26",
        "year": 2025,
        "summary": "Relative State Estimation perform mutually localization between two mobile agents undergoing six-degree-of-freedom motion. Based on the principle of circular motion, the estimation accuracy is sensitive to nonlinear rotations of the reference platform, particularly under large inter-platform distances. This phenomenon is even obvious for linearized kinematics, because cumulative linearization errors significantly degrade precision. In virtual reality (VR) applications, this manifests as substantial positional errors in 6-DoF controller tracking during rapid rotations of the head-mounted display. The linearization errors introduce drift in the estimate and render the estimator inconsistent. In the field of odometry, IMU preintegration is proposed as a kinematic observation to enable efficient relinearization, thus mitigate linearized error. Building on this theory, we propose dual preintegration, a novel observation integrating IMU preintegration from both platforms. This method serves as kinematic constraints for consecutive relative state and supports efficient relinearization. We also perform observability analysis of the state and analytically formulate the accordingly null space. Algorithm evaluation encompasses both simulations and real-world experiments. Multiple nonlinear rotations on the reference platform are simulated to compare the precision of the proposed method with that of other state-of-the-art (SOTA) algorithms. The field test compares the proposed method and SOTA algorithms in the application of VR controller tracking from the perspectives of bias observability, nonlinear rotation, and background texture. The results demonstrate that the proposed method is more precise and robust than the SOTA algorithms.",
        "pdf_url": "https://arxiv.org/pdf/2511.21189v1",
        "entry_id": "http://arxiv.org/abs/2511.21189v1",
        "categories": [
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
        "authors": [
          "Ziheng Ouyang",
          "Yiren Song",
          "Yaoli Liu",
          "Shihao Zhu",
          "Qibin Hou",
          "Ming-Ming Cheng",
          "Mike Zheng Shou"
        ],
        "published": "2025-11-25",
        "year": 2025,
        "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
        "pdf_url": "https://arxiv.org/pdf/2511.20614v1",
        "entry_id": "http://arxiv.org/abs/2511.20614v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "Failure Modes in LLM Systems: A System-Level Taxonomy for Reliable AI Applications",
        "authors": [
          "Vaishali Vinay"
        ],
        "published": "2025-11-25",
        "year": 2025,
        "summary": "Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.",
        "pdf_url": "https://arxiv.org/pdf/2511.19933v2",
        "entry_id": "http://arxiv.org/abs/2511.19933v2",
        "categories": [
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Microseismic Noise Mitigation with Machine Learning for Advanced LIGO",
        "authors": [
          "Christina Reissel",
          "Devin Lai",
          "Shivanshu Dwivedi",
          "Edgard Bonilla",
          "Claudia Geer",
          "Christopher Wipf",
          "Richard Mittleman",
          "Philip Harris",
          "Eyal Schwartz",
          "Dovi Poznanski",
          "Brian Lantz",
          "Erik Katsavounidis"
        ],
        "published": "2025-11-24",
        "year": 2025,
        "summary": "The unprecedented sensitivity of the Laser Interferometer Gravitational-Wave Observatory, which enables the detection of distant astrophysical sources, also renders the detectors highly susceptible to low-frequency ground motion. Persistent microseisms in the 0.1-0.3 Hz band couple into the instruments, degrade lock stability, and contribute substantially to detector downtime during observing runs. The multi-stage seismic isolation system has achieved remarkable success in mitigating such disturbances through active feedback control, yet residual platform motion remains a key factor limiting low-frequency sensitivity and duty cycle. Further reduction of this residual motion is therefore critical for improving the long-term stability and overall astrophysical reach of the observatories.\n  In this work, we develop a data-driven approach that uses machine learning to model and suppress residual seismic motion within the isolation system. Ground and platform sensor data from the detectors are used to train a neural network that predicts platform motion driven by microseismic activity. When incorporated into the control scheme, the network's predictions yield up to an order-of-magnitude reduction in residual motion compared to conventional linear filtering methods, revealing that nonlinear couplings play a significant role in limiting current isolation performance. These results demonstrate that machine-learning-based control can provide a powerful new pathway for enhancing active seismic isolation, improving lock robustness, and extending the low-frequency observational capabilities of gravitational-wave detectors.",
        "pdf_url": "https://arxiv.org/pdf/2511.19682v1",
        "entry_id": "http://arxiv.org/abs/2511.19682v1",
        "categories": [
          "gr-qc",
          "astro-ph.IM"
        ],
        "us_affiliated": false
      },
      {
        "title": "CHAOS - A Consistent Large-scale Database for Sigma-Profiles and Other Molecular Descriptors",
        "authors": [
          "Dominik Gond",
          "Justus Arweiler",
          "Thomas Specht",
          "Hans Hasse",
          "Fabian Jirasek"
        ],
        "published": "2025-11-24",
        "year": 2025,
        "summary": "Sigma-profiles obtained from quantum-chemical calculations are key molecular descriptors for solvent selection, thermodynamic modeling, and data-driven molecular design. However, existing sigma-profile libraries are limited in size and inconsistent in quality, which restricts their utility. In this work, we introduce CHAOS (Computed High-Accuracy Observables and Sigma Profiles), a large-scale and internally consistent database providing sigma-profiles for 53091 molecules, along with additional quantum-chemical observables including gas-phase geometries, single-point conductor-like polarizable continuum (C-PCM) data, infrared spectra, ideal-gas heat capacities and entropies, and atomic orbital nuclear magnetic resonance (NMR) shielding tensors. All data were generated using a standardized quantum-chemical workflow based on an wB97X-D/def2-TZVP level of theory. The CHAOS database covers molecules composed of a diverse set of elements, with molar masses up to 400 amu and dipole moments up to 15 D, and is freely available on Zenodo under an open license. It extends the number of molecules for which sigma-profiles are publicly available by more than an order of magnitude and systematically links them to a broad range of other quantum-chemical molecular descriptors. CHAOS provides a comprehensive and consistent foundation for developing models of molecular and thermodynamic properties -- both physics-based and machine-learning approaches -- across chemistry, chemical engineering, and materials science, greatly extending the possibilities and the available quantum-chemical data basis.",
        "pdf_url": "https://arxiv.org/pdf/2511.19002v1",
        "entry_id": "http://arxiv.org/abs/2511.19002v1",
        "categories": [
          "physics.chem-ph"
        ],
        "us_affiliated": false
      },
      {
        "title": "OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting",
        "authors": [
          "Haoming Jia",
          "Yi Han",
          "Xiang Wang",
          "Huizan Wang",
          "Wei Wu",
          "Jianming Zheng",
          "Peikun Xiao"
        ],
        "published": "2025-11-24",
        "year": 2025,
        "summary": "Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.",
        "pdf_url": "https://arxiv.org/pdf/2511.18732v1",
        "entry_id": "http://arxiv.org/abs/2511.18732v1",
        "categories": [
          "cs.LG",
          "stat.ML"
        ],
        "us_affiliated": false
      },
      {
        "title": "An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms",
        "authors": [
          "Hannah Lee",
          "James D. Motes",
          "Marco Morales",
          "Nancy M. Amato"
        ],
        "published": "2025-11-23",
        "year": 2025,
        "summary": "This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis",
        "pdf_url": "https://arxiv.org/pdf/2511.18604v1",
        "entry_id": "http://arxiv.org/abs/2511.18604v1",
        "categories": [
          "cs.RO",
          "cs.AI",
          "cs.MA"
        ],
        "us_affiliated": false
      },
      {
        "title": "Connectivity-Preserving Multi-Agent Area Coverage via Optimal-Transport-Based Density-Driven Optimal Control (D2OC)",
        "authors": [
          "Kooktae Lee",
          "Ethan Brook"
        ],
        "published": "2025-11-23",
        "year": 2025,
        "summary": "Multi-agent systems play a central role in area coverage tasks across search-and-rescue, environmental monitoring, and precision agriculture. Achieving non-uniform coverage, where spatial priorities vary across the domain, requires coordinating agents while respecting dynamic and communication constraints. Density-driven approaches can distribute agents according to a prescribed reference density, but existing methods do not ensure connectivity. This limitation often leads to communication loss, reduced coordination, and degraded coverage performance.\n  This letter introduces a connectivity-preserving extension of the Density-Driven Optimal Control (D2OC) framework. The coverage objective, defined using the Wasserstein distance between the agent distribution and the reference density, admits a convex quadratic program formulation. Communication constraints are incorporated through a smooth connectivity penalty, which maintains strict convexity, supports distributed implementation, and preserves inter-agent communication without imposing rigid formations.\n  Simulation studies show that the proposed method consistently maintains connectivity, improves convergence speed, and enhances non-uniform coverage quality compared with density-driven schemes that do not incorporate explicit connectivity considerations.",
        "pdf_url": "https://arxiv.org/pdf/2511.18579v3",
        "entry_id": "http://arxiv.org/abs/2511.18579v3",
        "categories": [
          "eess.SY",
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "End-to-End Automated Logging via Multi-Agent Framework",
        "authors": [
          "Renyi Zhong",
          "Yintong Huo",
          "Wenwei Gu",
          "Yichen Li",
          "Michael R. Lyu"
        ],
        "published": "2025-11-23",
        "year": 2025,
        "summary": "Software logging is critical for system observability, yet developers face a dual crisis of costly overlogging and risky underlogging. Existing automated logging tools often overlook the fundamental whether-to-log decision and struggle with the composite nature of logging. In this paper, we propose Autologger, a novel hybrid framework that addresses the complete the end-to-end logging pipeline. Autologger first employs a fine-tuned classifier, the Judger, to accurately determine if a method requires new logging statements. If logging is needed, a multi-agent system is activated. The system includes specialized agents: a Locator dedicated to determining where to log, and a Generator focused on what to log. These agents work together, utilizing our designed program analysis and retrieval tools. We evaluate Autologger on a large corpus from three mature open-source projects against state-of-the-art baselines. Our results show that Autologger achieves 96.63\\% F1-score on the crucial whether-to-log decision. In an end-to-end setting, Autologger improves the overall quality of generated logging statements by 16.13\\% over the strongest baseline, as measured by an LLM-as-a-judge score. We also demonstrate that our framework is generalizable, consistently boosting the performance of various backbone LLMs.",
        "pdf_url": "https://arxiv.org/pdf/2511.18528v1",
        "entry_id": "http://arxiv.org/abs/2511.18528v1",
        "categories": [
          "cs.SE"
        ],
        "us_affiliated": false
      },
      {
        "title": "stable-pretraining-v1: Foundation Model Research Made Simple",
        "authors": [
          "Randall Balestriero",
          "Hugues Van Assel",
          "Sami BuGhanem",
          "Lucas Maes"
        ],
        "published": "2025-11-23",
        "year": 2025,
        "summary": "Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.",
        "pdf_url": "https://arxiv.org/pdf/2511.19484v1",
        "entry_id": "http://arxiv.org/abs/2511.19484v1",
        "categories": [
          "cs.SE",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses",
        "authors": [
          "Enrico Pallotta",
          "Sina Mokhtarzadeh Azar",
          "Lars Doorenbos",
          "Serdar Ozsoy",
          "Umar Iqbal",
          "Juergen Gall"
        ],
        "published": "2025-11-22",
        "year": 2025,
        "summary": "Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.",
        "pdf_url": "https://arxiv.org/pdf/2511.18173v1",
        "entry_id": "http://arxiv.org/abs/2511.18173v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "MEDIC: a network for monitoring data quality in collider experiments",
        "authors": [
          "Juvenal Bassa",
          "Arghya Chattopadhyay",
          "Sudhir Malik",
          "Mario Escabi Rivera"
        ],
        "published": "2025-11-22",
        "year": 2025,
        "summary": "Data Quality Monitoring (DQM) is a crucial component of particle physics experiments and ensures that the recorded data is of the highest quality, and suitable for subsequent physics analysis. Due to the extreme environmental conditions, unprecedented data volumes, and the sheer scale and complexity of the detectors, DQM orchestration has become a very challenging task. Therefore, the use of Machine Learning (ML) to automate anomaly detection, improve efficiency, and reduce human error in the process of collecting high-quality data is unavoidable. Since DQM relies on real experimental data, it is inherently tied to the specific detector substructure and technology in operation. In this work, a simulation-driven approach to DQM is proposed, enabling the study and development of data-quality methodologies in a controlled environment. Using a modified version of Delphes -- a fast, multi-purpose detector simulation -- the preliminary realization of a framework is demonstrated which leverages ML to identify detector anomalies as well as localize the malfunctioning components responsible. We introduce MEDIC (Monitoring for Event Data Integrity and Consistency), a neural network designed to learn detector behavior and perform DQM tasks to look for potential faults. Although the present implementation adopts a simplified setup for computational ease, where large detector regions are deliberately deactivated to mimic faults, this work represents an initial step toward a comprehensive ML-based DQM framework. The encouraging results underline the potential of simulation-driven studies as a foundation for developing more advanced, data-driven DQM systems for future particle detectors.",
        "pdf_url": "https://arxiv.org/pdf/2511.18172v1",
        "entry_id": "http://arxiv.org/abs/2511.18172v1",
        "categories": [
          "hep-ex",
          "cs.AI",
          "cs.LG"
        ],
        "us_affiliated": false
      }
    ]
  },
  "recovery_patterns": {
    "description": "Recovery patterns for AI - checkpointing strategies, state reconstruction, graceful degradation",
    "query": "(abs:\"checkpointing\" OR abs:\"state recovery\" OR abs:\"graceful degradation\") AND (abs:\"AI\" OR abs:\"machine learning\" OR abs:\"inference\")",
    "total_found": 50,
    "filtered_count": 50,
    "papers_2025": 50,
    "papers_2024": 0,
    "us_affiliated": 2,
    "papers": [
      {
        "title": "NVIDIA Nemotron Nano V2 VL",
        "authors": [
          "NVIDIA",
          ":",
          "Amala Sanjay Deshmukh",
          "Kateryna Chumachenko",
          "Tuomas Rintamaki",
          "Matthieu Le",
          "Tyler Poon",
          "Danial Mohseni Taheri",
          "Ilia Karmanov",
          "Guilin Liu",
          "Jarno Seppanen",
          "Guo Chen",
          "Karan Sapra",
          "Zhiding Yu",
          "Adi Renduchintala",
          "Charles Wang",
          "Peter Jin",
          "Arushi Goel",
          "Mike Ranzinger",
          "Lukas Voegtle",
          "Philipp Fischer",
          "Timo Roman",
          "Wei Ping",
          "Boxin Wang",
          "Zhuolin Yang",
          "Nayeon Lee",
          "Shaokun Zhang",
          "Fuxiao Liu",
          "Zhiqi Li",
          "Di Zhang",
          "Greg Heinrich",
          "Hongxu Yin",
          "Song Han",
          "Pavlo Molchanov",
          "Parth Mannan",
          "Yao Xu",
          "Jane Polak Scowcroft",
          "Tom Balough",
          "Subhashree Radhakrishnan",
          "Paris Zhang",
          "Sean Cha",
          "Ratnesh Kumar",
          "Zaid Pervaiz Bhat",
          "Jian Zhang",
          "Darragh Hanley",
          "Pritam Biswas",
          "Jesse Oliver",
          "Kevin Vasques",
          "Roger Waleffe",
          "Duncan Riach",
          "Oluwatobi Olabiyi",
          "Ameya Sunil Mahabaleshwarkar",
          "Bilal Kartal",
          "Pritam Gundecha",
          "Khanh Nguyen",
          "Alexandre Milesi",
          "Eugene Khvedchenia",
          "Ran Zilberstein",
          "Ofri Masad",
          "Natan Bagrov",
          "Nave Assaf",
          "Tomer Asida",
          "Daniel Afrimi",
          "Amit Zuker",
          "Netanel Haber",
          "Zhiyu Cheng",
          "Jingyu Xin",
          "Di Wu",
          "Nik Spirin",
          "Maryam Moosaei",
          "Roman Ageev",
          "Vanshil Atul Shah",
          "Yuting Wu",
          "Daniel Korzekwa",
          "Unnikrishnan Kizhakkemadam Sreekumar",
          "Wanli Jiang",
          "Padmavathy Subramanian",
          "Alejandra Rico",
          "Sandip Bhaskar",
          "Saeid Motiian",
          "Kedi Wu",
          "Annie Surla",
          "Chia-Chih Chen",
          "Hayden Wolff",
          "Matthew Feinberg",
          "Melissa Corpuz",
          "Marek Wawrzos",
          "Eileen Long",
          "Aastha Jhunjhunwala",
          "Paul Hendricks",
          "Farzan Memarian",
          "Benika Hall",
          "Xin-Yu Wang",
          "David Mosallanezhad",
          "Soumye Singhal",
          "Luis Vega",
          "Katherine Cheung",
          "Krzysztof Pawelec",
          "Michael Evans",
          "Katherine Luna",
          "Jie Lou",
          "Erick Galinkin",
          "Akshay Hazare",
          "Kaustubh Purandare",
          "Ann Guan",
          "Anna Warno",
          "Chen Cui",
          "Yoshi Suhara",
          "Shibani Likhite",
          "Seph Mard",
          "Meredith Price",
          "Laya Sleiman",
          "Saori Kaji",
          "Udi Karpas",
          "Kari Briski",
          "Joey Conway",
          "Michael Lightstone",
          "Jan Kautz",
          "Mohammad Shoeybi",
          "Mostofa Patwary",
          "Jonathen Cohen",
          "Oleksii Kuchaiev",
          "Andrew Tao",
          "Bryan Catanzaro"
        ],
        "published": "2025-11-06",
        "year": 2025,
        "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.",
        "pdf_url": "https://arxiv.org/pdf/2511.03929v2",
        "entry_id": "http://arxiv.org/abs/2511.03929v2",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.CV"
        ],
        "us_affiliated": true
      },
      {
        "title": "World Simulation with Video Foundation Models for Physical AI",
        "authors": [
          "NVIDIA",
          ":",
          "Arslan Ali",
          "Junjie Bai",
          "Maciej Bala",
          "Yogesh Balaji",
          "Aaron Blakeman",
          "Tiffany Cai",
          "Jiaxin Cao",
          "Tianshi Cao",
          "Elizabeth Cha",
          "Yu-Wei Chao",
          "Prithvijit Chattopadhyay",
          "Mike Chen",
          "Yongxin Chen",
          "Yu Chen",
          "Shuai Cheng",
          "Yin Cui",
          "Jenna Diamond",
          "Yifan Ding",
          "Jiaojiao Fan",
          "Linxi Fan",
          "Liang Feng",
          "Francesco Ferroni",
          "Sanja Fidler",
          "Xiao Fu",
          "Ruiyuan Gao",
          "Yunhao Ge",
          "Jinwei Gu",
          "Aryaman Gupta",
          "Siddharth Gururani",
          "Imad El Hanafi",
          "Ali Hassani",
          "Zekun Hao",
          "Jacob Huffman",
          "Joel Jang",
          "Pooya Jannaty",
          "Jan Kautz",
          "Grace Lam",
          "Xuan Li",
          "Zhaoshuo Li",
          "Maosheng Liao",
          "Chen-Hsuan Lin",
          "Tsung-Yi Lin",
          "Yen-Chen Lin",
          "Huan Ling",
          "Ming-Yu Liu",
          "Xian Liu",
          "Yifan Lu",
          "Alice Luo",
          "Qianli Ma",
          "Hanzi Mao",
          "Kaichun Mo",
          "Seungjun Nah",
          "Yashraj Narang",
          "Abhijeet Panaskar",
          "Lindsey Pavao",
          "Trung Pham",
          "Morteza Ramezanali",
          "Fitsum Reda",
          "Scott Reed",
          "Xuanchi Ren",
          "Haonan Shao",
          "Yue Shen",
          "Stella Shi",
          "Shuran Song",
          "Bartosz Stefaniak",
          "Shangkun Sun",
          "Shitao Tang",
          "Sameena Tasmeen",
          "Lyne Tchapmi",
          "Wei-Cheng Tseng",
          "Jibin Varghese",
          "Andrew Z. Wang",
          "Hao Wang",
          "Haoxiang Wang",
          "Heng Wang",
          "Ting-Chun Wang",
          "Fangyin Wei",
          "Jiashu Xu",
          "Dinghao Yang",
          "Xiaodong Yang",
          "Haotian Ye",
          "Seonghyeon Ye",
          "Xiaohui Zeng",
          "Jing Zhang",
          "Qinsheng Zhang",
          "Kaiwen Zheng",
          "Andrew Zhu",
          "Yuke Zhu"
        ],
        "published": "2025-10-28",
        "year": 2025,
        "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.",
        "pdf_url": "https://arxiv.org/pdf/2511.00062v1",
        "entry_id": "http://arxiv.org/abs/2511.00062v1",
        "categories": [
          "cs.CV",
          "cs.AI",
          "cs.LG",
          "cs.RO"
        ],
        "us_affiliated": true
      },
      {
        "title": "LongCat-Image Technical Report",
        "authors": [
          "Meituan LongCat Team",
          "Hanghang Ma",
          "Haoxian Tan",
          "Jiale Huang",
          "Junqiang Wu",
          "Jun-Yan He",
          "Lishuai Gao",
          "Songlin Xiao",
          "Xiaoming Wei",
          "Xiaoqi Ma",
          "Xunliang Cai",
          "Yayong Guan",
          "Jie Hu"
        ],
        "published": "2025-12-08",
        "year": 2025,
        "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
        "pdf_url": "https://arxiv.org/pdf/2512.07584v1",
        "entry_id": "http://arxiv.org/abs/2512.07584v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "$\\mathrm{D}^{\\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction",
        "authors": [
          "Changliang Xia",
          "Chengyou Jia",
          "Minnan Luo",
          "Zhuohang Dang",
          "Xin Shen",
          "Bowen Ping"
        ],
        "published": "2025-12-08",
        "year": 2025,
        "summary": "Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\\mathrm{D}^{\\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\\mathrm{D}^{\\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\\mathrm{D}^{\\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.",
        "pdf_url": "https://arxiv.org/pdf/2512.07062v1",
        "entry_id": "http://arxiv.org/abs/2512.07062v1",
        "categories": [
          "cs.CV",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
        "authors": [
          "Yuchuan Tian",
          "Yuchen Liang",
          "Jiacheng Sun",
          "Shuo Zhang",
          "Guangwen Yang",
          "Yingte Shu",
          "Sibo Fang",
          "Tianyu Guo",
          "Kai Han",
          "Chao Xu",
          "Hanting Chen",
          "Xinghao Chen",
          "Yunhe Wang"
        ],
        "published": "2025-12-07",
        "year": 2025,
        "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
        "pdf_url": "https://arxiv.org/pdf/2512.06776v1",
        "entry_id": "http://arxiv.org/abs/2512.06776v1",
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance",
        "authors": [
          "Junjie Zheng",
          "Chunbo Hao",
          "Guobin Ma",
          "Xiaoyu Zhang",
          "Gongyu Chen",
          "Chaofan Ding",
          "Zihao Chen",
          "Lei Xie"
        ],
        "published": "2025-12-04",
        "year": 2025,
        "summary": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.",
        "pdf_url": "https://arxiv.org/pdf/2512.04779v1",
        "entry_id": "http://arxiv.org/abs/2512.04779v1",
        "categories": [
          "cs.SD",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\\{\\pm 1, \\pm i\\}$",
        "authors": [
          "Feiyu Wang",
          "Xinyu Tan",
          "Bokai Huang",
          "Yihao Zhang",
          "Guoan Wang",
          "Peizhuang Cong",
          "Tong Yang"
        ],
        "published": "2025-12-02",
        "year": 2025,
        "summary": "Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.",
        "pdf_url": "https://arxiv.org/pdf/2512.02901v2",
        "entry_id": "http://arxiv.org/abs/2512.02901v2",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "ThetaEvolve: Test-time Learning on Open Problems",
        "authors": [
          "Yiping Wang",
          "Shao-Rong Su",
          "Zhiyuan Zeng",
          "Eva Xu",
          "Liliang Ren",
          "Xinyu Yang",
          "Zeyi Huang",
          "Xuehai He",
          "Luyao Ma",
          "Baolin Peng",
          "Hao Cheng",
          "Pengcheng He",
          "Weizhu Chen",
          "Shuohang Wang",
          "Simon Shaolei Du",
          "Yelong Shen"
        ],
        "published": "2025-11-28",
        "year": 2025,
        "summary": "Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve",
        "pdf_url": "https://arxiv.org/pdf/2511.23473v1",
        "entry_id": "http://arxiv.org/abs/2511.23473v1",
        "categories": [
          "cs.LG",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts",
        "authors": [
          "Ivan Novikov"
        ],
        "published": "2025-11-26",
        "year": 2025,
        "summary": "Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1",
        "pdf_url": "https://arxiv.org/pdf/2511.21089v1",
        "entry_id": "http://arxiv.org/abs/2511.21089v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models",
        "authors": [
          "Zhidong Gao",
          "Zimeng Pan",
          "Yuhang Yao",
          "Chenyue Xie",
          "Wei Wei"
        ],
        "published": "2025-11-25",
        "year": 2025,
        "summary": "Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling",
        "pdf_url": "https://arxiv.org/pdf/2512.03056v1",
        "entry_id": "http://arxiv.org/abs/2512.03056v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "NNGPT: Rethinking AutoML with Large Language Models",
        "authors": [
          "Roman Kochnev",
          "Waleed Khalid",
          "Tolgay Atinc Uzun",
          "Xi Zhang",
          "Yashkumar Sanjaybhai Dhameliya",
          "Furui Qin",
          "Chandini Vysyaraju",
          "Raghuvir Duvvuri",
          "Avi Goyal",
          "Dmitry Ignatov",
          "Radu Timofte"
        ],
        "published": "2025-11-25",
        "year": 2025,
        "summary": "Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.",
        "pdf_url": "https://arxiv.org/pdf/2511.20333v1",
        "entry_id": "http://arxiv.org/abs/2511.20333v1",
        "categories": [
          "cs.AI",
          "cs.LG",
          "cs.NE"
        ],
        "us_affiliated": false
      },
      {
        "title": "Crash-Consistent Checkpointing for AI Training on macOS/APFS",
        "authors": [
          "Juha Jeon"
        ],
        "published": "2025-11-23",
        "year": 2025,
        "summary": "Deep learning training relies on periodic checkpoints to recover from failures, but unsafe checkpoint installation can leave corrupted files on disk. This paper presents an experimental study of checkpoint installation protocols and integrity validation for AI training on macOS/APFS. We implement three write modes with increasing durability guarantees: unsafe (baseline, no fsync), atomic_nodirsync (file-level durability via fsync()), and atomic_dirsync (file + directory durability). We design a format-agnostic integrity guard using SHA-256 checksums with automatic rollback. Through controlled experiments including crash injection (430 unsafe-mode trials) and corruption injection (1,600 atomic-mode trials), we demonstrate that the integrity guard detects 99.8-100% of corruptions with zero false positives. Performance overhead is 56.5-108.4% for atomic_nodirsync and 84.2-570.6% for atomic_dirsync relative to the unsafe baseline. Our findings quantify the reliability-performance trade-offs and provide deployment guidance for production AI infrastructure.",
        "pdf_url": "https://arxiv.org/pdf/2511.18323v1",
        "entry_id": "http://arxiv.org/abs/2511.18323v1",
        "categories": [
          "cs.OS",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
        "authors": [
          "Quentin Anthony",
          "Yury Tokpanov",
          "Skyler Szot",
          "Srivatsan Rajagopal",
          "Praneeth Medepalli",
          "Anna Golubeva",
          "Vasu Shyam",
          "Robert Washbourne",
          "Rishi Iyer",
          "Ansh Chaurasia",
          "Tomas Figliolia",
          "Xiao Yang",
          "Abhinav Sarje",
          "Drew Thorstensen",
          "Amartey Pearson",
          "Zack Grossbart",
          "Jason van Patten",
          "Emad Barsoum",
          "Zhenyu Gu",
          "Yao Fu",
          "Beren Millidge"
        ],
        "published": "2025-11-21",
        "year": 2025,
        "summary": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs and Pollara networking. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts over Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE, available at https://huggingface.co/Zyphra/ZAYA1-base) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
        "pdf_url": "https://arxiv.org/pdf/2511.17127v2",
        "entry_id": "http://arxiv.org/abs/2511.17127v2",
        "categories": [
          "cs.CL",
          "cs.AI",
          "cs.DC"
        ],
        "us_affiliated": false
      },
      {
        "title": "BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks",
        "authors": [
          "Samuel Stevens"
        ],
        "published": "2025-11-20",
        "year": 2025,
        "summary": "ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.",
        "pdf_url": "https://arxiv.org/pdf/2511.16315v1",
        "entry_id": "http://arxiv.org/abs/2511.16315v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "Green Distributed AI Training: Orchestrating Compute Across Renewable-Powered Micro Datacenters",
        "authors": [
          "Giuseppe Tomei",
          "Andrea Mayer",
          "Giuseppe Alcini",
          "Stefano Salsano"
        ],
        "published": "2025-11-20",
        "year": 2025,
        "summary": "The accelerating expansion of AI workloads is colliding with an energy landscape increasingly dominated by intermittent renewable generation. While vast quantities of zero-carbon energy are routinely curtailed, today's centralized datacenter architectures remain poorly matched to this reality in both energy proportionality and geographic flexibility. This work envisions a shift toward a distributed fabric of renewable-powered micro-datacenters that dynamically follow the availability of surplus green energy through live workload migration.\n  At the core of this vision lies a formal feasibility-domain model that delineates when migratory AI computation is practically achievable. By explicitly linking checkpoint size, wide-area bandwidth, and renewable-window duration, the model reveals that migration is almost always energetically justified, and that time-not energy-is the dominant constraint shaping feasibility. This insight enables the design of a feasibility-aware orchestration framework that transforms migration from a best-effort heuristic into a principled control mechanism. Trace-driven evaluation shows that such orchestration can simultaneously reduce non-renewable energy use and improve performance stability, overcoming the tradeoffs of purely energy-driven strategies.\n  Beyond the immediate feasibility analysis, the extended version explores the architectural horizon of renewable-aware AI infrastructures. It examines the role of emerging ultra-efficient GPU-enabled edge platforms, anticipates integration with grid-level control and demand-response ecosystems, and outlines paths toward supporting partially migratable and distributed workloads. The work positions feasibility-aware migration as a foundational building block for a future computing paradigm in which AI execution becomes fluid, geographically adaptive, and aligned with renewable energy availability.",
        "pdf_url": "https://arxiv.org/pdf/2511.16182v1",
        "entry_id": "http://arxiv.org/abs/2511.16182v1",
        "categories": [
          "cs.NI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs",
        "authors": [
          "Rayen Dhahri",
          "Steffen Urban"
        ],
        "published": "2025-11-19",
        "year": 2025,
        "summary": "Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric, per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes. Across models and tasks, it narrows the FP-to-low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy per inference, and cost under static/dynamic activation scaling and varying operator coverage.",
        "pdf_url": "https://arxiv.org/pdf/2511.15300v2",
        "entry_id": "http://arxiv.org/abs/2511.15300v2",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
        "authors": [
          "Vladimir Arkhipkin",
          "Vladimir Korviakov",
          "Nikolai Gerasimenko",
          "Denis Parkhomenko",
          "Viacheslav Vasilev",
          "Alexey Letunovskiy",
          "Nikolai Vaulin",
          "Maria Kovaleva",
          "Ivan Kirillov",
          "Lev Novitskiy",
          "Denis Koposov",
          "Nikita Kiselev",
          "Alexander Varlamov",
          "Dmitrii Mikhailov",
          "Vladimir Polovnikov",
          "Andrey Shutkin",
          "Julia Agafonova",
          "Ilya Vasiliev",
          "Anastasiia Kargapoltseva",
          "Anna Dmitrienko",
          "Anastasia Maltseva",
          "Anna Averchenkova",
          "Olga Kim",
          "Tatiana Nikulina",
          "Denis Dimitrov"
        ],
        "published": "2025-11-19",
        "year": 2025,
        "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
        "pdf_url": "https://arxiv.org/pdf/2511.14993v2",
        "entry_id": "http://arxiv.org/abs/2511.14993v2",
        "categories": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images",
        "authors": [
          "Roman Kinakh",
          "Gonzalo R. R\u00edos-Mu\u00f1oz",
          "Arrate Mu\u00f1oz-Barrutia"
        ],
        "published": "2025-11-14",
        "year": 2025,
        "summary": "Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows.",
        "pdf_url": "https://arxiv.org/pdf/2511.11486v1",
        "entry_id": "http://arxiv.org/abs/2511.11486v1",
        "categories": [
          "cs.CV",
          "q-bio.QM"
        ],
        "us_affiliated": false
      },
      {
        "title": "On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization",
        "authors": [
          "Prabodh Katti",
          "Sangwoo Park",
          "Bipin Rajendran",
          "Osvaldo Simeone"
        ],
        "published": "2025-11-14",
        "year": 2025,
        "summary": "On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.",
        "pdf_url": "https://arxiv.org/pdf/2511.11362v1",
        "entry_id": "http://arxiv.org/abs/2511.11362v1",
        "categories": [
          "cs.LG",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?",
        "authors": [
          "Tuan Anh Tran",
          "Duy M. H. Nguyen",
          "Hoai-Chau Tran",
          "Michael Barz",
          "Khoa D. Doan",
          "Roger Wattenhofer",
          "Ngo Anh Vien",
          "Mathias Niepert",
          "Daniel Sonntag",
          "Paul Swoboda"
        ],
        "published": "2025-11-07",
        "year": 2025,
        "summary": "Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at https://gitmerge3d.github.io",
        "pdf_url": "https://arxiv.org/pdf/2511.05449v1",
        "entry_id": "http://arxiv.org/abs/2511.05449v1",
        "categories": [
          "cs.CV",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning",
        "authors": [
          "Md Ahasanul Arafath",
          "Abhijit Kumar Ghosh",
          "Md Rony Ahmed",
          "Sabrin Afroz",
          "Minhazul Hosen",
          "Md Hasan Moon",
          "Md Tanzim Reza",
          "Md Ashad Alam"
        ],
        "published": "2025-11-05",
        "year": 2025,
        "summary": "Colorectal cancer (CRC) grading is a critical prognostic factor but remains hampered by inter-observer variability and the privacy constraints of multi-institutional data sharing. While deep learning offers a path to automation, centralized training models conflict with data governance regulations and neglect the diagnostic importance of multi-scale analysis. In this work, we propose a scalable, privacy-preserving federated learning (FL) framework for CRC histopathological grading that integrates multi-scale feature learning within a distributed training paradigm. Our approach employs a dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear detail and broader tissue-level context. This architecture is integrated into a robust FL system stabilized using FedProx to mitigate client drift across heterogeneous data distributions from multiple hospitals. Extensive evaluation on the CRC-HGD dataset demonstrates that our framework achieves an overall accuracy of 83.5%, outperforming a comparable centralized model (81.6%). Crucially, the system excels in identifying the most aggressive Grade III tumors with a high recall of 87.5%, a key clinical priority to prevent dangerous false negatives. Performance further improves with higher magnification, reaching 88.0% accuracy at 40x. These results validate that our federated multi-scale approach not only preserves patient privacy but also enhances model performance and generalization. The proposed modular pipeline, with built-in preprocessing, checkpointing, and error handling, establishes a foundational step toward deployable, privacy-aware clinical AI for digital pathology.",
        "pdf_url": "https://arxiv.org/pdf/2511.03693v1",
        "entry_id": "http://arxiv.org/abs/2511.03693v1",
        "categories": [
          "stat.ML",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model",
        "authors": [
          "Annabelle Martin",
          "Daphne Kontogiorgos-Heintz",
          "Jeff Nivala"
        ],
        "published": "2025-11-03",
        "year": 2025,
        "summary": "Nanopore protein sequencing produces long, noisy ionic current traces in which key molecular phases, such as protein capture and translocation, are embedded. Capture phases mark the successful entry of a protein into the pore and serve as both a checkpoint and a signal that a channel merits further analysis. However, manual identification of capture phases is time-intensive, often requiring several days for expert reviewers to annotate the data due to the need for domain-specific interpretation of complex signal patterns. To address this, a lightweight one-dimensional convolutional neural network (1D CNN) was developed and trained to detect capture phases in down-sampled signal windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids, histogram-based classifiers, and other CNN variants using run-level data splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and precision of 93.39% on held-out test data. The model supports low-latency inference and is integrated into a dashboard for Oxford Nanopore experiments, reducing the total analysis time from several days to under thirty minutes. These results show that efficient, real-time capture detection is possible using simple, interpretable architectures and suggest a broader role for lightweight ML models in sequencing workflows.",
        "pdf_url": "https://arxiv.org/pdf/2511.01277v1",
        "entry_id": "http://arxiv.org/abs/2511.01277v1",
        "categories": [
          "cs.LG",
          "q-bio.QM"
        ],
        "us_affiliated": false
      },
      {
        "title": "Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence",
        "authors": [
          "Yi Zhang",
          "Che Liu",
          "Xiancong Ren",
          "Hanchu Ni",
          "Shuai Zhang",
          "Zeyuan Ding",
          "Jiayu Hu",
          "Hanzhe Shan",
          "Zhenwei Niu",
          "Zhaoyang Liu",
          "Shuang Liu",
          "Yue Zhao",
          "Junbo Qi",
          "Qinfan Zhang",
          "Dengjie Li",
          "Yidong Wang",
          "Jiachen Luo",
          "Yong Dai",
          "Zenglin Xu",
          "Bin Shen",
          "Qifan Wang",
          "Jian Tang",
          "Xiaozhu Ju"
        ],
        "published": "2025-10-30",
        "year": 2025,
        "summary": "This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.",
        "pdf_url": "https://arxiv.org/pdf/2511.00108v2",
        "entry_id": "http://arxiv.org/abs/2511.00108v2",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.RO"
        ],
        "us_affiliated": false
      },
      {
        "title": "One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning",
        "authors": [
          "Renhao Li",
          "Jianhong Tu",
          "Yang Su",
          "Hamid Alinejad-Rokny",
          "Derek F. Wong",
          "Junyang Lin",
          "Min Yang"
        ],
        "published": "2025-10-30",
        "year": 2025,
        "summary": "Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.",
        "pdf_url": "https://arxiv.org/pdf/2510.26167v1",
        "entry_id": "http://arxiv.org/abs/2510.26167v1",
        "categories": [
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Human Resilience in the AI Era -- What Machines Can't Replace",
        "authors": [
          "Shaoshan Liu",
          "Anina Schwarzenbach",
          "Yiyu Shi"
        ],
        "published": "2025-10-29",
        "year": 2025,
        "summary": "AI is displacing tasks, mediating high-stakes decisions, and flooding communication with synthetic content, unsettling work, identity, and social trust. We argue that the decisive human countermeasure is resilience. We define resilience across three layers: psychological, including emotion regulation, meaning-making, cognitive flexibility; social, including trust, social capital, coordinated response; organizational, including psychological safety, feedback mechanisms, and graceful degradation. We synthesize early evidence that these capacities buffer individual strain, reduce burnout through social support, and lower silent failure in AI-mediated workflows through team norms and risk-responsive governance. We also show that resilience can be cultivated through training that complements rather than substitutes for structural safeguards. By reframing the AI debate around actionable human resilience, this article offers policymakers, educators, and operators a practical lens to preserve human agency and steer responsible adoption.",
        "pdf_url": "https://arxiv.org/pdf/2510.25218v1",
        "entry_id": "http://arxiv.org/abs/2510.25218v1",
        "categories": [
          "cs.CY",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Relative Scaling Laws for LLMs",
        "authors": [
          "William Held",
          "David Hall",
          "Percy Liang",
          "Diyi Yang"
        ],
        "published": "2025-10-28",
        "year": 2025,
        "summary": "Scaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. We introduce relative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP) budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we find diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, we release all model checkpoints from this work to enable practitioners to measure relative alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson.",
        "pdf_url": "https://arxiv.org/pdf/2510.24626v1",
        "entry_id": "http://arxiv.org/abs/2510.24626v1",
        "categories": [
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Exploration of Machine Learning Methods to Seismic Event Discrimination in the Pacific Northwest",
        "authors": [
          "Akash Kharita",
          "Marine Denolle",
          "Alexander R Hutko",
          "J. Renate Hartog",
          "Stephen D. Malone"
        ],
        "published": "2025-10-27",
        "year": 2025,
        "summary": "Accurately separating tectonic, anthropogenic, and geomorphologic seismic sources is essential for Pacific Northwest (PNW) monitoring but remains difficult as networks densify and signals overlap. Prior work largely treats binary discrimination and seldom compares classic ML (feature-engineered) and deep learning (end-to-end) approaches under a common, multi-class setting with operational constraints. We evaluate methods and features for four-way source discrimination - earthquakes, explosions, surface events, and noise - and identify models that are both accurate and deployable. Using ~200k three-component waveforms from >70k events in an AI-curated PNW dataset, we test random-forest classifiers on TSFEL, physics-informed, and scattering features, and CNNs that ingest time series (1D) or spectrograms (2D); we benchmark on a balanced common test set, a 10k event network dataset, and out-of-domain data (global surface events; near-field blasts). CNNs taking spectrograms lead with accuracy performance over 92% for within-domain (as a short-and-fat CNN SeismicCNN 2D) and out-of-domain (as a long and skinny CNN QuakeXNet 2D), versus 89% for the best random forest; performance remains strong at low SNR and longer distances, and generalizes to independent network and global datasets. QuakeXNet-2D is lightweight (~70k parameters; ~1.2 MB), implemented into seisbench, scans a full day of 100 Hz, three-component data in ~9 s on commodity hardware, with released checkpoints. These results show spectrogram-based CNNs provide state-of-the-art accuracy, efficiency, and robustness for real-time PNW operations and transferable surface-event monitoring.",
        "pdf_url": "https://arxiv.org/pdf/2510.23795v1",
        "entry_id": "http://arxiv.org/abs/2510.23795v1",
        "categories": [
          "physics.geo-ph"
        ],
        "us_affiliated": false
      },
      {
        "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality",
        "authors": [
          "Shayne Longpre",
          "Sneha Kudugunta",
          "Niklas Muennighoff",
          "I-Hung Hsu",
          "Isaac Caswell",
          "Alex Pentland",
          "Sercan Arik",
          "Chen-Yu Lee",
          "Sayna Ebrahimi"
        ],
        "published": "2025-10-24",
        "year": 2025,
        "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.",
        "pdf_url": "https://arxiv.org/pdf/2510.22037v1",
        "entry_id": "http://arxiv.org/abs/2510.22037v1",
        "categories": [
          "cs.CL",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)",
        "authors": [
          "Robert Gigiu"
        ],
        "published": "2025-10-24",
        "year": 2025,
        "summary": "Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.",
        "pdf_url": "https://arxiv.org/pdf/2511.11590v2",
        "entry_id": "http://arxiv.org/abs/2511.11590v2",
        "categories": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "us_affiliated": false
      },
      {
        "title": "Mirror-Neuron Patterns in AI Alignment",
        "authors": [
          "Robyn Wyrick"
        ],
        "published": "2025-10-23",
        "year": 2025,
        "summary": "As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.\n  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?\n  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.\n  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.",
        "pdf_url": "https://arxiv.org/pdf/2511.01885v2",
        "entry_id": "http://arxiv.org/abs/2511.01885v2",
        "categories": [
          "cs.AI",
          "cs.LG",
          "q-bio.NC"
        ],
        "us_affiliated": false
      },
      {
        "title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning",
        "authors": [
          "Liangyu Chen",
          "Hanzhang Zhou",
          "Chenglin Cai",
          "Jianan Zhang",
          "Panrong Tong",
          "Quyu Kong",
          "Xu Zhang",
          "Chen Liu",
          "Yuqi Liu",
          "Wenxuan Wang",
          "Yue Wang",
          "Qin Jin",
          "Steven Hoi"
        ],
        "published": "2025-10-23",
        "year": 2025,
        "summary": "GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.",
        "pdf_url": "https://arxiv.org/pdf/2510.20286v1",
        "entry_id": "http://arxiv.org/abs/2510.20286v1",
        "categories": [
          "cs.CV",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
        "authors": [
          "Qianli Ma",
          "Siyu Wang",
          "Yilin Chen",
          "Yinhao Tang",
          "Yixiang Yang",
          "Chang Guo",
          "Bingjie Gao",
          "Zhening Xing",
          "Yanan Sun",
          "Zhipeng Zhang"
        ],
        "published": "2025-10-22",
        "year": 2025,
        "summary": "In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce $\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated \"Checker\" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct $\\textbf{PageBench}$, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \\$0.1. Code and dataset will be released at $\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.",
        "pdf_url": "https://arxiv.org/pdf/2510.19600v1",
        "entry_id": "http://arxiv.org/abs/2510.19600v1",
        "categories": [
          "cs.SE",
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Benchmarking On-Device Machine Learning on Apple Silicon with MLX",
        "authors": [
          "Oluwaseun A. Ajayi",
          "Ogundepo Odunayo"
        ],
        "published": "2025-10-21",
        "year": 2025,
        "summary": "The recent widespread adoption of Large Language Models (LLMs) and machine learning in general has sparked research interest in exploring the possibilities of deploying these models on smaller devices such as laptops and mobile phones. This creates a need for frameworks and approaches that are capable of taking advantage of on-device hardware. The MLX framework was created to address this need. It is a framework optimized for machine learning (ML) computations on Apple silicon devices, facilitating easier research, experimentation, and prototyping.\n  This paper presents a performance evaluation of MLX, focusing on inference latency of transformer models. We compare the performance of different transformer architecture implementations in MLX with their Pytorch counterparts. For this research we create a framework called MLX-transformers which includes different transformer implementations in MLX and downloads the model checkpoints in pytorch and converts it to the MLX format. By leveraging the advanced architecture and capabilities of Apple Silicon, MLX-Transformers enables seamless execution of transformer models directly sourced from Hugging Face, eliminating the need for checkpoint conversion often required when porting models between frameworks.\n  Our study benchmarks different transformer models on two Apple Silicon macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the inference latency performance of models with the same parameter sizes and checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa models, with the intention of extending future work to include models of different modalities, thus providing a more comprehensive assessment of MLX's capabilities. The results highlight MLX's potential in enabling efficient and more accessible on-device ML applications within Apple's ecosystem.",
        "pdf_url": "https://arxiv.org/pdf/2510.18921v1",
        "entry_id": "http://arxiv.org/abs/2510.18921v1",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales",
        "authors": [
          "Tung Nguyen",
          "Tuan Pham",
          "Troy Arcomano",
          "Veerabhadra Kotamarthi",
          "Ian Foster",
          "Sandeep Madireddy",
          "Aditya Grover"
        ],
        "published": "2025-10-20",
        "year": 2025,
        "summary": "Accurate weather forecasting across time scales is critical for anticipating and mitigating the impacts of climate change. Recent data-driven methods based on deep learning have achieved significant success in the medium range, but struggle at longer subseasonal-to-seasonal (S2S) horizons due to error accumulation in their autoregressive approach. In this work, we propose OmniCast, a scalable and skillful probabilistic model that unifies weather forecasting across timescales. OmniCast consists of two components: a VAE model that encodes raw weather data into a continuous, lower-dimensional latent space, and a diffusion-based transformer model that generates a sequence of future latent tokens given the initial conditioning tokens. During training, we mask random future tokens and train the transformer to estimate their distribution given conditioning and visible tokens using a per-token diffusion head. During inference, the transformer generates the full sequence of future tokens by iteratively unmasking random subsets of tokens. This joint sampling across space and time mitigates compounding errors from autoregressive approaches. The low-dimensional latent space enables modeling long sequences of future latent states, allowing the transformer to learn weather dynamics beyond initial conditions. OmniCast performs competitively with leading probabilistic methods at the medium-range timescale while being 10x to 20x faster, and achieves state-of-the-art performance at the subseasonal-to-seasonal scale across accuracy, physics-based, and probabilistic metrics. Furthermore, we demonstrate that OmniCast can generate stable rollouts up to 100 years ahead. Code and model checkpoints are available at https://github.com/tung-nd/omnicast.",
        "pdf_url": "https://arxiv.org/pdf/2510.18707v1",
        "entry_id": "http://arxiv.org/abs/2510.18707v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs",
        "authors": [
          "Junlan Feng",
          "Fanyu Meng",
          "Chong Long",
          "Pengyu Cong",
          "Duqing Wang",
          "Yan Zheng",
          "Yuyao Zhang",
          "Xuanchang Gao",
          "Ye Yuan",
          "Yunfei Ma",
          "Zhijie Ren",
          "Fan Yang",
          "Na Wu",
          "Di Jin",
          "Chao Deng"
        ],
        "published": "2025-10-20",
        "year": 2025,
        "summary": "The hallucination and credibility concerns of large language models (LLMs) are global challenges that the industry is collectively addressing. Recently, a significant amount of advances have been made on post-training and inference techniques to mitigate these challenges. However, it is widely agreed that unsafe and hallucinations of LLMs intrinsically originate from pre-training, involving pre-training data and the next-token prediction learning mechanism. In this paper, we focus on enhancing pre-training data to improve the trustworthiness and safety of LLMs. Since the data is vast, it's almost impossible to entirely purge the data of factual errors, logical inconsistencies, or distributional biases. Moreover, the pre-training data lack grounding in real-world knowledge. Each piece of data is treated as a sequence of tokens rather than as a representation of a part of the world. To overcome these issues, we propose approaches to enhancing our pre-training data with its context in the world and increasing a substantial amount of data reflecting industrial scenarios. We argue that most source data are created by the authors for specific purposes in a certain spatial-temporal context. They have played a role in the real world. By incorporating related world context information, we aim to better anchor pre-training data within real-world scenarios, thereby reducing uncertainty in model training and enhancing the model's safety and trustworthiness. We refer to our Data with World Context as DWC. We continue pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC tokens. We introduce our post-training procedures to activate the potentials of DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an average performance improvement of 1.79% on the Safety and Trustworthy evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.",
        "pdf_url": "https://arxiv.org/pdf/2510.17918v1",
        "entry_id": "http://arxiv.org/abs/2510.17918v1",
        "categories": [
          "cs.CL",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models",
        "authors": [
          "Xiaohan Zhao",
          "Hongyu Xiang",
          "Shengze Ye",
          "Song Li",
          "Zhengkun Tian",
          "Guanyu Chen",
          "Ke Ding",
          "Guanglu Wan"
        ],
        "published": "2025-10-17",
        "year": 2025,
        "summary": "This paper presents LongCat-Audio-Codec, an audio tokenizer and detokenizer solution designed for industrial grade end-to-end speech large language models. By leveraging a decoupled model architecture and a multistage training strategy, LongCat-Audio-Codec exhibits robust semantic modeling capabilities, flexible acoustic feature extraction capabilities, and low-latency streaming synthesis capabilities. It encodes speech at an ultra-low frame rate of 16.67 Hz, with a minimum bitrate of 0.43 kbps and a maximum bitrate of 0.87 kbps. Evaluation results demonstrate that LongCat-Audio-Codec achieves strong speech intelligibility and is capable of synthesizing highquality speech at low bitrate, thus effectively balancing coding efficiency and decoding quality. The inference code and model checkpoints of LongCat-Audio-Codec are available at: https://github.com/meituan-longcat/LongCat-Audio-Codec.",
        "pdf_url": "https://arxiv.org/pdf/2510.15227v1",
        "entry_id": "http://arxiv.org/abs/2510.15227v1",
        "categories": [
          "eess.AS",
          "cs.SD"
        ],
        "us_affiliated": false
      },
      {
        "title": "QLENS: Towards A Quantum Perspective of Language Transformers",
        "authors": [
          "Aditya Gupta",
          "Kirandeep Kaur",
          "Vinayak Gupta"
        ],
        "published": "2025-10-13",
        "year": 2025,
        "summary": "In natural language processing, current methods for understanding Transformers are successful at identifying intermediate predictions during a model's inference. However, these approaches function as limited diagnostic checkpoints, lacking a mathematical framework for mechanistically modeling how each layer facilitates transitions between these evolving states. This interpretability gap and past successes of interdisciplinary outlooks inspire us to turn to physics in search of a descriptive mathematical framework for Transformers. We observe that language models are intrinsically probabilistic, an attribute that is echoed in the core postulates of quantum mechanics. This parallel inspires us to translate insights from this discipline to that of natural language processing. Towards this objective, we propose QLENS a novel attempt to develop a physics-based perspective on the Transformer generation process. Under QLENS, a Transformer is studied by converting its latent activations into a state vector in a Hilbert space derived from the model's output units. This state subsequently evolves through hidden layers - reformulated as unitary operators and analogously defined Hamiltonians - during inference. The model's final probability distribution is obtained by applying the Born rule to the end state using a specific measurement operator. To demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy Transformer to investigate the influence of individual layers in a model's prediction trajectory. We present our work as a foundation for cross-domain insights to be leveraged towards a broader understanding of Transformers.",
        "pdf_url": "https://arxiv.org/pdf/2510.11963v1",
        "entry_id": "http://arxiv.org/abs/2510.11963v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Z0-Inf: Zeroth Order Approximation for Data Influence",
        "authors": [
          "Narine Kokhlikyan",
          "Kamalika Chaudhuri",
          "Saeed Mahloujifar"
        ],
        "published": "2025-10-13",
        "year": 2025,
        "summary": "A critical aspect of analyzing and improving modern machine learning systems lies in understanding how individual training examples influence a model's predictive behavior. Estimating this influence enables critical applications, including data selection and model debugging; in particular, self-influence, which quantifies the influence of a training point on itself, has found many uses in data quality assessment and outlier detection. Existing methods for measuring data influence, however, are often impractical for large models due to low accuracy or prohibitive computational costs: most approaches either provide poor approximations or rely on gradients and inverse-Hessian computations that remain challenging to scale. In this work, we introduce a highly efficient zeroth-order approximation for estimating the influence of training data that requires only a fraction of the time and memory footprint of prior methods. Notably, our method relies solely on loss values of intermediate checkpoints on the training and test data, along with the checkpoints themselves, making it broadly applicable even when the loss function of interest is non-differentiable. Beyond its computational efficiency, our approach achieves superior accuracy in estimating self-influence and comparable or improved accuracy in estimating train-test influence for fine-tuned large language models, enabling scalable and practical analysis of how training data shapes model behavior.",
        "pdf_url": "https://arxiv.org/pdf/2510.11832v1",
        "entry_id": "http://arxiv.org/abs/2510.11832v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety",
        "authors": [
          "Yuyi Huang",
          "Runzhe Zhan",
          "Lidia S. Chao",
          "Ailin Tao",
          "Derek F. Wong"
        ],
        "published": "2025-10-11",
        "year": 2025,
        "summary": "As large language models (LLMs) are increasingly deployed for complex reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a key paradigm for structured inference. Despite early-stage safeguards enabled by alignment techniques such as RLHF, we identify a previously underexplored vulnerability: reasoning trajectories in Long-CoT models can drift from aligned paths, resulting in content that violates safety constraints. We term this phenomenon Path Drift. Through empirical analysis, we uncover three behavioral triggers of Path Drift: (1) first-person commitments that induce goal-driven reasoning that delays refusal signals; (2) ethical evaporation, where surface-level disclaimers bypass alignment checkpoints; (3) condition chain escalation, where layered cues progressively steer models toward unsafe completions. Building on these insights, we introduce a three-stage Path Drift Induction Framework comprising cognitive load amplification, self-role priming, and condition chain hijacking. Each stage independently reduces refusal rates, while their combination further compounds the effect. To mitigate these risks, we propose a path-level defense strategy incorporating role attribution correction and metacognitive reflection (reflective safety cues). Our findings highlight the need for trajectory-level alignment oversight in long-form reasoning beyond token-level alignment.",
        "pdf_url": "https://arxiv.org/pdf/2510.10013v1",
        "entry_id": "http://arxiv.org/abs/2510.10013v1",
        "categories": [
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "Don't Throw Away Your Pretrained Model",
        "authors": [
          "Shangbin Feng",
          "Wenhao Yu",
          "Yike Wang",
          "Hongming Zhang",
          "Yulia Tsvetkov",
          "Dong Yu"
        ],
        "published": "2025-10-10",
        "year": 2025,
        "summary": "Alignment training has tradeoffs: it helps language models (LMs) gain in reasoning and instruction following but might lose out on skills such as creativity and calibration, where unaligned base models are better at. We aim to make the best of both worlds through model collaboration, where different models in the training pipeline collaborate and complement each other. Since LM responses feature interleaving skills that favor different models, we propose Switch Generation, where pretrained and aligned model versions take turns to ``speak'' in a response sequence. Specifically, we train a switcher LM by learning from outcomes of choosing different models to generate the next segment across diverse queries and contexts. At inference time, the switcher LM guides different model checkpoints to dynamically generate the next segment where their strengths are most needed. Extensive experiments with 8 model collaboration baselines and 18 datasets show that 1) model collaboration consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch Generation further outperforms baselines by 12.9% on average. Further analysis reveals that Switch Generation discovers compositional skills to solve problems where individual models struggle and generalizes to unseen models and tasks, reusing and repurposing by-products in expensive model training pipelines that are otherwise discarded.",
        "pdf_url": "https://arxiv.org/pdf/2510.09913v1",
        "entry_id": "http://arxiv.org/abs/2510.09913v1",
        "categories": [
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions",
        "authors": [
          "Sanjari Srivastava",
          "Gang Li",
          "Cheng Chang",
          "Rishu Garg",
          "Manpreet Kaur",
          "Charlene Y. Lee",
          "Yuezhang Li",
          "Yining Mao",
          "Ignacio Cases",
          "Yanan Xie",
          "Peng Qi"
        ],
        "published": "2025-10-10",
        "year": 2025,
        "summary": "Training web agents to navigate complex, real-world websites requires them to master $\\textit{subtasks}$ - short-horizon interactions on multiple UI components (e.g., choosing the correct date in a date picker, or scrolling in a container to extract information). We introduce WARC-Bench (Web Archive Benchmark), a novel web navigation benchmark featuring 438 tasks designed to evaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed interactions with dynamic and realistic webpages using Web ARChive files. We show that WARC-Bench is challenging for leading computer-use models, with the highest observed success rate being 64.8%. To improve open source models on subtask, we explore two common training techniques: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments show that SFT models obtain a 48.8% success rate on the benchmark. Training with RLVR over SFT checkpoints, even in data-scarce settings, improves the score to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis concludes that mastering these subtasks is essential for robust web planning and navigation, and is a capability not extensively evaluated by existing benchmarks.",
        "pdf_url": "https://arxiv.org/pdf/2510.09872v1",
        "entry_id": "http://arxiv.org/abs/2510.09872v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis",
        "authors": [
          "Ming Jie Ong",
          "Sze Yinn Ung",
          "Sim Kuan Goh",
          "Jimmy Y. Zhong"
        ],
        "published": "2025-10-09",
        "year": 2025,
        "summary": "The current study investigated the use of Explainable Artificial Intelligence (XAI) to improve the accuracy of brain tumor segmentation in MRI images, with the goal of assisting physicians in clinical decision-making. The study focused on applying UNet models for brain tumor segmentation and using the XAI techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization to enhance the understanding of these models. Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet (AttUNet) - were evaluated to identify the best-performing model. XAI was employed with the aims of clarifying model decisions and increasing physicians' trust in these models. We compared the performance of two UNet variants (ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM and attention-based visualization. Using the latest computer hardware, we trained and validated each model using the Adam optimizer and assessed their performance with respect to: (i) training, validation, and inference times, (ii) segmentation similarity coefficients and loss functions, and (iii) classification performance. Notably, during the final testing phase, ResUNet outperformed the other models with respect to Dice and Jaccard similarity scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided visuospatial insights into the tumor subregions each UNet model focused on while attention-based visualization provided valuable insights into the working mechanisms of AttUNet's attention modules. These results demonstrated ResUNet as the best-performing model and we conclude by recommending its use for automated brain tumor segmentation in future clinical assessments. Our source code and checkpoint are available at https://github.com/ethanong98/MultiModel-XAI-Brats2020",
        "pdf_url": "https://arxiv.org/pdf/2510.07785v1",
        "entry_id": "http://arxiv.org/abs/2510.07785v1",
        "categories": [
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking",
        "authors": [
          "Mitchell Keren Taraday",
          "Shahaf Wagner",
          "Chaim Baskin"
        ],
        "published": "2025-10-08",
        "year": 2025,
        "summary": "Multimodal retrieval still leans on embedding-based models like CLIP for fast vector search over pre-computed image embeddings. Yet, unlike text retrieval, where joint-encoder rerankers are standard, comparable vision--language rerankers are largely absent. We find that seminal joint encoders such as BLIP are severely bottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale. Motivated by this bottleneck, we introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes vision tokens offline and compresses them via a lightweight attention-based adapter, so online inference runs only a compact joint encoder over a small set of visual tokens plus the text. EDJE preserves strong retrieval performance while drastically reducing storage and online compute, enabling high-throughput inference. Specifically, EDJE processes 50k image--text pairs/second while requiring 49kB of disk storage per image, matching prior art on Flickr (zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints will be made publicly available shortly.",
        "pdf_url": "https://arxiv.org/pdf/2510.06820v1",
        "entry_id": "http://arxiv.org/abs/2510.06820v1",
        "categories": [
          "cs.CV",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Control-Augmented Autoregressive Diffusion for Data Assimilation",
        "authors": [
          "Prakhar Srivastava",
          "Farrin Marouf Sofian",
          "Francesco Immorlano",
          "Kushagra Pandey",
          "Stephan Mandt"
        ],
        "published": "2025-10-08",
        "year": 2025,
        "summary": "Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments pretrained ARDMs with a lightweight controller network, trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), a setting where existing methods are often computationally prohibitive and prone to forecast drift under sparse observations. Our approach reduces DA inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and/or optimizations during inference. We demonstrate that our method consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes. We will release code and checkpoints publicly.",
        "pdf_url": "https://arxiv.org/pdf/2510.06637v1",
        "entry_id": "http://arxiv.org/abs/2510.06637v1",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.CV"
        ],
        "us_affiliated": false
      },
      {
        "title": "POME: Post Optimization Model Edit via Muon-style Projection",
        "authors": [
          "Yong Liu",
          "Di Fu",
          "Yang Luo",
          "Zirui Zhu",
          "Minhao Cheng",
          "Cho-Jui Hsieh",
          "Yang You"
        ],
        "published": "2025-10-08",
        "year": 2025,
        "summary": "We introduce Post-Optimization Model Edit (POME), a new algorithm that enhances the performance of fine-tuned large language models using only their pretrained and fine-tuned checkpoints, without requiring extra data or further optimization. The core idea is to apply a muon-style projection to $\u0394W$, the difference between the fine-tuned and pretrained weights. This projection uses truncated singular value decomposition (SVD) to equalize the influence of dominant update directions and prune small singular values, which often represent noise. As a simple post-processing step, POME is completely decoupled from the training pipeline. It requires zero modifications and imposes no overhead, making it universally compatible with any optimizer or distributed framework. POME delivers consistent gains, boosting average performance by +2.5\\% on GSM8K and +1.0\\% on code generation. Its broad applicability -- from 7B foundation models to 72B RLHF-instructed models -- establishes it as a practical, zero-cost enhancement for any fine-tuning pipeline. Code is available at https://github.com/NUS-HPC-AI-Lab/POME.",
        "pdf_url": "https://arxiv.org/pdf/2510.06627v1",
        "entry_id": "http://arxiv.org/abs/2510.06627v1",
        "categories": [
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment",
        "authors": [
          "Ibrahim Salihu Yusuf",
          "Iffanice Houndayi",
          "Rym Oualha",
          "Mohamed Aziz Cherif",
          "Kobby Panford-Quainoo",
          "Arnu Pretorius"
        ],
        "published": "2025-10-07",
        "year": 2025,
        "summary": "Open-access multispectral imagery from missions like Landsat 8-9 and Sentinel-2 has fueled the development of geospatial foundation models (GFMs) for humanitarian and environmental applications. Yet, their deployment remains limited by (i) the absence of automated geospatial data pipelines and (ii) the large size of fine-tuned models. Existing GFMs lack workflows for processing raw satellite imagery, and downstream adaptations often retain the full complexity of the original encoder.\n  We present InstaGeo, an open-source, end-to-end framework that addresses these challenges by integrating: (1) automated data curation to transform raw imagery into model-ready datasets; (2) task-specific model distillation to derive compact, compute-efficient models; and (3) seamless deployment as interactive web-map applications. Using InstaGeo, we reproduced datasets from three published studies and trained models with marginal mIoU differences of -0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for desert locust prediction. The distilled models are up to 8x smaller than standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal accuracy loss.\n  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp improvement over prior baselines. Moreover, InstaGeo enables users to progress from raw data to model deployment within a single working day.\n  By unifying data preparation, model compression, and deployment, InstaGeo transforms research-grade GFMs into practical, low-carbon tools for real-time, large-scale Earth observation. This approach shifts geospatial AI toward data quality and application-driven innovation. Source code, datasets, and model checkpoints are available at: https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git",
        "pdf_url": "https://arxiv.org/pdf/2510.05617v1",
        "entry_id": "http://arxiv.org/abs/2510.05617v1",
        "categories": [
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "Artificial-Intelligence Grading Assistance for Handwritten Components of a Calculus Exam",
        "authors": [
          "Gerd Kortemeyer",
          "Alexander Caspar",
          "Daria Horica"
        ],
        "published": "2025-10-04",
        "year": 2025,
        "summary": "We investigate whether contemporary multimodal LLMs can assist with grading open-ended calculus at scale without eroding validity. In a large first-year exam, students' handwritten work was graded by GPT-5 against the same rubric used by teaching assistants (TAs), with fractional credit permitted; TA rubric decisions served as ground truth. We calibrated a human-in-the-loop filter that combines a partial-credit threshold with an Item Response Theory (2PL) risk measure based on the deviation between the AI score and the model-expected score for each student-item. Unfiltered AI-TA agreement was moderate, adequate for low-stakes feedback but not for high-stakes use. Confidence filtering made the workload-quality trade-off explicit: under stricter settings, AI delivered human-level accuracy, but also left roughly 70% of the items to be graded by humans. Psychometric patterns were constrained by low stakes on the open-ended portion, a small set of rubric checkpoints, and occasional misalignment between designated answer regions and where work appeared. Practical adjustments such as slightly higher weight and protected time, a few rubric-visible substeps, stronger spatial anchoring should raise ceiling performance. Overall, calibrated confidence and conservative routing enable AI to reliably handle a sizable subset of routine cases while reserving expert judgment for ambiguous or pedagogically rich responses.",
        "pdf_url": "https://arxiv.org/pdf/2510.05162v2",
        "entry_id": "http://arxiv.org/abs/2510.05162v2",
        "categories": [
          "cs.CY",
          "cs.AI"
        ],
        "us_affiliated": false
      },
      {
        "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning",
        "authors": [
          "Eadom Dessalene",
          "Pavan Mantripragada",
          "Michael Maynord",
          "Yiannis Aloimonos"
        ],
        "published": "2025-10-04",
        "year": 2025,
        "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic robot overlays over human video. We employ EmbodiSwap for zero-shot imitation learning, bridging the embodiment gap between in-the-wild ego-centric human video and a target robot embodiment. We train a closed-loop robot manipulation policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a visual backbone, repurposing V-JEPA from the domain of video understanding to imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms alternative vision backbones more conventionally used within robotics. In real-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success rate, outperforming a few-shot trained $\u03c0_0$ network as well as $\u03c0_0$ trained over data produced by EmbodiSwap. We release (i) code for generating the synthetic robot overlays which takes as input human videos and an arbitrary robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference code, to facilitate reproducible research and broader adoption.",
        "pdf_url": "https://arxiv.org/pdf/2510.03706v1",
        "entry_id": "http://arxiv.org/abs/2510.03706v1",
        "categories": [
          "cs.RO",
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ],
        "us_affiliated": false
      },
      {
        "title": "A Granular Study of Safety Pretraining under Model Abliteration",
        "authors": [
          "Shashank Agnihotri",
          "Jonas Jakubassa",
          "Priyam Dey",
          "Sachin Goyal",
          "Bernt Schiele",
          "Venkatesh Babu Radhakrishnan",
          "Margret Keuper"
        ],
        "published": "2025-10-03",
        "year": 2025,
        "summary": "Open-weight LLMs can be modified at inference time with simple activation edits, which raises a practical question for safety: do common safety interventions like refusal training or metatag training survive such edits? We study model abliteration, a lightweight projection technique designed to remove refusal-sensitive directions, and conduct a controlled evaluation across a granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside widely used open baselines. For each of 20 systems, original and abliterated, we issue 100 prompts with balanced harmful and harmless cases, classify responses as **Refusal** or **Non-Refusal** using multiple judges, and validate judge fidelity on a small human-labeled subset. We also probe whether models can identify refusal in their own outputs. Our study produces a checkpoint-level characterization of which data-centric safety components remain robust under abliteration, quantifies how judge selection influences evaluation outcomes, and outlines a practical protocol for integrating inference-time edits into safety assessments. Code: https://github.com/shashankskagnihotri/safety_pretraining.",
        "pdf_url": "https://arxiv.org/pdf/2510.02768v1",
        "entry_id": "http://arxiv.org/abs/2510.02768v1",
        "categories": [
          "cs.LG",
          "cs.CL"
        ],
        "us_affiliated": false
      },
      {
        "title": "UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies",
        "authors": [
          "Harsh Gupta",
          "Xiaofeng Guo",
          "Huy Ha",
          "Chuer Pan",
          "Muqing Cao",
          "Dongjae Lee",
          "Sebastian Scherer",
          "Shuran Song",
          "Guanya Shi"
        ],
        "published": "2025-10-02",
        "year": 2025,
        "summary": "We introduce UMI-on-Air, a framework for embodiment-aware deployment of embodiment-agnostic manipulation policies. Our approach leverages diverse, unconstrained human demonstrations collected with a handheld gripper (UMI) to train generalizable visuomotor policies. A central challenge in transferring these policies to constrained robotic embodiments-such as aerial manipulators-is the mismatch in control and robot dynamics, which often leads to out-of-distribution behaviors and poor execution. To address this, we propose Embodiment-Aware Diffusion Policy (EADP), which couples a high-level UMI policy with a low-level embodiment-specific controller at inference time. By integrating gradient feedback from the controller's tracking cost into the diffusion sampling process, our method steers trajectory generation towards dynamically feasible modes tailored to the deployment embodiment. This enables plug-and-play, embodiment-aware trajectory adaptation at test time. We validate our approach on multiple long-horizon and high-precision aerial manipulation tasks, showing improved success rates, efficiency, and robustness under disturbances compared to unguided diffusion baselines. Finally, we demonstrate deployment in previously unseen environments, using UMI demonstrations collected in the wild, highlighting a practical pathway for scaling generalizable manipulation skills across diverse-and even highly constrained-embodiments. All code, data, and checkpoints will be publicly released after acceptance. Result videos can be found at umi-on-air.github.io.",
        "pdf_url": "https://arxiv.org/pdf/2510.02614v2",
        "entry_id": "http://arxiv.org/abs/2510.02614v2",
        "categories": [
          "cs.RO"
        ],
        "us_affiliated": false
      }
    ]
  }
}