{
  "arxiv_id": "2512.23747v1",
  "title": "State-of-the-art Small Language Coder Model: Mify-Coder",
  "published": "2025-12-26T18:16:02Z",
  "summary": "We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic",
  "category": "cs.SE",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2512.23747v1",
  "cluster": 12,
  "issue": 171,
  "downloaded_at": "2026-01-11T10:27:49.551244"
}