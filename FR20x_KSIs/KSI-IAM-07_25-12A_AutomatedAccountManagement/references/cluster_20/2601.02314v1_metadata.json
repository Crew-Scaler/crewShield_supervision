{
  "arxiv_id": "2601.02314v1",
  "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
  "published": "2026-01-05T18:05:29Z",
  "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI fra",
  "category": "cs.AI",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2601.02314v1",
  "cluster": 20,
  "issue": 171,
  "downloaded_at": "2026-01-11T10:27:49.562712"
}