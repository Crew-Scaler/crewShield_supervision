{
  "arxiv_id": "2512.21373v1",
  "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories",
  "published": "2025-12-24T08:11:11Z",
  "summary": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade s",
  "category": "cs.SE",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2512.21373v1",
  "cluster": 14,
  "issue": 171,
  "downloaded_at": "2026-01-11T10:27:49.554350"
}