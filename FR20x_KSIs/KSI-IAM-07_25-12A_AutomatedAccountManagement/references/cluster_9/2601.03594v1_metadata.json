{
  "arxiv_id": "2601.03594v1",
  "title": "Jailbreaking LLMs & VLMs: Mechanisms, Evaluation, and Unified Defense",
  "published": "2026-01-07T05:25:33Z",
  "summary": "This paper provides a systematic survey of jailbreak attacks and defenses on Large Language Models (LLMs) and Vision-Language Models (VLMs), emphasizing that jailbreak vulnerabilities stem from structural factors such as incomplete training data, linguistic ambiguity, and generative uncertainty. It further differentiates between hallucinations and jailbreaks in terms of intent and triggering mechanisms. We propose a three-dimensional survey framework: (1) Attack dimension-including template/enco",
  "category": "cs.CR",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2601.03594v1",
  "cluster": 9,
  "issue": 171,
  "downloaded_at": "2026-01-11T10:27:49.546168"
}