{
  "arxiv_id": "2601.04795v1",
  "title": "Defense Against Indirect Prompt Injection via Tool Result Parsing",
  "published": "2026-01-08T10:21:56Z",
  "summary": "As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection",
  "category": "cs.AI",
  "relevance": 85,
  "url": "https://arxiv.org/abs/2601.04795v1",
  "cluster": 9,
  "issue": 171,
  "downloaded_at": "2026-01-11T10:27:49.545262"
}