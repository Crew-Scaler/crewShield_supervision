{
  "arxiv_id": "2510.18176v1",
  "title": "Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains",
  "authors": [
    "Soumya Rani Samineni",
    "Durgesh Kalwar",
    "Vardaan Gangal",
    "Siddhant Bhambri",
    "Subbarao Kambhampati"
  ],
  "published": "2025-10-20T23:58:31Z",
  "summary": "Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of Large Language Models (LLMs) has been shown to improve accuracy on reasoning tasks and continues to attract significant attention. Existing RLVR methods, however, typically treat all tokens uniformly without accounting for token-level advantages. These methods primarily evaluate performance based on final answer correctness or Pass@K accuracy, and yet make claims about RL post-training leading to improved reasoning trac",
  "cluster": "cluster_3",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.172012"
}