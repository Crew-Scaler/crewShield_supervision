{
  "arxiv_id": "2512.01037v2",
  "title": "When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals",
  "authors": [
    "Riad Ahmed Anonto",
    "Md Labid Al Nahiyan",
    "Md Tanvir Hassan"
  ],
  "published": "2025-11-30T19:11:45Z",
  "summary": "Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-pr",
  "cluster": "cluster_3",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.176754"
}