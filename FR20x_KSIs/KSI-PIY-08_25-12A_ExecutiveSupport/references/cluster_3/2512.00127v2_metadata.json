{
  "arxiv_id": "2512.00127v2",
  "title": "Generating Verifiable Chain of Thoughts from Exection-Traces",
  "authors": [
    "Shailja Thakur",
    "Vaibhav Saxena",
    "Rohan Kulkarni",
    "Shivdeep Singh",
    "Parameswaran Selvam",
    "Hima Patel",
    "Hiroshi Kanayama"
  ],
  "published": "2025-11-28T07:43:43Z",
  "summary": "Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.\n  We add",
  "cluster": "cluster_3",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.171585"
}