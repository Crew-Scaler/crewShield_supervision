{
  "arxiv_id": "2512.23213v1",
  "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process",
  "authors": [
    "Zhijun Chen",
    "Zeyu Ji",
    "Qianren Mao",
    "Junhang Cheng",
    "Bangjie Qin",
    "Hao Wu",
    "Zhuoran Li",
    "Jingzheng Li",
    "Kai Sun",
    "Zizhe Wang",
    "Yikun Ban",
    "Zhu Sun",
    "Xiangyang Ji",
    "Hailong Sun"
  ],
  "published": "2025-12-29T05:25:49Z",
  "summary": "We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerg",
  "cluster": "cluster_2",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.169752"
}