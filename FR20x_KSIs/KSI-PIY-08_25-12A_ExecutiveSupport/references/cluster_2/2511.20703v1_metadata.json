{
  "arxiv_id": "2511.20703v1",
  "title": "PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach",
  "authors": [
    "Udari Madhushani Sehwag",
    "Shayan Shabihi",
    "Alex McAvoy",
    "Vikash Sehwag",
    "Yuancheng Xu",
    "Dalton Towers",
    "Furong Huang"
  ],
  "published": "2025-11-24T18:46:44Z",
  "summary": "Recent advances in Large Language Models (LLMs) have sparked concerns over their potential to acquire and misuse dangerous or high-risk capabilities, posing frontier risks. Current safety evaluations primarily test for what a model \\textit{can} do - its capabilities - without assessing what it $\\textit{would}$ do if endowed with high-risk capabilities. This leaves a critical blind spot: models may strategically conceal capabilities or rapidly acquire them, while harboring latent inclinations tow",
  "cluster": "cluster_2",
  "classification": "GREEN",
  "relevance_score": 85,
  "screened_at": "2026-01-11T11:46:04.167515"
}