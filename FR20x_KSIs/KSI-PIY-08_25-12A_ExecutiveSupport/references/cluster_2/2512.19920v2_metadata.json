{
  "arxiv_id": "2512.19920v2",
  "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
  "authors": [
    "Jiayun Wu",
    "Jiashuo Liu",
    "Zhiyuan Zeng",
    "Tianyang Zhan",
    "Tianle Cai",
    "Wenhao Huang"
  ],
  "published": "2025-12-22T22:51:48Z",
  "summary": "LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently ince",
  "cluster": "cluster_2",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.165127"
}