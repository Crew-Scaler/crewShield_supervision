{
  "arxiv_id": "2511.19171v2",
  "title": "Can LLMs Threaten Human Survival? Benchmarking Potential Existential Threats from LLMs via Prefix Completion",
  "authors": [
    "Yu Cui",
    "Yifei Liu",
    "Hang Fu",
    "Sicheng Pan",
    "Haibin Zhang",
    "Cong Zuo",
    "Licheng Wang"
  ],
  "published": "2025-11-24T14:34:13Z",
  "summary": "Research on the safety evaluation of large language models (LLMs) has become extensive, driven by jailbreak studies that elicit unsafe responses. Such response involves information already available to humans, such as the answer to \"how to make a bomb\". When LLMs are jailbroken, the practical threat they pose to humans is negligible. However, it remains unclear whether LLMs commonly produce unpredictable outputs that could pose substantive threats to human safety. To address this gap, we study w",
  "cluster": "cluster_2",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.167533"
}