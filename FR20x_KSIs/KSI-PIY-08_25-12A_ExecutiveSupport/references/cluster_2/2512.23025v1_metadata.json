{
  "arxiv_id": "2512.23025v1",
  "title": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models",
  "authors": [
    "Wenxuan Xu",
    "Arvind Pillai",
    "Subigya Nepal",
    "Amanda C Collins",
    "Daniel M Mackin",
    "Michael V Heinz",
    "Tess Z Griffin",
    "Nicholas C Jacobson",
    "Andrew Campbell"
  ],
  "published": "2025-12-28T18:00:57Z",
  "summary": "Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large",
  "cluster": "cluster_2",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.169943"
}