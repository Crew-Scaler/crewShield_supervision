{
  "issue": 174,
  "title": "KSI-PIY-08_25-12A_ExecutiveSupport",
  "campaign_date": "2026-01-11T11:46:04.204815",
  "total_papers_screened": 2495,
  "papers_by_classification": {
    "green": 1272,
    "yellow": 1014,
    "red": 209
  },
  "green_papers": [
    {
      "arxiv_id": "2511.19644v1",
      "title": "IRSDA: An Agent-Orchestrated Framework for Enterprise Intrusion Response",
      "authors": [
        "Damodar Panigrahi",
        "Raj Patel",
        "Shaswata Mitra",
        "Sudip Mittal",
        "Shahram Rahimi"
      ],
      "published": "2025-11-24T19:21:09Z",
      "summary": "Modern enterprise systems face escalating cyber threats that are increasingly dynamic, distributed, and multi-stage in nature. Traditional intrusion detection and response systems often rely on static rules and manual workflows, which limit their ability to respond with the speed and precision required in high-stakes environments. To address these challenges, we present the Intrusion Response System Digital Assistant (IRSDA), an agent-based framework designed to deliver autonomous and policy-com",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 100,
      "screened_at": "2026-01-11T11:46:04.164278"
    },
    {
      "arxiv_id": "2511.19644v1",
      "title": "IRSDA: An Agent-Orchestrated Framework for Enterprise Intrusion Response",
      "authors": [
        "Damodar Panigrahi",
        "Raj Patel",
        "Shaswata Mitra",
        "Sudip Mittal",
        "Shahram Rahimi"
      ],
      "published": "2025-11-24T19:21:09Z",
      "summary": "Modern enterprise systems face escalating cyber threats that are increasingly dynamic, distributed, and multi-stage in nature. Traditional intrusion detection and response systems often rely on static rules and manual workflows, which limit their ability to respond with the speed and precision required in high-stakes environments. To address these challenges, we present the Intrusion Response System Digital Assistant (IRSDA), an agent-based framework designed to deliver autonomous and policy-com",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 100,
      "screened_at": "2026-01-11T11:46:04.167494"
    },
    {
      "arxiv_id": "2512.20405v2",
      "title": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected",
      "authors": [
        "Kanchon Gharami",
        "Sanjiv Kumar Sarkar",
        "Yongxin Liu",
        "Shafika Showkat Moni"
      ],
      "published": "2025-12-23T14:54:45Z",
      "summary": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical ",
      "cluster": "cluster_4",
      "classification": "GREEN",
      "relevance_score": 100,
      "screened_at": "2026-01-11T11:46:04.178028"
    },
    {
      "arxiv_id": "2512.20405v2",
      "title": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected",
      "authors": [
        "Kanchon Gharami",
        "Sanjiv Kumar Sarkar",
        "Yongxin Liu",
        "Shafika Showkat Moni"
      ],
      "published": "2025-12-23T14:54:45Z",
      "summary": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical ",
      "cluster": "cluster_4",
      "classification": "GREEN",
      "relevance_score": 100,
      "screened_at": "2026-01-11T11:46:04.182989"
    },
    {
      "arxiv_id": "2512.20405v2",
      "title": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected",
      "authors": [
        "Kanchon Gharami",
        "Sanjiv Kumar Sarkar",
        "Yongxin Liu",
        "Shafika Showkat Moni"
      ],
      "published": "2025-12-23T14:54:45Z",
      "summary": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical ",
      "cluster": "cluster_4",
      "classification": "GREEN",
      "relevance_score": 100,
      "screened_at": "2026-01-11T11:46:04.184571"
    },
    {
      "arxiv_id": "2510.03368v1",
      "title": "An Adaptive Responsible AI Governance Framework for Decentralized Organizations",
      "authors": [
        "Kiana Jafari Meimandi",
        "Anka Reuel",
        "Gabriela Aranguiz-Dias",
        "Hatim Rahama",
        "Ala-Eddine Ayadi",
        "Xavier Boullier",
        "J\u00e9r\u00e9my Verdo",
        "Louis Montanie",
        "Mykel Kochenderfer"
      ],
      "published": "2025-10-03T05:55:48Z",
      "summary": "This paper examines the assessment challenges of Responsible AI (RAI) governance efforts in globally decentralized organizations through a case study collaboration between a leading research university and a multinational enterprise. While there are many proposed frameworks for RAI, their application in complex organizational settings with distributed decision-making authority remains underexplored. Our RAI assessment, conducted across multiple business units and AI use cases, reveals four key p",
      "cluster": "cluster_1",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.156837"
    },
    {
      "arxiv_id": "2510.03368v1",
      "title": "An Adaptive Responsible AI Governance Framework for Decentralized Organizations",
      "authors": [
        "Kiana Jafari Meimandi",
        "Anka Reuel",
        "Gabriela Aranguiz-Dias",
        "Hatim Rahama",
        "Ala-Eddine Ayadi",
        "Xavier Boullier",
        "J\u00e9r\u00e9my Verdo",
        "Louis Montanie",
        "Mykel Kochenderfer"
      ],
      "published": "2025-10-03T05:55:48Z",
      "summary": "This paper examines the assessment challenges of Responsible AI (RAI) governance efforts in globally decentralized organizations through a case study collaboration between a leading research university and a multinational enterprise. While there are many proposed frameworks for RAI, their application in complex organizational settings with distributed decision-making authority remains underexplored. Our RAI assessment, conducted across multiple business units and AI use cases, reveals four key p",
      "cluster": "cluster_1",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.160265"
    },
    {
      "arxiv_id": "2512.21048v1",
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": [
        "Savvy Sharma",
        "George Petrovic",
        "Sarthak Kaushik"
      ],
      "published": "2025-12-24T08:29:28Z",
      "summary": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. W",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.163680"
    },
    {
      "arxiv_id": "2512.04416v2",
      "title": "DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows",
      "authors": [
        "Zhou Liu",
        "Zhaoyang Han",
        "Guochen Yan",
        "Hao Liang",
        "Bohan Zeng",
        "Xing Chen",
        "Yuanfeng Song",
        "Wentao Zhang"
      ],
      "published": "2025-12-04T03:25:12Z",
      "summary": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance:",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.164091"
    },
    {
      "arxiv_id": "2512.18043v1",
      "title": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "authors": [
        "Sunil Arora",
        "John Hastings"
      ],
      "published": "2025-12-19T20:22:25Z",
      "summary": "Securing Agentic Artificial Intelligence (AI) systems requires addressing the complex cyber risks introduced by autonomous, decision-making, and adaptive behaviors. Agentic AI systems are increasingly deployed across industries, organizations, and critical sectors such as cybersecurity, finance, and healthcare. However, their autonomy introduces unique security challenges, including unauthorized actions, adversarial manipulation, and dynamic environmental interactions. Existing AI security frame",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.165396"
    },
    {
      "arxiv_id": "2512.21048v1",
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": [
        "Savvy Sharma",
        "George Petrovic",
        "Sarthak Kaushik"
      ],
      "published": "2025-12-24T08:29:28Z",
      "summary": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. W",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.166196"
    },
    {
      "arxiv_id": "2512.20985v1",
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": [
        "Salman Jan",
        "Hassan Ali Razzaqi",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum"
      ],
      "published": "2025-12-24T06:20:28Z",
      "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guara",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.166215"
    },
    {
      "arxiv_id": "2512.21048v1",
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": [
        "Savvy Sharma",
        "George Petrovic",
        "Sarthak Kaushik"
      ],
      "published": "2025-12-24T08:29:28Z",
      "summary": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. W",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.166672"
    },
    {
      "arxiv_id": "2512.20985v1",
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": [
        "Salman Jan",
        "Hassan Ali Razzaqi",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum"
      ],
      "published": "2025-12-24T06:20:28Z",
      "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guara",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.166688"
    },
    {
      "arxiv_id": "2512.14860v1",
      "title": "Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks",
      "authors": [
        "Viet K. Nguyen",
        "Mohammad I. Husain"
      ],
      "published": "2025-12-16T19:22:50Z",
      "summary": "Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, G",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.166953"
    },
    {
      "arxiv_id": "2512.04416v2",
      "title": "DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows",
      "authors": [
        "Zhou Liu",
        "Zhaoyang Han",
        "Guochen Yan",
        "Hao Liang",
        "Bohan Zeng",
        "Xing Chen",
        "Yuanfeng Song",
        "Wentao Zhang"
      ],
      "published": "2025-12-04T03:25:12Z",
      "summary": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance:",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.167348"
    },
    {
      "arxiv_id": "2512.20985v1",
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": [
        "Salman Jan",
        "Hassan Ali Razzaqi",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum"
      ],
      "published": "2025-12-24T06:20:28Z",
      "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guara",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.168165"
    },
    {
      "arxiv_id": "2512.20275v1",
      "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
      "authors": [
        "Divya Vijay",
        "Vignesh Ethiraj"
      ],
      "published": "2025-12-23T11:27:17Z",
      "summary": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning wi",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.168231"
    },
    {
      "arxiv_id": "2512.21048v1",
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": [
        "Savvy Sharma",
        "George Petrovic",
        "Sarthak Kaushik"
      ],
      "published": "2025-12-24T08:29:28Z",
      "summary": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. W",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.168641"
    },
    {
      "arxiv_id": "2512.20985v1",
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": [
        "Salman Jan",
        "Hassan Ali Razzaqi",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum"
      ],
      "published": "2025-12-24T06:20:28Z",
      "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guara",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.168675"
    },
    {
      "arxiv_id": "2512.20275v1",
      "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
      "authors": [
        "Divya Vijay",
        "Vignesh Ethiraj"
      ],
      "published": "2025-12-23T11:27:17Z",
      "summary": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning wi",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.168711"
    },
    {
      "arxiv_id": "2512.18043v1",
      "title": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "authors": [
        "Sunil Arora",
        "John Hastings"
      ],
      "published": "2025-12-19T20:22:25Z",
      "summary": "Securing Agentic Artificial Intelligence (AI) systems requires addressing the complex cyber risks introduced by autonomous, decision-making, and adaptive behaviors. Agentic AI systems are increasingly deployed across industries, organizations, and critical sectors such as cybersecurity, finance, and healthcare. However, their autonomy introduces unique security challenges, including unauthorized actions, adversarial manipulation, and dynamic environmental interactions. Existing AI security frame",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.168782"
    },
    {
      "arxiv_id": "2512.14860v1",
      "title": "Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks",
      "authors": [
        "Viet K. Nguyen",
        "Mohammad I. Husain"
      ],
      "published": "2025-12-16T19:22:50Z",
      "summary": "Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, G",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.169000"
    },
    {
      "arxiv_id": "2512.11931v1",
      "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy",
      "authors": [
        "Alexander K. Saeri",
        "Sophia Lloyd George",
        "Jess Graham",
        "Clelia D. Lacarriere",
        "Peter Slattery",
        "Michael Noetel",
        "Neil Thompson"
      ],
      "published": "2025-12-12T03:26:29Z",
      "summary": "Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-20",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.169205"
    },
    {
      "arxiv_id": "2512.11893v1",
      "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI",
      "authors": [
        "Haocheng Lin"
      ],
      "published": "2025-12-09T20:25:24Z",
      "summary": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.169282"
    },
    {
      "arxiv_id": "2512.08592v1",
      "title": "The SMART+ Framework for AI Systems",
      "authors": [
        "Laxmiraju Kandikatla",
        "Branislav Radeljic"
      ],
      "published": "2025-12-09T13:33:14Z",
      "summary": "Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enh",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.169321"
    },
    {
      "arxiv_id": "2512.07909v1",
      "title": "Agentic Artificial Intelligence for Ethical Cybersecurity in Uganda: A Reinforcement Learning Framework for Threat Detection in Resource-Constrained Environments",
      "authors": [
        "Ibrahim Adabara",
        "Bashir Olaniyi Sadiq",
        "Aliyu Nuhu Shuaibu",
        "Yale Ibrahim Danjuma",
        "Venkateswarlu Maninti",
        "Mutebi Joe"
      ],
      "published": "2025-12-08T05:44:25Z",
      "summary": "Uganda's rapid digital transformation, supported by national strategies such as Vision 2040 and the Digital Transformation Roadmap, has expanded reliance on networked services while simultaneously increasing exposure to sophisticated cyber threats. In resource-constrained settings, commonly deployed rule-based intrusion detection systems lack the adaptability and ethical safeguards needed to address evolving attack patterns, leading to undetected breaches and excessive blocking of legitimate tra",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.169394"
    },
    {
      "arxiv_id": "2512.21048v1",
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": [
        "Savvy Sharma",
        "George Petrovic",
        "Sarthak Kaushik"
      ],
      "published": "2025-12-24T08:29:28Z",
      "summary": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. W",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.170310"
    },
    {
      "arxiv_id": "2512.20985v1",
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": [
        "Salman Jan",
        "Hassan Ali Razzaqi",
        "Ali Akarma",
        "Mohammad Riyaz Belgaum"
      ],
      "published": "2025-12-24T06:20:28Z",
      "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guara",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.170343"
    },
    {
      "arxiv_id": "2512.21048v1",
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": [
        "Savvy Sharma",
        "George Petrovic",
        "Sarthak Kaushik"
      ],
      "published": "2025-12-24T08:29:28Z",
      "summary": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. W",
      "cluster": "cluster_2",
      "classification": "GREEN",
      "relevance_score": 95,
      "screened_at": "2026-01-11T11:46:04.170538"
    }
  ],
  "yellow_papers": [
    {
      "arxiv_id": "2512.24754v1",
      "title": "AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement",
      "authors": [
        "Yutong Wang",
        "Yunxiang Xiao",
        "Yonglin Tian",
        "Junyong Li",
        "Jing Wang",
        "Yisheng Lv"
      ],
      "published": "2025-12-31T09:55:18Z",
      "summary": "Competitive access to modern observatories has intensified as proposal volumes outpace available telescope time, making timely, consistent, and transparent peer review a critical bottleneck for the advancement of astronomy. Automating parts of this process is therefore both scientifically significant and operationally necessary to ensure fair allocation and reproducible decisions at scale. We present AstroReview, an open-source, agent-based framework that automates proposal review in three stage",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 80,
      "screened_at": "2026-01-11T11:46:04.172183"
    },
    {
      "arxiv_id": "2512.18561v1",
      "title": "Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale",
      "authors": [
        "Saad Alqithami"
      ],
      "published": "2025-12-21T02:04:47Z",
      "summary": "Large-scale networked multi-agent systems increasingly underpin critical infrastructure, yet their collective behavior can drift toward undesirable emergent norms that elude conventional governance mechanisms. We introduce an adaptive accountability framework that (i) continuously traces responsibility flows through a lifecycle-aware audit ledger, (ii) detects harmful emergent norms online via decentralized sequential hypothesis tests, and (iii) deploys local policy and reward-shaping interventi",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 80,
      "screened_at": "2026-01-11T11:46:04.172900"
    },
    {
      "arxiv_id": "2512.18561v1",
      "title": "Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale",
      "authors": [
        "Saad Alqithami"
      ],
      "published": "2025-12-21T02:04:47Z",
      "summary": "Large-scale networked multi-agent systems increasingly underpin critical infrastructure, yet their collective behavior can drift toward undesirable emergent norms that elude conventional governance mechanisms. We introduce an adaptive accountability framework that (i) continuously traces responsibility flows through a lifecycle-aware audit ledger, (ii) detects harmful emergent norms online via decentralized sequential hypothesis tests, and (iii) deploys local policy and reward-shaping interventi",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 80,
      "screened_at": "2026-01-11T11:46:04.177272"
    },
    {
      "arxiv_id": "2512.22894v1",
      "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents",
      "authors": [
        "Phil Cuvin",
        "Hao Zhu",
        "Diyi Yang"
      ],
      "published": "2025-12-28T11:55:20Z",
      "summary": "Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 80,
      "screened_at": "2026-01-11T11:46:04.178879"
    },
    {
      "arxiv_id": "2508.01390v2",
      "title": "Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research",
      "authors": [
        "Raluca Rilla",
        "Tobias Werner",
        "Hiromu Yakura",
        "Iyad Rahwan",
        "Anne-Marie Nussberger"
      ],
      "published": "2025-08-02T14:40:54Z",
      "summary": "Online behavioural research faces an emerging threat as participants increasingly turn to large language models (LLMs) for advice, translation, or task delegation: LLM Pollution. We identify three interacting variants through which LLM Pollution threatens the validity and integrity of online behavioural research. First, Partial LLM Mediation occurs when participants make selective use of LLMs for specific aspects of a task, such as translation or wording support, leading researchers to (mis)inte",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.156938"
    },
    {
      "arxiv_id": "2505.12012v1",
      "title": "Empowering Sustainable Finance with Artificial Intelligence: A Framework for Responsible Implementation",
      "authors": [
        "Georgios Pavlidis"
      ],
      "published": "2025-05-17T14:05:39Z",
      "summary": "This chapter explores the convergence of two major developments: the rise of environmental, social, and governance (ESG) investing and the exponential growth of artificial intelligence (AI) technology. The increased demand for diverse ESG instruments, such as green and ESG-linked loans, will be aligned with the rapid growth of the global AI market, which is expected to be worth $1,394.30 billion by 2029. AI can assist in identifying and pricing climate risks, setting more ambitious ESG goals, an",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.157044"
    },
    {
      "arxiv_id": "2410.01818v1",
      "title": "Integrating AI's Carbon Footprint into Risk Management Frameworks: Strategies and Tools for Sustainable Compliance in Banking Sector",
      "authors": [
        "Nataliya Tkachenko"
      ],
      "published": "2024-09-15T23:09:27Z",
      "summary": "This paper examines the integration of AI's carbon footprint into the risk management frameworks (RMFs) of the banking sector, emphasising its importance in aligning with sustainability goals and regulatory requirements. As AI becomes increasingly central to banking operations, its energy-intensive processes contribute significantly to carbon emissions, posing environmental, regulatory, and reputational risks. Regulatory frameworks such as the EU AI Act, Corporate Sustainability Reporting Direct",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.157216"
    },
    {
      "arxiv_id": "2512.21552v1",
      "title": "Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments",
      "authors": [
        "Hua Shen"
      ],
      "published": "2025-12-25T07:50:56Z",
      "summary": "Artificial intelligence (AI) is transforming education, offering unprecedented opportunities to personalize learning, enhance assessment, and support educators. Yet these opportunities also introduce risks related to equity, privacy, and student autonomy. This chapter develops the concept of bidirectional human-AI alignment in education, emphasizing that trustworthy learning environments arise not only from embedding human values into AI systems but also from equipping teachers, students, and in",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.157292"
    },
    {
      "arxiv_id": "2512.24470v2",
      "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models",
      "authors": [
        "Kim Alexander Christensen",
        "Andreas Gudahl Tufte",
        "Alexey Gusev",
        "Rohan Sinha",
        "Milan Ganai",
        "Ole Andreas Alsos",
        "Marco Pavone",
        "Martin Steinert"
      ],
      "published": "2025-12-30T21:20:41Z",
      "summary": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., d",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.158152"
    },
    {
      "arxiv_id": "2505.12012v1",
      "title": "Empowering Sustainable Finance with Artificial Intelligence: A Framework for Responsible Implementation",
      "authors": [
        "Georgios Pavlidis"
      ],
      "published": "2025-05-17T14:05:39Z",
      "summary": "This chapter explores the convergence of two major developments: the rise of environmental, social, and governance (ESG) investing and the exponential growth of artificial intelligence (AI) technology. The increased demand for diverse ESG instruments, such as green and ESG-linked loans, will be aligned with the rapid growth of the global AI market, which is expected to be worth $1,394.30 billion by 2029. AI can assist in identifying and pricing climate risks, setting more ambitious ESG goals, an",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.160670"
    },
    {
      "arxiv_id": "2504.02000v1",
      "title": "AI Regulation and Capitalist Growth: Balancing Innovation, Ethics, and Global Governance",
      "authors": [
        "Vikram Kulothungan",
        "Priya Ranjani Mohan",
        "Deepti Gupta"
      ],
      "published": "2025-04-01T10:59:02Z",
      "summary": "Artificial Intelligence (AI) is increasingly central to economic growth, promising new efficiencies and markets. This economic significance has sparked debate over AI regulation: do rules and oversight bolster long term growth by building trust and safeguarding the public, or do they constrain innovation and free enterprise? This paper examines the balance between AI regulation and capitalist ideals, focusing on how different approaches to AI data privacy can impact innovation in AI-driven appli",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.160751"
    },
    {
      "arxiv_id": "2411.13585v1",
      "title": "Artificial Intelligence in Cybersecurity: Building Resilient Cyber Diplomacy Frameworks",
      "authors": [
        "Michael Stoltz"
      ],
      "published": "2024-11-17T17:57:17Z",
      "summary": "This paper explores how automation and artificial intelligence (AI) are transforming U.S. cyber diplomacy. Leveraging these technologies helps the U.S. manage the complexity and urgency of cyber diplomacy, improving decision-making, efficiency, and security. As global inter connectivity grows, cyber diplomacy, managing national interests in the digital space has become vital. The ability of AI and automation to quickly process vast data volumes enables timely responses to cyber threats and oppor",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.161007"
    },
    {
      "arxiv_id": "2410.01818v1",
      "title": "Integrating AI's Carbon Footprint into Risk Management Frameworks: Strategies and Tools for Sustainable Compliance in Banking Sector",
      "authors": [
        "Nataliya Tkachenko"
      ],
      "published": "2024-09-15T23:09:27Z",
      "summary": "This paper examines the integration of AI's carbon footprint into the risk management frameworks (RMFs) of the banking sector, emphasising its importance in aligning with sustainability goals and regulatory requirements. As AI becomes increasingly central to banking operations, its energy-intensive processes contribute significantly to carbon emissions, posing environmental, regulatory, and reputational risks. Regulatory frameworks such as the EU AI Act, Corporate Sustainability Reporting Direct",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.161558"
    },
    {
      "arxiv_id": "2512.21552v1",
      "title": "Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments",
      "authors": [
        "Hua Shen"
      ],
      "published": "2025-12-25T07:50:56Z",
      "summary": "Artificial intelligence (AI) is transforming education, offering unprecedented opportunities to personalize learning, enhance assessment, and support educators. Yet these opportunities also introduce risks related to equity, privacy, and student autonomy. This chapter develops the concept of bidirectional human-AI alignment in education, emphasizing that trustworthy learning environments arise not only from embedding human values into AI systems but also from equipping teachers, students, and in",
      "cluster": "cluster_1",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.162796"
    },
    {
      "arxiv_id": "2512.21251v2",
      "title": "Uncertainty in security: managing cyber senescence",
      "authors": [
        "Martijn Dekker"
      ],
      "published": "2025-12-24T15:56:12Z",
      "summary": "My main worry, and the core of my research, is that our cybersecurity ecosystem is slowly but surely aging and getting old and that aging is becoming an operational risk. This is happening not only because of growing complexity, but more importantly because of accumulation of controls and measures whose effectiveness are uncertain. I introduce a new term for this aging phenomenon: cyber senescence. I will begin my lecture with a short historical overview in which I sketch a development over time",
      "cluster": "cluster_2",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.166100"
    },
    {
      "arxiv_id": "2512.09959v1",
      "title": "TRUCE: TRUsted Compliance Enforcement Service for Secure Health Data Exchange",
      "authors": [
        "Dae-young Kim",
        "Karuna Pande Joshi"
      ],
      "published": "2025-12-09T21:47:46Z",
      "summary": "Organizations are increasingly sharing large volumes of sensitive Personally Identifiable Information (PII), like health records, with each other to better manage their services. Protecting PII data has become increasingly important in today's digital age, and several regulations have been formulated to ensure the secure exchange and management of sensitive personal data. However, at times some of these regulations are at loggerheads with each other, like the Health Insurance Portability and Acc",
      "cluster": "cluster_2",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.171150"
    },
    {
      "arxiv_id": "2510.26217v1",
      "title": "Hybrid LLM and Higher-Order Quantum Approximate Optimization for CSA Collateral Management",
      "authors": [
        "Tao Jin",
        "Stuart Florescu",
        " Heyu",
        " Jin"
      ],
      "published": "2025-10-30T07:46:40Z",
      "summary": "We address finance-native collateral optimization under ISDA Credit Support Annexes (CSAs), where integer lots, Schedule A haircuts, RA/MTA gating, and issuer/currency/class caps create rugged, legally bounded search spaces. We introduce a certifiable hybrid pipeline purpose-built for this domain: (i) an evidence-gated LLM that extracts CSA terms to a normalized JSON (abstain-by-default, span-cited); (ii) a quantum-inspired explorer that interleaves simulated annealing with micro higher order QA",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.171795"
    },
    {
      "arxiv_id": "2510.25890v2",
      "title": "ATLAS: Artifact Generation Through Layered Constraints and LLM x MDE Synergy",
      "authors": [
        "Tong Ma",
        "Hui Lai",
        "Hui Wang",
        "Zhenhu Tian",
        "Jizhou Wang",
        "Haichao Wu",
        "Yongfan Gao",
        "Chaochao Li",
        "Fengjie Xu",
        "Ling Fang"
      ],
      "published": "2025-10-29T18:44:22Z",
      "summary": "ATLAS unifies Large Language Models with Model-Driven Engineering to generate regulator-ready artifacts and machine-checkable evidence for safety- and compliance-critical domains. ATLAS integrates three pillars: a Unified Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a single semantic space; an Integrated Constraint Model (ICM) extends our prior Dual-Stage(S2D2) extraction logic to compile layered requirements into deterministic generation-time automata (Layer~1) and",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.171813"
    },
    {
      "arxiv_id": "2512.17538v1",
      "title": "Binding Agent ID: Unleashing the Power of AI Agents with accountability and credibility",
      "authors": [
        "Zibin Lin",
        "Shengli Zhang",
        "Guofu Liao",
        "Dacheng Tao",
        "Taotao Wang"
      ],
      "published": "2025-12-19T13:01:54Z",
      "summary": "Autonomous AI agents lack traceable accountability mechanisms, creating a fundamental dilemma where systems must either operate as ``downgraded tools'' or risk real-world abuse. This vulnerability stems from the limitations of traditional key-based authentication, which guarantees neither the operator's physical identity nor the agent's code integrity. To bridge this gap, we propose BAID (Binding Agent ID), a comprehensive identity infrastructure establishing verifiable user-code binding. BAID i",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.174385"
    },
    {
      "arxiv_id": "2512.17538v1",
      "title": "Binding Agent ID: Unleashing the Power of AI Agents with accountability and credibility",
      "authors": [
        "Zibin Lin",
        "Shengli Zhang",
        "Guofu Liao",
        "Dacheng Tao",
        "Taotao Wang"
      ],
      "published": "2025-12-19T13:01:54Z",
      "summary": "Autonomous AI agents lack traceable accountability mechanisms, creating a fundamental dilemma where systems must either operate as ``downgraded tools'' or risk real-world abuse. This vulnerability stems from the limitations of traditional key-based authentication, which guarantees neither the operator's physical identity nor the agent's code integrity. To bridge this gap, we propose BAID (Binding Agent ID), a comprehensive identity infrastructure establishing verifiable user-code binding. BAID i",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.175111"
    },
    {
      "arxiv_id": "2512.17538v1",
      "title": "Binding Agent ID: Unleashing the Power of AI Agents with accountability and credibility",
      "authors": [
        "Zibin Lin",
        "Shengli Zhang",
        "Guofu Liao",
        "Dacheng Tao",
        "Taotao Wang"
      ],
      "published": "2025-12-19T13:01:54Z",
      "summary": "Autonomous AI agents lack traceable accountability mechanisms, creating a fundamental dilemma where systems must either operate as ``downgraded tools'' or risk real-world abuse. This vulnerability stems from the limitations of traditional key-based authentication, which guarantees neither the operator's physical identity nor the agent's code integrity. To bridge this gap, we propose BAID (Binding Agent ID), a comprehensive identity infrastructure establishing verifiable user-code binding. BAID i",
      "cluster": "cluster_3",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.177381"
    },
    {
      "arxiv_id": "2512.18102v1",
      "title": "From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines",
      "authors": [
        "Kishan Kumar Ganguly",
        "Tim Menzies"
      ],
      "published": "2025-12-19T22:15:53Z",
      "summary": "Context: Exhaustive fuzzing of modern JavaScript engines is infeasible due to the vast number of program states and execution paths. Coverage-guided fuzzers waste effort on low-risk inputs, often ignoring vulnerability-triggering ones that do not increase coverage. Existing heuristics proposed to mitigate this require expert effort, are brittle, and hard to adapt.\n  Objective: We propose a data-centric, LLM-boosted alternative that learns from historical vulnerabilities to automatically identify",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.179340"
    },
    {
      "arxiv_id": "2512.24470v2",
      "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models",
      "authors": [
        "Kim Alexander Christensen",
        "Andreas Gudahl Tufte",
        "Alexey Gusev",
        "Rohan Sinha",
        "Milan Ganai",
        "Ole Andreas Alsos",
        "Marco Pavone",
        "Martin Steinert"
      ],
      "published": "2025-12-30T21:20:41Z",
      "summary": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., d",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.179759"
    },
    {
      "arxiv_id": "2512.23881v1",
      "title": "Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack",
      "authors": [
        "Roee Ziv",
        "Raz Lapid",
        "Moshe Sipper"
      ],
      "published": "2025-12-29T21:56:13Z",
      "summary": "Audio-language models combine audio encoders with large language models to enable multimodal reasoning, but they also introduce new security vulnerabilities. We propose a universal targeted latent space attack, an encoder-level adversarial attack that manipulates audio latent representations to induce attacker-specified outputs in downstream language generation. Unlike prior waveform-level or input-specific attacks, our approach learns a universal perturbation that generalizes across inputs and ",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.179936"
    },
    {
      "arxiv_id": "2512.23881v1",
      "title": "Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack",
      "authors": [
        "Roee Ziv",
        "Raz Lapid",
        "Moshe Sipper"
      ],
      "published": "2025-12-29T21:56:13Z",
      "summary": "Audio-language models combine audio encoders with large language models to enable multimodal reasoning, but they also introduce new security vulnerabilities. We propose a universal targeted latent space attack, an encoder-level adversarial attack that manipulates audio latent representations to induce attacker-specified outputs in downstream language generation. Unlike prior waveform-level or input-specific attacks, our approach learns a universal perturbation that generalizes across inputs and ",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.180665"
    },
    {
      "arxiv_id": "2512.24661v1",
      "title": "Do Large Language Models Know What They Are Capable Of?",
      "authors": [
        "Casey O. Barkan",
        "Sid Black",
        "Oliver Sourbut"
      ],
      "published": "2025-12-31T06:14:46Z",
      "summary": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLM",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.181384"
    },
    {
      "arxiv_id": "2512.24470v2",
      "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models",
      "authors": [
        "Kim Alexander Christensen",
        "Andreas Gudahl Tufte",
        "Alexey Gusev",
        "Rohan Sinha",
        "Milan Ganai",
        "Ole Andreas Alsos",
        "Marco Pavone",
        "Martin Steinert"
      ],
      "published": "2025-12-30T21:20:41Z",
      "summary": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., d",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.181430"
    },
    {
      "arxiv_id": "2512.17538v1",
      "title": "Binding Agent ID: Unleashing the Power of AI Agents with accountability and credibility",
      "authors": [
        "Zibin Lin",
        "Shengli Zhang",
        "Guofu Liao",
        "Dacheng Tao",
        "Taotao Wang"
      ],
      "published": "2025-12-19T13:01:54Z",
      "summary": "Autonomous AI agents lack traceable accountability mechanisms, creating a fundamental dilemma where systems must either operate as ``downgraded tools'' or risk real-world abuse. This vulnerability stems from the limitations of traditional key-based authentication, which guarantees neither the operator's physical identity nor the agent's code integrity. To bridge this gap, we propose BAID (Binding Agent ID), a comprehensive identity infrastructure establishing verifiable user-code binding. BAID i",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.182283"
    },
    {
      "arxiv_id": "2512.19570v1",
      "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
      "authors": [
        "Angjelin Hila"
      ],
      "published": "2025-12-22T16:52:37Z",
      "summary": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justif",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.183144"
    },
    {
      "arxiv_id": "2512.18986v1",
      "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "authors": [
        "Kun Zhao",
        "Siyuan Dai",
        "Yingying Zhang",
        "Guodong Liu",
        "Pengfei Gu",
        "Chenghua Lin",
        "Paul M. Thompson",
        "Alex Leow",
        "Heng Huang",
        "Lifang He",
        "Liang Zhan",
        "Haoteng Tang"
      ],
      "published": "2025-12-22T02:54:10Z",
      "summary": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each an",
      "cluster": "cluster_4",
      "classification": "YELLOW",
      "relevance_score": 75,
      "screened_at": "2026-01-11T11:46:04.183279"
    }
  ],
  "summary": {
    "total_queries": 47,
    "total_clusters": 7,
    "green_rate": "50%",
    "target_papers": "20-30 green papers",
    "status": "COMPLETE"
  }
}