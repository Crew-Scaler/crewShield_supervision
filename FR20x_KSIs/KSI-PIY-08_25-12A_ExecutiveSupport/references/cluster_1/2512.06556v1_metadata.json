{
  "arxiv_id": "2512.06556v1",
  "title": "Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks",
  "authors": [
    "Saeid Jamshidi",
    "Kawser Wazed Nafi",
    "Arghavan Moradi Dakhel",
    "Negar Shahabi",
    "Foutse Khomh",
    "Naser Ezzati-Jivan"
  ],
  "published": "2025-12-06T20:07:58Z",
  "summary": "The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-i",
  "cluster": "cluster_1",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.161027"
}