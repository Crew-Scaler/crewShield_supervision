{
  "arxiv_id": "2512.22088v1",
  "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
  "authors": [
    "Chiwun Yang"
  ],
  "published": "2025-12-26T17:20:09Z",
  "summary": "The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stocha",
  "cluster": "cluster_6",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.199601"
}