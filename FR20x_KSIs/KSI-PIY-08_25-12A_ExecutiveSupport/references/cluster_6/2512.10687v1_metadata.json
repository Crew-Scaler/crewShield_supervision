{
  "arxiv_id": "2512.10687v1",
  "title": "Challenges of Evaluating LLM Safety for User Welfare",
  "authors": [
    "Manon Kempermann",
    "Sai Suresh Macharla Vasu",
    "Mahalakshmi Raveenthiran",
    "Theo Farrell",
    "Ingmar Weber"
  ],
  "published": "2025-12-11T14:34:40Z",
  "summary": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-triv",
  "cluster": "cluster_6",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.193507"
}