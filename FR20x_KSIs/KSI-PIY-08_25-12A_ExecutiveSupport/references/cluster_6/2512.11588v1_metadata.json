{
  "arxiv_id": "2512.11588v1",
  "title": "AI Benchmark Democratization and Carpentry",
  "authors": [
    "Gregor von Laszewski",
    "Wesley Brewer",
    "Jeyan Thiyagalingam",
    "Juri Papay",
    "Armstrong Foundjem",
    "Piotr Luszczek",
    "Murali Emani",
    "Shirley V. Moore",
    "Vijay Janapa Reddi",
    "Matthew D. Sinclair",
    "Sebastian Lobentanzer",
    "Sujata Goswami",
    "Benjamin Hawks",
    "Marco Colombo",
    "Nhan Tran",
    "Christine R. Kirkpatrick",
    "Abdulkareem Alsudais",
    "Gregg Barrett",
    "Tianhao Li",
    "Kirsten Morehouse",
    "Shivaram Venkataraman",
    "Rutwik Jain",
    "Kartik Mathur",
    "Victor Lu",
    "Tejinder Singh",
    "Khojasteh Z. Mirza",
    "Kongtao Chen",
    "Sasidhar Kunapuli",
    "Gavin Farrell",
    "Renato Umeton",
    "Geoffrey C. Fox"
  ],
  "published": "2025-12-12T14:20:05Z",
  "summary": "Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.\n  Beyond traditional static benchmarks, continuous ",
  "cluster": "cluster_6",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.193444"
}