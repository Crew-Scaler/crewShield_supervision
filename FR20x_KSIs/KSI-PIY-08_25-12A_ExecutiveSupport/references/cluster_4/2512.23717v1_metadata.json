{
  "arxiv_id": "2512.23717v1",
  "title": "HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate",
  "authors": [
    "Shenzhe Zhu"
  ],
  "published": "2025-12-09T17:56:38Z",
  "summary": "Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framewor",
  "cluster": "cluster_4",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.185116"
}