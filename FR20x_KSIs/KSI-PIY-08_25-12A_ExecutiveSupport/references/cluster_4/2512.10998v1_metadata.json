{
  "arxiv_id": "2512.10998v1",
  "title": "SCOUT: A Defense Against Data Poisoning Attacks in Fine-Tuned Language Models",
  "authors": [
    "Mohamed Afane",
    "Abhishek Satyam",
    "Ke Chen",
    "Tao Li",
    "Junaid Farooq",
    "Juntao Chen"
  ],
  "published": "2025-12-10T17:25:55Z",
  "summary": "Backdoor attacks create significant security threats to language models by embedding hidden triggers that manipulate model behavior during inference, presenting critical risks for AI systems deployed in healthcare and other sensitive domains. While existing defenses effectively counter obvious threats such as out-of-context trigger words and safety alignment violations, they fail against sophisticated attacks using contextually-appropriate triggers that blend seamlessly into natural language. Th",
  "cluster": "cluster_4",
  "classification": "GREEN",
  "relevance_score": 90,
  "screened_at": "2026-01-11T11:46:04.185047"
}