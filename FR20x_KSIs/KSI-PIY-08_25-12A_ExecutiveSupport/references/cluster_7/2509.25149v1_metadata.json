{
  "arxiv_id": "2509.25149v1",
  "title": "Pretraining Large Language Models with NVFP4",
  "authors": [
    " NVIDIA",
    "Felix Abecassis",
    "Anjulie Agrusa",
    "Dong Ahn",
    "Jonah Alben",
    "Stefania Alborghetti",
    "Michael Andersch",
    "Sivakumar Arayandi",
    "Alexis Bjorlin",
    "Aaron Blakeman",
    "Evan Briones",
    "Ian Buck",
    "Bryan Catanzaro",
    "Jinhang Choi",
    "Mike Chrzanowski",
    "Eric Chung",
    "Victor Cui",
    "Steve Dai",
    "Bita Darvish Rouhani",
    "Carlo del Mundo",
    "Deena Donia",
    "Burc Eryilmaz",
    "Henry Estela",
    "Abhinav Goel",
    "Oleg Goncharov",
    "Yugi Guvvala",
    "Robert Hesse",
    "Russell Hewett",
    "Herbert Hum",
    "Ujval Kapasi",
    "Brucek Khailany",
    "Mikail Khona",
    "Nick Knight",
    "Alex Kondratenko",
    "Ronny Krashinsky",
    "Ben Lanir",
    "Simon Layton",
    "Michael Lightstone",
    "Daniel Lo",
    "Paulius Micikevicius",
    "Asit Mishra",
    "Tim Moon",
    "Deepak Narayanan",
    "Chao Ni",
    "Abhijit Paithankar",
    "Satish Pasumarthi",
    "Ankit Patel",
    "Mostofa Patwary",
    "Ashwin Poojary",
    "Gargi Prasad",
    "Sweta Priyadarshi",
    "Yigong Qin",
    "Xiaowei Ren",
    "Oleg Rybakov",
    "Charbel Sakr",
    "Sanjeev Satheesh",
    "Stas Sergienko",
    "Pasha Shamis",
    "Kirthi Shankar",
    "Nishant Sharma",
    "Mohammad Shoeybi",
    "Michael Siu",
    "Misha Smelyanskiy",
    "Darko Stosic",
    "Dusan Stosic",
    "Bor-Yiing Su",
    "Frank Sun",
    "Nima Tajbakhsh",
    "Shelby Thomas",
    "Przemek Tredak",
    "Evgeny Tsykunov",
    "Gandhi Vaithilingam",
    "Aditya Vavre",
    "Rangharajan Venkatesan",
    "Roger Waleffe",
    "Qiyu Wan",
    "Hexin Wang",
    "Mengdi Wang",
    "Lizzie Wei",
    "Hao Wu",
    "Evan Wu",
    "Keith Wyss",
    "Ning Xu",
    "Jinze Xue",
    "Charlene Yang",
    "Yujia Zhai",
    "Ruoxi Zhang",
    "Jingyang Zhu",
    "Zhongbo Zhu"
  ],
  "published": "2025-09-29T17:53:17Z",
  "summary": "Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation ",
  "cluster": "cluster_7",
  "classification": "GREEN",
  "relevance_score": 80,
  "screened_at": "2026-01-11T11:46:04.200292"
}