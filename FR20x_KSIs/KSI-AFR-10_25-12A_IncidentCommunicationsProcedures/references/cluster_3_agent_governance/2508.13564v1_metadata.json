{
  "arxiv_id": "2508.13564v1",
  "title": "The 9th AI City Challenge",
  "authors": [
    "Zheng Tang",
    "Shuo Wang",
    "David C. Anastasiu",
    "Ming-Ching Chang",
    "Anuj Sharma",
    "Quan Kong",
    "Norimasa Kobori",
    "Munkhjargal Gochoo",
    "Ganzorig Batnasan",
    "Munkh-Erdene Otgonbold",
    "Fady Alnajjar",
    "Jun-Wei Hsieh",
    "Tomasz Kornuta",
    "Xiaolong Li",
    "Yilin Zhao",
    "Han Zhang",
    "Subhashree Radhakrishnan",
    "Arihant Jain",
    "Ratnesh Kumar",
    "Vidya N. Murali",
    "Yuxing Wang",
    "Sameer Satish Pusegaonkar",
    "Yizhou Wang",
    "Sujit Biswas",
    "Xunlei Wu",
    "Zhedong Zheng",
    "Pranamesh Chakraborty",
    "Rama Chellappa"
  ],
  "published": "2025-08-19T06:55:06Z",
  "url": "http://arxiv.org/abs/2508.13564v1",
  "pdf_url": "http://arxiv.org/pdf/2508.13564v1.pdf",
  "relevance_score": 100,
  "summary": "The ninth AI City Challenge continues to advance real-world applications of computer vision and AI in transportation, industrial automation, and public safety. The 2025 edition featured four tracks and saw a 17% increase in participation, with 245 teams from 15 countries registered on the evaluation server. Public release of challenge datasets led to over 30,000 downloads to date. Track 1 focused on multi-class 3D multi-camera tracking, involving people, humanoids, autonomous mobile robots, and forklifts, using detailed calibration and 3D bounding box annotations. Track 2 tackled video question answering in traffic safety, with multi-camera incident understanding enriched by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic warehouse environments, requiring AI systems to interpret RGB-D inputs and answer spatial questions that combine perception, geometry, and language. Both Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4 emphasized efficient road object detection from fisheye cameras, supporting lightweight, real-time deployment on edge devices. The evaluation framework enforced submission limits and used a partially held-out test set to ensure fair benchmarking. Final rankings were revealed after the competition concluded, fostering reproducibility and mitigating overfitting. Several teams achieved top-tier results, setting new benchmarks in multiple tasks."
}