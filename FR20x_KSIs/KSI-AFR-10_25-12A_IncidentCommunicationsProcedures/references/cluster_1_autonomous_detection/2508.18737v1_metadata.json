{
  "arxiv_id": "2508.18737v1",
  "title": "FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks",
  "authors": [
    "Enrique M\u00e1rmol Campos",
    "Aurora Gonz\u00e1lez Vidal",
    "Jos\u00e9 Luis Hern\u00e1ndez Ramos",
    "Antonio Skarmeta"
  ],
  "published": "2025-08-26T07:09:15Z",
  "url": "http://arxiv.org/abs/2508.18737v1",
  "pdf_url": "http://arxiv.org/pdf/2508.18737v1.pdf",
  "relevance_score": 85,
  "summary": "Federated Learning (FL) has become a powerful technique for training Machine Learning (ML) models in a decentralized manner, preserving the privacy of the training datasets involved. However, the decentralized nature of FL limits the visibility of the training process, relying heavily on the honesty of participating clients. This assumption opens the door to malicious third parties, known as Byzantine clients, which can poison the training process by submitting false model updates. Such malicious clients may engage in poisoning attacks, manipulating either the dataset or the model parameters to induce misclassification. In response, this study introduces FLAegis, a two-stage defensive framework designed to identify Byzantine clients and improve the robustness of FL systems. Our approach leverages symbolic time series transformation (SAX) to amplify the differences between benign and malicious models, and spectral clustering, which enables accurate detection of adversarial behavior. Furthermore, we incorporate a robust FFT-based aggregation function as a final layer to mitigate the impact of those Byzantine clients that manage to evade prior defenses. We rigorously evaluate our method against five poisoning attacks, ranging from simple label flipping to adaptive optimization-based strategies. Notably, our approach outperforms state-of-the-art defenses in both detection precision and final model accuracy, maintaining consistently high performance even under strong adversarial conditions."
}