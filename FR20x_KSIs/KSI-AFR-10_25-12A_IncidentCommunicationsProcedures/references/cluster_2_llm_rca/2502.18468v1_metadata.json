{
  "arxiv_id": "2502.18468v1",
  "title": "SOK: Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights for LLM Deployment",
  "authors": [
    "Ariful Haque",
    "Sunzida Siddique",
    "Md. Mahfuzur Rahman",
    "Ahmed Rafi Hasan",
    "Laxmi Rani Das",
    "Marufa Kamal",
    "Tasnim Masura",
    "Kishor Datta Gupta"
  ],
  "published": "2025-01-31T06:00:27Z",
  "url": "http://arxiv.org/abs/2502.18468v1",
  "pdf_url": "http://arxiv.org/pdf/2502.18468v1.pdf",
  "relevance_score": 100,
  "summary": "The integration of Large Language Models (LLMs) such as GitHub Copilot, ChatGPT, Cursor AI, and Codeium AI into software development has revolutionized the coding landscape, offering significant productivity gains, automation, and enhanced debugging capabilities. These tools have proven invaluable for generating code snippets, refactoring existing code, and providing real-time support to developers. However, their widespread adoption also presents notable challenges, particularly in terms of security vulnerabilities, code quality, and ethical concerns. This paper provides a comprehensive analysis of the benefits and risks associated with AI-powered coding tools, drawing on user feedback, security analyses, and practical use cases. We explore the potential for these tools to replicate insecure coding practices, introduce biases, and generate incorrect or non-sensical code (hallucinations). In addition, we discuss the risks of data leaks, intellectual property violations and the need for robust security measures to mitigate these threats. By comparing the features and performance of these tools, we aim to guide developers in making informed decisions about their use, ensuring that the benefits of AI-assisted coding are maximized while minimizing associated risks."
}