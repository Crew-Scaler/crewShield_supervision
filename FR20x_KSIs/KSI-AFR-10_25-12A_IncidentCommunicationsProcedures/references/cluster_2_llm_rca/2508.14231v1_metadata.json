{
  "arxiv_id": "2508.14231v1",
  "title": "Incident Analysis for AI Agents",
  "authors": [
    "Carson Ezell",
    "Xavier Roberts-Gaal",
    "Alan Chan"
  ],
  "published": "2025-08-19T19:39:37Z",
  "url": "http://arxiv.org/abs/2508.14231v1",
  "pdf_url": "http://arxiv.org/pdf/2508.14231v1.pdf",
  "relevance_score": 95,
  "summary": "As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that directly or indirectly cause harm. For example, agents could be prompt-injected to exfiltrate private information or make unauthorized purchases. Structured information about such incidents (e.g., user prompts) can help us understand their causes and prevent future occurrences. However, existing incident reporting processes are not sufficient for understanding agent incidents. In particular, such processes are largely based on publicly available data, which excludes useful, but potentially sensitive, information such as an agent's chain of thought or browser history. To inform the development of new, emerging incident reporting processes, we propose an incident analysis framework for agents. Drawing on systems safety approaches, our framework proposes three types of factors that can cause incidents: system-related (e.g., CBRN training data), contextual (e.g., prompt injections), and cognitive (e.g., misunderstanding a user request). We also identify specific information that could help clarify which factors are relevant to a given incident: activity logs, system documentation and access, and information about the tools an agent uses. We provide recommendations for 1) what information incident reports should include and 2) what information developers and deployers should retain and make available to incident investigators upon request. As we transition to a world with more agents, understanding agent incidents will become increasingly crucial for managing risks."
}